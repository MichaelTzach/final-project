"","x"
"1","
Swift is a general-purpose, multi-paradigm, compiled programming language developed by Apple Inc. for iOS, macOS, watchOS, tvOS, and Linux. Swift is designed to work with Apple's Cocoa and Cocoa Touch frameworks and the large body of existing Objective-C (ObjC) code written for Apple products. It is built with the open source LLVM compiler framework and has been included in Xcode since version 6. On platforms other than Linux, it uses the Objective-C runtime library which allows C, Objective-C, C++ and Swift code to run within one program.
Apple intended Swift to support many core concepts associated with Objective-C, notably dynamic dispatch, widespread late binding, extensible programming and similar features, but ""safer"" (easier to catch software bugs); Swift has features addressing some common programming errors like null pointers and provides syntactic sugar to help avoid the pyramid of doom. Swift supports the concept of protocol extensibility, an extensibility system that can be applied to types, structs and classes, which Apple promotes as a real change in programming paradigms they term ""protocol-oriented programming"" (similar to traits).
Swift was introduced at Apple's 2014 Worldwide Developers Conference (WWDC). It underwent an upgrade to version 1.2 during 2014 and a more major upgrade to Swift 2 at WWDC 2015. Initially a proprietary language, version 2.2 was made open-source software under the Apache License 2.0 on December 3, 2015, for Apple's platforms and Linux.
In March 2017, Swift made the top 10 in the monthly TIOBE index ranking of popular programming languages, and was ranked 11th at the end of 2017.
Development of Swift started in July 2010 by Chris Lattner, with the eventual collaboration of many other programmers at Apple. Swift took language ideas ""from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list"". On June 2, 2014, the Apple Worldwide Developers Conference (WWDC) application became the first publicly released app written in Swift. A beta version of the programming language was released to registered Apple developers at the conference, but the company did not promise that the final version of Swift would be source code compatible with the test version. Apple planned to make source code converters available if needed for the full release.
The Swift Programming Language, a free 500-page manual, was also released at WWDC, and is available on the iBooks Store and the official website.
Swift reached the 1.0 milestone on September 9, 2014, with the Gold Master of Xcode 6.0 for iOS. Swift 1.1 was released on October 22, 2014, alongside the launch of Xcode 6.1. Swift 1.2 was released on April 8, 2015, along with Xcode 6.3. Swift 2.0 was announced at WWDC 2015, and was made available for publishing apps in the App Store in September 21, 2015. Swift 3.0 was released on September 13, 2016.
Swift won first place for Most Loved Programming Language in the Stack Overflow Developer Survey 2015 and second place in 2016.
In December 2015, IBM announced its Swift Sandbox website, which allows developers to write Swift code in one pane and display output in another.
During the WWDC 2016, Apple announced an iPad exclusive app, named Swift Playgrounds, intended to teach people how to code in Swift. The app is presented in a 3D video game-like interface which provides feedback when lines of code are placed in a certain order and executed.
In January 2017, Chris Lattner announced his departure from Apple for a new position with Tesla Motors, with the Swift project lead role going to team veteran Ted Kremenek.
Swift is an alternative to the Objective-C language that employs modern programming-language theory concepts and strives to present a simpler syntax. During its introduction, it was described simply as ""Objective-C without the C"".
By default, Swift does not expose pointers and other unsafe accessors, in contrast to Objective-C, which uses pointers pervasively to refer to object instances. Also, Objective-C's use of a Smalltalk-like syntax for making method calls has been replaced with a dot-notation style and namespace system more familiar to programmers from other common object-oriented (OO) languages like Java or C#. Swift introduces true named parameters and retains key Objective-C concepts, including protocols, closures and categories, often replacing former syntax with cleaner versions and allowing these concepts to be applied to other language structures, like enumerated types (enums)
Under the Cocoa and Cocoa Touch environments, many common classes were part of the Foundation Kit library. This included the NSString string library (using Unicode), the NSArray and NSDictionary collection classes, and others. Objective-C provided various bits of syntactic sugar to allow some of these objects to be created on-the-fly within the language, but once created, the objects were manipulated with object calls. For instance, in Objective-C concatenating two NSStrings required method calls similar to this: 
 In Swift, many of these basic types have been promoted to the language's core, and can be manipulated directly. For instance, strings are invisibly bridged to NSString (when Foundation is imported) and can now be concatenated with the + operator, allowing greatly simplified syntax; the prior example becoming:
Swift supports five access control levels for symbols: open, public, internal, fileprivate, and private. Unlike many object-oriented languages, these access controls ignore inheritance hierarchies: private indicates that a symbol is accessible only in the immediate scope, fileprivate indicates it is accessible only from within the file, internal indicates it is accessible within the containing module, public indicates it is accessible from any module, and open (only for classes and their methods) indicates that the class may be subclassed outside of the module.
An important new feature in Swift is option types, which allow references or values to operate in a manner similar to the common pattern in C, where a pointer may refer to a value or may be null. This implies that non-optional types cannot result in a null-pointer error; the compiler can ensure this is not possible.
Optional types are created with the Optional mechanism—to make an Integer that is nullable, one would use a declaration similar to var optionalInteger: Optional<Int>. As in C#, Swift also includes syntactic sugar for this, allowing one to indicate a variable is optional by placing a question mark after the type name, var optionalInteger: Int?. Variables or constants that are marked optional either have a value of the underlying type or are nil. Optional types wrap the base type, resulting in a different instance. String and String? are fundamentally different types, the latter has more in common with Int? than String.
To access the value inside, assuming it is not nil, it must be unwrapped to expose the instance inside. This is performed with the ! operator:
In this case, the ! operator unwraps anOptionalInstance to expose the instance inside, allowing the method call to be made on it. If anOptionalInstance is nil, a null-pointer error occurs. This can be annoying in practice, so Swift also includes the concept of optional chaining to test whether the instance is nil and then unwrap it if it is non-null:
In this case the runtime only calls someMethod if anOptionalInstance is not nil, suppressing the error. Normally this requires the programmer to test whether myValue is nil before proceeding. The origin of the term chaining comes from the more common case where several method calls/getters are chained together. For instance:
can be reduced to:
The ? syntax circumvents the pyramid of doom.
Swift 2 introduced the new keyword guard for cases in which code should stop executing if some condition is unmet:
Using guard has three benefits. While the syntax can act as an if statement, its primary benefit is inferring non-nullability. Where an if statement requires a case, guard assumes the case based on the condition provided. Also, since guard contains no scope, with exception of the else closure, leaseStart is presented as an unwrapped optional to the guard's super-scope. Lastly, if the guard statement's test fails, Swift requires the else to exit the current method or loop, ensuring leaseStart never is accessed when nil.  This is performed with the keywords return, continue, break, or throw.
ObjC was weakly typed, and allowed any method to be called on any object at any time. If the method call failed, there was a default handler in the runtime that returned nil. That meant that no unwrapping or testing was needed, the equivalent statement in ObjC:
would return nil and this could be tested. However, this also demanded that all method calls be dynamic, which introduces significant overhead. Swift's use of optionals provides a similar mechanism for testing and dealing with nils, but does so in a way that allows the compiler to use static dispatch because the unwrapping action is called on a defined instance (the wrapper), versus occurring in the runtime dispatch system.
In many object-oriented languages, objects are represented internally in two parts. The object is stored as a block of data placed on the heap, while the name (or ""handle"") to that object is represented by a pointer. Objects are passed between methods by copying the value of the pointer, allowing the same underlying data on the heap to be accessed by anyone with a copy. In contrast, basic types like integers and floating point values are represented directly; the handle contains the data, not a pointer to it, and that data is passed directly to methods by copying. These styles of access are termed pass-by-reference in the case of objects, and pass-by-value for basic types.
Both concepts have their advantages and disadvantages. Objects are useful when the data is large, like the description of a window or the contents of a document. In these cases, access to that data is provided by copying a 32- or 64-bit value, versus copying an entire data structure. However, smaller values like integers are the same size as pointers (typically both are one word), so there is no advantage to passing a pointer, versus passing the value. Also, pass-by-reference inherently requires a dereferencing operation, which can produce noticeable overhead in some operations, typically those used with these basic value types, like mathematics.
Similarly to C# and in contrast to most other OO languages,[citation needed] Swift offers built-in support for objects using either pass-by-reference or pass-by-value semantics, the former using the class declaration and the latter using struct. Structs in Swift have almost all the same features as classes: methods, implementing protocols, and using the extension mechanisms. For this reason, Apple terms all data generically as instances, versus objects or values. Structs do not support inheritance, however.
The programmer is free to choose which semantics are more appropriate for each data structure in the application. Larger structures like windows would be defined as classes, allowing them to be passed around as pointers. Smaller structures, like a 2D point, can be defined as structs, which will be pass-by-value and allow direct access to their internal data with no dereference. The performance improvement inherent to the pass-by-value concept is such that Swift uses these types for almost all common data types, including Int and Double, and types normally represented by objects, like String and Array. Using value types can result in significant performance improvements in user applications also.
To ensure that even the largest structs do not cause a performance penalty when they are handed off, Swift uses copy on write so that the objects are copied only if and when the program attempts to change a value in them. This means that the various accessors have what is in effect a pointer to the same data storage, but this takes place far below the level of the language, in the computer's memory management unit (MMU). So while the data is physically stored as one instance in memory, at the level of the application, these values are separate, and physical separation is enforced by copy on write only if needed.
A key feature of ObjC is its support for categories, methods that can be added to extend classes at runtime. Categories allow extending classes in-place to add new functions with no need to subclass or even have access to the original source code. An example might be to add spell checker support to the base NSString class, which means all instances of NSString in the application gain spell checking. The system is also widely used as an organizational technique, allowing related code to be gathered into library-like extensions. Swift continues to support this concept, although they are now termed extensions, and declared with the keyword extension. Unlike ObjC, Swift can also add new properties accessors, types and enums to extant instances.
Another key feature of ObjC is its use of protocols, known in most modern languages as interfaces. Protocols promise that a particular class implements a set of methods, meaning that other objects in the system can call those methods on any object supporting that protocol. This is often used in modern OO languages as a substitute for multiple inheritance, although the feature sets are not entirely similar. A common example of a protocol in Cocoa is the NSCopying protocol, which defines one method, copyWithZone, that implements deep copying on objects.
In ObjC, and most other languages implementing the protocol concept, it is up to the programmer to ensure that the required methods are implemented in each class. Swift adds the ability to add these methods using extensions, and to use generic programming (generics) to implement them. Combined, these allow protocols to be written once and support a wide variety of instances. Also, the extension mechanism can be used to add protocol conformance to an object that does not list that protocol in its definition.
For example, a protocol might be declared called SupportsToString, which ensures that instances that conform to the protocol implement a toString method that returns a String. In Swift, this can be declared with code like this:
This protocol can now be added to String, with no access to the base class's source:
In Swift, like many modern languages supporting interfaces, protocols can be used as types, which means variables and methods can be defined by protocol instead of their specific type:
It does not matter what sort of instance someSortOfPrintableObject is, the compiler will ensure that it conforms to the protocol and thus this code is safe. This syntax also means that collections can be based on protocols also, like let printableArray = [SupportsToString].
As Swift treats structs and classes as similar concepts, both extensions and protocols are extensively used in Swift's runtime to provide a rich API based on structs. For instance, Swift uses an extension to add the Equatable protocol to many of their basic types, like Strings and Arrays, allowing them to be compared with the == operator. A concrete example of how all of these features interact can be seen in the concept of default protocol implementations:
This function defines a method that works on any instance conforming to Equatable, providing a not equals function. Any instance, class or struct, automatically gains this implementation simply by conforming to Equatable. As many instances gain Equatable through their base implementations or other generic extensions, most basic objects in the runtime gain equals and not equals with no code.
This combination of protocols, defaults, protocol inheritance, and extensions allows many of the functions normally associated with classes and inheritance to be implemented on value types. Properly used, this can lead to dramatic performance improvements with no significant limits in API. This concept is so widely used within Swift, that Apple has begun calling it a protocol-oriented programming language. They suggest addressing many of the problem domains normally solved though classes and inheritance using protocols and structs instead.
Swift uses the same runtime as the extant Objective-C system, but requires iOS 7 or macOS 10.9 or higher. Swift and Objective-C code can be used in one program, and by extension, C and C++ also. In contrast to C, C++ code cannot be used directly from Swift. An Objective-C or C wrapper must be created between Swift and C++. In the case of Objective-C, Swift has considerable access to the object model, and can be used to subclass, extend and use Objective-C code to provide protocol support. The converse is not true: a Swift class cannot be subclassed in Objective-C.
To aid development of such programs, and the re-use of extant code, Xcode 6 offers a semi-automated system that builds and maintains a bridging header to expose Objective-C code to Swift. This takes the form of an additional header file that simply defines or imports all of the Objective-C symbols that are needed by the project's Swift code. At that point, Swift can refer to the types, functions, and variables declared in those imports as though they were written in Swift. Objective-C code can also use Swift code directly, by importing an automatically maintained header file with Objective-C declarations of the project's Swift symbols. For instance, an Objective-C file in a mixed project called ""MyApp"" could access Swift classes or functions with the code #import ""MyApp-Swift.h"". Not all symbols are available through this mechanism, however—use of Swift-specific features like generic types, non-object optional types, sophisticated enums, or even Unicode identifiers may render a symbol inaccessible from Objective-C.
Swift also has limited support for attributes, metadata that is read by the development environment, and is not necessarily part of the compiled code. Like Objective-C, attributes use the @ syntax, but the currently available set is small. One example is the @IBOutlet attribute, which marks a given value in the code as an outlet, available for use within Interface Builder (IB). An outlet is a device that binds the value of the on-screen display to an object in code.
Swift uses Automatic Reference Counting (ARC) to manage memory. Apple used to require manual memory management in Objective-C, but introduced ARC in 2011 to allow for easier memory allocation and deallocation. One problem with ARC is the possibility of creating a strong reference cycle, where objects reference each other in a way that you can reach the object you started from by following references (e.g. A references B, B references A). This causes them to become leaked into memory as they are never released.  Swift provides the keywords weak and unowned to prevent strong reference cycles. Typically a parent-child relationship would use a strong reference while a child-parent would use either weak reference, where parents and children can be unrelated, or unowned where a child always has a parent, but parent may not have a child. Weak references must be optional variables, since they can change and become nil.
A closure within a class can also create a strong reference cycle by capturing self references. Self references to be treated as weak or unowned can be indicated using a capture list.
A key element of the Swift system is its ability to be cleanly debugged and run within the development environment, using a read–eval–print loop (REPL), giving it interactive properties more in common with the scripting abilities of Python than traditional system programming languages. The REPL is further enhanced with the new concept playgrounds. These are interactive views running within the Xcode environment that respond to code or debugger changes on-the-fly. Playgrounds allow programmers to add in Swift code along with markdown documentation. If some code changes over time or with regard to some other ranged input value, the view can be used with the Timeline Assistant to demonstrate the output in an animated way. In addition, Xcode has debugging features for Swift development including breakpoints, step through and step over statements, as well as UI element placement breakdowns for app developers. 
Apple says that Swift ""is the first industrial-quality systems programming language that is as expressive and enjoyable as a scripting language"".
Many of the features introduced with Swift also have well-known performance and safety trade-offs. Apple has implemented optimizations that reduce this overhead.
Swift is similar to C in various ways:
It also has similarities to Objective-C:
Differences from Objective-C include:
Since the language is open-source, there are prospects of it being ported to the web. Some web frameworks have already been developed, such as IBM's Kitura, Perfect and Vapor.
An official ""Server APIs"" work group has also been started by Apple, with members of the Swift developer community playing a central role.
A second free implementation of Swift that targets Cocoa, Microsoft's Common Language Infrastructure (.NET), and the Java and Android platform exists as part of the Elements Compiler from RemObjects Software."
"2","
Cocoa is Apple's native object-oriented application programming interface (API) for their operating system macOS.
For iOS, tvOS, and watchOS, a similar API exists, named Cocoa Touch, which includes gesture recognition, animation, and a different set of graphical control elements. It is used in applications for Apple devices such as iPhone, iPad, iPod Touch, Apple TV, and Apple Watch.
Cocoa consists of the Foundation Kit, Application Kit, and Core Data frameworks, as included by the Cocoa.h header file, and the libraries and frameworks included by those, such as the C standard library and the Objective-C runtime.
Cocoa applications are typically developed using the development tools provided by Apple, specifically Xcode (formerly Project Builder) and Interface Builder, using the languages Objective-C or Swift. However, the Cocoa programming environment can be accessed using other tools, such as Clozure CL, LispWorks, Object Pascal, Python, Perl, Ruby, and AppleScript with the aid of bridge mechanisms such as PasCocoa, PyObjC, CamelBones, RubyCocoa, and a D/Objective-C Bridge. A Ruby language implementation named MacRuby, which removes the need for a bridge mechanism, was formerly developed by Apple, while Nu is a Lisp-like language that can be used with Cocoa with no bridge.  It is also possible to write Objective-C Cocoa programs in a simple text editor and build it manually with GNU Compiler Collection (GCC) or clang from the command line or from a makefile.
For end-users, Cocoa applications are those written using the Cocoa programming environment. Such applications usually have a distinctive feel, since the Cocoa programming environment automates many aspects of an application to comply with Apple's human interface guidelines.
Cocoa continues the lineage of several software frameworks (mainly the App Kit and Foundation Kit)  from the NeXTSTEP and OpenStep programming environments developed by NeXT in the 1980s and 1990s. Apple acquired NeXT in December 1996, and subsequently went to work on the Rhapsody operating system that was to be the direct successor of OpenStep. It was to have had an emulation base for classic Mac OS applications, named Blue Box. The OpenStep base of libraries and binary support was termed Yellow Box. Rhapsody evolved into Mac OS X, and the Yellow Box became Cocoa. Thus, Cocoa classes begin with the letters NS, such as NSString or NSArray.  These stand for either the NeXT-Sun creation of OpenStep, or for the original proprietary term for the OpenStep framework, NeXTSTEP.
Much of the work that went into developing OpenStep was applied to developing Mac OS X, Cocoa being the most visible part. However, differences exist. For example, NeXTSTEP and OpenStep used Display PostScript for on-screen display of text and graphics, while Cocoa depends on Apple's Quartz (which uses the Portable Document Format (PDF) imaging model, but not its underlying technology). Cocoa also has a level of Internet support, including the NSURL and WebKit HTML classes, and others, while OpenStep had only rudimentary support for managed network connections via NSFileHandle classes and Berkeley sockets.
The resulting software framework received the name Cocoa for the sake of expediency, because the name had already been trademarked by Apple. For many years before this present use of the name, Apple's Cocoa trademark had originated as the name of a multimedia project design application for children. The application was originally developed at the Apple Advanced Technology Group under the name KidSim, and was then renamed and trademarked as ""Cocoa"". The name, coined by Peter Jensen who was hired to develop Cocoa for Apple, was intended to evoke ""Java for kids"", as it ran embedded in web pages.  The trademark, and thus the name ""Cocoa"", was re-used to avoid the delay which would have occurred while registering a new trademark for this software framework.  The original ""Cocoa"" program was discontinued at Apple in one of the rationalizations that followed Steve Jobs's return to Apple.  It was then licensed to a third party and marketed as Stagecast Creator as of  2011[update].
One feature of the Cocoa environment is its facility for managing dynamically allocated memory. Cocoa's NSObject class, from which most classes, both vendor and user, are derived, implements a reference counting scheme for memory management. Objects that derive from the NSObject root class respond to a retain and a release message, and keep a retain count. A method titled retainCount exists, but contrary to its name, will usually not return the exact retain count of an object. It is mainly used for system-level purposes. Invoking it manually is not recommended by Apple.
A newly allocated object created with alloc or copy has a retain count of one. Sending that object a retain message increments the retain count, while sending it a release message decrements the retain count. When an object's retain count reaches zero, it is deallocated by a procedure similar to a C++ destructor. dealloc is not guaranteed to be invoked.
Starting with Objective-C 2.0, the Objective-C runtime implemented an optional garbage collector, which is now obsolete and deprecated in favor of Automatic Reference Counting (ARC). In this model, the runtime turned Cocoa reference counting operations such as ""retain"" and ""release"" into no-ops.  The garbage collector does not exist on the iOS implementation of Objective-C 2.0. Garbage collection in Objective-C ran on a low-priority background thread, and can halt on Cocoa's user events, with the intention of keeping the user experience responsive.  The legacy garbage collector is still available on Mac OS X version 10.13, but no Apple-provided applications use it.
In 2011, the LLVM compiler introduced Automatic Reference Counting (ARC), which replaces the conventional garbage collector by performing static analysis of Objective-C source code and inserting retain and release messages as necessary.
Cocoa consists of three Objective-C object libraries called frameworks. Frameworks are functionally similar to shared libraries, a compiled object that can be dynamically loaded into a program's address space at runtime, but frameworks add associated resources, header files, and documentation.  The Cocoa frameworks are implemented as a type of bundle, containing the aforementioned items in standard locations.
A key part of the Cocoa architecture is its comprehensive views model. This is organized along conventional lines for an application framework, but is based on the Portable Document Format (PDF) drawing model provided by Quartz. This allows creating custom drawing content using PostScript-like drawing commands, which also allows automatic printer support and so forth. Since the Cocoa framework manages all the clipping, scrolling, scaling and other chores of drawing graphics, the programmer is freed from implementing basic infrastructure and can concentrate on the unique aspects of an application's content.
The Smalltalk teams at Xerox PARC eventually settled on a design philosophy that led to easy development and high code reuse. Named model-view-controller (MVC), the concept breaks an application into three sets of interacting object classes.
Cocoa's design is a fairly, but not absolutely strict application of MVC principles. Under OpenStep, most of the classes provided were either high-level View classes (in AppKit) or one of a number of relatively low-level model classes like NSString. Compared to similar MVC systems, OpenStep lacked a strong model layer. No stock class represented a ""document,"" for instance. During the transition to Cocoa, the model layer was expanded greatly, introducing a number of pre-rolled classes to provide functionality common to desktop applications.
In Mac OS X 10.3, Apple introduced the NSController family of classes, which provide predefined behavior for the controller layer. These classes are considered part of the Cocoa Bindings system, which also makes extensive use of protocols such as Key-Value Observing and Key-Value Binding. The term 'binding' refers to a relationship between two objects, often between a view and a controller. Bindings allow the developer to focus more on declarative relationships rather than orchestrating fine-grained behavior.
With the arrival of Mac OS X 10.4, Apple extended this foundation further by introducing the Core Data framework, which standardizes change tracking and persistence in the model layer. In effect, the framework greatly simplifies the process of making changes to application data, undoing changes (if necessary), saving data to disk, and reading it back in.
By providing framework support for all three MVC layers, Apple's goal is to reduce the amount of boilerplate or ""glue"" code that developers have to write, freeing up resources to spend time on application-specific features.
In most object-oriented languages, calls to methods are represented physically by a pointer to the code in memory. This restricts the design of an application since specific command handling classes are needed, usually organized according to the chain-of-responsibility pattern. While Cocoa retains this approach for the most part, Objective-C's late binding opens up more flexibility.
Under Objective-C, methods are represented by a selector, a string describing the method to call. When a message is sent, the selector is sent into the Objective-C runtime, matched against a list of available methods, and the method's implementation is called. Since the selector is text data, this lets it be saved to a file, transmitted over a network or between processes, or manipulated in other ways. The implementation of the method is looked up at runtime, not compile time. There is a small performance penalty for this, but late binding allows the same selector to reference different implementations.
By a similar token, Cocoa provides a pervasive data manipulation method called key-value coding (KVC). This allows a piece of data or property of an object to be looked up or changed at runtime by name. The property name acts as a key to the value. In traditional languages, this late binding is impossible. KVC leads to great design flexibility. An object's type need not be known, yet any property of that object can be discovered using KVC. Also, by extending this system using something Cocoa terms key-value observing (KVO), automatic support for undo-redo is provided.
Late static binding is a variant of binding somewhere between static and dynamic binding. The binding of names before the program is run is called static (early); bindings performed as the program runs are dynamic (late or virtual).
One of the most useful features of Cocoa is the powerful base objects the system supplies. As an example, consider the Foundation classes NSString and NSAttributedString, which provide Unicode strings, and the NSText system in AppKit, which allows the programmer to place string objects in the GUI.
NSText and its related classes are used to display and edit strings. The collection of objects involved permit an application to implement anything from a simple single-line text entry field to a complete multi-page, multi-column text layout schema, with full professional typography features such as kerning, ligatures, running text around arbitrary shapes, rotation, full Unicode support and anti-aliased glyph rendering. Paragraph layout can be controlled automatically or by the user, using a built-in ""ruler"" object that can be attached to any text view. Spell checking is automatic, using a single dictionary used by all applications that uses the squiggly underlining convention introduced by Microsoft (actually a dashed red underline in Cocoa). Unlimited undo-redo support is built in. Using only the built-in features, one can write a text editor application in as few as 10 lines of code. With new controller objects, this may fall to zero[clarification needed]. This is in contrast to the TextEdit APIs found in the earlier Mac OS.
When extensions are needed, Cocoa's use of Objective-C makes this a straightforward task. Objective-C includes the concept of ""categories,"" which allows modifying existing class ""in-place"". Functionality can be accomplished in a category without any changes to the original classes in the framework, or even access to its source. Under more common frameworks, this same task requires making a new subclass supporting the added features, and then changing all instances of the classes to this new class.
The Cocoa frameworks are written in Objective-C, and hence that is the preferred language for developing Cocoa applications.[citation needed] Java bindings for the Cocoa frameworks (termed the Java bridge) were also made available with the aim of replacing Objective-C with a more popular language but these bindings were unpopular among Cocoa developers and Cocoa's message passing semantics did not translate well to a statically-typed language such as Java. Cocoa's need for runtime binding means many of Cocoa's key features are not available with Java. In 2005, Apple announced that the Java bridge was to be deprecated, meaning that features added to Cocoa in macOS versions later than 10.4 would not be added to the Cocoa-Java programming interface.
At Apple Worldwide Developers Conference (WWDC) 2014, Apple introduced a new programming language named Swift, which is intended to replace Objective-C.
Originally, AppleScript Studio could be used to develop simpler Cocoa applications.  However, as of Snow Leopard, it has been deprecated. It was replaced with AppleScriptObjC, which allows programming in AppleScript, while using Cocoa frameworks.
Third-party bindings available for other languages include Clozure CL, Monobjc and NObjective (C#), Cocoa# (CLI), Cocodao and D/Objective-C Bridge,LispWorks, CamelBones (Perl), PyObjC (Python), FPC PasCocoa (Lazarus and Free Pascal), RubyCocoa (Ruby).Nu uses the Objective-C object model directly, and thus can use the Cocoa frameworks without needing a binding.
There are also open source implementations of major parts of the Cocoa framework, such as GNUstep and Cocotron, which allow cross-platform Cocoa application development to target other operating systems, such as Microsoft Windows and Linux."
"3","In object-oriented computer programming, a null object is an object with no referenced value or with defined neutral (""null"") behavior. The null object design pattern describes the uses of such objects and their behavior (or lack thereof). It was first published in the Pattern Languages of Program Design book series.
In most object-oriented languages, such as Java or C#, references may be null. These references need to be checked to ensure they are not null before invoking any methods, because methods typically cannot be invoked on null references.
The Objective-C language takes another approach to this problem and does nothing when sending a message to nil; if a return value is expected, nil (for objects), 0 (for numeric values), NO (for BOOL values), or a struct (for struct types) with all its members initialised to null/0/NO/zero-initialised struct is returned.
Instead of using a null reference to convey absence of an object (for instance, a non-existent customer), one uses an object which implements the expected interface, but whose method body is empty. The advantage of this approach over a working default implementation is that a null object is very predictable and has no side effects: it does nothing.
For example, a function may retrieve a list of files in a folder and perform some action on each. In the case of an empty folder, one response may be to throw an exception or return a null reference rather than a list. Thus, the code which expects a list must verify that it in fact has one before continuing, which can complicate the design.
By returning a null object (i.e. an empty list) instead, there is no need to verify that the return value is in fact a list. The calling function may simply iterate the list as normal, effectively doing nothing. It is, however, still possible to check whether the return value is a null object (an empty list) and react differently if desired.
The null object pattern can also be used to act as a stub for testing, if a certain feature such as a database is not available for testing.
Given a binary tree, with this node structure:
One may implement a tree size procedure recursively:
Since the child nodes may not exist, one must modify the procedure by adding non-existence or null checks:
This however makes the procedure more complicated by mixing boundary checks with normal logic, and it becomes harder to read. Using the null object pattern, one can create a special version of the procedure but only for null nodes:
This separates normal logic from special case handling, and makes the code easier to understand.
It can be regarded as a special case of the State pattern and the Strategy pattern.
It is not a pattern from Design Patterns, but is mentioned in Martin Fowler's Refactoring and Joshua Kerievsky's Refactoring To Patterns as the Insert Null Object refactoring.
Chapter 17 of Robert Cecil Martin's Agile Software Development: Principles, Patterns and Practices is dedicated to the pattern.
From C# 6.0 it is possible to use the ""?."" operator (aka null-conditional operator), which will simply evaluate to null if its left operand is null.
In some Microsoft .NET languages, Extension methods can be used to perform what is called 'null coalescing'. This is because extension methods can be called on null values as if it concerns an 'instance method invocation' while in fact extension methods are static. Extension methods can be made to check for null values, thereby freeing code that uses them from ever having to do so. Note that the example below uses the C# Null coalescing operator to guarantee error free invocation, where it could also have used a more mundane if...then...else.
A language with statically typed references to objects illustrates how the null object becomes a more complicated pattern:
Here, the idea is that there are situations where a pointer or reference to an animal object is required, but there is no appropriate object available. A null reference is impossible in standard-conforming C++. A null animal * pointer is possible, and could be useful as a place-holder, but may not be used for direct dispatch: a->make_sound() is undefined behavior if a is a null pointer.
The null object pattern solves this problem by providing a special null_animal class which can be instantiated bound to an animal pointer or reference.
The special null class must be created for each class hierarchy that is to have a null object, since a null_animal is of no use when what is needed is a null object with regard to some widget base class that is not related to the animal hierarchy.
Note, that NOT having a null class at all is an important feature, in contrast to languages where ""anything is a reference"" (e.g. Java and C#). In C++, the design of a function or method may explicitly state whether null is allowed or not.
C# is a language in which the null object pattern can be properly implemented. This example shows animal objects that display sounds and a NullAnimal instance used in place of the C# null keyword. The null object provides consistent behaviour and prevents a runtime null reference exception that would occur if the C# null keyword were used instead.
Following the Smalltalk principle, everything is an object, the absence of an object is itself modeled by an object, called nil. In the GNU Smalltalk for example, the class of  nil is UndefinedObject, a direct descendant of Object.
Any operation that fails to return a sensible object for its purpose may return nil instead, thus avoiding the special case of returning ""no object"". This method has the advantage of simplicity (no need for a special case) over the classical ""null"" or ""no object"" or ""null reference"" approach. Especially useful messages to be used with nil are isNil or ifNil:, which make it practical and safe to deal with possible references to nil in Smalltalk programs.
In Lisp, functions can gracefully accept the special object nil, which reduces the amount of special case testing in application code. For instance, although nil is an atom and does not have any fields, the functions car and cdr accept nil and just return it, which is very useful and results in shorter code.
Since nil is the empty list in Lisp, the situation described in the introduction above doesn't exist. Code which returns nil is returning what is in fact the empty list (and not anything resembling a  null reference to a list type), so the caller does not need to test the value to see whether or not it has a list.
The null object pattern is also supported in multiple value processing. If the program attempts to extract a value from an expression which returns no values, the behavior is that the null object nil is substituted.
Thus (list (values)) returns (nil) (a one-element list containing nil). The (values) expression returns no values at all, but since the function call to list needs to reduce its argument expression to a value, the null object is automatically substituted.
In Common Lisp, the object nil is the one and only instance of the special class null. What this means is that a method can be specialized to the null class, thereby implementing the null design pattern. Which is to say, it is essentially built into the object system:
The class null is a subclass of the symbol class, because nil is a symbol.
Since nil also represents the empty list, null is a subclass of the list class, too. Methods parameters specialized to symbol or list will thus take a nil argument. Of course, a null specialization can still be defined which is a more specific match for nil.
Unlike Common Lisp, and many dialects of Lisp, the Scheme dialect does not have a nil value which works this way; the functions car and cdr may not be applied to an empty list; Scheme application code therefore has to use the empty? or pair? predicate functions to sidestep this situation, even in situations where very similar Lisp would not need to distinguish the empty and non-empty cases thanks to the behavior of nil.
In duck-typed languages like Ruby, language inheritance is not necessary to provide expected behavior. 
Attempts to directly monkey-patch NilClass instead of providing explicit implementations give more unexpected side effects than benefits.
In duck-typed languages like JavaScript, language inheritance is not necessary to provide expected behavior.
This code illustrates a variation of the C++ example, above, using the Java language. As with C++, a null class can be instantiated in situations where a reference to an Animal object is required, but there is no appropriate object available. A null Animal object is possible (Animal myAnimal = null;) and could be useful as a place-holder, but may not be used for calling a method. In this example, myAnimal.makeSound(); will throw a NullPointerException. Therefore, additional code may be necessary to test for null objects.
The null object pattern solves this problem by providing a special NullAnimal class which can be instantiated as an object of type Animal. As with C++ and related languages, that special null class must be created for each class hierarchy that needs a null object, since a NullAnimal is of no use when what is needed is a null object that does not implement the Animal interface.
The following null object pattern implementation demonstrates the concrete class providing its corresponding null object in a static field Empty. This approach is frequently used in the .NET Framework (String.Empty, EventArgs.Empty, Guid.Empty, etc.).
This pattern should be used carefully as it can make errors/bugs appear as normal program execution.
Care should be taken not to implement this pattern just to avoid null checks and make code more readable, since the harder to read code may just move to another place and be less standard - such as when different logic must execute in case the object provided is indeed the null object. The common pattern in most languages with reference types is to compare a reference to a single value referred to as null or nil. Also, there is additional need for testing that no code anywhere ever assigns null instead of the null object, because in most cases and languages with static typing, this is not a compiler error if the null object is of a reference type, although it would certainly lead to errors at run time in parts of the code where the pattern was used to avoid null checks. On top of that, in most languages and assuming there can be many null objects (i.e. the null object is a reference type but doesn't implement the singleton pattern in one or another way), checking for the null object instead of for the null or nil value introduces overhead, as does the singleton pattern likely itself upon obtaining the singleton reference."
"4","Chris Lattner (born 1978) is an American software developer, best known as the main author of LLVM and related projects, such as the compiler Clang and the programming language Swift. He works at Google Brain, following a brief stint at Tesla, Inc.  as Vice President of Autopilot Software. Prior to that, he worked at Apple Inc. as Senior Director of the Developer Tools department, leading the Xcode, Instruments, and compiler teams.
Lattner studied computer science at the University of Portland, Oregon, graduating in 2000. While in Oregon, he worked as an operating system developer, enhancing Sequent Computer Systems's DYNIX/ptx. He is married to compiler engineer Tanya Lattner, who has been serving as president of the LLVM Foundation since 2015.
In late 2000, Lattner joined the University of Illinois at Urbana-Champaign as a research assistant and M.Sc. student. While working with Vikram Adve, he designed and began implementing LLVM, an innovative infrastructure for optimizing compilers, which was the subject of his 2002 M.Sc. thesis. He completed his Ph.D. in 2005, researching new techniques for optimizing pointer-intensive programs and adding them to LLVM.[citation needed]
In 2005, Apple Inc. hired Lattner to begin work bringing LLVM to production quality for use in Apple products.  Over time, Lattner built out the technology, personally implementing many major new features in LLVM, formed and built a team of LLVM developers at Apple, started the Clang project, took responsibility for evolving Objective-C (contributing to the blocks language feature, and driving the ARC and Objective-C literals features), and nurtured the open source community (leading it through many open source releases). Apple first shipped LLVM-based technology in the 10.5 (and 10.4.8) OpenGL stack as a just-in-time (JIT) compiler, shipped the llvm-gcc compiler in the integrated development environment (IDE) Xcode 3.1, Clang 1.0 in Xcode 3.2, Clang 2.0 (with C++ support) in Xcode 4.0, and LLDB, libc++, assemblers, and disassembler technology in later releases.[citation needed]
Lattner's recent work involves designing, implementing, and evangelizing the LLVM and Clang compilers, productizing and driving the debugger LLDB, and overseeing development of the low-level toolchain. As of 2016, LLVM technologies are the core of Apple's developer tools and the default toolchain on FreeBSD.[citation needed]
In June 2010, the Association for Computing Machinery (ACM) Special Interest Group on programming languages (SIGPLAN) gave Lattner its inaugural ACM SIGPLAN Programming Languages Software Award ""for his design and development of the Low Level Virtual Machine"", noting that Professor Adve has stated: ""Lattner’s talent as a compiler architect, together with his programming skills, technical vision, and leadership ability were crucial to the success of LLVM.""
In April 2013, the ACM awarded Lattner its Software System Award, which is presented to anyone ""recognized for developing a software system that has had a lasting influence, reflected in contributions to concepts, in commercial acceptance, or both"".
Swift is an open sourceprogramming language with first-class functions for iOS and macOS development, created by Apple and introduced at Apple's developer conference Apple Worldwide Developers Conference (WWDC) 2014.
Swift is designed to coexist with Objective-C, the object-oriented programming language formerly preferred by Apple, and to be more resilient against erroneous code. It is built with the LLVM compiler included in Xcode 6.
Lattner began developing Swift in 2010, with the eventual collaboration of many other programmers. On June 2, 2014, the WWDC app became the first publicly released app that used Swift.
He then decided to give the Project Lead role to Ted Kremenek in January 2017."
"5","Automatic Reference Counting (ARC) is a memory management feature of the Clang compiler providing automatic reference counting for the Objective-C and Swift programming languages. At compile time, it inserts into the object code messages retain and release which increase and decrease the reference count at run time, marking for deallocation those objects when the number of references to them reaches zero.
ARC differs from tracing garbage collection in that there is no background process that deallocates the objects asynchronously at runtime. Unlike garbage collection, ARC does not handle reference cycles automatically. This means that as long as there are ""strong"" references to an object, it will not be deallocated. Strong cross-references can accordingly create deadlocks and memory leaks. It is up to the developer to break cycles by using weak references.
Apple Inc. deploys ARC in their operating systems, such as macOS (OS X) and iOS. Limited support (ARCLite) has been available since Mac OS X Snow Leopard and iOS 4, with complete support following in Mac OS X Lion and iOS 5. Garbage collection was declared deprecated in OS X Mountain Lion, in favor of ARC, and removed from the Objective-C runtime library in macOS Sierra.
The following rules are enforced by the compiler when ARC is turned on:
ARC introduces some new property declaration attributes, some of which replace the old attributes.
Zeroing weak references is a feature in Objective-C ARC that automatically clears (sets to nil) weak-reference local variables, instance variables, and declared properties immediately before the object being pointed to starts deallocating. This ensures that the pointer goes to either a valid object or nil, and avoids dangling pointers. Prior to the introduction of this feature, ""weak references"" referred to references that were not retaining, but were not set to nil when the object they pointed to was deallocated (equivalent to unsafe_unretained in ARC), thus possibly leading to a dangling pointer. The programmer typically had to ensure that all possible weak references to an object were set to nil manually when it was being deallocated. Zeroing weak references obviates the need to do this.
Zeroing weak references are indicated by using the declared property attribute weak or by using the variable attribute __weak.
Zeroing weak references are only available in Mac OS X Lion (10.7) or later and iOS 5 or later, because they require additional support from the Objective-C runtime.  However, some OS X classes do not currently support weak references. Code that uses ARC but needs to support versions of the OS older than those above cannot use zeroing weak references, and therefore must use unsafe_unretained weak references. There exists a third-party library called PLWeakCompatibility  that allows one to use zeroing weak references even on these older OS versions.
Xcode 4.2 or later provides a way to convert code to ARC.  As of Xcode 4.5, it is found by choosing Edit > Refactor > Convert to Objective-C ARC... Although Xcode will automatically convert most code, some code may have to be converted manually.  Xcode will inform the developer when more complex use cases arise, such as when a variable is declared inside an autorelease pool and used outside it or when two objects need to be toll-free bridged with special casts.
In Swift, references to objects are strong, unless they are declared weak or unowned. Swift requires explicit handling of nil with the Optional type: a value type that can either have a value or be nil. An Optional type must be handled by ""unwrapping"" it with a conditional statement, allowing safe usage of the value, if present. Conversely, any non-Optional type will always have a value and cannot be nil.
Accordingly, a strong reference to an object cannot be of type Optional, as the object will be kept in the heap until the reference itself is deallocated. A weak reference is of type Optional, as the object can be deallocated and the reference be set to nil. Unowned references fall in-between; they are neither strong nor of type Optional. Instead, the compiler assumes that the object to which an unowned reference points is not deallocated as long the reference itself remains allocated. This is typically used in situations where the target object itself holds a reference to the object that holds the unowned reference.
Swift also differs from Objective-C in its usage and encouragement of value types instead of reference types. Most types in the Swift standard library are value types and they are copied by reference, whereas classes and closures are reference types and passed by reference. Because value types are copied when passed around, they are deallocated automatically with the reference that created them."
"6","Objective-C is a general-purpose, object-oriented programming language that adds Smalltalk-style messaging to the C programming language. It was the main programming language used by Apple for the OS X and iOS operating systems, and their respective application programming interfaces (APIs) Cocoa and Cocoa Touch prior to the introduction of Swift.
The programming language Objective-C was originally developed in the early 1980s. It was selected as the main language used by NeXT for its NeXTSTEP operating system, from which OS X and iOS are derived. Portable Objective-C programs that do not use the Cocoa or Cocoa Touch libraries, or those using parts that may be ported or reimplemented for other systems, can also be compiled for any system supported by GNU Compiler Collection (GCC) or Clang.
Objective-C source code 'implementation' program files usually have .m filename extensions, while Objective-C 'header/interface' files have .h extensions, the same as C header files. Objective-C++ files are denoted with a .mm file extension.
Objective-C was created primarily by Brad Cox and Tom Love in the early 1980s at their company Stepstone. Both had been introduced to Smalltalk while at ITT Corporation's Programming Technology Center in 1981. The earliest work on Objective-C traces back to around that time.
Cox was intrigued by problems of true reusability in software design and programming. He realized that a language like Smalltalk would be invaluable in building development environments for system developers at ITT. However, he and Tom Love also recognized that backward compatibility with C was critically important in ITT's telecom engineering milieu.
Cox began writing a pre-processor for C to add some of the abilities of Smalltalk. He soon had a working implementation of an object-oriented extension to the C language, which he called ""OOPC"" for Object-Oriented Pre-Compiler.
Love was hired by Schlumberger Research in 1982 and had the opportunity to acquire the first commercial copy of Smalltalk-80, which further influenced the development of their brainchild.
In order to demonstrate that real progress could be made, Cox showed that making interchangeable software components really needed only a few practical changes to existing tools. Specifically, they needed to support objects in a flexible manner, come supplied with a usable set of libraries, and allow for the code (and any resources needed by the code) to be bundled into one cross-platform format.
Love and Cox eventually formed a new venture, Productivity Products International (PPI), to commercialize their product, which coupled an Objective-C compiler with class libraries. In 1986, Cox published the main description of Objective-C in its original form in the book Object-Oriented Programming, An Evolutionary Approach. Although he was careful to point out that there is more to the problem of reusability than just the language, Objective-C often found itself compared feature for feature with other languages.
In 1988, NeXT licensed Objective-C from StepStone (the new name of PPI, the owner of the Objective-C trademark) and extended the GCC compiler to support Objective-C. NeXT developed the AppKit and Foundation Kit libraries on which the NeXTSTEP user interface and Interface Builder were based. While the NeXT workstations failed to make a great impact in the marketplace, the tools were widely lauded in the industry. This led NeXT to drop hardware production and focus on software tools, selling NeXTSTEP (and OpenStep) as a platform for custom programming.
In order to circumvent the terms of the GPL, NeXT had originally intended to ship the Objective-C frontend separately, allowing the user to link it with GCC to produce the compiler executable. After being initially accepted by Richard M. Stallman, this plan was rejected after Stallman consulted with GNU's lawyers and NeXT agreed to make Objective-C part of GCC.
The work to extend GCC was led by Steve Naroff, who joined NeXT from StepStone. The compiler changes were made available as per GPL license terms, but the runtime libraries were not, rendering the open source contribution unusable to the general public.  This led to other parties developing such runtime libraries under open source license.  Later, Steve Naroff was also principal contributor to work at Apple to build the Objective-C frontend to Clang.
The GNU project started work on its free software implementation of Cocoa, named GNUstep, based on the OpenStep standard. Dennis Glatting wrote the first GNU Objective-C runtime in 1992. The GNU Objective-C runtime, which has been in use since 1993, is the one developed by Kresten Krab Thorup when he was a university student in Denmark.[citation needed] Thorup also worked at NeXT from 1993 to 1996.
After acquiring NeXT in 1996, Apple Computer used OpenStep in its new operating system, OS X. This included Objective-C, NeXT's Objective-C-based developer tool, Project Builder, and its interface design tool, Interface Builder, both now merged into one application, Xcode. Most of Apple's current Cocoa API is based on OpenStep interface objects and is the most significant Objective-C environment being used for active development.
At WWDC 2014, Apple introduced a new language, Swift, which was characterized as ""Objective-C without the C"".
Objective-C is a thin layer atop C, and is a ""strict superset"" of C, meaning that it is possible to compile any C program with an Objective-C compiler, and to freely include C language code within an Objective-C class.
Objective-C derives its object syntax from Smalltalk. All of the syntax for non-object-oriented operations (including primitive variables, pre-processing, expressions, function declarations, and function calls) are identical to those of C, while the syntax for object-oriented features is an implementation of Smalltalk-style messaging.
The Objective-C model of object-oriented programming is based on message passing to object instances. In Objective-C one does not call a method; one sends a message. This is unlike the Simula-style programming model used by C++. The difference between these two concepts is in how the code referenced by the method or message name is executed. In a Simula-style language, the method name is in most cases bound to a section of code in the target class by the compiler. In Smalltalk and Objective-C, the target of a message is resolved at runtime, with the receiving object itself interpreting the message. A method is identified by a selector or SEL — a NUL-terminated string representing its name — and resolved to a C method pointer implementing it: an IMP. A consequence of this is that the message-passing system has no type checking. The object to which the message is directed — the receiver — is not guaranteed to respond to a message, and if it does not, it raises an exception.
Sending the message method to the object pointed to by the pointer obj would require the following code in C++:
In Objective-C, this is written as follows:
Both styles of programming have their strengths and weaknesses. Object-oriented programming in the Simula (C++) style allows multiple inheritance and faster execution by using compile-time binding whenever possible, but it does not support dynamic binding by default. It also forces all methods to have a corresponding implementation unless they are abstract. The Smalltalk-style programming as used in Objective-C allows messages to go unimplemented, with the method resolved to its implementation at runtime. For example, a message may be sent to a collection of objects, to which only some will be expected to respond, without fear of producing runtime errors. Message passing also does not require that an object be defined at compile time. An implementation is still required for the method to be called in the derived object. (See the dynamic typing section below for more advantages of dynamic (late) binding.)
Objective-C requires that the interface and implementation of a class be in separately declared code blocks. By convention, developers place the interface in a header file and the implementation in a code file. The header files, normally suffixed .h, are similar to C header files while the implementation (method) files, normally suffixed .m, can be very similar to C code files.
In other programming languages, this is called a ""class declaration"".
The interface of a class is usually defined in a header file. A common convention is to name the header file after the name of the class, e.g. Ball.h would contain the interface for the class Ball.
An interface declaration takes the form:
In the above, plus signs denote class methods, or methods that can be called on the class itself (not on an instance), and minus signs denote instance methods, which can only be called on a particular instance of the class. Class methods also have no access to instance variables.
The code above is roughly equivalent to the following C++ interface:
Note that instanceMethod2With2Parameters:param2_callName: demonstrates the interleaving of selector segments with argument expressions, for which there is no direct equivalent in C/C++.
Return types can be any standard C type, a pointer to a generic Objective-C object, a pointer to a specific type of object such as NSArray *, NSImage *, or NSString *, or a pointer to the class to which the method belongs (instancetype). The default return type is the generic Objective-C type id.
Method arguments begin with a name labeling the argument that is part of the method name, followed by a colon followed by the expected argument type in parentheses and the argument name. The label can be omitted.
The interface only declares the class interface and not the methods themselves: the actual code is written in the implementation file. Implementation (method) files normally have the file extension .m, which originally signified ""messages"".
Methods are written using their interface declarations.
Comparing Objective-C and C:
The syntax allows pseudo-naming of arguments.
Internal representations of a method vary between different implementations of Objective-C. If myColor is of the class Color, instance method -changeColorToRed:green:blue: might be internally labeled _i_Color_changeColorToRed_green_blue. The i is to refer to an instance method, with the class and then method names appended and colons changed to underscores. As the order of parameters is part of the method name, it cannot be changed to suit coding style or expression as with true named parameters.
However, internal names of the function are rarely used directly. Generally, messages are converted to function calls defined in the Objective-C runtime library. It is not necessarily known at link time which method will be called because the class of the receiver (the object being sent the message) need not be known until runtime.
Once an Objective-C class is written, it can be instantiated. This is done by first allocating an uninitialized instance of the class (an object) and then by initializing it. An object is not fully functional until both steps have been completed. These steps should be accomplished with one line of code so that there is never an allocated object that hasn't undergone initialization (and because it is unwise to keep the intermediate result since -init can return a different object than that on which it is called).
Instantiation with the default, no-parameter initializer:
Instantiation with a custom initializer:
In the case where no custom initialization is being performed, the ""new"" method can often be used in place of the alloc-init messages:
Also, some classes implement class method initializers. Like +new, they combine +alloc and -init, but unlike +new, they return an autoreleased instance. Some class method initializers take parameters:
The alloc message allocates enough memory to hold all the instance variables for an object, sets all the instance variables to zero values, and turns the memory into an instance of the class; at no point during the initialization is the memory an instance of the superclass.
The init message performs the set-up of the instance upon creation. The init method is often written as follows:
In the above example, notice the id return type. This type stands for ""pointer to any object"" in Objective-C (See the Dynamic typing section).
The initializer pattern is used to assure that the object is properly initialized by its superclass before the init method performs its initialization. It performs the following actions:
A non-valid object pointer has the value nil; conditional statements like ""if"" treat nil like a null pointer, so the initialization code will not be executed if [super init] returned nil. If there is an error in initialization the init method should perform any necessary cleanup, including sending a ""release"" message to self, and return nil to indicate that initialization failed. Any checking for such errors must only be performed after having called the superclass initialization to ensure that destroying the object will be done correctly.
If a class has more than one initialization method, only one of them (the ""designated initializer"") needs to follow this pattern; others should call the designated initializer instead of the superclass initializer.
In other programming languages, these are called ""interfaces"".
Objective-C was extended at NeXT to introduce the concept of multiple inheritance of specification, but not implementation, through the introduction of protocols. This is a pattern achievable either as an abstract multiple inherited base class in C++, or as an ""interface"" (as in Java and C#). Objective-C makes use of ad hoc protocols called informal protocols and compiler-enforced protocols called formal protocols.
An informal protocol is a list of methods that a class can opt to implement. It is specified in the documentation, since it has no presence in the language. Informal protocols are implemented as a category (see below) on NSObject and often include optional methods, which, if implemented, can change the behavior of a class. For example, a text field class might have a delegate that implements an informal protocol with an optional method for performing auto-completion of user-typed text. The text field discovers whether the delegate implements that method (via reflection) and, if so, calls the delegate's method to support the auto-complete feature.
A formal protocol is similar to an interface in Java, C#, and Ada 2005. It is a list of methods that any class can declare itself to implement. Versions of Objective-C before 2.0 required that a class must implement all methods in a protocol it declares itself as adopting; the compiler will emit an error if the class does not implement every method from its declared protocols. Objective-C 2.0 added support for marking certain methods in a protocol optional, and the compiler will not enforce implementation of optional methods.
A class must be declared to implement that protocol to be said to conform to it. This is detectable at runtime. Formal protocols cannot provide any implementations; they simply assure callers that classes that conform to the protocol will provide implementations. In the NeXT/Apple library, protocols are frequently used by the Distributed Objects system to represent the abilities of an object executing on a remote system.
The syntax
denotes that there is the abstract idea of locking. By stating in the class definition that the protocol is implemented,
instances of NSLock claim that they will provide an implementation for the two instance methods.
Objective-C, like Smalltalk, can use dynamic typing: an object can be sent a message that is not specified in its interface. This can allow for increased flexibility, as it allows an object to ""capture"" a message and send the message to a different object that can respond to the message appropriately, or likewise send the message on to another object. This behavior is known as message forwarding or delegation (see below). Alternatively, an error handler can be used in case the message cannot be forwarded. If an object does not forward a message, respond to it, or handle an error, then the system will generate a runtime exception.  If messages are sent to nil (the null object pointer), they will be silently ignored or raise a generic exception, depending on compiler options.
Static typing information may also optionally be added to variables. This information is then checked at compile time. In the following four statements, increasingly specific type information is provided. The statements are equivalent at runtime, but the extra information allows the compiler to warn the programmer if the passed argument does not match the type specified.
In the above statement, foo may be of any class.
In the above statement, foo may be an instance of any class that conforms to the NSCopying protocol.
In the above statement, foo must be an instance of the NSNumber class.
In the above statement, foo must be an instance of the NSNumber class, and it must conform to the NSCopying protocol.
Objective-C permits the sending of a message to an object that may not respond. Rather than responding or simply dropping the message, an object can forward the message to an object that can respond. Forwarding can be used to simplify implementation of certain design patterns, such as the observer pattern or the proxy pattern.
The Objective-C runtime specifies a pair of methods in Object
An object wishing to implement forwarding needs only to override the forwarding method with a new method to define the forwarding behavior. The action method performv:: need not be overridden, as this method merely performs an action based on the selector and arguments. Notice the SEL type, which is the type of messages in Objective-C.
Note: in OpenStep, Cocoa, and GNUstep, the commonly used frameworks of Objective-C, one does not use the Object class. The - (void)forwardInvocation:(NSInvocation *)anInvocation method of the NSObject class is used to do forwarding.
Here is an example of a program that demonstrates the basics of forwarding.
When compiled using gcc, the compiler reports:
The compiler is reporting the point made earlier, that Forwarder does not respond to hello messages. In this circumstance, it is safe to ignore the warning since forwarding was implemented. Running the program produces this output:
During the design of Objective-C, one of the main concerns was the maintainability of large code bases. Experience from the structured programming world had shown that one of the main ways to improve code was to break it down into smaller pieces. Objective-C borrowed and extended the concept of categories from Smalltalk implementations to help with this process.
Furthermore, the methods within a category are added to a class at run-time. Thus, categories permit the programmer to add methods to an existing class without the need to recompile that class or even have access to its source code. For example, if a system does not contain a spell checker in its String implementation, it could be added without modifying the String source code.
Methods within categories become indistinguishable from the methods in a class when the program is run. A category has full access to all of the instance variables within the class, including private variables.
If a category declares a method with the same method signature as an existing method in a class, the category's method is adopted. Thus categories can not only add methods to a class, but also replace existing methods. This feature can be used to fix bugs in other classes by rewriting their methods, or to cause a global change to a class's behavior within a program. If two categories have methods with the same name but different method signatures, it is undefined which category's method is adopted.
Other languages have attempted to add this feature in a variety of ways. TOM took the Objective-C system a step further and allowed for the addition of variables also. Other languages have used prototype-based solutions instead, the most notable being Self.
The C# and Visual Basic.NET languages implement superficially similar functionality in the form of extension methods, but these lack access to the private variables of the class.Ruby and several other dynamic programming languages refer to the technique as ""monkey patching"".
Logtalk implements a concept of categories (as first-class entities) that subsumes Objective-C categories functionality (Logtalk categories can also be used as fine-grained units of composition when defining e.g. new classes or prototypes; in particular, a Logtalk category can be virtually imported by any number of classes and prototypes).
This example builds up an Integer class, by defining first a basic class with only accessor methods implemented, and adding two categories, Arithmetic and Display, which extend the basic class. While categories can access the base class's private data members, it is often good practice to access these private data members through the accessor methods, which helps keep categories more independent from the base class. Implementing such accessors is one typical usage of categories. Another is to use categories to add methods to the base class. However, it is not regarded as good practice to use categories for subclass overriding, also known as monkey patching. Informal protocols are implemented as a category on the base NSObject class. By convention, files containing categories that extend base classes will take the name BaseClass+ExtensionClass.h.
Compilation is performed, for example, by:
One can experiment by leaving out the #import ""Integer+Arithmetic.h""  and [num1 add:num2] lines and omitting Integer+Arithmetic.m in compilation. The program will still run. This means that it is possible to mix-and-match added categories if needed; if a category does not need to have some ability, it can simply not be compile in.
Objective-C permits a class to wholly replace another class within a program. The replacing class is said to ""pose as"" the target class.
Class posing was declared deprecated with Mac OS X v10.5, and is unavailable in the 64-bit runtime. Similar functionality can be achieved by using method swizzling in categories, that swaps one method's implementation with another's that have the same signature.
For the versions still supporting posing, all messages sent to the target class are instead received by the posing class. There are several restrictions:
Posing, similarly with categories, allows global augmentation of existing classes. Posing permits two features absent from categories:
For example,
This intercepts every invocation of setMainMenu to NSApplication.
In the C language, the #include pre-compile directive always causes a file's contents to be inserted into the source at that point. Objective-C has the #import directive, equivalent except that each file is included only once per compilation unit, obviating the need for include guards.
Objective-C's features often allow for flexible, and often easy, solutions to programming issues.
Objective-C++ is a language variant accepted by the front-end to the GNU Compiler Collection and Clang, which can compile source files that use a combination of C++ and Objective-C syntax. Objective-C++ adds to C++ the extensions that Objective-C adds to C. As nothing is done to unify the semantics behind the various language features, certain restrictions apply:
At the 2006 Worldwide Developers Conference, Apple announced the release of ""Objective-C 2.0,"" a revision of the Objective-C language to include ""modern garbage collection, syntax enhancements, runtime performance improvements, and 64-bit support"". Mac OS X v10.5, released in October 2007, included an Objective-C 2.0 compiler. GCC 4.6 supports many new Objective-C features, such as declared and synthesized properties, dot syntax, fast enumeration, optional protocol methods, method/protocol/class attributes, class extensions and a new GNU Objective-C runtime API.
Objective-C 2.0 provided an optional conservative, generational garbage collector. When run in backwards-compatible mode, the runtime turned reference counting operations such as ""retain"" and ""release"" into no-ops. All objects were subject to garbage collection when garbage collection was enabled. Regular C pointers could be qualified with ""__strong"" to also trigger the underlying write-barrier compiler intercepts and thus participate in garbage collection. A zero-ing weak subsystem was also provided such that pointers marked as ""__weak"" are set to zero when the object (or more simply, GC memory) is collected. The garbage collector does not exist on the iOS implementation of Objective-C 2.0. Garbage collection in Objective-C runs on a low-priority background thread, and can halt on user events, with the intention of keeping the user experience responsive.
Garbage collection was deprecated in OS X v10.8 in favor of Automatic Reference Counting (ARC).  Objective-C on iOS 7 running on ARM64 uses 19 bits out of a 64-bit word to store the reference count, as a form of tagged pointers.
Objective-C 2.0 introduces a new syntax to declare instance variables as properties, with optional attributes to configure the generation of accessor methods. Properties are, in a sense, public instance variables; that is, declaring an instance variable as a property provides external classes with access (possibly limited, e.g. read only) to that property. A property may be declared as ""readonly"", and may be provided with storage semantics such as assign, copy or retain. By default, properties are considered atomic, which results in a lock preventing multiple threads from accessing them at the same time. A property can be declared as nonatomic, which removes this lock.
Properties are implemented by way of the @synthesize keyword, which generates getter (and setter, if not read-only) methods according to the property declaration. Alternatively, the getter and setter methods must be implemented explicitly, or the @dynamic keyword can be used to indicate that accessor methods will be provided by other means. When compiled using clang 3.1 or higher, all properties which are not explicitly declared with @dynamic, marked readonly or have complete user-implemented getter and setter will be automatically implicitly @synthesize'd.
Properties can be accessed using the traditional message passing syntax, dot notation, or, in Key-Value Coding, by name via the ""valueForKey:""/""setValue:forKey:"" methods.
In order to use dot notation to invoke property accessors within an instance method, the ""self"" keyword should be used:
A class or protocol's properties may be dynamically introspected.
Objective-C 2.0 provides non-fragile instance variables where supported by the runtime (i.e. when building code for 64-bit Mac OS X, and all iOS). Under the modern runtime, an extra layer of indirection is added to instance variable access, allowing the dynamic linker to adjust instance layout at runtime. This feature allows for two important improvements to Objective-C code:
Instead of using an NSEnumerator object or indices to iterate through a collection, Objective-C 2.0 offers the fast enumeration syntax. In Objective-C 2.0, the following loops are functionally equivalent, but have different performance traits.
Fast enumeration generates more efficient code than standard enumeration because method calls to enumerate over objects are replaced by pointer arithmetic using the NSFastEnumeration protocol.
A class extension has the same syntax as a category declaration with no category name, and the methods and properties declared in it are added directly to the main class. It is mostly used as an alternative to a category to add methods to a class without advertising them in the public headers, with the advantage that for class extensions the compiler checks that all the privately declared methods are actually implemented.
All Objective-C applications developed for Mac OS X that make use of the above improvements for Objective-C 2.0 are incompatible with all operating systems prior to 10.5 (Leopard). Since fast enumeration does not generate exactly the same binaries as standard enumeration, its use will cause an application to crash on OS X version 10.4 or earlier.
Blocks is a nonstandard extension for Objective-C (and C and C++) that uses special syntax to create closures. Blocks are only supported in Mac OS X 10.6 ""Snow Leopard"" or later, iOS 4 or later, and GNUstep with libobjc2 1.7 and compiling with clang 3.1 or later.
Automatic Reference Counting (ARC) is a compile-time feature that eliminates the need for programmers to manually manage retain counts using retain and release.  Unlike garbage collection, which occurs at run time, ARC eliminates the overhead of a separate process managing retain counts. ARC and manual memory management are not mutually exclusive; programmers can continue to use non-ARC code in ARC-enabled projects by disabling ARC for individual code files. XCode can also attempt to automatically upgrade a project to ARC.
NeXT and Apple Obj-C runtimes have long included a short-form way to create new strings, using the literal syntax @""a new string"", or drop to CoreFoundation constants kCFBooleanTrue and kCFBooleanFalse for NSNumber with Boolean values. Using this format saves the programmer from having to use the longer initWithString or similar methods when doing certain operations.
When using Apple LLVM compiler 4.0 or later, arrays, dictionaries, and numbers (NSArray, NSDictionary, NSNumber classes) can also be created using literal syntax instead of methods.
Example without literals:
Example with literals:
However, different from string literals, which compile to constants in the executable, these literals compile to code equivalent to the above method calls. In particular, under manually reference-counted memory management, these objects are autoreleased, which requires added care when e.g., used with function-static variables or other kinds of globals.
When using Apple LLVM compiler 4.0 or later, arrays and dictionaries (NSArray and NSDictionary classes) can be manipulated using subscripting.  Subscripting can be used to retrieve values from indexes (array) or keys (dictionary), and with mutable objects, can also be used to set objects to indexes or keys. In code, subscripting is represented using brackets [ ].
Example without subscripting:
Example with subscripting:
After the purchase of NeXT by Apple, attempts were made to make the language more acceptable to programmers more familiar with Java than Smalltalk. One of these attempts was introducing what was dubbed ""Modern Syntax"" for Objective-C at the time (as opposed to the current, ""classic"" syntax). There was no change in behaviour, this was merely an alternative syntax. Instead of writing a method invocation like
It was instead written as
Similarly, declarations went from the form
to
This ""modern"" syntax is no longer supported in current dialects of the Objective-C language.
Besides the GCC/NeXT/Apple implementation, which added several extensions to the original Stepstone implementation, another free, open-source Objective-C implementation called the Portable Object Compiler also exists. The set of extensions implemented by the Portable Object Compiler differs from the GCC/NeXT/Apple implementation; in particular, it includes Smalltalk-like blocks for Objective-C, while it lacks protocols and categories, two features used extensively in OpenStep and its derivatives and relatives. Overall, POC represents an older, pre-NeXT stage in the language's evolution, roughly conformant to Brad Cox's 1991 book.
It also includes a runtime library called ObjectPak, which is based on Cox's original ICPak101 library (which in turn derives from the Smalltalk-80 class library), and is quite radically different from the OpenStep FoundationKit.
The PC GEOS system used a programming language known as GEOS Objective-C or goc; despite the name similarity, the two languages are similar only in overall concept and the use of keywords prefixed with an @ sign.
The Clang compiler suite, part of the LLVM project, implements Objective-C, and other languages.
WinObjC (Also known as ""The Bridge"") is a open-source ObjC compiler project started by Microsoft on GitHub as a way to allow the reuse of iOS Application code inside of a Windows Universal Applications.
On Windows, Objective-C Development tools are provided for download on GNUStep's website.  The GNUStep Development System consists of the following packages: GNUstep MSYS System, GNUstep Core, GNUstep Devel, GNUstep Cairo, ProjectCenter IDE (Like Xcode, but not as complex), Gorm (Interface Builder Like Xcode NIB builder).
Objective-C today is often used in tandem with a fixed library of standard objects (often known as a ""kit"" or ""framework""), such as Cocoa, GNUstep or ObjFW. These libraries often come with the operating system: the GNUstep libraries often come with Linux-based distributions and Cocoa comes with OS X. The programmer is not forced to inherit functionality from the existing base class (NSObject / OFObject). Objective-C allows for the declaration of new root classes that do not inherit any existing functionality. Originally, Objective-C-based programming environments typically offered an Object class as the base class from which almost all other classes inherited. With the introduction of OpenStep, NeXT created a new base class named NSObject, which offered additional features over Object (an emphasis on using object references and reference counting instead of raw pointers, for example). Almost all classes in Cocoa inherit from NSObject.
Not only did the renaming serve to differentiate the new default behavior of classes within the OpenStep API, but it allowed code that used Object—the original base class used on NeXTSTEP (and, more or less, other Objective-C class libraries)—to co-exist in the same runtime with code that used NSObject (with some limitations). The introduction of the two letter prefix also became a simplistic form of namespaces, which Objective-C lacks. Using a prefix to create an informal packaging identifier became an informal coding standard in the Objective-C community, and continues to this day.
More recently, package managers have started appearing, such as CocoaPods, which aims to be both a package manager and a repository of packages. A lot of open-source Objective-C code that was written in the last few years can now be installed using CocoaPods.
Objective-C implementations use a thin runtime system written in C, which adds little to the size of the application. In contrast, most object-oriented systems at the time that it was created used large virtual machine runtimes. Programs written in Objective-C tend to be not much larger than the size of their code and that of the libraries (which generally do not need to be included in the software distribution), in contrast to Smalltalk systems where a large amount of memory was used just to open a window. Objective-C applications tend to be larger than similar C or C++ applications because Objective-C dynamic typing does not allow methods to be stripped or inlined. Since the programmer has such freedom to delegate, forward calls, build selectors on the fly and pass them to the runtime system, the Objective-C compiler cannot assume it is safe to remove unused methods or to inline calls.
Likewise, the language can be implemented atop extant C compilers (in GCC, first as a preprocessor, then as a module) rather than as a new compiler. This allows Objective-C to leverage the huge existing collection of C code, libraries, tools, etc. Existing C libraries can be wrapped in Objective-C wrappers to provide an OO-style interface. In this aspect, it is similar to GObject library and Vala language, which are widely used in development of GTK applications.
All of these practical changes lowered the barrier to entry, likely the biggest problem for the widespread acceptance of Smalltalk in the 1980s.
A common criticism is that Objective-C does not have language support for namespaces. Instead, programmers are forced to add prefixes to their class names, which are traditionally shorter than namespace names and thus more prone to collisions. As of 2007, all Mac OS X classes and functions in the Cocoa programming environment are prefixed with ""NS"" (e.g. NSObject, NSButton) to identify them as belonging to the Mac OS X or iOS core; the ""NS"" derives from the names of the classes as defined during the development of NeXTSTEP.
Since Objective-C is a strict superset of C, it does not treat C primitive types as first-class objects.
Unlike C++, Objective-C does not support operator overloading. Also unlike C++, Objective-C allows an object to directly inherit only from one class (forbidding multiple inheritance). However, in most cases, categories and protocols may be used as alternative ways to achieve the same results.
Because Objective-C uses dynamic runtime typing and because all method calls are function calls (or, in some cases, syscalls), many common performance optimizations cannot be applied to Objective-C methods (for example: inlining, constant propagation, interprocedural optimizations, and scalar replacement of aggregates). This limits the performance of Objective-C abstractions relative to similar abstractions in languages such as C++ where such optimizations are possible.
The first versions of Objective-C did not support garbage collection. At the time this decision was a matter of some debate, and many people considered long ""dead times"" (when Smalltalk performed collection) to render the entire system unusable. Some 3rd party implementations have added this feature (most notably GNUstep) and Apple has implemented it as of Mac OS X v10.5. However, in more recent versions of Mac OS X and iOS, garbage collection has been deprecated in favor of Automatic Reference Counting (ARC), introduced in 2011.
With ARC, the compiler inserts retain and release calls automatically into Objective-C code based on static code analysis. The automation relieves the programmer of having to write in memory management code. ARC also adds weak references to the Objective-C language.
The design and implementation of C++ and Objective-C represent fundamentally different approaches to extending C.
In addition to C's style of procedural programming, C++ directly supports certain forms of object-oriented programming, generic programming, and metaprogramming. C++ also comes with a large standard library that includes several container classes. Similarly, Objective-C adds object-oriented programming, dynamic typing, and reflection to C. Objective-C does not provide a standard library per se, but in most places where Objective-C is used, it is used with an OpenStep-like library such as OPENSTEP, Cocoa, or GNUstep, which provides functionality similar to C++'s standard library.
One notable difference is that Objective-C provides runtime support for reflective features, whereas C++ adds only a small amount of runtime support to C. In Objective-C, an object can be queried about its own properties, e.g., whether it will respond to a certain message. In C++, this is not possible without the use of external libraries.
The use of reflection is part of the wider distinction between dynamic (run-time) features and static (compile-time) features of a language. Although Objective-C and C++ each employ a mix of both features, Objective-C is decidedly geared toward run-time decisions while C++ is geared toward compile-time decisions. The tension between dynamic and static programming involves many of the classic trade-offs in programming: dynamic features add flexibility, static features add speed and type checking.
Generic programming and metaprogramming can be implemented in both languages using runtime polymorphism. In C++ this takes the form of virtual functions and runtime type identification, while Objective-C offers dynamic typing and reflection.  Objective-C lacks compile-time polymorphism (generic functions) entirely, while C++ supports it via function overloading and templates.
"
"7","
NeXTSTEP is a discontinued object-oriented, multitasking operating system based on UNIX. It was developed by NeXT Computer in the late 1980s and early 1990s and was initially used for its range of proprietary workstation computers such as the NeXTcube and later ported to several other computer architectures.
Although relatively unsuccessful at the time, it attracted interest from computer scientists and researchers. It was used as the original platform for the development of the Electronic AppWrapper, the first commercial electronic software distribution catalog to collectively manage encryption and provide digital rights for application software and digital media, a forerunner of the modern ""app store"" concept. It was also the platform on which Tim Berners-Lee created the first web browser. After the purchase of NeXT by Apple, it became the source of the popular operating systems macOS, iOS, watchOS, tvOS, and audioOS. Many bundled macOS applications, such as TextEdit, Mail, and Chess, are descendants of NeXTSTEP applications.
NeXTSTEP (also stylized as NeXTstep, NeXTStep, and NEXTSTEP) is a combination of several parts:
NeXTSTEP is notable for having been a preeminent implementation of the latter three items. The toolkits offer considerable power, and are the canonical development system for all of the software on the machine.
It introduced the idea of the Dock (carried through OpenStep and into today's macOS) and the Shelf. NeXTSTEP also originated or innovated a large number of other GUI concepts which became common in other operating systems: 3D ""chiseled"" widgets, large full-color icons, system-wide drag and drop of a wide range of objects beyond file icons, system-wide piped services, real-time scrolling and window dragging, properties dialog boxes called ""inspectors"", and window modification notices (such as the saved status of a file). The system is among the first general-purpose user interfaces to handle publishing color standards, transparency, sophisticated sound and music processing (through a Motorola 56000 DSP), advanced graphics primitives, internationalization, and modern typography, in a consistent manner across all applications.
Additional kits were added to the product line to make the system more attractive. These include Portable Distributed Objects (PDO), which allow easy remote invocation, and Enterprise Objects Framework, a powerful object-relational database system. The kits made the system particularly interesting to custom application programmers, and NeXTSTEP had a long history in the financial programming community.[citation needed]
A preview release of NeXTSTEP (version 0.8) was shown with the launch of the NeXT Computer on October 12, 1988. The first full release, NeXTSTEP 1.0, shipped on September 18, 1989. The last version, 3.3, was released in early 1995, by which time it ran on not only the Motorola 68000 family processors used in NeXT computers, but also on Intel x86, Sun SPARC, and HP PA-RISC-based systems.
NeXTSTEP was later modified to separate the underlying operating system from the higher-level object libraries. The result was the OpenStep API, which ran on multiple underlying operating systems, including NeXT's own OPENSTEP, Windows NT and SUN Solaris. NeXTSTEP's legacy stands today in the form of its direct descendents, Apple's macOS, iOS, watchOS, tvOS, and audioOS operating systems.
From day one, the operating system of NeXTSTEP was built upon Mach/BSD.
The first web browser, WorldWideWeb, and the first ever app store were all invented on the NeXTSTEP platform.
Some features and keyboard shortcuts now commonly found in web browsers can be traced back to NeXTSTEP conventions. The basic layout options of HTML 1.0 and 2.0 are attributable to those features available in NeXT's Text class.
Features seen first on NeXTSTEP:
In the 1990s, the pioneering PC games Doom (with its WAD level editor), Doom II, and Quake (with its respective level editor) were developed by id Software on NeXT machines. Other games based on the Doom engine such as Heretic and its sequel Hexen by Raven Software as well as Strife by Rogue Entertainment were also developed on NeXT hardware using id's tools.
Altsys made a NeXTSTEP application called Virtuoso, version 2 of which was ported to Mac OS and Windows to become Macromedia FreeHand version 4. The modern ""Notebook"" interface for Mathematica, and the advanced spreadsheet Lotus Improv, were developed using NeXTSTEP. The software that controlled MCI's Friends and Family calling plan program was developed using NeXTSTEP.
About the time of the release of NeXTSTEP 3.2, NeXT partnered with Sun Microsystems to develop OpenStep. It is the product of an effort to separate the underlying operating system from the higher-level object libraries to create a cross-platform object-oriented API standard derived from NeXTSTEP. The OpenStep API targets multiple underlying operating systems, including NeXT's own OPENSTEP. Implementations of that standard were released for Sun's Solaris, Windows NT, and NeXT's version of the Mach kernel. NeXT's implementation is called ""OPENSTEP for Mach"" and its first release (4.0) superseded NeXTSTEP 3.3 on NeXT, Sun, and Intel IA-32 systems.
Following an announcement on December 20, 1996,Apple Computer acquired NeXT on February 4, 1997 for $429 million. Based upon the ""OPENSTEP for Mach"" operating system, and developing the OPENSTEP API to become Cocoa, Apple created the basis of Mac OS X, and eventually, in turn, of iOS, watchOS, tvOS, and audioOS.
A free software implementation of the OpenStep standard, GNUstep, also exists.
Delivered on 2 CDs: NeXTSTEP CISC and NeXTSTEP RISC. The Developer CD includes libraries for all architectures, so that programs can be cross-compiled on any architecture for all architectures.
Allegedly dropped due to complaints of having to re-teach users but not for technical reasons (the new UI worked well in the beta).
Versions up to 4.1 are general releases. OPENSTEP 4.2 pre-release 2 is a bug-fix release published by Apple and was supported for five years after its September 1997 release.
This article is based on material taken from  the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later."
"8","OpenStep is an object-oriented application programming interface (API) specification for a legacy object-oriented operating system, with the basic goal of offering a NeXTSTEP-like environment on a non-NeXTSTEP operating system. OpenStep was principally developed by NeXT with Sun Microsystems, to allow NeXTSTEP (like) development on Sun's operating systems, specifically Solaris. NeXT produced a version of OpenStep for their own Mach-based Unix, known as OPENSTEP (all capitalized),  as well as a version that ran on Windows NT. The software libraries that shipped with OPENSTEP are a superset of the original OpenStep specification, including many features from the original NeXTSTEP.
The OpenStep API was created as the result of a 1993 collaboration between NeXT and Sun Microsystems, allowing this cut-down version of the NeXTSTEP operating system object layers to be run on Sun's Solaris operating system (more specifically, Solaris on SPARC-based hardware). Most of the OpenStep effort was to strip away those portions of NeXTSTEP that depended on Mach or NeXT-specific hardware being present. This resulted in a smaller system that consisted primarily of Display PostScript, the Objective-C runtime and compilers, and the majority of the NeXTSTEP Objective-C libraries. Not included was the basic operating system, or the lower-level display system.
The first draft of the API was published by NeXT in summer 1994. Later that year they released an OpenStep compliant version of NeXTSTEP as OPENSTEP, supported on several of their platforms as well as Sun SPARC systems. The official OpenStep API, published in September 1994, was the first to split the API between Foundation and Application Kit and the first to use the ""NS"" prefix. Early versions of NeXTSTEP used an ""NX"" prefix and contained only the Application Kit, relying on standard Unix libc types for low-level data structures. OPENSTEP remained NeXT's primary operating system product until they were purchased by Apple Computer in 1996. OPENSTEP was then combined with technologies from the existing classic Mac OS to produce Mac OS X. iPhone and iPad's iOS is also a descendant of OPENSTEP, but targeted at touch devices. 
Sun originally adopted the OpenStep environment with the intent of complementing Sun's CORBA-compliant object system, Solaris NEO (formerly known as Project DOE), by providing an object-oriented user interface toolkit to complement the object-oriented CORBA plumbing. The port involved integrating the OpenStep AppKit with the Display PostScript layer of the Sun X11 server, making the AppKit tolerant of multi-threaded code (as Project DOE was inherently heavily multi-threaded), implementing a Solaris daemon to simulate the behavior of Mach ports, extending the SunPro C++ compiler to support Objective-C using NeXT's ObjC runtime, writing an X11 window manager to implement the NeXTSTEP look and feel as much as possible, and integrating the NeXT development tools, such as Project Manager and Interface Builder, with the SunPro compiler. In order to provide a complete end-user environment, Sun also ported the NeXTSTEP-3.3 versions of several end-user applications, including Mail.app, Preview.app, Edit.app, Workspace Manager, and the Dock.
The OpenStep and CORBA parts of the products were later split, and NEO was released in late 1995 without the OpenStep environment. In March 1996, Sun announced Joe, a product to integrate NEO with Java. Sun shipped a beta release of the OpenStep environment for Solaris on July 22, 1996, and made it freely available for download in August 1996 for non-commercial use, and for sale in September 1996. OpenStep/Solaris was shipped only for the SPARC architecture.
The API OpenStep contrasts with the earlier NeXTSTEP primarily in five ways:
The API specification itself is composed of the two main sets of object-oriented classes: the GUI and graphics front-end known as the Application Kit, and the aforementioned Foundation Kit.
However, OpenStep also specified the use of Display PostScript, a versatile and powerful PostScript-based method of drawing windows and graphics on screen. NeXT, with its devotion to implementing object-oriented solutions, supplied pswraps for interfacing C code to Display PostScript. pswraps acted in an encapsulative way similar to Embedded SQL, and was somewhat object oriented. The Application Kit, Foundation, and Display PostScript comprise the three key technologies in the OpenStep specification; many of the specific calls in the API were available in NeXTSTEP as well, but many were modified or re-packaged for OpenStep.
The standardization on OpenStep also allowed for the creation of several new library packages that were delivered on the OPENSTEP platform. Unlike the operating system as a whole, these packages were designed to run stand-alone on practically any operating system. The idea was to use OpenStep code as a basis for network-wide applications running across different platforms, as opposed to using CORBA or some other system.
Primary among these packages was Portable Distributed Objects (PDO). PDO was essentially an even more ""stripped down"" version of OpenStep containing only the Foundation Kit technologies, combined with new libraries to provide remote invocation with very little code. Unlike OpenStep, which defined an operating system that applications would run in, under PDO the libraries were compiled into the application itself, creating a stand-alone ""native"" application for a particular platform. PDO was small enough to be easily portable, and versions were released for all major server vendors.
In the mid-1990s, NeXT staff took to writing in solutions to various CORBA magazine articles in a few lines of code, whereas the original article would fill several pages. Even though using PDO required the installation of a considerable amount of supporting code (Objective-C and the libraries), PDO applications were nevertheless considerably smaller than similar CORBA solutions, typically about one-half to one-third the size.
The similar D'OLE provided the same types of services, but presented the resulting objects as COM objects, with the goal of allowing programmers to create COM services running on high-powered platforms, called from Microsoft Windows applications. For instance one could develop a high-powered financial modeling application using D'OLE, and then call it directly from within Microsoft Excel. When D'OLE was first released, OLE by itself only communicated between applications running on a single machine. PDO enabled NeXT to demonstrate Excel talking to other Microsoft applications across a network before Microsoft themselves were able to implement this functionality (DCOM).
Another package developed on OpenStep was Enterprise Objects Framework (EOF), a tremendously powerful (for the time) object-relational mapping product. EOF became very popular in the enterprise market, notably in the financial sector where OPENSTEP caused something of a minor revolution.[citation needed]
NeXT's first operating system was NeXTSTEP, a sophisticated Mach-UNIX based operating system that originally ran only on NeXT's Motorola 68k-based workstations and that was then ported to run on 32-bit Intel x86-based ""IBM-compatible"" personal computers, PA-RISC-based workstations from Hewlett-Packard, and SPARC-based workstations from Sun Microsystems.
NeXT completed an implementation of OpenStep on their existing Mach-based OS and called it OPENSTEP for Mach 4.0 (July, 1996), 4.1 (December, 1996), and 4.2 (January, 1997). It was, for all intents, NeXTSTEP 4.0, and still retained flagship NeXTSTEP technologies (such as DPS, UNIX underpinnings, user interface characteristics like the Dock and Shelf, and so on), and retained the classic NeXTSTEP user interface and styles. OPENSTEP for Mach was further improved, in comparison to NeXTSTEP 3.3, with vastly improved driver support – however the environment to actually write drivers was changed with the introduction of the object-oriented DriverKit.
OPENSTEP for Mach supported Intel x86-based PC's, Sun's SPARC workstations, and NeXT's own 68k-based architectures, while the HP PA-RISC version was dropped. These versions continued to run on the underlying Mach-based OS used in NeXTSTEP. OPENSTEP for Mach became NeXT's primary OS from 1995 on, and was used mainly on the Intel platform. In addition to being a complete OpenStep implementation, the system was delivered with a complete set of NeXTSTEP libraries for backward compatibility. This was an easy thing to do in OpenStep due to library versioning, and OPENSTEP did not suffer in bloat because of it.
In addition to the OPENSTEP for Mach port for SPARC, Sun and NeXT developed an OpenStep compliant set of frameworks to run on Sun's Solaris operating system. After developing Solaris OpenStep, Sun lost interest in OpenStep and shifted its attention toward Java. As a virtual machine development environment, Java served as a direct competitor to OpenStep.
NeXT also delivered an implementation running on top of Windows NT 4.0 called OPENSTEP Enterprise (often abbreviated OSE). This was an unintentional demonstration on the true nature of the portability of programs created under the OpenStep specification. Programs for OPENSTEP for Mach could be ported to OSE with little difficulty. This allowed their existing customer base to continue using their tools and applications, but running them on Windows, to which many of them were in the process of switching. Never a clean match from the UI perspective, probably due to OPENSTEP's routing of window graphics through the Display Postscript server—which was also ported to Windows—OSE nevertheless managed to work fairly well and extended OpenStep's commercial lifespan.
OPENSTEP and OSE had two revisions (and one major one that was never released) before NeXT was purchased by Apple in 1997.
After acquiring NeXT, Apple intended to ship Rhapsody as a reworked version of OPENSTEP for Mach for both the Mac and standard PCs. Rhapsody was OPENSTEP for Mach with a Copland appearance from Mac OS 8 and support for Java and Apple's own technologies, including ColorSync and QuickTime; it could be regarded as OPENSTEP 5. Two developer versions of Rhapsody were released, known as Developer Preview 1 and 2; these ran on a limited subset of both Intel and PowerPC hardware.  Mac OS X Server 1.0 was the first commercial release of this operating system, and was delivered exclusively for PowerPC Mac hardware.
After replacing the Display Postscript WindowServer with Quartz, and responding to developers by including better backward compatibility for classic Mac OS applications through the addition of Carbon, Apple released Mac OS X and Mac OS X Server, starting at version 10.0; Mac OS X is now named macOS.
macOS's primary programming environment is essentially OpenStep (with certain additions such as XML property lists and URL classes for Internet connections) with macOS ports of the development libraries and tools, now called Cocoa.
macOS has since become the single most popular desktop Unix-like operating system in the world, although macOS is no longer an OpenStep compliant operating system.[citation needed]
GNUstep, a free software implementation of the NeXT libraries, began at the time of NeXTSTEP, predating OPENSTEP. While OPENSTEP and OSE were purchased by Apple, who effectively ended the commercial development of implementing OpenStep for other platforms, GNUstep is an ongoing open source project aiming to create a portable, free software implementation of the Cocoa/OPENSTEP libraries.
GNUstep also features a fully functional development environment, reimplementations of some of the newer innovations from macOS's Cocoa framework, as well as its own extensions to the API."
"9"," (Learn how and when to remove this template message)

GNUstep is a free software implementation of the Cocoa (formerly OpenStep) Objective-C frameworks, widget toolkit, and application development tools for Unix-like operating systems and Microsoft Windows. It is part of the GNU Project.
GNUstep features a cross-platform, object-oriented IDE.  Apart from the default Objective-C interface, GNUstep also has bindings for Java, Ruby,Guile and Scheme. The GNUstep developers track some additions to Apple's Cocoa to remain compatible. The roots of the GNUstep application interface are the same as the roots of Cocoa: NeXTSTEP and OpenStep. GNUstep thus predates Cocoa, which emerged when Apple acquired NeXT's technology and incorporated it into the development of the original Mac OS X, while GNUstep was initially an effort by GNU developers to replicate the technically ambitious NeXTSTEP's programmer-friendly features.
GNUstep began when Paul Kunz and others at Stanford Linear Accelerator Center wanted to port HippoDraw from NeXTSTEP to another platform. Instead of rewriting HippoDraw from scratch and reusing only the application design, they decided to rewrite the NeXTSTEP object layer on which the application depended. This was the first version of libobjcX. It enabled them to port HippoDraw to Unix systems running the X Window System without changing a single line of their application source. After the OpenStep specification was released to the public in 1994, they decided to write a new objcX which would adhere to the new APIs. The software would become known as ""GNUstep"".
GNUstep contain a set of graphical control elements written in the Objective-C programming language. The graphical user interface (GUI) of e.g. GNUMail is composed of graphics control element. GNUMail has to interact with the windowing system, e.g. X11 or Wayland, and its graphical user interface has to be rendered. GNUstep's backend provides a small set of functions used by the user interface library to interface to the actual windowing system. It also has a rendering engine which emulates common PostScript functions. The package gnustep-back provides the following backends:
GNUstep inherits some design principles proposed in OPENSTEP (GNUstep predates Cocoa, but Cocoa is based on OPENSTEP) as well as the Objective-C language.

Here are some examples of applications written for or ported to GNUstep.
The Foundation Kit provides basic classes such as wrapper classes and data structure classes.
The Application Kit provides classes oriented around graphical user interface capabilities."
"10","The architecture of macOS describes the layers of the operating system that is the culmination of Apple Inc.'s decade-long search and development process to replace the classic Mac OS.
After the failures of their previous attempts; Pink, which started as an Apple project but evolved into a joint venture with IBM called Taligent, and Copland, which started in 1994 and was cancelled two years later, Apple began development of Mac OS X with the acquisition of NeXT's NeXTSTEP in 1997.
Note that Mac OS X was renamed to OS X in 2012 and then again to macOS in 2016.
NeXTSTEP used a hybrid kernel that combined the Mach 2.5 kernel developed at Carnegie Mellon University with subsystems from 4.3BSD. NeXTSTEP also introduced a new windowing system based on Display PostScript that intended to achieve better WYSIWYG systems by using the same language to draw content on monitors that drew content on printers. NeXT also included object-oriented programming tools based on the Objective-C language that they had acquired from Stepstone and a collection of Frameworks (or Kits) that were intended to speed software development. NeXTSTEP originally ran on Motorola's 68k processors, but was later ported to Intel's x86, Hewlett-Packard's PA-RISC and Sun Microsystems' SPARC processors. Later on, the developer tools and frameworks were released, as OpenStep, as a development platform that would run on other operating systems.
On February 4, 1997, Apple acquired NeXT and began development of the Rhapsody operating system. Rhapsody built on NeXTSTEP, porting the core system to the PowerPC architecture and adding a redesigned user interface based on the Platinum user interface from Mac OS 8. An emulation layer called Blue Box allowed Mac OS applications to run within an actual instance of the Mac OS and an integrated Java platform. The Objective-C developer tools and Frameworks were referred to as the Yellow Box and also made available separately for Microsoft Windows. The Rhapsody project eventually bore the fruit of all Apple's efforts to develop a new generation Mac OS, which finally shipped in the form of Mac OS X Server.
At the 1998 Worldwide Developers Conference (WWDC), Apple announced a move that was intended as a response to complaints from Macintosh software developers who were not happy with the two options (Yellow Box and Blue Box) available in Rhapsody. Mac OS X would add another developer API to the existing ones in Rhapsody. Key APIs from the Macintosh Toolbox would be implemented in Mac OS X to run directly on the BSD layers of the operating system instead of in the emulated Macintosh layer. This modified interface, called Carbon, would eliminate approximately 2000 troublesome API calls (of about 8000 total) and replace them with calls compatible with a modern OS.
At the same conference, Apple announced that the Mach side of the kernel had been updated with sources from the OSFMK 7.3 (Open Source Foundation Mach Kernel)  and the BSD side of the kernel had been updated with sources from the FreeBSD, NetBSD and OpenBSD projects. They also announced a new driver model called I/O Kit, intended to replace the Driver Kit used in NeXTSTEP citing Driver Kit's lack of power management and hot-swap capabilities and its lack of automatic configuration capability.
At the 1999 WWDC, Apple revealed Quartz, a new Portable Document Format (PDF) based windowing system for the operating system that was not encumbered with licensing fees to Adobe like the Display PostScript windowing system of NeXTSTEP. Apple also announced that the Yellow Box layer had been renamed Cocoa and began to move away from their commitment to providing the Yellow Box on Windows. At this WWDC, Apple also showed Mac OS X booting off of a HFS Plus formatted drive for the first time.
The first public release of Mac OS X released to consumers was a Public Beta released on September 13, 2000."
"11","In programming languages and type theory, polymorphism (from Greek πολύς, polys, ""many, much"" and μορφή, morphē, ""form, shape"") is the provision of a single interface to entities of different types. A polymorphic type is one whose operations can also be applied to values of some other type, or types. There are several fundamentally different kinds of polymorphism:
The interaction between parametric polymorphism and subtyping leads to the concepts of variance and bounded quantification.
Ad hoc polymorphism and parametric polymorphism were originally described in Fundamental Concepts in Programming Languages, a set of lecture notes written in 1967 by British computer scientist Christopher Strachey.
In a 1985 paper, Peter Wegner and Luca Cardelli introduced the term inclusion polymorphism to model subtypes and inheritance. However, implementations of subtyping and inheritance predate the term ""inclusion polymorphism"", having appeared with Simula in 1967.
Christopher Strachey chose the term ad hoc polymorphism to refer to polymorphic functions that can be applied to arguments of different types, but that behave differently depending on the type of the argument to which they are applied (also known as function overloading or operator overloading). The term ""ad hoc"" in this context is not intended to be pejorative; it refers simply to the fact that this type of polymorphism is not a fundamental feature of the type system. In the Pascal / Delphi example below, the Add functions seem to work generically over various types when looking at the invocations, but are considered to be two entirely distinct functions by the compiler for all intents and purposes:
In dynamically typed languages the situation can be more complex as the correct function that needs to be invoked might only be determinable at run time.
Implicit type conversion has also been defined as a form of polymorphism, referred to as ""coercion polymorphism"".
Parametric polymorphism allows a function or a data type to be written generically, so that it can handle values uniformly without depending on their type. Parametric polymorphism is a way to make a language more expressive while still maintaining full static type-safety.
The concept of parametric polymorphism applies to both data types and functions. A function that can evaluate to or be applied to values of different types is known as a polymorphic function. A data type that can appear to be of a generalized type (e.g. a list with elements of arbitrary type) is designated polymorphic data type like the generalized type from which such specializations are made.
Parametric polymorphism is ubiquitous in functional programming, where it is often simply referred to as ""polymorphism"". The following example in Haskell shows a parametrized list data type and two parametrically polymorphic functions on them:
Parametric polymorphism is also available in several object-oriented languages. For instance, templates in C++ and D, or under the name generics in C# and Java:
John C. Reynolds (and later Jean-Yves Girard) formally developed this notion of polymorphism as an extension to lambda calculus (called the polymorphic lambda calculus or System F). Any parametrically polymorphic function is necessarily restricted in what it can do, working on the shape of the data instead of its value, leading to the concept of parametricity.
Some languages employ the idea of subtyping (also called subtype polymorphism or inclusion polymorphism) to restrict the range of types that can be used in a particular case of polymorphism. In these languages, subtyping allows a function to be written to take an object of a certain type T, but also work correctly, if passed an object that belongs to a type S that is a subtype of T (according to the Liskov substitution principle). This type relation is sometimes written S <: T. Conversely, T is said to be a supertype of S—written T :> S. Subtype polymorphism is usually resolved dynamically (see below).
In the following example we make cats and dogs subtypes of animals. The procedure letsHear() accepts an animal, but will also work correctly if a subtype is passed to it:
In another example, if Number, Rational, and Integer are types such that Number :> Rational and Number :> Integer, a function written to take a Number will work equally well when passed an Integer or Rational as when passed a Number. The actual type of the object can be hidden from clients into a black box, and accessed via object identity.
In fact, if the Number type is abstract, it may not even be possible to get your hands on an object whose most-derived type is Number (see abstract data type, abstract class). This particular kind of type hierarchy is known—especially in the context of the Scheme programming language—as a numerical tower, and usually contains many more types.
Object-oriented programming languages offer subtype polymorphism using subclassing (also known as inheritance). In typical implementations, each class contains what is called a virtual table—a table of functions that implement the polymorphic part of the class interface—and each object contains a pointer to the ""vtable"" of its class, which is then consulted whenever a polymorphic method is called. This mechanism is an example of:
The same goes for most other popular object systems. Some, however, such as Common Lisp Object System, provide multiple dispatch, under which method calls are polymorphic in all arguments.
Row polymorphism is a similar, but distinct concept from subtyping. It deals with structural types.  It allows the usage of all values whose types have certain properties, without losing the remaining type information.
A related concept is polytypism (or data type genericity). A polytypic function is more general than polymorphic, and in such a function, ""though one can provide fixed ad hoc cases for specific data types, an ad hoc combinator is absent"".
Polymorphism can be distinguished by when the implementation is selected: statically (at compile time) or dynamically (at run time, typically via a virtual function). This is known respectively as static dispatch and dynamic dispatch, and the corresponding forms of polymorphism are accordingly called static polymorphism and dynamic polymorphism.
Static polymorphism executes faster, because there is no dynamic dispatch overhead, but requires additional compiler support. Further, static polymorphism allows greater static analysis by compilers (notably for optimization), source code analysis tools, and human readers (programmers). Dynamic polymorphism is more flexible but slower—for example, dynamic polymorphism allows duck typing, and a dynamically linked library may operate on objects without knowing their full type.
Static polymorphism typically occurs in ad hoc polymorphism and parametric polymorphism, whereas dynamic polymorphism is usual for subtype polymorphism. However, it is possible to achieve static polymorphism with subtyping through more sophisticated use of template metaprogramming, namely the curiously recurring template pattern."
"12","In the area of mathematical logic and computer science known as type theory, a unit type is a type that allows only one value (and thus can hold no information). The carrier (underlying set) associated with a unit type can be any singleton set. There is an isomorphism between any two such sets, so it is customary to talk about the unit type and ignore the details of its value. One may also regard the unit type as the type of 0-tuples, i.e. the product of no types.
The unit type is the terminal object in the category of types and typed functions. It should not be confused with the zero or bottom type, which allows no values and is the initial object in this category. Similarly, the Boolean is the type with two values.
The unit type is implemented in most functional programming languages. The void type that is used in some imperative programming languages serves some of its functions, but because its carrier set is empty, it has some limitations (as detailed below).
Several computer programming languages provide a unit type to specify the result type of a function with the sole purpose of causing a side effect, and the argument type of a function that does not require arguments. 
In C, C++, C#, D and Java, void is used to designate a function that does not return anything useful, or a function that accepts no arguments. The unit type in C is conceptually similar to an empty struct, but a struct without members is not allowed in the C language specification. Instead, 'void' is used in a manner that simulates some, but not all, of the properties of the unit type, as detailed below. Like most imperative languages, C allows functions that do not return a value; these are specified as having the void return type. Such functions are called procedures in other imperative languages like Pascal, where a syntactic distinction, instead of type-system distinction, is made between functions and procedures.
The first notable difference between a true unit type and the void type is that the unit type may always be the type of the argument to a function, but the void type cannot be the type of an argument in C, despite the fact that it may appear as the sole argument in the list. This problem is best illustrated by the following program, which is a compile-time error in C:
This issue does not arise in most programming practice in C, because since the void type carries no information, it is useless to pass it anyway; but it may arise in generic programming, such as C++ templates, where void must be treated differently from other types. In C++ however, empty classes are allowed, so it is possible to implement a real unit type; the above example becomes compilable as:
(For brevity, we're not worried in the above example whether the_unit is really a singleton; see singleton pattern for details on that issue.)
The second notable difference is that the void type is special and can never be stored in a record type, i.e. in a struct or a class in C/C++. In contrast, the unit type can be stored in records in functional programming languages, i.e. it can appear as the type of a field; the above implementation of the unit type in C++ can also be stored. While this may seem a useless feature, it does allow one for instance to elegantly implement a set as a map to the unit type; in the absence of a unit type, one can still implement a set this way by storing some dummy value of another type for each key.
In Java Generics, type parameters must be reference types. The wrapper type Void is often used when a unit type parameter is needed. Although the Void type can never have any instances, it does have one value, null (like all other reference types), so it acts as a unit type. In practice, any other non-instantiable type, e.g. Math, can also be used for this purpose, since they also have exactly one value, null."
"13","
In programming, nullable types are a feature of the type system of some programming languages which allow the value to be set to the special value NULL instead of the usual possible values of the data type. In statically-typed languages, a nullable type is an option type (in functional programming terms), while in dynamically-typed languages (where values have types, but variables do not), equivalent behavior is provided by having a single null value.
Primitive types such as integers and booleans cannot generally be null, but the corresponding nullable types (nullable integer and nullable boolean, respectively) can also assume the NULL value. NULL is frequently used to represent a missing value or invalid value, such as from a function that failed to return or a missing field in a database, as in NULL in SQL.
An integer variable may represent integers, but 0 (zero) is a special case because 0 in many programming languages can mean ""false"". Also this doesn't give us any notion of saying that the variable is empty, a need for which occurs in many circumstances. This need can be achieved with a nullable type. In programming languages like C# 2.0, a nullable integer, for example, can be declared by a question mark (int? x).  In programming languages like C# 1.0, nullable types can be defined by an external library as new types (e.g. NullableInteger, NullableBoolean).
A boolean variable makes the effect more clear. Its values can be either ""true"" or ""false"", while a nullable boolean may also contain a representation for ""undecided"". However, the interpretation or treatment of a logical operation involving such a variable depends on the language.
In contrast, object pointers can be set to NULL by default in most common languages, meaning that the pointer or reference points to nowhere, that no object is assigned (the variable does not point to any object).
Nullable references were invented by C. A. R. Hoare in 1965 as part of the Algol W language.  Hoare later described their invention as a ""billion-dollar mistake"".  This is because object pointers that can be NULL require the user to check the pointer before using it and require specific code to handle the case when the object pointer is NULL. 
Java has classes that correspond to scalar values, such as Integer, Boolean and Float.  Combined with autoboxing (automatic usage-driven conversion between object and value), this effectively allows nullable variables for scalar values.
Nullable type implementations usually adhere to the null object pattern.
There is a more general and formal concept that extend the nullable type concept, it comes from option types, which enforce explicit handling of the exceptional case.
Option type implementations usually adhere to the Special Case pattern.
The following programming languages support nullable types.
Statically typed languages with native null support include:
Statically typed languages with library null support include:
Dynamically-typed languages with null include:"
"14","This article describes the syntax of the C# programming language. The features described are compatible with .NET Framework and Mono.
An identifier is the name of an element in the code. There are certain standard naming conventions to follow when selecting names for elements.
An identifier can:
An identifier cannot:
Keywords are predefined reserved words with special syntactic meaning. The language has two types of keyword — contextual and reserved. The reserved keywords such as false or byte may only be used as keywords. The contextual keywords such as where or from are only treated as keywords in certain situations. If an identifier is needed which would be the same as a reserved keyword, it may be prefixed by the @ character to distinguish it. This facilitates reuse of .NET code written in other languages.
Using a keyword as an identifier:
The underscore symbol separates digits in number values for readability purposes. Compiler will ignore it.
Generally, it may be put only between digit characters. It cannot be put at the beginning (_121) or the end of the value (121_ or 121.05_), next to the decimal in floating point values (10_.0), next to the exponent character (1.1e_1) and next to the type specifier (10_f).
Variables are identifiers associated with values. They are declared by writing the variable's type and name, and are optionally initialized in the same statement.
Declare
Assigning
Initialize
Multiple variables of the same type can be declared and initialized in one statement.
C# 3.0 introduced type inference, allowing the type specifier of a variable declaration to be replaced by the keyword var, if its actual type can be statically determined from the initializer. This reduces repetition, especially for types with multiple generic type-parameters, and adheres more closely to the DRY principle.
See also
Constants are immutable values.
When declaring a local variable or a field with the const keyword as a prefix the value must be given when it is declared. After that it is locked and cannot change. They can either be declared in the context as a field or a local variable. Constants are implicitly static.
This shows all the uses of the keyword.
The readonly keyword does a similar thing to fields. Like fields marked as const they cannot change once initialized. The difference is that you can choose to initialize them in a constructor. This only works on fields. Read-only fields can either be members of an instance or static class members.
The operators { ... } are used to signify a code block and a new scope. Class members and the body of a method are examples of what can live inside these braces in various contexts.
Inside of method bodies you can use the braces to create new scopes like so:
A C# application consists of classes and their members. Classes and other types exist in namespaces but can also be nested inside other classes.
Whether it is a console or a graphical interface application, the program must have an entry point of some sort. The entry point of the C# application is the Main method. There can only be one, and it is a static method in a class. The method usually returns void and is passed command-line arguments as an array of strings.
A Main method is also allowed to return an integer value if specified.
Namespaces are a part of a type name and they are used to group and/or distinguish named entities from other ones.
A namespace is defined like this:
The using statement loads a specific namespace from a referenced assembly. It is usually placed in the top (or header) of a code file but it can be placed elsewhere if wanted, e.g. inside classes.
The statement can also be used to define another name for an existing namespace or type. This is sometimes useful when names are too long and less readable.
Some of the existing operators can be overloaded by writing an overload method.
These are the overloadable operators:
See also
The cast operator is not overloadable but you can write a conversion operator method which lives in the target class. Conversion methods can define two varieties of operators, implicit and explicit conversion operators. The implicit operator will cast without specifying with the cast operator (( )) and the explicit operator requires it to be used.
Implicit conversion operator
Explicit conversion operator
The as operator will attempt to do a silent cast to a given type. If it succeeds it will return the object as the new type, if it fails it will return a null reference.
The following:
is shorthand for:
Meaning that if the content of variable ifNotNullValue is not null, that content will be returned, otherwise the content of variable otherwiseValue is returned.
C# inherits most of the control structures of C/C++ and also adds new ones like the foreach statement.
These structures control the flow of the program through given conditions.
The if statement is entered when the given condition is true. Single-line case statements do not require block braces although it is mostly preferred by convention.
Simple one-line statement:
Multi-line with else-block (without any braces):
Recommended coding conventions for an if-statement.
The switch construct serves as a filter for different values. Each value leads to a ""case"". It is not allowed to fall through case sections and therefore the keyword break is typically used to end a case. An unconditional return in a case section can also be used to end a case. See also how goto statement can be used to fall through from one case to the next. Many cases may lead to the same code though. The default case handles all the other cases not handled by the construct.
Iteration statements are statements that are repeatedly executed when a given condition is evaluated as true.
The for loop consists of three parts: declaration, condition and increment. Any of them can be left out as they are optional.
Is equivalent to this code represented with a while statement, except here the i variable is not local to the loop.
The foreach statement is derived from the for statement and makes use of a certain pattern described in C#'s language specification in order to obtain and use an enumerator of elements to iterate over.
Each item in the given collection will be returned and reachable in the context of the code block. When the block has been executed the next item will be returned until there are no items remaining.
Jump statements are inherited from C/C++ and ultimately assembly languages through it. They simply represent the jump-instructions of an assembly language that controls the flow of a program.
Labels are given points in code that can be jumped to by using the goto statement.
The goto statement can be used in switch statements to jump from one case to another or to fall through from one case to the next.
The break statement breaks out of the closest loop or switch statement. Execution continues in the statement after the terminated statement, if any.
The continue statement discontinues the current iteration of the current control statement and begins the next iteration.
The while loop in the code above reads characters by calling GetChar(), skipping the statements in the body of the loop if the characters are spaces.
Runtime exception handling method in C# is inherited from Java and C++.
The base class library has a class called System.Exception from which all other exception classes are derived. An Exception-object contains all the information about a specific exception and also the inner exceptions that were caused.
Programmers may define their own exceptions by deriving from the Exception class.
An exception can be thrown this way:
Exceptions are managed within try ... catch blocks.
The statements within the try block are executed, and if any of them throws an exception, execution of the block is discontinued and the exception is handled by the catch block. There may be multiple catch blocks, in which case the first block with an exception variable whose type matches the type of the thrown exception is executed.
If no catch block matches the type of the thrown exception, the execution of the outer block (or method) containing the try ... catch statement is discontinued, and the exception is passed up and outside the containing block or method. The exception is propagated upwards through the call stack until a matching catch block is found within one of the currently active methods. If the exception propagates all the way up to the top-most Main() method without a matching catch block being found, the entire program is terminated and a textual description of the exception is written to the standard output stream.
The statements within the finally block are always executed after the try and catch blocks, whether or not an exception was thrown. Such blocks are useful for providing clean-up code.
Either a catch block, a finally block, or both, must follow the try block.
C# is a statically typed language like C and C++. That means that every variable and constant gets a fixed type when it is being declared. There are two kinds of types: value types and reference types.
Instances of value types reside on the stack, i.e. they are bound to their variables. If you declare a variable for a value type the memory gets allocated directly. If the variable gets out of scope the object is destroyed with it.
Structures are more commonly known as structs. Structs are user-defined value types that are declared using the struct keyword. They are very similar to classes but are more suitable for lightweight types. Some important syntactical differences between a class and a struct are presented later in this article.
The primitive data types are all structs.
These are the primitive datatypes.
Note: string (System.String) is not a struct and is not a primitive type.
Enumerated types (enums) are named values representing integer values.
enum variables are initialized by default to zero. They can be assigned or initialized to the named values defined by the enumeration type.
enum type variables are integer values. Addition and subtraction between variables of the same type is allowed without any specific cast but multiplication and division is somewhat more risky and requires an explicit cast. Casts are also required for converting enum variables to and from integer types. However, the cast will not throw an exception if the value is not specified by the enum type definition.
Values can be combined using the bitwise-OR operator 
.
See also
Variables created for reference types are typed managed references. When the constructor is called, an object is created on the heap and a reference is assigned to the variable. When a variable of an object goes out of scope the reference is broken and when there are no references left the object gets marked as garbage. The garbage collector will then soon collect and destroy it.
A reference variable is null when it does not reference any object.
An array type is a reference type that refers to a space containing one or more elements of a certain type. All array types derive from a common base class, System.Array. Each element is referenced by its index just like in C++ and Java.
An array in C# is what would be called a dynamic array in C++.
Array initializers provide convenient syntax for initialization of arrays.
Arrays can have more than one dimension, for example 2 dimensions to represent a grid.
See also
Classes are self-describing user-defined reference types. Essentially all types in the .NET Framework are classes, including structs and enums, that are compiler generated classes. Class members are private by default, but can be declared as public to be visible outside of the class or protected to be visible by any descendants of the class.
The System.String class, or simply string, represents an immutable sequence of unicode characters (char).
Actions performed on a string will always return a new string.
The System.StringBuilder class can be used when a mutable ""string"" is wanted.
Interfaces are data structures that contain member definitions with no actual implementation. A variable of an interface type is a reference to an instance of a class which implements this interface. See #Interfaces.
C# provides type-safe object-oriented function pointers in the form of delegates.
Initializing the delegate with an anonymous method. 
Initializing the delegate with lambda expression. 
Events are pointers that can point to multiple methods. More exactly they bind method pointers to one identifier. This can therefore be seen as an extension to delegates. They are typically used as triggers in UI development. The form used in C# and the rest of the Common Language Infrastructure is based on that in the classic Visual Basic.
An event requires an accompanied event handler that is made from a special delegate that in a platform specific library like in Windows Presentation Foundation and Windows Forms usually takes two parameters: sender and the event arguments. The type of the event argument-object derive from the EventArgs class that is a part of the CLI base library.
Once declared in its class the only way of invoking the event is from inside of the owner. A listener method may be implemented outside to be triggered when the event is fired.
Custom event implementation is also possible:
See also
Nullable types were introduced in C# 2.0 firstly to enable value types to be null (useful when working with a database).
In reality this is the same as using the Nullable<T> struct.
C# has and allows pointers to selected types (some primitives, enums, strings, pointers, and even arrays and structs if they contain only types that can be pointed) in unsafe context: methods and codeblock marked unsafe. These are syntactically the same as pointers in C and C++. However, runtime-checking is disabled inside unsafe blocks.
Structs are required only to be pure structs with no members of a managed reference type, e.g. a string or any other class.
In use:
See also
Type dynamic is a feature that enables dynamic runtime lookup to C# in a static manner. Dynamic denotes a variable with an object with a type that is resolved at runtime, as opposed to compile-time, as normally is done.
This feature takes advantage of the Dynamic Language Runtime (DLR) and has been designed specifically with the goal of interoping with dynamically typed languages like IronPython and IronRuby (Implementations of Python and Ruby for .NET).
Dynamic-support also eases interop with COM objects.
Anonymous types are nameless classes that are generated by the compiler. They are only consumable and yet very useful in a scenario like where you have a LINQ query which returns an object on select and you just want to return some specific values. Then you can define an anonymous type containing auto-generated read-only fields for the values.
When instantiating another anonymous type declaration with the same signature the type is automatically inferred by the compiler.
Boxing is the operation of converting a value of a value type into a value of a corresponding reference type.  Boxing in C# is implicit.
Unboxing is the operation of converting a value of a reference type (previously boxed) into a value of a value type. Unboxing in C# requires an explicit type cast.
Example:
C# has direct support for object-oriented programming.
An object is created with the type as a template and is called an instance of that particular type.
In C#, objects are either references or values. No further syntactical distinction is made between those in code.
All types, even value types in their boxed form, implicitly inherit from the System.Object class, the ultimate base class of all objects. This class contains the most common methods shared by all objects. Some of these are virtual and can be overridden.
Classes inherit System.Object either directly or indirectly through another base class.
Members
Some of the members of the Object class:
Classes are fundamentals of an object-oriented language such as C#. They serve as a template for objects. They contain members that store and manipulate data in a real-lifelike way.
See also
Although classes and structures are similar in both the way they are declared and how they are used, there are some significant differences. Classes are reference types and structs are value types. A structure is allocated on the stack when it is declared and the variable is bound to its address. It directly contains the value. Classes are different because the memory is allocated as objects on the heap. Variables are rather managed pointers on the stack which point to the objects. They are references.
Structures require some more work than classes. For example, you need to explicitly create a default constructor which takes no arguments to initialize the struct and its members. The compiler will create a default one for classes. All fields and properties of a struct must have been initialized before an instance is created. Structs do not have finalizers and cannot inherit from another class like classes do. However, they inherit from System.ValueType, that inherits from System.Object. Structs are more suitable for smaller constructs of data.
This is a short summary of the differences:
2Always auto generated, and cannot be written by the programmer
A class is declared like this:
A partial class is a class declaration whose code is divided into separate files. The different parts of a partial class must be marked with keyword partial.
Before you can use the members of the class you need to initialize the variable with a reference to an object. To create it you call the appropriate constructor using the new keyword. It has the same name as the class.
For structs it is optional to explicitly call a constructor because the default one is called automatically. You just need to declare it and it gets initialized with standard values.
Provides a more convenient way of initializing public fields and properties of an object. Constructor calls are optional when there is a default constructor.
Collection initializers give an array-like syntax for initializing collections. The compiler will simply generate calls to the Add-method. This works for classes that implement the interface ICollection.
Members of an instance and static members of a class are accessed using the . operator.
Accessing an instance member
Instance members can be accessed through the name of a variable.
Accessing a static class member
Static members are accessed by using the name of the class or other type.
Accessing a member through a pointer
In unsafe code, members of a value (struct type) referenced by a pointer are accessed with the -> operator just like in C and C++.
Modifiers are keywords used to modify declarations of types and type members. Most notably there is a sub-group containing the access modifiers.
The static modifier states that a member belongs to the class and not to a specific object. Classes marked static are only allowed to contain static members. Static members are sometimes referred to as class members since they apply to the class as a whole and not to its instances.
The access modifiers, or inheritance modifiers, set the accessibility of classes, methods, and other members. Something marked public can be reached from anywhere. private members can only be accessed from inside of the class they are declared in and will be hidden when inherited. Members with the protected modifier will be private, but accessible when inherited. internal classes and members will only be accessible from the inside of the declaring assembly.
Classes and structs are implicitly internal and members are implicitly private if they do not have an access modifier.
This table defines where the access modifiers can be used.
A constructor is a special method that is called automatically when an object is created. Its purpose is to initialize the members of the object. Constructors have the same name as the class and do not return anything. They may take parameters like any other method.
Constructors can be public, private, protected or internal.
See also
The destructor is called when the object is being collected by the garbage collector to perform some manual clean-up. There is a default destructor method called finalize that can be overridden by declaring your own.
The syntax is similar to the one of constructors. The difference is that the name is preceded by a ~ and it cannot contain any parameters. There cannot be more than one destructor..
Finalizers are always private.
See also
Like in C and C++ there are functions that group reusable code. The main difference is that functions, just like in Java, have to reside inside of a class. A function is therefore called a method. A method has a return value, a name and usually some parameters initialized when it is called with some arguments. It can either belong to an instance of a class or be a static member.
A method is called using . notation on a specific variable, or as in the case of static methods, the name of a type.
See also
One can explicitly make arguments be passed by reference when calling a method with parameters preceded by keywords ref or out. These managed pointers come in handy when passing variables that you want to be modified inside the method by reference. The main difference between the two is that an out parameter must have been assigned within the method by the time the method returns, while ref need not assign a value.
C# 4.0 introduces optional parameters with default values as seen in C++. For example:
In addition, to complement optional parameters, it is possible to explicitly specify parameter names in method calls, allowing to selectively pass any given subset of optional parameters for a method. The only restriction is that named parameters must be placed after the unnamed parameters. Parameter names can be specified for both optional and required parameters, and can be used to improve readability or arbitrarily reorder arguments in a call. For example:
Optional parameters make interoperating with COM easier. Previously, C# had to pass in every parameter in the method of the COM component, even those that are optional. For example:
With support for optional parameters, the code can be shortened as
A feature of C# is the ability to call native code. A method signature is simply declared without a body and is marked as extern. The DllImport attribute also needs to be added to reference the desired DLL file.
Fields, or class variables, can be declared inside the class body to store data.
Fields can be initialized directly when declared (unless declared in struct).
Modifiers for fields:
Properties bring field-like syntax and combine them with the power of methods. A property can have two accessors: get and set.
Modifiers for properties:
Modifiers for property accessors:
The default modifiers for the accessors are inherited from the property. Note that the accessor's modifiers can only be equal or more restrictive than the property's modifier.
A feature of C# 3.0 is auto-implemented properties. You define accessors without bodies and the compiler will generate a backing field and the necessary code for the accessors.
Indexers add array-like indexing capabilities to objects. They are implemented in a way similar to properties.
Classes in C# may only inherit from one class. A class may derive from any class that is not marked as sealed.
See also
Methods marked virtual provide an implementation, but they can be overridden by the inheritors by using the override keyword.
The implementation is chosen by the actual type of the object and not the type of the variable.
When overloading a non-virtual method with another signature, the keyword new may be used. The used method will be chosen by the type of the variable instead of the actual type of the object.
This demonstrates the case:
Abstract classes are classes that only serve as templates and you can not initialize an object of that type. Otherwise it is just like an ordinary class.
There may be abstract members too. Abstract members are members of abstract classes that do not have any implementation. They must be overridden by the class that inherits the member.
The sealed modifier can be combined with the others as an optional modifier for classes to make them uninheritable.
Interfaces are data structures that contain member definitions and not actual implementation. They are useful when you want to define a contract between members in different types that have different implementations. You can declare definitions for methods, properties, and indexers. Interface members are implicitly public. An interface can either be implicitly or explicitly implemented.
An interface is implemented by a class or extended by another interface in the same way you derive a class from another class using the : notation.
Implicit implementation
When implicitly implementing an interface the members of the interface have to be public.
In use:
Explicit implementation
You can also explicitly implement members. The members of the interface that are explicitly implemented by a class are accessible only when the object is handled as the interface type.
In use:
Note: The properties in the class that extends IBinaryOperation are auto-implemented by the compiler and a backing field is automatically added (see #Automatic properties).
Extending multiple interfaces
Interfaces and classes are allowed to extend multiple interfaces.
Here is an interface that extends two interfaces.
Interfaces and abstract classes are similar. The following describes some important differences:
Generics (or parameterized types, parametric polymorphism) use type parameters, which make it possible to design classes and methods that do not specify the type used until the class or method is instantiated. The main advantage is that one can use generic type parameters to create classes and methods that can be used without incurring the cost of runtime casts or boxing operations, as shown here:
When compared with C++ templates, C# generics can provide enhanced safety, but also have somewhat limited capabilities. For example, it is not possible to call arithmetic operators on a C# generic type. Unlike C++ templates, .NET parameterized types are instantiated at runtime rather than by the compiler; hence they can be cross-language whereas C++ templates cannot. They support some features not supported directly by C++ templates such as type constraints on generic parameters by use of interfaces. On the other hand, C# does not support non-type generic parameters.
Unlike generics in Java, .NET generics use reification to make parameterized types first-class objects in the Common Language Infrastructure (CLI) Virtual Machine, which allows for optimizations and preservation of the type information.
Classes and structs can be generic.
Type-parameters are names used in place of concrete types when defining a new generic. They may be associated with classes or methods by placing the type parameter in angle brackets < >. When instantiating (or calling) a generic, you can then substitute a concrete type for the type-parameter you gave in its declaration. Type parameters may be constrained by use of the where keyword and a constraint specification, any of the six comma separated constraints may be used:
Generic interfaces and delegates can have their type parameters marked as covariant or contravariant, using keywords out and in, respectively. These declarations are then respected for type conversions, both implicit and explicit, and both compile-time and run-time. For example, the existing interface IEnumerable<T> has been redefined as follows:
Therefore, any class that implements IEnumerable<Derived> for some class Derived is also considered to be compatible with IEnumerable<Base> for all classes and interfaces Base that Derived extends, directly, or indirectly. In practice, it makes it possible to write code such as:
For contravariance, the existing interface IComparer<T> has been redefined as follows:
Therefore, any class that implements IComparer<Base> for some class Base is also considered to be compatible with IComparer<Derived> for all classes and interfaces Derived that are extended from Base. It makes it possible to write code such as:
An enumerator is an iterator.
Enumerators are typically obtained by calling the GetEnumerator() method of an object implementing the IEnumerable interface. Container classes typically implement this interface. However, the foreach statement in C# can operate on any object providing such a method, even if it doesn't implement IEnumerable. This interface was expanded into generic version in .NET 2.0.
The following shows a simple use of iterators in C# 2.0:
The .NET 2.0 Framework allowed C# to introduce an iterator that provides generator functionality, using a yield return construct similar to yield in Python. With a yield return, the function automatically keeps its state during the iteration.
LINQ, short for Language Integrated Queries, is a .NET Framework feature which simplifies the handling of data. Mainly it adds support that allows you to query arrays, collections, and databases. It also introduces binders, which makes it easier to access to databases and their data.
The LINQ query syntax was introduced in C# 3.0 and lets you write SQL-like queries in C#.
The statements are compiled into method calls, whereby almost only the names of the methods are specified. Which methods are ultimately used is determined by normal overload resolution. Thus, the end result of the translation is affected by what symbols are in scope.
What differs from SQL is that the from-statement comes first and not last as in SQL. This is because it seems more natural writing like this in C# and supports ""Intellisense"" (Code completion in the editor).
Anonymous methods, or in their present form more commonly referred to as ""lambda expressions"", is a feature which allows you to write inline closure-like functions in your code.
There are various ways to create anonymous methods. Prior to C# 3.0 there was limited support by using delegates.
See also
Anonymous delegates are functions pointers that hold anonymous methods. The purpose is to make it simpler to use delegates by simplifying the process of assigning the function. Instead of declaring a separate method in code the programmer can use the syntax to write the code inline and the compiler will then generate an anonymous function for it.
Lambda expressions provide a simple syntax for inline functions that are similar to closures. Functions with parameters infer the type of the parameters if other is not explicitly specified.
Multi-statement lambdas have bodies enclosed by braces and inside of them code can be written like in standard methods.
Lambda expressions can be passed as arguments directly in method calls similar to anonymous delegates but with a more aesthetic syntax.
Lambda expressions are essentially compiler-generated methods that are passed via delegates. These methods are reserved for the compiler only and can not be used in any other context.
Extension methods are a form of syntactic sugar providing the illusion of adding new methods to the existing class outside its definition. In practice, an extension method is a static method that is callable as if it were an instance method; the receiver of the call is bound to the first parameter of the method, decorated with keyword this:
See also
Local functions can be defined in the body of another method, constructor or property’s getter and setter. Such functions have access to all variables in the enclosing scope, including parent method local variables. They are in scope for the entire method, regardless of whether they’re invoked before or after their declaration. Access modifiers (public, private, protected) cannot be used with local functions. Also they do not support function overloading. It means there cannot be two local functions in the same method with the same name even if the signatures don’t overlap. After a compilation, a local function is transformed into a private static method, but when defined it cannot be marked static.
In code example below, the Sum method is a local function inside Main method. So it can be used only inside its parent method Main:
C# implements closure blocks by means of the using statement. The using statement accepts an expression which results in an object implementing IDisposable, and the compiler generates code that guarantees the object's disposal when the scope of the using-statement is exited. The using statement is syntactic sugar. It makes the code more readable than the equivalent try ... finally block.
C# provides the lock statement, which is yet another example of beneficial syntactic sugar. It works by marking a block of code as a critical section by mutual exclusion of access to a provided object. Like the using statement, it works by the compiler generating a try ... finally block in its place.
Attributes are entities of data that are stored as metadata in the compiled assembly. An attribute can be added to types and members like properties and methods. Attributes can be used for better maintenance of preprocessor directives.
The .NET Framework comes with predefined attributes that can be used. Some of them serve an important role at runtime while some are just for syntactic decoration in code like CompilerGenerated. It does only mark that it is a compiler-generated element. Programmer-defined attributes can also be created.
An attribute is essentially a class which inherits from the System.Attribute class. By convention, attribute classes end with ""Attribute"" in their name. This will not be required when using it.
Showing the attribute in use using the optional constructor parameters.
C# features ""preprocessor directives"" (though it does not have an actual preprocessor) based on the C preprocessor that allow programmers to define symbols, but not macros. Conditionals such as #if, #endif, and #else are also provided.
Directives such as #region give hints to editors for code folding. The #region block must be terminated with a #endregion directive.
C# utilizes a double slash (//) to indicate the rest of the line is a comment.
Multi-line comments can be indicated by a starting slash/asterisk (/*) and ending asterisk/forward slash (*/).
Comments do not nest. These are two single comments:
Single-line comments beginning with three slashes are used for XML documentation. This, however, is a convention used by Visual Studio and is not part of the language definition:
C#'s documentation system is similar to Java's Javadoc, but based on XML. Two methods of documentation are currently supported by the C# compiler.
Single-line documentation comments, such as those commonly found in Visual Studio generated code, are indicated on a line beginning with // /.
Multi-line documentation comments, while defined in the version 1.0 language specification, were not supported until the .NET 1.1 release. These comments are designated by a starting forward slash/asterisk/asterisk (/**) and ending asterisk/forward slash (*/).
There are some stringent criteria regarding white space and XML documentation when using the forward slash/asterisk/asterisk (/**) technique.
This code block:
produces a different XML comment than this code block:
Syntax for documentation comments and their XML markup is defined in a non-normative annex of the ECMA C# standard. The same standard also defines rules for processing of such comments, and their transformation to a plain XML document with precise rules for mapping of Common Language Infrastructure (CLI) identifiers to their related documentation elements. This allows any C# integrated development environment (IDE) or other development tool to find documentation for any symbol in the code in a certain well-defined way.
As of .NET Framework 4 there is a task library that makes it easier to write parallel and multi-threaded applications through tasks.
C# 5.0 has native language support for asynchrony.
Consider this code that takes advantage of the task library directly:
Here is the same logic written in the async-await syntax:
Spec# is a dialect of C# that is developed in parallel with the standard implementation from Microsoft. It extends C# with specification language features and is a possible future feature to the C# language. It also adds syntax for the code contracts API that was introduced in .NET Framework 4.0. Spec# is being developed by Microsoft Research.
This sample shows two of the basic structures that are used when adding contracts to your code.
Spec# extends C# with non-nullable types that simply checks so the variables of nullable types that has been set as non-nullable are not null. If is null then an exception will be thrown.
In use:
Preconditions are checked before a method is executed.
Postconditions are conditions that are ensured to be correct when a method has been executed.
Spec# adds checked exceptions like those in Java.
Checked exceptions are problematic, because when a lower-level function adds a new exception type, the whole chain of methods using this method at some nested lower level must also change its contract. This violates the open/closed principle."
"15","In software engineering, the module pattern is a design pattern used to implement the concept of software modules, defined by modular programming, in a programming language with incomplete direct support for the concept.
This pattern can be implemented in several ways depending on the host programming language, such as the singleton design pattern, object-oriented static members in a class and procedural global functions. In Python, the pattern is built into the language, and each .py file is automatically a module.
The module software design pattern provides the features and syntactic structure defined by the modular programming paradigm to programming languages that have incomplete support for the concept.
In software development, source code can be organized into components that accomplish a particular function or contain everything necessary to accomplish a particular task. Modular programming is one of those approaches.
The concept of a ""module"" is not fully supported in many common programming languages.
In order to consider that a Singleton or any group of related code implements this pattern, the following features must be supplied:
The semantics and syntax of each programming language may affect the implementation of this pattern.
Although Java supports the notion of a namespace, a reduced version of a module, some scenarios benefit from employing the design pattern instead of using namespaces.
The following example uses the singleton pattern.
C#, like Java, supports namespaces although the pattern remains useful in specific cases.
The following example uses the singleton pattern.
JavaScript is commonly used to automate web pages.
This pattern may be seen as a procedural extension to object-oriented languages.
Although the procedural and modular programming paradigms are often used together, there are cases where a procedural programming language may not fully support modules, hence requiring a design pattern implementation.
This example applies to procedural PHP before namespaces (introduced in version 5.3.0). It is recommended that each member of a module is given a prefix related to the filename or module name in order to avoid identifier collisions.
Note that this example applies to procedural C without namespaces. It is recommended that each member of a module is given a prefix related to the filename or module name in order to avoid identifier collisions.
Note that this example applies to procedural non-modular Pascal. Many Pascal dialects have namespace support, called ""unit (s)"". Some dialects also support initialization and finalization.
If namespaces are not supported, it is recommended to give all member names a prefix related to the filename or module name in order to prevent identifier collisions.
Both namespaces and modules allow to group several related entities by a single identifier, and in some situations, used interchangeably. Those entities can be globally accessed. The main purpose of both concepts is the same.
In some scenarios a namespace requires that the global elements that compose it are initialized and finalized by a function or method call.
In many programming languages, namespaces are not directly intended to support an initialization process nor a finalization process, and are therefore not equivalent to modules. That limitation can be worked around in two ways. In namespaces that support global functions, a function for initialization and a function for finalization are coded directly, and called directly in the main program code.
Classes are used sometimes used as or with namespaces. In programming languages that don't support namespaces (e.g., JavaScript) but do support classes and objects, classes are often used to substitute for namespaces. These classes are usually not instantiated and consist exclusively of static members.
In object-oriented programming languages where namespaces are incompletely supported, the singleton pattern may be used instead of static members within a non-instantiable class.
The module pattern can be implemented using a specialization of the singleton pattern. However, other design patterns may be applied and combined, in the same class.
This pattern can be used as a decorator, a flyweight, or an adapter.
The Module pattern can be considered a creational pattern and a structural pattern. It manages the creation and organization of other elements, and groups them as the structural pattern does.
An object that applies this pattern can provide the equivalent of a namespace, providing the initialization and finalization process of a static class or a class with static members with cleaner, more concise syntax and semantics.
It supports specific cases where a class or object can be considered structured, procedural data. And, vice versa, migrate structured, procedural data, and considered as object-oriented."
"16","
Xcode is an integrated development environment (IDE) for macOS containing a suite of software development tools developed by Apple for developing software for macOS, iOS, watchOS, and tvOS. First released in 2003, the latest stable release is version 9.2 and is available via the Mac App Store free of charge for macOS High Sierra and macOS Sierra users.Registered developers can download preview releases and prior versions of the suite through the Apple Developer website.
Xcode supports source code for the programming languages C, C++, Objective-C, Objective-C++, Java, AppleScript, Python, Ruby, ResEdit (Rez), and Swift, with a variety of programming models, including but not limited to Cocoa, Carbon, and Java. Third parties have added support for GNU Pascal,Free Pascal,Ada,C#,Perl, and D.
Thanks to the Mach-O executable format, which allows fat binary files, containing code for multiple architectures, Xcode can build universal binary files, which allow software to run on both PowerPC and Intel-based (x86) platforms and that can include both 32-bit and 64-bit code for both architectures. Using the iOS SDK, Xcode can also be used to compile and debug applications for iOS that run on ARM architecture processors.
Xcode includes the GUI tool Instruments, which runs atop a dynamic tracing framework, DTrace, created by Sun Microsystems and released as part of OpenSolaris.
The main application of the suite is the integrated development environment (IDE), also named Xcode. The Xcode suite includes most of Apple's developer documentation, and built-in Interface Builder, an application used to construct graphical user interfaces.
Up to Xcode 4.1, the Xcode suite included a modified version of the GNU Compiler Collection.  In Xcode 3.1 up to Xcode 4.6.3, it included the LLVM-GCC compiler, with front ends from the GNU Compiler Collection and a code generator based on LLVM. In Xcode 3.2 and later, it included the Clang C/C++/Objective-C compiler, with newly-written front ends and a code generator based on LLVM, and the Clang static analyzer.  Starting with Xcode 4.2, the Clang compiler became the default compiler, Starting with Xcode 5.0, Clang was the only compiler provided.
Up to Xcode 4.6.3, the Xcode suite used the GNU Debugger (GDB) as the back-end for the IDE's debugger. Starting with Xcode 4.3, the LLDB debugger was also provided; starting with Xcode 4.5 LLDB replaced GDB as the default back-end for the IDE's debugger. Starting with Xcode 5.0, GDB was no longer supplied.
Formerly, Xcode supported distributing a product build process over multiple systems. One technology involved was named Shared Workgroup Build, which used the Bonjour protocol to automatically discover systems providing compiler services, and a modified version of the free software product distcc to facilitate the distribution of workloads. Earlier versions of Xcode provided a system named Dedicated Network Builds. These features are absent in the supported versions of Xcode.
Xcode also includes Apple's WebObjects tools and frameworks for building Java web applications and web services (formerly sold as a separate product). As of Xcode 3.0, Apple dropped WebObjects development inside Xcode; WOLips should be used instead. Xcode 3 still includes the WebObjects frameworks.
Xcode 1.0 was released in fall 2003. Xcode 1.0 was based on Project Builder, but had an updated user interface (UI), ZeroLink, Fix & Continue, distributed build support, and Code Sense indexing.
The next significant release, Xcode 1.5, had better code completion and an improved debugger.
Xcode 2.0 was released with Mac OS X v10.4 ""Tiger"". It included the Quartz Composer visual programming language, better Code Sense indexing for Java, and Ant support. It also included the Apple Reference Library tool, which allows searching and reading online documentation from Apple’s website and documentation installed on a local computer.
Xcode 2.1 could create universal binary files. It supported shared precompiled headers, unit testing targets, conditional breakpoints, and watchpoints. It also had better dependency analysis.
The final version of Xcode for Mac OS X v10.4 was 2.5.
Xcode 3.0 was released with Mac OS X v10.5 ""Leopard"". Notable changes since 2.1 include the DTrace debugging tool (now named Instruments), refactoring support, context-sensitive documentation, and Objective-C 2.0 with garbage collection.  It also supports Project Snapshots, which provide a basic form of version control; Message Bubbles, which show build errors debug values alongside code; and building four-architecture fat binaries (32 and 64-bit Intel and PowerPC).
Xcode 3.1 was an update release of the developer tools for Mac OS X, and was the same version included with the iPhone SDK. It could target non-Mac OS X platforms, including iPhone OS 2.0. It included the GCC 4.2 and LLVM GCC 4.2 compilers. Another new feature since Xcode 3.0 is that Xcode's SCM support now includes Subversion 1.5.
Xcode 3.2 was released with Mac OS X v10.6 ""Snow Leopard"" and installs on no earlier version of OS X.  It supports static program analysis, among other features. It also drops official support for targeting versions earlier than iPhone OS 3.0. But it is still possible to target older versions, and the simulator supports iPhone OS 2.0 through 3.1. Also, Java support is ""exiled"" in 3.2 to the organizer.
Xcode 3.2.6 is the last version that can be downloaded for free for users of Mac OS X v10.6. Downloading it requires a free registration at Apple's developer site.
In June 2010, at the Apple Worldwide Developers Conference version 4 of Xcode was announced during the Developer Tools State of the Union address. Version 4 of the developer tools consolidates the Xcode editing tools and Interface Builder into one application, among other enhancements. Apple released the final version of Xcode 4.0 on March 9, 2011. The software was made available for free to all registered members of the $99 per year Mac Developer program and the $99 per year iOS Developer program. It was also sold for $4.99 to non-members on the Mac App Store (no longer available).  Xcode 4.0 drops support for many older systems, including all PowerPC development and software development kits (SDKs) for Mac OS X 10.4 and 10.5, and all iOS SDKs older than 4.3.  The deployment target can still be set to produce binaries for those older platforms, but for Mac OS platforms, one is then limited to creating x86 and x86-64 binaries. Later, Xcode was free to the general public. Before version 4.1, Xcode cost $4.99.
Xcode 4.1 was made available for free on July 20, 2011 (the day of Mac OS X Lion's release) to all users of Mac OS X Lion on the Mac App Store. On August 29, 2011, Xcode 4.1 was made available for Mac OS X Snow Leopard for members of the paid Mac or iOS developer programs. Xcode 4.1 was the last version to include GNU Compiler Collection (GCC) instead of only LLVM GCC or Clang.
On October 12, 2011, Xcode 4.2 was released concurrently with the release of iOS 5.0, and it included many more and improved features, such as storyboarding and automatic reference counting (ARC). Xcode 4.2 is the last version to support Mac OS X 10.6 ""Snow Leopard"", but is only available to registered developers with paid accounts; without a paid account, 3.2.6 is the latest download that appears for Snow Leopard.
Xcode 4.3, released on February 16, 2012, is distributed as one application bundle, Xcode.app, installed from the Mac App Store. Xcode 4.3 reorganizes the Xcode menu to include development tools. Xcode 4.3.1 was released on March 7, 2012 to add support for iOS 5.1. Xcode 4.3.2 was released on March 22, 2012 with enhancements to the iOS Simulator and a suggested move to the LLDB debugger versus the GDB debugger (which appear to be undocumented changes).[citation needed] Xcode 4.3.3, released in May 2012, featured an updated SDK for Mac OS X 10.7.4 ""Lion"" and a few bug fixes.
Xcode 4.4 was released on July 25, 2012.
It runs on both Mac OS X Lion (10.7) and OS X Mountain Lion (10.8) and is the first version of Xcode to contain the OS X 10.8 ""Mountain Lion"" SDK. Xcode 4.4 includes support for automatic synthesizing of declared properties, new Objective-C features such as literal syntax and subscripting, improved localization, and more. On August 7, 2012, Xcode 4.4.1 was released with a few bug fixes.
On September 19, 2012, iOS 6 and Xcode 4.5 were released. Xcode added support for iOS 6 and the 4-inch Retina Display on iPhone 5 and iPod touch 5th generation. It also brought some new Objective-C features to iOS, simplified localization, and added auto-layout support for iOS. On October 3, 2012, Xcode 4.5.1 was released with bug fixes and stability improvements. Less than a month later, Xcode 4.5.2 was released, with support for iPad Mini and iPad with Retina Display, and bug fixes and stability improvements.
On January 28, 2013, iOS 6.1 and Xcode 4.6 were released.
In June 2013, at the Apple Worldwide Developers Conference, version 5 of Xcode was announced.
On September 18, 2013 Xcode 5.0 was released. It added support for iOS 7 SDK, with always support of OS X 10.8 Mountain Lion SDK but not the support of OS X 10.9 Mavericks SDK. This latest was only included in the betas version. It also added a version of Clang generating 64-bit ARM code for iOS 7. Apple removed support for building garbage collected Cocoa binaries in Xcode 5.1.
On June 2, 2014 at the World Wide Developers Conference, Apple announced version 6 of Xcode. Features include Playgrounds, live debugging tools, and a new programming language named Swift. On September 17, 2014, at the same time, iOS and Xcode 6 were released. Xcode could be downloaded on the Mac App Store.
On June 8, 2015 at the Apple Worldwide Developers Conference Xcode version 7 was announced. It introduced support for Swift 2, and Metal for OS X, and added support for deploying on iOS devices without an Apple Developer license. Xcode 7 was released on September 16, 2015.
On June 13, 2016 at the Apple Worldwide Developers Conference Xcode version 8 was announced. It introduced support for Swift 3. Xcode 8 was released on September 13, 2016.
On June 5, 2017 at the Apple Worldwide Developers Conference Xcode version 9 was announced. It introduced support for Swift 4 and Metal 2. It also introduced remotely debugging iOS and tvOS devices wirelessly through WiFi.
Xcode 9 was publicly released on September 19, 2017.
1.0
1.1
1.2
1.5
2.0
2.1
2.2
2.2.1
2.3
2.4
2.4.1
2.5
3.0
3.1
3.1.1
3.1.2
3.1.3
3.1.4
3.2
3.2.1
3.2.2
3.2.3
3.2.4
3.2.5
3.2.6
4.0
4.0.1
4.0.2
4.1
4.1.1
4.2
4.2.1
4.3
4.3.1
4.3.2
4.3.3
4.4
4.4.1
4.5
4.5.1
4.5.2
4.6
4.6.1
4.6.2
4.6.3
5.0
5.0.1
5.0.2
5.1
5.1.1
6.0.1
6.1
6.1.1
6.2
6.3
6.3.1
6.3.2
6.4
7.0
7.0.1
7.1
7.1.1
7.2
7.2.1
7.3
7.3.1
8.0
8.1
8.2
8.2.1
8.3
8.3.1
8.3.2
8.3.3
9.0
9.0.1
9.1
9.2
"
"17","
The LLVM compiler infrastructure project is a ""collection of modular and reusable compiler and toolchain technologies"" used to develop compiler front ends and back ends.
LLVM is written in C++ and is designed for compile-time, link-time, run-time, and ""idle-time"" optimization of programs written in arbitrary programming languages. Originally implemented for C and C++, the language-agnostic design of LLVM has since spawned a wide variety of front ends: languages with compilers that use LLVM include ActionScript, Ada, C#,Common Lisp, Crystal, CUDA, D, Delphi, Fortran, Halide, Haskell, Java bytecode, Julia, Kotlin, Lua, Objective-C, OpenGL Shading Language, Pony,Python, R, Ruby,Rust, Scala,Swift, and Xojo.
The LLVM project started in 2000 at the University of Illinois at Urbana–Champaign, under the direction of Vikram Adve and Chris Lattner. LLVM was originally developed as a research infrastructure to investigate dynamic compilation techniques for static and dynamic programming languages. LLVM was released under the University of Illinois/NCSA Open Source License, a permissive free software licence. In 2005, Apple Inc. hired Lattner and formed a team to work on the LLVM system for various uses within Apple's development systems. LLVM is an integral part of Apple's latest development tools for macOS and iOS. Since 2013, Sony has been using LLVM's primary front end Clang compiler in the software development kit (SDK) of its PS4 console.
The name LLVM was originally an initialism for Low Level Virtual Machine. This initialism has officially been removed to avoid confusion, as the LLVM has evolved into an umbrella project that has little relationship to what most current developers think of as virtual machines. Now, LLVM is a brand that applies to the LLVM umbrella project, the LLVM intermediate representation (IR), the LLVM debugger, the LLVM implementation of the C++ Standard Library (with full support of C++11 and C++14), etc. LLVM is administered by the LLVM Foundation. Its president is compiler engineer Tanya Lattner.
The Association for Computing Machinery presented Adve, Lattner, and Evan Cheng with the 2012 ACM Software System Award for LLVM.
LLVM can provide the middle layers of a complete compiler system, taking intermediate representation (IR) code from a compiler and emitting an optimized IR. This new IR can then be converted and linked into machine-dependent assembly language code for a target platform. LLVM can accept the IR from the GNU Compiler Collection (GCC) toolchain, allowing it to be used with a wide array of extant compilers written for that project.
LLVM can also generate relocatable machine code at compile-time or link-time or even binary machine code at run-time.
LLVM supports a language-independent instruction set and type system. Each instruction is in static single assignment form (SSA), meaning that each variable (called a typed register) is assigned once and then frozen. This helps simplify the analysis of dependencies among variables. LLVM allows code to be compiled statically, as it is under the traditional GCC system, or left for late-compiling from the IR to machine code via just-in-time compilation (JIT), similar to Java. The type system consists of basic types such as integer or floating point numbers and five derived types: pointers, arrays, vectors, structures, and functions. A type construct in a concrete language can be represented by combining these basic types in LLVM. For example, a class in C++ can be represented by a mix of structures, functions and arrays of function pointers.
The LLVM JIT compiler can optimize unneeded static branches out of a program at runtime, and thus is useful for partial evaluation in cases where a program has many options, most of which can easily be determined unneeded in a specific environment. This feature is used in the OpenGL pipeline of Mac OS X Leopard (v10.5) to provide support for missing hardware features.
Graphics code within the OpenGL stack can be left in intermediate representation, and then compiled when run on the target machine. On systems with high-end graphics processing units (GPUs), the resulting code remains quite thin, passing the instructions on to the GPU with minimal changes. On systems with low-end GPUs, LLVM will compile optional procedures that run on the local central processing unit (CPU) that emulate instructions that the GPU cannot run internally. LLVM improved performance on low-end machines using Intel GMA chipsets. A similar system was developed under the Gallium3D LLVMpipe, and incorporated into the GNOME shell to allow it to run without a proper 3D hardware driver loaded.
For run-time performance of the compiled programs, GCC formerly outperformed LLVM by 10% on average. Newer results indicate that LLVM has now caught up with GCC in this area, and is now compiling binaries of approximately equal performance.
LLVM has become an umbrella project containing multiple components.
LLVM was originally written to be a replacement for the existing code generator in the GCC stack,
and many of the GCC front ends have been modified to work with it. LLVM currently supports compiling of Ada, C, C++, D, Delphi, Fortran, Haskell, Objective-C and Swift using various front ends, some derived from version 4.0.1 and 4.2 of the GNU Compiler Collection (GCC).
Widespread interest in LLVM has led to several efforts to develop new front ends for a variety of languages. The one that has received the most attention is Clang, a new compiler supporting C, C++, and Objective-C. Primarily supported by Apple, Clang is aimed at replacing the C/Objective-C compiler in the GCC system with a system that is more easily integrated with integrated development environments (IDEs) and has wider support for multithreading. Support for OpenMP directives has been included in Clang since release 3.8.
The Utrecht Haskell compiler can generate code for LLVM. Though the generator is in the early stages of development, in many cases it has been more efficient than the C code generator. The Glasgow Haskell Compiler (GHC) has a working LLVM backend that achieves a 30% speed-up of the compiled code relative to native code compiling via GHC or C code generation followed by compiling, missing only one of the many optimizing techniques implemented by the GHC.
Many other components are in various stages of development, including, but not limited to, the Rust compiler, a Java bytecode front end, a Common Intermediate Language (CIL) front end, the MacRuby implementation of Ruby 1.9, various front ends for Standard ML, and a new graph coloring register allocator.[citation needed]
The core of LLVM is the intermediate representation (IR), a low-level programming language similar to assembly. IR is a strongly typed reduced instruction set computing (RISC) instruction set which abstracts away details of the target. For example, the calling convention is abstracted through call and ret instructions with explicit arguments. Also, instead of a fixed set of registers, IR uses an infinite set of temporaries of the form %0, %1, etc. LLVM supports three isomorphic (i.e., functionally equivalent) forms of IR: a human-readable assembly format, a C++ object format suitable for frontends, and a dense bitcode format for serializing. A simple ""Hello, world!"" program in the assembly format:

At version 3.4, LLVM supports many instruction sets, including ARM, Qualcomm Hexagon, MIPS, Nvidia Parallel Thread Execution (PTX; called NVPTX in LLVM documentation), PowerPC,  AMD TeraScale, AMD Graphics Core Next (GCN), SPARC, z/Architecture (called SystemZ in LLVM documentation), x86, x86-64, and XCore. Some features are not available on some platforms. Most features are present for x86, x86-64, z/Architecture, ARM, and PowerPC.
The LLVM machine code (MC) subproject is LLVM's framework for translating machine instructions between textual forms and machine code. Formerly, LLVM relied on the system assembler, or one provided by a toolchain, to translate assembly into machine code. LLVM MC's integrated assembler supports most LLVM targets, including x86, x86-64, ARM, and ARM64. For some targets, including the various MIPS instruction sets, integrated assembly support is usable but still in the beta stage.
The lld subproject is an attempt to develop a built-in, platform-independent linker for LLVM. lld aims to remove dependence on a third-party linker. As of  May 2017[update], lld supports ELF, PE/COFF, and Mach-O in descending order of completeness. In cases where lld is insufficient, another linker such as GNU ld can be used.
Using lld allows link-time optimization. When link-time optimization is enabled, the compiler generates LLVM bitcode instead of native code, and native code generation is done by the linker.
The LLVM project includes an implementation of the C++ Standard Library, dual-licensed under the MIT License and the UIUC license.
The LLVM project started in 2000 at the University of Illinois at Urbana–Champaign, under the direction of Vikram Adve and Chris Lattner. LLVM was originally developed as a research infrastructure to investigate dynamic compilation techniques for static and dynamic programming languages. LLVM was released under the University of Illinois/NCSA Open Source License, a permissive free software licence. In 2005, Apple Inc. hired Lattner and formed a team to work on the LLVM system for various uses within Apple's development systems. LLVM is an integral part of Apple's latest development tools for macOS and iOS. Since 2013, Sony has been using LLVM's primary front end Clang compiler in the software development kit (SDK) of its PS4 console.
"
"18","
Clang /ˈklæŋ/ is a compiler front end for the programming languages C, C++, Objective-C, Objective-C++, OpenMP,OpenCL, and CUDA. It uses LLVM as its back end and has been part of the LLVM release cycle since LLVM 2.6.
It is designed to act as a drop-in replacement for the GNU Compiler Collection, supporting most of its compilation flags and unofficial language extensions. Its contributors include Apple, Microsoft, Google, ARM, Sony, Intel and Advanced Micro Devices (AMD). It is open-source software, with source code released under the University of Illinois/NCSA License, a permissive free software licence.
The Clang project includes the Clang front end and the Clang static analyzer and several code analysis tools.
Starting in 2005, Apple made extensive use of LLVM in a number of commercial systems, including the iPhone software development kit (SDK) and integrated development environment (IDE) Xcode 3.1.
One of the first uses of LLVM was an OpenGL code compiler for OS X that converts OpenGL calls into more fundamental calls for graphics processing units (GPU) that do not support certain features. This allowed Apple to support the entire OpenGL application programming interface (API) on computers using Intel Graphics Media Accelerator (GMA) chipsets, increasing performance on those machines. For GPUs that support it, the code is compiled to exploit fully the underlying hardware, but on GMA machines, LLVM compiles the same OpenGL code into subroutines to ensure continued proper function.
LLVM was intended originally to use GCC's front end, but GCC turned out to cause some problems for developers of LLVM and at Apple. The GCC source code is a large and somewhat cumbersome system for developers to work with; as one long-time GCC developer put it, ""Trying to make the hippo dance is not really a lot of fun"".
Apple software makes heavy use of Objective-C, but the Objective-C front-end in GCC is a low priority for GCC developers. Also, GCC does not integrate smoothly into Apple's IDE.
Finally, GCC is licensed under the GNU General Public License (GPL) version 3, which requires developers who distribute extensions for, or modified versions of, GCC to make their source code available, whereas LLVM has a BSD-like license which does not require users to release their source code changes when publishing compiled binaries of those changes.
Apple chose to develop a new compiler front end from scratch, supporting C, Objective-C and C++.
This ""clang"" project was open-sourced in July 2007.
Clang is intended to work atop LLVM. The combination of Clang and LLVM provides most of the toolchain, to allow replacing the full GCC stack. Because it is built with a library-based design, like the rest of LLVM, Clang is easy to embed into other applications. This is one reason why most OpenCL implementations are built with Clang and LLVM.[citation needed]
One of Clang's main goals is to provide a library-based architecture, to allow the compiler to be more tightly tied to tools that interact with source code, such as an integrated development environment (IDE) graphical user interface (GUI). In contrast, GCC is designed to work in a classic compile-link-debug cycle, and integrating it with other tools is not always easy. For instance, GCC uses a step called fold that is key to the overall compile process, which has the side effect of translating the code tree into a form that looks unlike the original source code. If an error is found during or after the fold step, it can be difficult to translate that back into one location in the original source. Also, vendors using the GCC stack within IDEs use separate tools to index the code, to provide features like syntax highlighting and autocomplete.
Clang is designed to retain more information during the compiling process than GCC, and to preserve the overall form of the original code. The goal of this is to make it easier to map errors back into the original source. The error reports offered by Clang are also aimed to be more detailed and specific, as well as machine-readable, so IDEs can index the output of the compiler during compiling. Modular design of the compiler can offer source code indexing, syntax checking, and other features normally associated with rapid application development systems. The parse tree is also more suitable for supporting automated code refactoring, as it directly represents the original source code.
Clang is a compiler for only C-like languages, including C, C++, Objective-C, Objective-C++, OpenCL, and CUDA. For other languages, like Ada, LLVM remains dependent on GCC or another compiler frontend. In many cases, Clang can be used or swapped out for GCC as needed, with no other effects on the toolchain as a whole.[citation needed] It supports most of the commonly used GCC options. An unofficial sub-project Flang by Nvidia added Fortran support.
Clang is designed to be highly compatible with GCC. Clang's command-line interface is similar to and shares many flags and options with GCC. Clang implements many GNU language extensions and enables them by default. Clang implements many GCC compiler intrinsics purely for compatibility. For example, even though Clang implements atomic intrinsics which correspond exactly with C11 atomics, it also implements GCC's __sync_* intrinsics for compatibility with GCC and libstdc++. Clang also maintains ABI compatibility with GCC-generated object code. In practice Clang can often be used as a drop-in replacement for GCC.[citation needed]
Clang's developers aim to reduce memory footprint and increase compilation speed compared to competing compilers, such as GCC. In October 2007, they report that Clang compiled the Carbon libraries more than twice as fast as GCC, while using about one-sixth GCC's memory and disk space. However, as of 2011 this was not a typical result. As of mid-2014, Clang won more than a third of the benchmarks, with GCC winning most. As of 2014, performance of Clang-compiled programs lagged behind performance of the GCC-compiled program, sometimes by large factors (up to 5.5x),, replicating earlier reports of slower performance.
More recent comparisons indicate that both compilers have evolved to increase their performance. As of GCC 4.8.1 versus clang 3.3, on a large harness of test files, clang had improved significantly, outperforming GCC by ~ 25%. Test results are code-specific, and optimized C source code can reverse such differences. The two compilers now seem broadly comparable.
This table presents only significant steps and releases in Clang history.
"
"19","

The LLDB Debugger (LLDB) is a software debugger. It is built as a set of reusable components which extensively use existing libraries from the larger LLVM Project, such as the Clang expression parser and LLVM disassembler.
All of the code in the LLDB project is free and open-source software subject to the terms of the University of Illinois/NCSA Open Source License, a permissive free software licence, as is the case with other parts of the LLVM project.
Although LLDB is in early development, it is mature enough to support basic debugging of programs written in C, Objective-C, C++ and Swift.
LLDB is known to work on macOS, Linux, FreeBSD, and Windows and supports i386, x86-64 and ARM instruction sets. It is used as a default debugger for Xcode 5 and later versions."
"20","
In computer science, garbage collection (GC) is a form of automatic memory management. The garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program. Garbage collection was invented by John McCarthy around 1959 to simplify manual memory management in Lisp.
Garbage collection is often portrayed as the opposite of manual memory management, which requires the programmer to specify which objects to deallocate and return to the memory system. However, many systems use a combination of approaches, including other techniques such as stack allocation and region inference. Like other memory management techniques, garbage collection may take a significant proportion of total processing time in a program and, as a result, can have significant influence on performance. With good implementations, enough memory, and depending on application, garbage collection can be faster than manual memory management, while the opposite can also be true and has often been the case in the past with sub-optimal GC algorithms.
Resources other than memory, such as network sockets, database handles, user interaction windows, and file and device descriptors, are not typically handled by garbage collection. Methods used to manage such resources, particularly destructors, may suffice to manage memory as well, leaving no need for GC. Some GC systems allow such other resources to be associated with a region of memory that, when collected, causes the other resource to be reclaimed; this is called finalization. Finalization may introduce complications limiting its usability, such as intolerable latency between disuse and reclaim of especially limited resources, or a lack of control over which thread performs the work of reclaiming.
The basic principles of garbage collection are to find data objects in a program that cannot be accessed in the future, and to reclaim the resources used by those objects.
Many programming languages require garbage collection, either as part of the language specification (for example, Java, C#, D,Go and most scripting languages) or effectively for practical implementation (for example, formal languages like lambda calculus); these are said to be garbage collected languages. Other languages were designed for use with manual memory management, but have garbage-collected implementations available (for example, C and C++). Some languages, like Ada, Modula-3, and C++/CLI, allow both garbage collection and manual memory management to co-exist in the same application by using separate heaps for collected and manually managed objects; others, like D, are garbage-collected but allow the user to manually delete objects and also entirely disable garbage collection when speed is required.
While integrating garbage collection into the language's compiler and runtime system enables a much wider choice of methods,[citation needed]post-hoc GC systems exist, such as Automatic Reference Counting (ARC), including some that do not require recompilation. (Post-hoc GC is sometimes distinguished as litter collection.) The garbage collector will almost always be closely integrated with the memory allocator.
Garbage collection frees the programmer from manually dealing with memory deallocation. As a result, certain categories of bugs are eliminated or substantially reduced:
Some of the bugs addressed by garbage collection have security implications.
Typically, garbage collection has certain disadvantages, including consuming additional resources, performance impacts, possible stalls in program execution, and incompatibility with manual resource management.
Garbage collection consumes computing resources in deciding which memory to free, even though the programmer may have already known this information. The penalty for the convenience of not annotating object lifetime manually in the source code is overhead, which can lead to decreased or uneven performance. A peer-reviewed paper came to the conclusion that GC needs five times the memory to compensate for this overhead and to perform as fast as explicit memory management. Interaction with memory hierarchy effects can make this overhead intolerable in circumstances that are hard to predict or to detect in routine testing. The impact on performance was also given by Apple as a reason for not adopting garbage collection in iOS despite being the most desired feature.
The moment when the garbage is actually collected can be unpredictable, resulting in stalls (pauses to shift/free memory) scattered throughout a session. Unpredictable stalls can be unacceptable in real-time environments, in transaction processing, or in interactive programs. Incremental, concurrent, and real-time garbage collectors address these problems, with varying trade-offs.
The modern GC implementations try to minimize blocking ""stop-the-world"" stalls by doing as much work as possible on the background (i.e. on a separate thread), for example marking unreachable garbage instances while the application process continues to run. In spite of these advancements, for example in the .NET CLR paradigm it is still very difficult to maintain large heaps (millions of objects) with resident objects that get promoted to older generations without incurring noticeable delays (sometimes a few seconds).
Non-deterministic GC is incompatible with resource acquisition is initialization (RAII) based management of non-GCed resources[citation needed]. As a result, the need for explicit manual resource management (release/close) for non-GCed resources becomes transitive to composition. That is: in a non-deterministic GC system, if a resource or a resource-like object requires manual resource management (release/close), and this object is used as ""part of"" another object, then the composed object will also become a resource-like object that itself requires manual resource management (release/close).
Tracing garbage collection is the most common type of garbage collection, so much so that ""garbage collection"" often refers to tracing garbage collection, rather than other methods such as reference counting. The overall strategy consists of determining which objects should be garbage collected by tracing which objects are reachable by a chain of references from certain root objects, and considering the rest as garbage and collecting them. However, there are a large number of algorithms used in implementation, with widely varying complexity and performance characteristics.
Reference counting garbage collection is where each object has a count of the number of references to it. Garbage is identified by having a reference count of zero. An object's reference count is incremented when a reference to it is created, and decremented when a reference is destroyed. When the count reaches zero, the object's memory is reclaimed. 
As with manual memory management, and unlike tracing garbage collection, reference counting guarantees that objects are destroyed as soon as their last reference is destroyed, and usually only accesses memory which is either in CPU caches, in objects to be freed, or directly pointed by those, and thus tends to not have significant negative side effects on CPU cache and virtual memory operation.
There are a number of disadvantages to reference counting; this can generally be solved or mitigated by more sophisticated algorithms:
Escape analysis can be used to convert heap allocations to stack allocations, thus reducing the amount of work needed to be done by the garbage collector. This is done using a compile-time analysis to determine whether an object allocated within a function is not accessible outside of it (i.e. escape) to other functions or threads. In such a case the object may be allocated directly on the thread stack and released when the function returns, reducing its potential garbage collection overhead.
Generally speaking, higher-level programming languages are more likely to have garbage collection as a standard feature. In some languages that do not have built in garbage collection, it can be added through a library, as with the Boehm garbage collector for C and C++.
Most functional programming languages, such as ML, Haskell, and APL, have garbage collection built in. Lisp is especially notable as both the first functional programming language and the first language to introduce garbage collection.
Other dynamic languages, such as Ruby and Julia (but not Perl 5 or PHP before version 5.3, which both use reference counting), JavaScript and ECMAScript also tend to use GC. Object-oriented programming languages such as Smalltalk and Java usually provide integrated garbage collection. Notable exceptions are C++ and Delphi which have destructors.
Historically, languages intended for beginners, such as BASIC and Logo, have often used garbage collection for heap-allocated variable-length data types, such as strings and lists, so as not to burden programmers with manual memory management. On early microcomputers, with their limited memory and slow processors, BASIC garbage collection could often cause apparently random, inexplicable pauses in the midst of program operation.
Some BASIC interpreters, such as Applesoft BASIC on the Apple II family, repeatedly scanned the string descriptors for the string having the highest address in order to compact it toward high memory, resulting in O(n2) performance, which could introduce minutes-long pauses in the execution of string-intensive programs. A replacement garbage collector for Applesoft BASIC published in Call-A.P.P.L.E. (January 1981, pages 40–45, Randy Wigginton) identified a group of strings in every pass over the heap, which cut collection time dramatically. BASIC.System, released with ProDOS in 1983, provided a windowing garbage collector for BASIC that reduced most collections to a fraction of a second.
While the Objective-C traditionally had no garbage collection, with the release of OS X 10.5 in 2007 Apple introduced garbage collection for Objective-C 2.0, using an in-house developed runtime collector.
However, with the 2012 release of OS X 10.8, garbage collection was deprecated in favor of LLVM's automatic reference counter (ARC) that was introduced with OS X 10.7. Furthermore, since May 2015 Apple even forbids the usage of garbage collection for new OS X applications in the App Store. For iOS, garbage collection has never been introduced due to problems in application responsivity and performance; instead, iOS uses ARC.
Garbage collection is rarely used on embedded or real-time systems because of the perceived need for very tight control over the use of limited resources. However, garbage collectors compatible with such limited environments have been developed. The Microsoft .NET Micro Framework and Java Platform, Micro Edition are embedded software platforms that, like their larger cousins, include garbage collection.
Compile-time garbage collection is a form of static analysis allowing memory to be reused and reclaimed based on invariants known during compilation. This form of garbage collection has been studied in the Mercury programming language, and it saw greater usage with the introduction of LLVM's automatic reference counter (ARC) into Apple's ecosystem (iOS and OS X) in 2011.
Incremental, concurrent, and real-time garbage collectors have been developed, such as  Baker's algorithm or Lieberman's algorithm.
In Baker's algorithm, the allocation is done in either half of a single region of memory. When it becomes half full, a garbage collection is performed which moves the live objects into the other half and the remaining objects are implicitly deallocated. The running program (the 'mutator') has to check that any object it references is in the correct half, and if not move it across, while a background task is finding all of the objects.
Generational garbage collection schemes are based on the empirical observation that most objects die young. In generational garbage collection two or more allocation regions (generations) are kept, which are kept separate based on object's age. New objects are created in the ""young"" generation that is regularly collected, and when a generation is full, the objects that are still referenced from older regions are copied into the next oldest generation. Occasionally a full scan is performed.
Some high-level language computer architectures include hardware support for real-time garbage collection.
Most implementations of real-time garbage collectors use tracing. Such real-time garbage collectors meet hard real-time constraints when used with a real-time operating system."
"21","In computer programming, a weak reference is a reference that does not protect the referenced object from collection by a garbage collector, unlike a strong reference. An object referenced only by weak references – meaning ""every chain of references that reaches the object includes at least one weak reference as a link"" – is considered weakly reachable, and can be treated as unreachable and so may be collected at any time. Some garbage-collected languages feature or support various levels of weak references, such as C#, Java, Lisp, OCaml, Perl, and Python.
Weak references have a number of common use cases. When using reference counting garbage collection, weak references can break reference cycles, by using a weak reference for a link in the cycle. When one has an associative array (mapping, hash map) whose keys are (references to) objects, for example to hold auxiliary data about objects, using weak references for the keys avoids keeping the objects alive just because of their use as a key. When one has an object where other objects are registered, such as in the observer pattern (particularly in event handling), if a strong reference is kept, objects must be explicitly unregistered, otherwise a memory leak occurs (the lapsed listener problem), while a weak reference removes the need to unregister. When holding cached data that can be recreated if necessary, weak references allow the cache to be reclaimed, effectively producing discardable memory. This last case (a cache) is distinct from others, as it is preferable that the objects only be garbage collected if necessary, and there is thus a need for finer distinctions within weak references, here a stronger form of a weak reference. In many cases weak references do not need to be directly used, instead simply using a weak array or other container whose keys or values are weak references.
Garbage collection is used to clean up unused objects and so reduce the potential for memory leaks and data corruption. There are two main types of garbage collection: tracing and reference counting. Reference counting schemes record the number of references to a given object and collect the object when the reference count becomes zero. Reference-counting cannot collect cyclic (or circular) references because only one object may be collected at a time. Groups of mutually referencing objects which are not directly referenced by other objects and are unreachable can thus become permanently resident; if an application continually generates such unreachable groups of unreachable objects this will have the effect of a memory leak. Weak references (references which are not counted in reference counting) may be used to solve the problem of circular references if the reference cycles are avoided by using weak references for some of the references within the group.
A very common case of such strong vs. weak reference distinctions is in tree structures, such as the Document Object Model (DOM), where parent-to-child references are strong, but child-to-parent references are weak. For example, Apple's Cocoa framework recommends this approach. Indeed, even when the object graph is not a tree, a tree structure can often be imposed by the notion of object ownership, where ownership relationships are strong and form a tree, and non-ownership relationships are weak and not needed to form the tree – this approach is common in C++ (pre-C++11), using raw pointers as weak references. This approach, however, has the downside of not allowing the ability to detect when a parent branch has been removed and deleted. Since the C++11 standard, a solution was added by using shared_ptr and weak_ptr, probably inherited from the Boost framework.
Weak references are also used to minimize the number of unnecessary objects in memory by allowing the program to indicate which objects are of minor importance by only weakly referencing them.
Some languages have multiple levels of weak reference strength. For example, Java has, in order of decreasing strength, soft, weak, and phantom references, defined in the package java.lang.ref. Each reference type has an associated notion of reachability. The garbage collector (GC) uses an object's type of reachability to determine when to free the object. It is safe for the GC to free an object that is softly reachable, but the GC may decide not to do so if it believes the JVM can spare the memory (e.g. the JVM has lots of unused heap space). The GC will free a weakly reachable object as soon as the GC notices the object. Unlike the other reference types, a phantom reference cannot be followed. On the other hand, phantom references provide a mechanism to notify the program when an object has been freed (notification is implemented using ReferenceQueues).
In C#, weak references are distinguished by whether they track object resurrection or not. This distinction does not occur for strong references, as objects are not finalized if they have any strong references to them. By default, in C# weak reference do not track resurrection, meaning a weak reference is not updated if an object is resurrected; these are called short weak references, and weak references that track resurrection are called long weak references.
Some non-garbage-collected languages, such as C++, provide weak/strong reference functionality as part of supporting garbage collection libraries. The Boost C++ library provides strong and weak references.  It is a mistake to use regular C++ pointers as the weak counterparts of smart pointers because such usage removes the ability to detect when the strong reference count has gone to 0 and the object has been deleted.  Worse yet, it doesn't allow for detection of whether another strong reference is already tracking a given plain pointer.  This introduces the possibility of having two (or more) smart pointers tracking the same plain pointer (which causes corruption as soon as one of these smart pointers' reference count reaches 0 and the object gets deleted).
Weak references can be useful when keeping a list of the current variables being referenced in the application. This list must have weak links to the objects. Otherwise, once objects are added to the list, they will be referenced by it and will persist for the duration of the program.
Java 1.2 in 1998 introduced two kinds of weak references, one known as a “soft reference” (intended to be used for maintaining GC-managed in-memory caches, but which doesn’t work very well in practice) and the other simply as a “weak reference”.  It also added a related experimental mechanism dubbed “phantom references” as an alternative to the dangerous and inefficient finalize() mechanism.
If a weak reference is created, and then elsewhere in the code get() is used to get the actual object, the weak reference isn't strong enough to prevent garbage collection, so it may be (if there are no strong references to the object) that get() suddenly starts returning null.
Another use of weak references is in writing a cache. Using, for example, a weak hash map, one can store in the cache the various referred objects via a weak reference. When the garbage collector runs — when for example the application's memory usage gets sufficiently high — those cached objects which are no longer directly referenced by other objects are removed from the cache.
In Objective-C 2.0, not only garbage collection, but also automatic reference counting will be affected by weak references. All variables and properties in the following example are weak.
The difference between weak (__weak) and unsafe_unretained (__unsafe_unretained) is that when the object the variable pointed to is being deallocated, whether the value of the variable is going to be changed or not. weak ones will be updated to nil and the unsafe_unretained one will be left unchanged, as a dangling pointer. The weak references is added to Objective-C since Mac OS X 10.7 ""Lion"" and iOS 5, together with Xcode 4.1 (4.2 for iOS), and only when using ARC. Older versions of Mac OS X, iOS, and GNUstep support only unsafe_unretained references as weak ones."
"22","Portable Distributed Objects (PDO) is an application programming interface (API) for creating object-oriented code that can be executed remotely on a network of computers. It was created by NeXT Computer, Inc. using their OpenStep system, whose use of Objective-C made the package very easy to write. It was characterized by its very light weight and high speed in comparison to similar systems such as CORBA.
Versions of PDO were available for Solaris, HP-UX and all versions of the OPENSTEP system. A version that worked with Microsoft OLE was also available called D'OLE, allowing distributed code written using PDO on any platform to be presented on Microsoft systems as if they were local OLE objects.
PDO was one of a number of distributed object systems created in the early 1990s, a design model where ""front end"" applications on GUI-based microcomputers would call code running on mainframe and minicomputers for their processing and data storage. Microsoft was evolving OLE into the Component Object Model (COM) and a similar distributed version called DCOM,[citation needed]IBM had their System Object Model (SOM/DSOM), Sun Microsystems was promoting their Distributed Objects Everywhere, and there were a host of smaller players as well. With the exception of the limited functionality in COM,[citation needed] most of these systems were extremely heavyweight, tended to be very large and slow, and often were very difficult to use.
PDO, on the other hand, relied on a small number of features in the Objective-C runtime to handle both portability as well as distribution. The key feature was the language's support for a ""second chance"" method in all classes; if a method call on an object failed because the object didn't support it (normally not allowed in most languages due to strong typing), the runtime would then bundle the message into a compact format and pass it back into the object's forwardInvocation method.
The normal behavior for forwardInvocation was to return an error, including details taken from the message (the ""invocation"").[clarification needed] PDO instead supplied a number of new objects with forwardInvocation methods that passed the invocation object to another machine on the network, with various versions to support different networks and platforms. Calling methods on remote objects was almost invisible; after some network setup (a few lines typically) PDO objects were instantiated locally and called the same way as any other object on the system. The PDO object then forwarded the invocation to the remote computer for processing and unbundled the results when they were returned.
In comparison with CORBA, PDO programs were typically 1/10 or less in size; it was common for NeXT staffers to write into magazines showing how to re-implement a multi-page CORBA article in perhaps 15 lines of code. From a programming standpoint, there was nearly nothing as easy to use as PDO.
However, PDO was also reliant entirely on Objective-C to function. This was a price most were unwilling to pay, as at the time C++ was more widely used and the effort to shift codebases to an entirely new language and paradigm was considered too onerous.[citation needed] PDO never saw much use, and NeXT's emphasis shifted to its new WebObjects framework in 1995.
PDO continues to be used by Mac OS X programmers as a method for interprocess and interapplication communication, and for communication between networked applications that only need compatibility with other Mac OS X applications.
In addition to the OS X platform, there is GNUstep, which has its own implementation of Distributed Objects."
"23","PyObjC is a bidirectional bridge between the Python and Objective-C programming languages, allowing programmers to use and extend existing Objective-C libraries, such as Apple's Cocoa framework, using Python.
PyObjC is used to develop macOS applications in pure Python.
There is also limited support for GNUstep, an open source, cross-platform implementation of Cocoa.
The most important usage of PyObjC is enabling programmers to create GUI applications using Cocoa libraries in pure Python.  Moreover, as an effect of Objective-C's close relationship with the C programming language (it is a pure superset), developers are also able to incorporate any C-based API by wrapping it with an Objective-C wrapper and then using the wrapped code over the PyObjC bridge. Using Objective-C++, the same can be done with C++ libraries.
Cocoa developers may also benefit, as tasks written in Python generally take fewer lines than the Objective-C equivalent. This can be used to their advantage as it enables faster prototyping.
PyObjC's origins date back to 1996, when Lele Gaifax built the original module in September of that year. Among the credited contributors were Guido van Rossum, creator of the Python programming language.
PyObjC was rewritten in 2002. Notable additions include the ability to directly subclass Objective-C classes from Python and nearly complete support for the Foundation, App Kit and Address Book frameworks.
Later the same year, support was added for non-framework Python builds, as well as subsequent support for the Python distribution included with Mac OS X. Along with these changes came project templates for standalone Cocoa applications for use with Project Builder, the predecessor to the current Apple platform IDE, Xcode.
Apple incorporated PyObjC into Mac OS X in 2007, with the release of Mac OS X 10.5 Leopard.
In Objective-C, objects communicate with each other by sending messages, which is analogous to method calls in other object-oriented languages. When an object receives a message, it looks up the message's name, or selector, and matches it up with a method designated the same selector, which it then invokes.
The syntax for these message expressions is inherited from Smalltalk, and appears as an object, called the receiver, placed to the left of the name of the message, or selector, and both are enclosed within a pair of square brackets (the square bracket syntax is not inherited from Smalltalk). Colons within a selector indicate that it accepts one or more arguments, one for each colon. Intended to improve code readability, colons are placed within the selector such that when the required arguments are in place, the expression's intent is unambiguous:
This is distinct from the syntax used in Python, and in many other languages, where an equivalent expression would read:
Translating Objective-C selectors to Python method names is accomplished by replacing each colon with a single underscore and listing the arguments within a pair of parentheses at the end, as demonstrated above.
Objective-C classes are subclassed in the same manner as a normal Python class:
"
"24","The term target–action design paradigm refers to a kind of software architecture, where a computer program is divided  into objects which dynamically establish relationships by telling each other which object they should target and what action or message to send to that target when an event occurs. This is especially useful when implementing graphical user interfaces, which are by nature event-driven.
The target–action approach to event-driven systems allows far more dynamism when compared to other, more static approaches, such as by subclassing. That is because subclassing is a relatively stiff way to program: a programmer must lay out the internal interconnection logic of a program at design time and this cannot be changed later, unless the program is stopped, reengineered, and rebuilt. On the other hand, target-action based programming can change these completely at run-time, thus allowing the program to create new interrelationships and novel behavior by itself.
A prime example of this approach is the OpenStep API, which partly thanks to being based on the dynamic Objective-C language, has much of its graphical user interface implemented by using the target-action paradigm. Consider the following example, written in Objective-C:
Now when the button identified by the button variable is pressed, the runtime system will try to send a message named doSomething to the object in which this code has been invoked. It is also very well possible to determine the message to be sent at run-time:
Here the message which is to be sent is determined by consulting a text field's string value (the string of text which the user typed into a text field). This string is afterwards converted into a message (using the NSSelectorFromString function) and passed to the button as its action. This is possible because, under Objective-C, methods are represented by a selector, a simple string describing the method to be called. When a message is sent, the selector is sent into the ObjC runtime, matched against a list of available methods, and the method's implementation is called. The implementation of the method is looked up at runtime, not compile time.
Because of the extreme dynamism and freedom of behavior given to programs designed with the target-action paradigm, it can happen that the program designer incorrectly implements a part of the interconnection logic and this can lead to sometimes hard to trace bugs. This is due to the lack of compile-time control provided by the compiler which cannot see the interconnections. Thus interconnection consistency control is left entirely to the programmer.
The result of an incorrectly connected target-action binding can differ based on how the particular system in which the program is implemented regards this:"
"25","
NeXT (later NeXT Computer and NeXT Software) was an American computer and software company founded in 1985 by Apple Computer co-founder Steve Jobs. Based in Redwood City, California, the company developed and manufactured a series of computer workstations intended for the higher education and business markets.  NeXT was founded by Jobs after he left Apple, along with several co-workers.  NeXT introduced the first NeXT Computer in 1988, and the smaller NeXTstation in 1990.  The NeXT computers experienced relatively limited sales, with estimates of about 50,000 units shipped in total. Nevertheless, their innovative object-oriented NeXTSTEP operating system and development environment were highly influential.
NeXT later released much of the NeXTSTEP system as a programming environment standard called OpenStep. NeXT withdrew from the hardware business in 1993 to concentrate on marketing OPENSTEP for Mach, its own OpenStep implementation, for several OEMs. NeXT also developed WebObjects, one of the first enterprise Web application frameworks. WebObjects never became very popular because of its initial high price of $50,000, but it remains a prominent early example of a Web server based on dynamic page generation rather than on static content.
Apple purchased NeXT in 1997 for $429 million (equivalent to $654 million in 2017), and 1.5 million shares of Apple stock. As part of the agreement, Steve Jobs, Chairman and CEO of NeXT Software, returned to Apple, the company he co-founded in 1976. The founder promised to merge software from NeXT with Apple's hardware platforms, eventually resulting in macOS, iOS, watchOS and tvOS. Parts of these operating systems incorporated the OPENSTEP foundation.
In 1985, Apple co-founder Steve Jobs led Apple's SuperMicro division, which was responsible for the development of the Macintosh and Lisa personal computers. The Macintosh had been successful on university campuses partly because of the Apple University Consortium, which allowed students and institutions to buy the computers at a discount. The consortium had earned more than $50 million on computers by February 1984.
While chairman, Jobs visited university departments and faculty members to sell Macintosh. Jobs met Paul Berg, a Nobel Laureate in chemistry, at a luncheon held in Silicon Valley to honor François Mitterrand, then President of France. Berg was frustrated by the expense of teaching students about recombinant DNA from textbooks instead of in wet laboratories, used for the testing and analysis of chemicals, drugs, and other materials or biological matter. Wet labs were prohibitively expensive for lower-level courses and were too complex to be simulated on personal computers of the time. Berg suggested to Jobs to use his influence at Apple to create a ""3M computer"" workstation for higher education, featuring more than one megabyte of random-access memory (RAM), a megapixel display and megaFLOP performance, hence the name ""3M"".
Jobs was intrigued by Berg's concept of a workstation and contemplated starting a higher education computer company in the fall of 1985, amidst increasing turmoil at Apple. Jobs' division did not release upgraded versions of the Macintosh and most of the Macintosh Office. As a result, sales plummeted, and Apple was forced to write off millions of dollars in unsold inventory. Apple's chief executive officer (CEO) John Sculley ousted Jobs from his day-to-day role at Apple, replacing him with Jean-Louis Gassée in 1985. Later that year, Jobs began a power struggle to regain control of the company. The board of directors sided with Sculley while Jobs took a business visit to Western Europe and the Soviet Union on behalf of Apple.
After several months of being sidelined, Jobs resigned from Apple on September 13, 1985. He told the board he was leaving to set up a new computer company, and that he would be taking several Apple employees from the SuperMicro division with him. He also told the board that his new company would not compete with Apple and might even consider licensing its designs back to them to market under the Macintosh brand.
Jobs named his new company Next, Inc. A number of former Apple employees followed him to Next, including  Joanna Hoffman, Bud Tribble, George Crow, Rich Page, Susan Barnes, Susan Kare, and Dan'l Lewin. After consulting with major educational buyers from around the country, including a follow-up meeting with Paul Berg, a tentative specification for the workstation was drawn up. It was designed to be powerful enough to run wet lab simulations and cheap enough for college students to use in their dormitory rooms. Before the specifications were finished, however, Apple sued Next for ""nefarious schemes"" to take advantage of the cofounders' insider information. Jobs remarked, ""It is hard to think that a $2 billion company with 4,300-plus people couldn't compete with six people in blue jeans."" The suit was eventually dismissed before trial.
In 1986, Jobs recruited the famous graphic designer Paul Rand to create a brand identity costing $100,000. Rand created a 20-page brochure detailing the brand, including the precise angle used for the logo (28°) and a new company name spelling, NeXT. The first major outside investment was from Ross Perot, who invested after seeing a segment about NeXT on The Entrepreneurs. In 1987, he invested $20 million in exchange for 16 percent of NeXT's stock and subsequently joined the board of directors in 1988.
NeXT changed its business plan in mid-1986. The company decided to develop both computer hardware and software, instead of just a low-end workstation. A team led by Avie Tevanian, who had joined the company after working as one of the Mach kernel engineers at Carnegie Mellon University, was to develop the NeXTSTEP operating system. The hardware division, led by Rich Page — one of the cofounders who had previously led the Apple Lisa team — designed and developed the hardware. NeXT's first factory was completed in Fremont, California in 1987. It was capable of producing 150,000 machines per year. NeXT's first workstation was officially named the NeXT Computer, although it was widely termed ""the cube"" because of its distinctive case, a 1 ft magnesium cube, designed by Apple IIc case designer Frogdesign in accordance with an edict from Jobs.
The original design team had anticipated releasing the computer for US$3,000 in spring of 1987 to be ready for sale by summer of that year. The NeXT Computer received standing ovations when revealed at a lavish, invitation-only gala event, ""NeXT Introduction — the Introduction to the NeXT Generation of Computers for Education"" at the Louise M. Davies Symphony Hall, San Francisco, California on Wednesday October 12, 1988. The following day, selected educators and software developers were invited (for $100 registration fee) to attend the first public technical overview of the NeXT computer at an event called ""The NeXT Day"" held at the San Francisco Hilton. This event gave developers interested in developing NeXT software an insight into the software architecture, object-oriented programming and developing for the NeXT Computer. The luncheon speaker was Steve Jobs.
The first machines were tested in 1989, after which NeXT started selling limited numbers to universities with a beta version of the NeXTSTEP operating system installed. Initially the NeXT Computer was targeted at U.S. higher education establishments only, with a base price of $6,500.
The machine was widely reviewed in magazines, generally concentrating on the hardware. When asked if he was upset that the computer's debut was delayed by several months, Jobs responded, ""Late? This computer is five years ahead of its time!""
The NeXT Computer was based on the new 25 MHz Motorola 68030 central processing unit (CPU). The Motorola 88000 RISC chip was originally considered, but was not available in sufficient quantities. It included between 8 and 64 MB of random-access memory (RAM), a 256 MB magneto-optical (MO) drive, a 40 MB (swap-only), 330 MB, or 660 MB hard disk drive, 10BASE2 Ethernet, NuBus and a 17-inch MegaPixel grayscale display measuring 1120 by 832 pixels. In 1989 a typical new PC, Macintosh, or Amiga computer included a few megabytes of RAM, a 640×480 16-color or 320x240 4000-color display, a 10 to 20 megabyte hard drive and few networking capabilities.
It also was the first computer to ship with a general-purpose DSP chip (Motorola 56001) on the motherboard. This was used to support sophisticated music and sound processing, including the Music Kit software.
The magneto-optical drive manufactured by Canon Inc. was used as the primary mass storage device. These drives were relatively new to the market, and the NeXT was the first computer to use them. They were cheaper than hard drives (blank media especially so: though each had a cost of $150 to Canon, Jobs's typically forthright negotiations saw Canon agree to a retail of only $50 apiece) but slower (with an average seek time of 96 ms). The design made it impossible to move files between computers without a network, since each NeXT Computer had only one MO drive and the disk could not be removed without shutting down the system. Storage options proved challenging for the first NeXT Computers. The magneto-optical media was relatively expensive and had performance and reliability problems despite being faster than a floppy drive. The drive was not sufficient to run as the primary medium running the NeXTSTEP operating system both in terms of speed or capacity.
In 1989, NeXT struck a deal for former Compaq reseller Businessland to sell NeXT computers in select markets nationwide. Selling through a retailer was a major change from NeXT's original business model of only selling directly to students and educational institutions. Businessland founder David Norman predicted that sales of the NeXT Computer would surpass sales of Compaq computers after 12 months.
In 1989, Canon invested US$100 million in NeXT, giving it a 16.67 percent stake, making NeXT worth almost $600 million. Canon invested in NeXT with the condition that it would be able to use the NeXTSTEP environment with its own workstations, which would mean a greatly expanded market for the software. After NeXT exited the hardware business, Canon produced a line of PCs, called object.station, including models 31, 41, 50 and 52, specifically designed to run NeXTSTEP/Intel. Canon also served as NeXT's distributor in Japan.
NeXT computers were first released on the retail market in 1990, for $9,999. NeXT's original investor Ross Perot resigned from the board of directors in June 1991 to dedicate more time to Perot Systems, a Plano, Texas-based systems integrator.
NeXT released a second generation of workstations in 1990. The new range included a revised NeXT Computer, renamed the NeXTcube, and the NeXTstation, nicknamed ""the slab,"" which used a ""pizza box"" case form-factor. Jobs was explicit in ensuring NeXT staff did not use the latter terminology, lest the NeXT machines be compared to competing Sun workstations. The magneto-optical drive was replaced with a 2.88 MB floppy drive to offer users a way to use their floppy disks. However, individual 2.88 MB floppies were expensive and the technology did not supplant the 1.44 MB floppy. Realizing this, NeXT utilized the CD-ROM drive, which eventually became an industry standard for storage. Color graphics were available on the NeXTstation Color and the NeXTdimension graphics processor hardware for the NeXTcube. The new computers were cheaper and faster than their predecessors, with the new Motorola 68040 processor.
In 1992, NeXT launched ""Turbo"" variants of the NeXTcube and NeXTstation with a 33 MHz 68040 processor and maximum RAM capacity increased to 128 MB. NeXT sold 20,000 computers in 1992 (NeXT counted upgraded motherboards on back order as sales) — a small number compared with their competitors. However, the company reported sales of $140 million for the year, encouraging Canon to invest a further $30 million to keep the company afloat.
In total, 50,000 NeXT machines were sold, including thousands to the then super secret National Reconnaissance Office located in Chantilly, Virginia.  NeXT's long-term aim was to migrate to the RISC (Reduced Instruction Set Computing) architecture, a processor design strategy intended to increase performance. The project was known as the NeXT RISC Workstation (NRW). Initially the NRW was to be based on the Motorola 88110 processor, but due to a lack of confidence in Motorola's commitment to the 88000-series architecture, it was later redesigned around dual PowerPC 601s. NeXT produced some motherboards and enclosures, but exited the hardware business before full production.
NeXT computers were delivered with Mathematica pre-installed. Several developers used the NeXT platform to write pioneering programs. Tim Berners-Lee used a NeXT Computer in 1990 to create the first Web browser and Web server; accordingly, NeXT was instrumental in the development of the World Wide Web.
NeXT was an engineering computer used by professors for the most serious science challenges, and also for developing finished newspaper layouts using News running on Next. George Mason University in the early 1990s had a set of them for publishing, as well as Silicon Graphics for CAD/GL and Mathematica for astrophysics. The games Doom, Doom II: Hell on Earth and Quake were developed by id Software on NeXT machines. Other games based on the Doom engine, such as Heretic and Hexen: Beyond Heretic by Raven Software, as well as Strife by Rogue Entertainment were also developed on NeXT hardware using id's tools.
Other commercial programs were released for NeXT computers, including Altsys Virtuoso, a vector drawing program with page-layout features which was ported to Mac OS and Microsoft Windows as Aldus FreeHand v4, and the Lotus Improv spreadsheet program. The systems also came with a number of smaller built-in applications, such as the Merriam-Webster Collegiate Dictionary, Oxford Quotations, the complete works of William Shakespeare, and the Digital Librarian search engine to access them.
NeXT started porting the NeXTSTEP operating system to IBM PC compatible computers using the Intel 80486 processor in late 1991. The operating system was ported to Intel's architecture because of a change in NeXT's business strategy, which was then to remove themselves from the hardware business entirely. A demonstration of the port was displayed at the NeXTWorld Expo in January 1992.  By mid-1993 the product was complete and version 3.1, also known as NeXTSTEP 486, was released. Prior to the release of NeXTSTEP, Chrysler planned to buy 3,000 copies in 1992.
NeXTSTEP 3.x was later ported to PA-RISC and SPARC-based platforms, for a total of four versions: NeXTSTEP/NeXT (for NeXT's 68k ""black boxes""), NeXTSTEP/Intel, NeXTSTEP/PA-RISC and NeXTSTEP/SPARC. Although these ports were not widely used, NeXTSTEP gained popularity at institutions such as First Chicago NBD, Swiss Bank Corporation, O'Connor and Company, and other organizations owing to its programming model. It was also used by many American federal agencies, such as United States Naval Research Laboratory, the National Security Agency, the Advanced Research Projects Agency, the Central Intelligence Agency and the National Reconnaissance Office. Some IBM PC clone vendors offered somewhat customized hardware solutions that were delivered running NeXTSTEP on Intel, such as the Elonex NextStation and the Canon object.station 41.
NeXT withdrew from the hardware business in 1993 and the company was renamed NeXT Software Inc; consequently, 300 of the 540 staff employees were laid off. NeXT negotiated to sell the hardware business, including the Fremont factory, to Canon. Canon later pulled out of the deal. Work on the PowerPC machines was stopped, along with all hardware production. CEO of Sun Microsystems Scott McNealy announced plans to invest $10 million in 1993 and use NeXT software (OpenStep) in future Sun systems. NeXT partnered with Sun to create OpenStep which was NeXTSTEP without the Mach-based kernel.
After dropping the hardware business, NeXT returned to selling a toolkit to run on other operating systems, in effect returning to the original business plan. New products based on OpenStep were released, including OpenStep Enterprise, a version for Microsoft's Windows NT. The company also launched WebObjects, a platform for building large-scale dynamic web applications. Many large businesses including Dell, Disney, WorldCom, and the BBC used this WebObjects software for a short time. Today, WebObjects is used almost solely to power Apple's iTunes Store and most of its corporate Web site.
Apple Computer announced an intention to acquire NeXT on December 20, 1996. Apple paid $429 million in cash, which went to the initial investors and 1.5 million Apple shares, which went to Steve Jobs, who was deliberately not given cash for his part in the deal. The main purpose of the acquisition was to use NeXTSTEP as a foundation to replace the dated classic Mac OS, instead of BeOS or the in-development Copland. The deal was finalized on February 7, 1997, bringing Jobs back to Apple as a consultant, who was later appointed as interim CEO. In 2000 Jobs took the CEO position as a permanent assignment.
Several NeXT executives replaced their Apple counterparts when Steve Jobs restructured the company's board of directors. Over the next five years the NeXTSTEP operating system was ported to the PowerPC architecture. At the same time, an Intel port and OpenStep Enterprise toolkit for Windows were both produced. The operating system was code named Rhapsody, while the toolkit for development on all platforms was called ""Yellow Box"". For backwards compatibility Apple added the ""Blue Box"" to Rhapsody, allowing existing Mac applications to be run in a self-contained cooperative multitasking environment.
A server version of the new operating system was released as Mac OS X Server 1.0 in 1999, and the first consumer version, Mac OS X 10.0, in 2001. The OpenStep developer toolkit was renamed Cocoa. Rhapsody's Blue Box was renamed Classic Environment and changed to run applications full-screen without requiring a separate window. Apple included an updated version of the original Macintosh toolbox, called Carbon, that gave existing Mac applications access to the environment without the constraints of Blue Box. Some of NeXTSTEP's interface features were used in Mac OS X, including the Dock, the Services menu, the Finder's ""browser"" view, and the Cocoa text system.
NeXTSTEP's processor-independent capabilities were retained in Mac OS X, leading to both PowerPC and Intel x86 versions (although only PowerPC versions were publicly available before 2006). Apple moved to Intel processors by August 2006.
Jobs created a different corporate culture at NeXT in terms of facilities, salaries, and benefits. Jobs had experimented with some structural changes at Apple but at NeXT he abandoned conventional corporate structures, instead making a ""community"" with ""members"" instead of employees. There were only two different salaries at NeXT until the early 1990s. Team members who joined before 1986 were paid $75,000 while those who joined afterwards were paid $50,000. This caused a few awkward situations where managers were paid less than their employees. Employees were given performance reviews and raises every six months because of the spartan salary plans. To foster openness, all employees had full access to the payrolls, although few employees ever took advantage of the privilege. NeXT's health insurance plan offered benefits to not only married couples but unmarried couples and same-sex couples, although the latter privilege was later withdrawn due to insurance complications. The payroll schedule was also very different from other companies in Silicon Valley at the time: instead of getting paid twice a month at the end of the pay period, employees would get paid once a month in advance.
Jobs found office space in Palo Alto, California on 3475 Deer Creek Road, occupying a glass and concrete building which featured a staircase designed by architect I. M. Pei. The first floor used hardwood flooring and large worktables where the workstations would be assembled. To avoid inventory errors, NeXT used the just-in-time (JIT) inventory strategy. The company contracted out for all major components such as mainboards and cases and had the finished components shipped to the first floor for assembly. The second floor was the office space with an open floor plan. The only enclosed rooms were Jobs's office and a few conference rooms.
As NeXT expanded, more office space was needed. The company rented an office at 800 and 900 Chesapeake Drive in Redwood City, also designed by Pei. The architectural centerpiece was a ""floating"" staircase with no visible supports. The open floor plan was retained, although furnishings became luxurious, with $5,000 chairs, $10,000 sofas and Ansel Adams prints.
NeXT's first former campus in Palo Alto was subsequently occupied by SAP AG. Its second former campus in Redwood City was occupied by ApniCure and OncoMed Pharmaceuticals Inc.
The first issue of NeXTWORLD magazine was printed in 1991. It was published in San Francisco by Integrated Media and edited by Michael Miley and later Dan Ruby. It was the only mainstream periodical to discuss NeXT computers, the operating system, and NeXT software. Publication was discontinued in 1994 after only four volumes. A NeXTWORLD Expo followed as a developer conference, held in 1991 and 1992 at the San Francisco Civic Center and in 1993 and 1994 at the Moscone Center in San Francisco, with Steve Jobs as the keynote speaker.
Steve Jobs pooled the finest of over-specified hardware and software (from PC standards) into NeXT and the company added its own innovations.  NeXT was the machine-of-choice for well-funded Unix-friendly science departments.[citation needed]
Despite NeXT's limited commercial success, the company had a wide-ranging impact on the computer industry. Object-oriented programming and graphical user interfaces became more common after the 1988 release of the NeXTcube and NeXTSTEP, when other companies started to emulate NeXT's object-oriented system. Apple started the Taligent project in 1989, with the goal of building a NeXT-like operating system for the Macintosh, with collaboration from Hewlett-Packard and IBM.
Microsoft announced the Cairo project in 1991; the Cairo specification included similar object-oriented user interface features for a coming consumer version of Windows NT. Although the project was ultimately abandoned, some elements were integrated into other projects. By 1994, Microsoft and NeXT were collaborating on a Windows NT port of OpenStep; the port, however, was never released.
WebObjects failed to achieve wide popularity partly because of the initial high price of US$50,000, but it remains the first and most prominent early example of a web application server that enabled dynamic page generation based on user interactions as opposed to static content. WebObjects is now bundled with macOS Server and Xcode.




"
"26","Mac OS X Server 1.0, released on March 16, 1999, is the first operating system released into the retail market by Apple Computer based on NeXT technology. It was the final release of the product code-named Rhapsody, which was an interim combination of the OpenStep system (Mach OS and OpenStep API) and Mac OS 8.
Although Mac OS X Server 1.0's graphical ""look and feel"" was a variation of the Platinum theme from Mac OS 8, its infrastructure is based on the OPENSTEP (and thus, NeXTSTEP) operating system instead of the classic Mac OS.  The resulting product gave users a preview of the operating system that was to become Mac OS X  (now referred to as macOS). Mac OS X Server was never officially known simply as Mac OS X, and was ultimately rendered obsolete by Mac OS X v10.0 in 2001 and macOS Server.
Server 1.0 contains a mix of features from the classic Mac OS, NeXTSTEP and Mac OS X. Like classic Mac OS, it has a single menu bar across the top of the screen, but file management is performed in Workspace Manager from NeXTSTEP instead of the classic Mac OS Finder. The user interface still uses the Display PostScript-based window server from NeXTSTEP, instead of the Quartz-based WindowServer, which would appear a year later in Mac OS X Public Beta. Unlike any version of Classic Mac OS, windows with unsaved content display a black dot in the window close button like NeXTSTEP did. The Dock and the Aqua appearance were not included; these were added later in Mac OS X.
""Carbon"", essentially a subset of ""classic"" Mac OS API calls, was also absent. This meant that the only native applications for OS X Server 1.0 were written for the ""Yellow Box"" API, which went on to become known as ""Cocoa"". Furthermore, Apple's own FireWire was not supported.
Server 1.0 also includes the first version of a NetBoot server, which allows computers to boot from a disk image over a local network. This was particularly useful in a school or other public-machine setting, as it allowed the machines to be booted from a single OS copy stored on Server 1.0.  This made it difficult for users to damage the OS by installing software – as soon as they signed out, the machine would re-boot with a fresh OS from the NetBoot server.
To run classic Mac OS applications, Mac OS X Server 1.0 includes the ""Blue Box"", which essentially ran a copy of Mac OS 8.5.1 (this could be updated to Mac OS 8.6 in version 1.2 and later) in a separate process as an emulation layer. Blue Box would eventually be renamed as the ""Classic Environment"" in Mac OS X, featuring the latest version of Mac OS 9."
"27","The Foundation Kit, or just Foundation for short, is an Objective-C framework in the OpenStep specification.  It provides basic classes such as wrapper classes and data structure classes.  This framework uses the prefix NS (for NeXTSTEP, or NeXT/Sun_Microsystems).
This class is the most common base class for Objective-C hierarchies and provides standard methods for working with objects by managing the memory associated with them and querying them.
A class used for string manipulation, representing a Unicode string (most typically using UTF-16 as its internal format). NSString is immutable, and thus can only be initialized but not modified. NSMutableString is a modifiable version.
NSValue is a wrapper class for C data types, and NSNumber is a wrapper class for C number data types such as int, double, and float.  The data structures in Foundation Kit can only hold objects, not primitive types, so wrappers such as NSValue and NSNumber are used in those data structures.
A dynamic array of objects, supporting constant-time indexing. NSArray is an immutable version that can only be initialized with objects but not modified. NSMutableArray may be modified by adding and removing objects.
An associative data container of key-value pairs with unique keys. Searching and element addition and removal (in the case of NSMutableDictionary) is faster-than-linear. However, the order of the elements within the container is not guaranteed.
An associative container of unique keys, similar to NSDictionary, with the difference that members do not contain a data object.
A wrapper for raw byte data. An object of this type can dynamically allocate and manage its data, or it can refer to data owned by and managed by something else (such as a static numeric array).
Classes that store times and dates and represent calendrical information. They offer methods for calculating date and time differences. Together with NSLocale, they provide methods for displaying dates and times in many formats, and for adjusting times and dates based on location in the world.
The Foundation Kit is part of the Cocoa API.  Beginning as the successor to OPENSTEP/Mach, this framework has deviated from OpenStep compliance, and is in some places incompatible.
The Foundation Kit is in the Cocoa Touch API.  This framework is based on the macOS Foundation, not OpenStep.[citation needed]
The Foundation Kit is implemented in GNUstep's Base Package.  This implementation is mostly comparable (4 classes are missing) and aims to be comparable with both the OpenStep API and later macOS additions.
The Foundation Kit is implemented in Cocotron, an open-source implementation of Cocoa.
PureFoundation is an open-source implementation of Foundation that implements Foundation by wrapping Core Foundation, just like in Cocoa, rather than create a separate Foundation from scratch like GNUstep and Cocotron.
SwiftFoundation is Apple's open-source Swift implementation of the Foundation API for platforms where there is no Objective-C runtime."
"28","

Window Maker is a free and open source window manager for the X Window System, allowing graphical applications to be run on Unix-like operating-systems. It is designed to emulate NeXTSTEP's GUI as an OpenStep-compatible environment. Window Maker is part of the GNU Project.
Window Maker has a reputation for being fast, efficient and highly stable.[citation needed] Window Maker has been characterized as reproducing ""the elegant look and feel of the NeXTstep GUI"" and is noted as ""easy to configure and easy to use."" A graphical tool called Wprefs is included and can be used to configure most aspects of the UI. The interface tends towards a minimalist, high performance environment directly supporting XPM, PNG, JPEG, TIFF, GIF and PPM icons with an alpha-channel and a right-click, sliding-scrolling application menu system which can throw off pinnable menus, along with window-icon miniaturization and other animations on multiple desktops. Menus and preferences can be changed without restarting. As with most window managers it supports themes and many are available. Owing to its NeXT inspiration, Window Maker has a dock like MacOS, but Window Maker's look and feel hews mostly to that of its NeXT forebear.
Window Maker has window hints which allow seamless integration with the GNUstep, GNOME, KDE, Motif and OpenLook environments. Significantly it has almost complete ICCCM compliance and internationalization support for at least 11 locales. Window Maker uses the lightweight WINGs widget set which was built specifically for Window Maker as a way to skirt what its developers said would have been the ""overkill"" (or bloat) of using GNUstep. WINGs is common to other applications including a login display manager called WINGs Display Manager (WDM) and many dockapps. Window Maker dock and clip applets are compatible with those from AfterStep's wharf.
Window Maker was written from scratch primarily by Brazilian programmer Alfredo Kojima as a window manager for the GNUstep desktop environment and originally meant as an improved take on the AfterStep window manager's design concept. The first release was in 1997. For a time it was included as a standard window manager in several Linux distributions and is also available in the FreeBSD and OpenBSD ports collection. Since the goal of the project has been to closely emulate the design of the defunct NeXTstep and OpenStep GUIs, further development has been light. In late 2007 the widely available, stable release version was at 0.92 from July 2005 with subsequent maintenance updates having been made to some distribution packages and ports.
In late June 2008 a post on the project's website said active development would resume, noting, ""...we are working very hard to revitalize Window Maker's presence on X Window (and perhaps beyond) desktops... We expect to once again provide the de-facto minimalist yet extremely functional window manager to the world.""  On 29 January 2012, Window Maker 0.95.1 was released, making it the first official release in almost seven years. This was followed by a number of releases; As of  October 2017[update] the latest release was 0.95.8, released on 11 March 2017.
The program's original name was WindowMaker (camelcased and without the space) but a naming conflict arose with an older product called Windowmaker from Windowmaker Software Ltd, a UK company producing software for companies that manufacture windows and doors. A 1998 agreement between the developers of Window Maker and Windowmaker Software specified that Window Maker (in the X sense) should never be used as a single word.
Though adhering closely to the NeXT interface, the default appearance can be confusing to someone expecting a Microsoft Windows-style taskbar and start menu. All applications can be accessed by right-clicking on the desktop background to access the fully configurable main menu. The menu can also be displayed using the keyboard, with F12 for the application menu and F11 for a window menu.
Window Maker can be configured by double-clicking the screwdriver icon on the dock. An icon depicting a computer monitor is used to launch a command-window and a paperclip icon is used to cycle between workspaces. Any icon in Window Maker, including application icons, can be easily changed.
Icons representing running applications appear at the bottom of the screen (the user can extend application windows to cover these). By default, the dock appears at upper right. Icons can be dragged onto the dock to make them permanent. The edge of an icon can be right-clicked to adjust its settings. A separate, dockable application called wmdrawer features a slide-out drawer which can hold application and file launching icons.
While any X application can be docked in Window Maker, the archetypical WM dockable applications are called dockapps. These tend to be clocks and system monitoring applications. There are many clock implementations, including wmcalclock, wmtime, wmclock (a NeXTStep-like calendar clock clone) and wmclockmon. Monitoring applets include wmload, wmavgload, wmmon, wmnet and wmnd. Many other dockapps are available, typically ones intended to interact with other ""full fledged"" applications.
The WPrefs configuration tool enables tuning of most Window Maker preferences. wmakerconf was developed to provide more configuration options, notably theme customization. Configuration files are typically stored in ~/GNUstep/. The background can be changed from the command line with wmsetbg -s -u [filename.jpg] (wmsetbg stands for ""window maker set background"").
FSViewer is a separate, configurable Miller Columns file browser developed for Window Maker in 1998 by George Clernon as a visual and functional analogy to NeXTstep's Workspace Manager. In 2002, it was adapted to later versions of the WINGs libraries and Window Maker by Guido Scholz.
aterm is an rxvt based terminal emulator developed for Afterstep mainly for visual appeal, featuring a NeXTstep style scrollbar (which matches Window Maker's look and feel) along with pseudo-transparency.
The application menu can be edited graphically with much versatility. The configuration is recorded in ~/GNUstep/Defaults/WMRootMenu as a text file which can be easily read and edited (in versions after 0.94.0 it can also be automatically generated from a list of installed applications using a program called wmgenmenu).
Menu items can be set to:
Many Linux distributions define their own applications menu for Window Maker. This cannot usually be edited using the configuration tool (which will instead offer to replace it with a generic default menu which can be edited).
Amanda the Panda is the mascot of Window Maker. She was designed by Agnieszka Czajkowska."
"29"," (Learn how and when to remove this template message)

Gorm (Graphical Object Relationship Modeller) is a graphical user interface builder application. It is part of the developer tools of GNUstep. Gorm is the equivalent of Interface Builder that was originally found on NeXTSTEP, then OPENSTEP, and finally on Mac OS X.
Gorm and Project Center represent the heart of the suite for GNUstep. Gorm follows Interface Builder so closely that using tutorials written for the latter is possible without much hassle and thus brings the power of Interface Builder to the open source world, being part of the GNU project.
Gorm allows developers to quickly create graphical applications and to design every little aspect of the application's user interface. The developer can drag and drop all types of objects such as menus, buttons, tables, lists and browsers into the interface. With the mouse, it is possible to resize, move or convert the objects or connect them to functions, as well as to edit nearly every aspect of them using Gorm's powerful inspectors."
"30","
Interface Builder is a software development application for Apple's Mac OS X operating system. It is part of Xcode (formerly Project Builder), the Apple Developer Connection developer's toolset. Interface Builder allows Cocoa and Carbon developers to create interfaces for applications using a graphical user interface. The resulting interface is stored as a .nib file, short for NeXT Interface Builder, or more recently, as a .xib file.
Interface Builder is descended from the NeXTSTEP development software of the same name. A version of Interface Builder is also used in the development of OpenStep software, and a very similar tool called Gorm exists for GNUstep. On March 27, 2008, a specialized iPhone version of Interface Builder allowing interface construction for iPhone applications was released with the iPhone SDK Beta 2. 
Interface Builder was intentionally developed as a separate application, to allow interaction designers to design interfaces without having to use a code-oriented IDE, but as of Xcode 4, Apple has integrated its functionality directly into Xcode.
Interface Builder first made its appearance in 1986 written in Lisp (for the ExperLisp product by ExperTelligence).  It was invented and developed by Jean-Marie Hullot using the object-oriented features in ExperLisp, and deeply integrated with the Macintosh toolbox. Denison Bollay took Jean-Marie Hullot to NeXT later that year to demonstrate it to Steve Jobs.  Jobs immediately recognized its value, and started incorporating it into NeXTSTEP, and by 1988 it was part of NeXTSTEP 0.8. It was the first commercial application that allowed interface objects, such as buttons, menus, and windows, to be placed in an interface using a mouse. One notable early use of Interface Builder was the development of the first WorldWideWeb web browser by Tim Berners-Lee at CERN, made using a NeXT workstation.
Interface Builder provides palettes, or collections, of user interface objects to an Objective-C developer. These user interface objects contain items like text fields, data tables, sliders, and pop-up menus. Interface Builder's palettes are completely extensible, meaning any developer can develop new objects and add palettes to Interface Builder.
To build an interface, a developer simply drags interface objects from the palette onto a window or menu. Actions (messages) which the objects can emit are connected to targets in the application's code and outlets (pointers) declared in the application's code are connected to specific objects. In this way all initialization is done before runtime, both improving performance[citation needed] and streamlining the development process. When Interface Builder was a standalone application, interface designers could ship nib files to developers, who would then drop them into their projects.
Interface Builder saves an application's interface as a bundle that contains the interface objects and relationships used in the application. These objects are archived (a process also known as serialization or  marshalling in other contexts) into either an XML file or a NeXT-style property list file with a .nib extension. Upon running an application, the proper NIB objects are unarchived, connected into the binary of their owning application, and awakened. Unlike almost all other GUI designer systems which generate code to construct the UI (notable exceptions being Glade, Embarcadero Technologies's Delphi and C++ Builder, which stream UI objects similarly), NIBs are often referred to as freeze dried because they contain the archived objects themselves, ready to run. As of Interface Builder version 3, a new file format (with extension .xib) has been added, which is functionally identical to .nib, except it is stored in a flat file, making it more suitable for storage in revision control systems and processing by tools such as diff."
"31","Rhapsody was the code name given to Apple Computer's next-generation operating system during the period of its development between Apple's purchase of NeXT in late 1996 and the announcement of Mac OS X (now macOS) in 1998. At first more than an operating system, Rhapsody represented a new strategy for Apple, who intended the operating system to run on x86-based PCs, DEC Alpha workstations, as well as PowerPC-based Macintosh hardware. In addition, the underlying API frameworks would be ported to run natively on Microsoft Windows NT. Eventually the non-Apple platforms were dropped and later versions consisted primarily of the OPENSTEP operating system ported to the Power Macintosh along with a new GUI to make it appear more Mac-like. Several existing ""classic"" Mac OS technologies were also ported to Rhapsody, including QuickTime and AppleSearch. Rhapsody could also run Mac OS 8 in a ""Blue Box"" emulation layer.
Rhapsody was announced at the MacWorld Expo in San Francisco on January 7, 1997 and first demonstrated at the 1997 Worldwide Developers Conference (WWDC). There were two subsequent general Developer Releases for computers with x86 or PowerPC processors. After this there was to be a ""Premier"" version somewhat analogous to the Mac OS X Public Beta, followed by the full ""Unified"" version in the second quarter of 1998. Apple's development schedule in integrating the features of two very different systems made it difficult to forecast the features of upcoming releases. At the 1998 MacWorld Expo in New York, Steve Jobs announced that Rhapsody would be released as Mac OS X Server 1.0 (which shipped in 1999). No home version of Rhapsody would be released. Its code base was forked into Darwin, the open source underpinnings of macOS.
Defining features of the Rhapsody operating system included a heavily modified ""hybrid"" OSFMK 7.3 (Open Source Foundation Mach Kernel) from the OSF, a BSD operating system layer (based on 4.4BSD), the object-oriented Yellow Box API framework, the Blue Box compatibility environment for running ""classic"" Mac OS applications, and a Java Virtual Machine.
The user interface was modeled after Mac OS 8's ""Platinum"" appearance. The file management functions served by the Finder in previous Mac OS versions were instead handled by a port of OPENSTEP's Workspace Manager. Additional features inherited from OPENSTEP and not found in the classic Mac OS Finder were included, such as the Shelf and column view. Although the Shelf was dropped in favor of Dock functionality, column view would later make its way to macOS's Finder. 
Rhapsody's Blue Box environment, available only when running on the PowerPC architecture, was responsible for providing runtime compatibility with existing Mac OS applications. Compared to the more streamlined and integrated Classic compatibility layer that was later featured in Mac OS X, Blue Box's interface presented users with a distinct barrier between emulated legacy software and native Rhapsody applications. All emulated applications and their associated windows were encapsulated within a single Blue Box emulation window instead of being interspersed with the other applications using the native Yellow Box API. This limited cross-environment interoperability and caused various user interface inconsistencies.
To avoid the pitfalls of running within the emulation environment and take full advantage of Rhapsody's features, software needed to be rewritten to use the new Yellow Box API. Inherited from OPENSTEP, Yellow Box used an object-oriented model completely unlike the procedural model used by the Classic APIs. The large difference between the two frameworks meant transition of legacy code required significant changes and effort on the part of the developer. The consequent lack of adoption as well as objections by prominent figures in the Macintosh software market, including Adobe Systems and Microsoft, became major factors in Apple's decision to cancel the Rhapsody project in 1998.
However, most of Yellow Box and other Rhapsody technologies went on to be used in macOS's Cocoa API. Bowing to developers' wishes, Apple also ported existing Classic Mac OS technologies into the new operating system and implemented the Carbon API to provide Classic Mac OS API compatibility. Widely used Mac OS libraries like QuickTime and AppleScript were ported and made available to developers. Carbon allowed developers to maintain full compatibility and native functionality using their current codebases, while enabling them to take advantage of new features at their discretion.
The name Rhapsody followed a pattern of music-related code names that Apple designated for operating system releases during the 1990s. Another next-generation operating system, which was to be the successor to the never-completed Copland operating system, was code-named Gershwin after George Gershwin, composer of Rhapsody in Blue. Copland itself was named after another American composer, Aaron Copland. Other musical code names include Harmony (Mac OS 7.6), Tempo (Mac OS 8), Allegro (Mac OS 8.5), and Sonata (Mac OS 9)."
"32","A hybrid kernel is an operating system kernel architecture that attempts to combine aspects and benefits of microkernel and monolithic kernel architectures used in computer operating systems.
The traditional kernel categories are monolithic kernels and microkernels (with nanokernels and exokernels seen as more extreme versions of microkernels). The ""hybrid"" category is controversial, due to the similarity of hybrid kernels and ordinary monolithic kernels; the term has been dismissed by Linus Torvalds as simple marketing.
The idea behind a hybrid kernel is to have a kernel structure similar to that of a microkernel, but to implement that structure in the manner of a monolithic kernel. In contrast to a microkernel, all (or nearly all) operating system services in a hybrid kernel are still in kernel space. So there are none of the reliability benefits of having services in user space, as with a microkernel. However, just as with an ordinary monolithic kernel, there is none of the performance overhead for message passing and context switching between kernel and user mode that normally comes with a microkernel.
One prominent example of a hybrid kernel is the Microsoft Windows NT kernel that powers all operating systems in the Windows NT family, up to and including Windows 10 and Windows Server 2016, and powers Windows Phone 8, Windows Phone 8.1, and Xbox One. NT-based Windows is classified as a hybrid kernel (or a macrokernel) rather than a monolithic kernel because the emulation subsystems run in user-mode server processes, rather than in kernel mode as on a monolithic kernel, and further because of the large number of design goals which resemble design goals of Mach (in particular the separation of OS personalities from a general kernel design). Conversely, the reason NT is not a microkernel system is because most of the system components run in the same address space as the kernel, as would be the case with a monolithic design (in a traditional monolithic design, there would not be a microkernel per se, but the kernel would implement broadly similar functionality to NT's microkernel and kernel-mode subsystems).
The Windows NT design includes many of the same objectives as Mach, the archetypal microkernel system, one of the most important being its structure as a collection of modules that communicate via well-known interfaces, with a small microkernel limited to core functions such as first-level interrupt handling, thread scheduling and synchronization primitives. This allows for the possibility of using either direct procedure calls or interprocess communication (IPC) to communicate between modules, and hence for the potential location of modules in different address spaces (for example in either kernel space or server processes). Other design goals shared with Mach included support for diverse architectures, a kernel with abstractions general enough to allow multiple operating system personalities to be implemented on top of it and an object-oriented organisation.
The reason NT is not a microkernel system is that nearly all of the subsystems providing system services, including the entire Executive, run in kernel mode, in the same address space as the microkernel itself, rather than in user-mode server processes, as would be the case with a microkernel design. This is an attribute NT shares with early versions of Mach, as well as all commercial systems based on Mach, and stems from the superior performance offered by using direct procedure calls in a single memory space, rather than IPC, for communication amongst subsystems. The user-mode subsystems on NT include one or more emulation subsystems, each of which provides an operating system personality to applications, the Session Manager Subsystem (smss.exe), which starts the emulation subsystems during system startup and the Local Security Authority Subsystem Service (lsass.exe), which enforces security on the system. The subsystems are not written to a particular OS personality, but rather to the native NT API (or Native API).
The primary operating system personality on Windows is the Windows API, which is always present. The emulation subsystem which implements the Windows personality is called the Client/Server Runtime Subsystem (csrss.exe). On versions of NT prior to 4.0, this subsystem process also contained the window manager, graphics device interface and graphics device drivers. For performance reasons, however, in version 4.0 and later, these modules (which are often implemented in user mode even on monolithic systems, especially those designed without internal graphics support) run as a kernel-mode subsystem.
As of 2007, one other operating system personality, UNIX, is offered as an optionally installed system component on certain versions of Windows Vista and Windows Server 2003 R2. The associated subsystem process is the Subsystem for UNIX-Based Applications (psxss.exe), which was formerly part of a Windows add-on called Windows Services for UNIX. An OS/2 subsystem (os2ss.exe) was supported in older versions of Windows NT, as was a very limited POSIX subsystem (psxss.exe). The POSIX subsystem was supplanted by the UNIX subsystem, hence the identical executable name.
In August 2016, Microsoft unveiled the latest Windows subsystem called the Windows Subsystem for Linux. This subsystem, available only on 64-bit Windows 10 version 1607 (Anniversary Update, codenamed Redstone), runs a slimmed down version of Ubuntu 14.04 LTS natively within the operating system without emulation to achieve this. It was marketed as ""Bash on Windows"", because it ran bash, a popular command line interface used on many Linux distributions and macOS, and allows binaries compiled for amd64 to run unmodified within the subsystem. This was intended so that developers could run their tools on Windows without having to emulate them, and thus requires developer mode to be enabled in Windows Settings. It is designed only to run command-line applications, although a reddit user has discovered a way to run GUI applications or even an entire desktop environment with it. Certain applications that strictly rely on the Linux kernel itself will not be able to run because it does not include the Linux kernel.
Applications that run on NT are written to one of the OS personalities (usually the Windows API), and not to the native NT API for which documentation is not publicly available (with the exception of routines used in device driver development). An OS personality is implemented via a set of user-mode DLLs (see Dynamic-link library), which are mapped into application processes' address spaces as required, together with an emulation subsystem server process (as described previously). Applications access system services by calling into the OS personality DLLs mapped into their address spaces, which in turn call into the NT run-time library (ntdll.dll), also mapped into the process address space. The NT run-time library services these requests by trapping into kernel mode to either call kernel-mode Executive routines or make Local Procedure Calls (LPCs) to the appropriate user-mode subsystem server processes, which in turn use the NT API to communicate with application processes, the kernel-mode subsystems and each other.
XNU is the kernel that Apple Inc. acquired and developed for use in the macOS, iOS, watchOS, and tvOS operating systems and released as free and open source software as part of the Darwin operating system. XNU is an acronym for X is Not Unix.
Originally developed by NeXT for the NeXTSTEP operating system, XNU was a hybrid kernel combining version 2.5 of the Mach kernel developed at Carnegie Mellon University with components from 4.3BSD and an object-oriented API for writing drivers called Driver Kit.
After Apple acquired NeXT, the Mach component was upgraded to OSFMK 7.3, which is a microkernel. Apple uses a heavily modified OSFMK 7.3 functioning as a hybrid kernel with parts of FreeBSD included. (OSFMK 7.3 includes applicable code from the University of Utah Mach 4 kernel and applicable code from the many Mach 3.0 variants that sprouted off from the original Carnegie Mellon University Mach 3.0 kernel.) The BSD components were upgraded with code from the FreeBSD project and the Driver Kit was replaced with a C++ API for writing drivers called I/O Kit.
Like some other modern kernels, XNU is a hybrid, containing features of both monolithic and microkernels, attempting to make the best use of both technologies, such as the message passing capability of microkernels enabling greater modularity[citation needed] and larger portions of the OS to benefit from protected memory,[citation needed] as well as retaining the speed of monolithic kernels for certain critical tasks.
XNU runs on ARM as part of iOS,IA-32, and x86-64 based processors.
"
"33"," (Learn how and when to remove this template message)
In programming languages, a type system is a set of rules that assigns a property called type to the various constructs of a computer program, such as variables, expressions, functions or modules.  These types formalize and enforce the otherwise implicit categories the programmer uses for data structures and components (e.g. ""string"", ""array of float"", ""function returning boolean""). The main purpose of a type system is to reduce possibilities for bugs in computer programs by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of static and dynamic checking. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc.
A type system associates a type with each computed value and, by examining the flow of these values, attempts to ensure or prove that no type errors can occur. The given type system in question determines exactly what constitutes a type error, but in general the aim is to prevent operations expecting a certain kind of value from being used with values for which that operation does not make sense (logic errors). Type systems are often specified as part of programming languages, and built into the interpreters and compilers for them; although the type system of a language can be extended by optional tools that perform added kinds of checks using the language's original type syntax and grammar.
An example of a simple type system is that of the C language. The portions of a C program are the function definitions.  One function is invoked by another function.  The interface of a function states the name of the function and a list of values that are passed to the function's code.  The code of an invoking function states the name of the invoked, along with the names of variables that hold values to pass to it.  During execution, the values are placed into temporary storage, then execution jumps to the code of the invoked function.  The invoked function's code accesses the values and makes use of them.  If the instructions inside the function are written with the assumption of receiving an integer value, but the calling code passed a floating-point value, then the wrong result will be computed by the invoked function.  The C compiler checks the type declared for each variable sent, against the type declared for each variable in the interface of the invoked function.  If the types do not match, the compiler throws a compile-time error.
A compiler may also use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).
The depth of type constraints and the manner of their evaluation affect the typing of the language. A programming language may further associate an operation with various resolutions for each type, in the case of type polymorphism. Type theory is the study of type systems. The concrete types of some programming languages, such as integers and strings, depend on practical issues of computer architecture, compiler implementation, and language design.
Formally, type theory studies type systems. A programming language must have occurrence to type check using the type system whether at compile time or runtime, manually annotated or automatically inferred.  As Mark Manasse concisely put it:
Assigning a data type, termed typing, gives meaning to a sequence of bits such as a value in memory or some object such as a variable. The hardware of a general purpose computer is unable to discriminate between for example a memory address and an instruction code, or between a character, an integer, or a floating-point number, because it makes no intrinsic distinction between any of the possible values that a sequence of bits might mean.[note 1] Associating a sequence of bits with a type conveys that meaning to the programmable hardware to form a symbolic system composed of that hardware and some program.
A program associates each value with at least one specific type, but it also can occur that one value is associated with many subtypes.  Other entities, such as objects, modules, communication channels, and dependencies can become associated with a type.  Even a type can become associated with a type.  An implementation of a type system could in theory associate identifications called data type (a type of a value), class (a type of an object), and kind (a type of a type, or metatype). These are the abstractions that typing can go through, on a hierarchy of levels contained in a system.
When a programming language evolves a more elaborate type system, it gains a more finely grained rule set than basic type checking, but this comes at a price when the type inferences (and other properties) become undecidable, and when more attention must be paid by the programmer to annotate code or to consider computer-related operations and functioning. It is challenging to find a sufficiently expressive type system that satisfies all programming practices in a type safe manner.
The more type restrictions that are imposed by the compiler, the more strongly typed a programming language is.  Strongly typed languages often require the programmer to make explicit conversions in contexts where an implicit conversion would cause no harm.  Pascal's type system has been described as ""too strong"" because, for example, the size of an array or string is part of its type, making some programming tasks difficult.Haskell is also strongly typed but its types are automatically inferred so that explicit conversions are often (but not always) unnecessary.
A programming language compiler can also implement a dependent type or an effect system, which enables even more program specifications to be verified by a type checker. Beyond simple value-type pairs, a virtual ""region"" of code is associated with an ""effect"" component describing what is being done with what, and enabling for example to ""throw"" an error report.  Thus the symbolic system may be a type and effect system, which endows it with more safety checking than type checking alone.
Whether automated by the compiler or specified by a programmer, a type system makes program behavior illegal if outside the type-system rules. Advantages provided by programmer-specified type systems include:
Advantages provided by compiler-specified type systems include:
Type safety contributes to program correctness, but can only guarantee correctness at the cost of making the type checking itself an undecidable problem.[citation needed]  In a type system with automated type checking a program may prove to run incorrectly yet be safely typed, and produce no compiler errors. Division by zero is an unsafe and incorrect operation, but a type checker running at compile time only doesn't scan for division by zero in most languages, and then it is left as a runtime error. To prove the absence of these more-general-than-types defects, other kinds of formal methods, collectively known as program analyses, are in common use. Alternatively, a sufficiently expressive type system, such as in dependently typed languages, can prevent these kinds of errors (for example, expressing the type of non-zero numbers).  In addition software testing is an empirical method for finding errors that the type checker cannot detect.
The process of verifying and enforcing the constraints of types—type checking—may occur either at compile-time (a static check) or at run-time. If a language specification requires its typing rules strongly (i.e., more or less allowing only those automatic type conversions that do not lose information), one can refer to the process as strongly typed, if not, as weakly typed. The terms are not usually used in a strict sense.
Static type checking is the process of verifying the type safety of a program based on analysis of a program's text (source code). If a program passes a static type checker, then the program is guaranteed to satisfy some set of type safety properties for all possible inputs.
Static type checking can be considered a limited form of program verification (see type safety), and in a type-safe language, can be considered also an optimization. If a compiler can prove that a program is well-typed, then it does not need to emit dynamic safety checks, allowing the resulting compiled binary to run faster and to be smaller.
Static type checking for Turing-complete languages is inherently conservative. That is, if a type system is both sound (meaning that it rejects all incorrect programs) and decidable (meaning that it is possible to write an algorithm that determines whether a program is well-typed), then it must be incomplete (meaning there are correct programs, which are also rejected, even though they do not encounter runtime errors). For example, consider a program containing the code:
if <complex test> then <do something> else <generate type error>
Even if the expression <complex test> always evaluates to true at run-time, most type checkers will reject the program as ill-typed, because it is difficult (if not impossible) for a static analyzer to determine that the else branch will not be taken. Conversely, a static type checker will quickly detect type errors in rarely used code paths. Without static type checking, even code coverage tests with 100% coverage may be unable to find such type errors. The tests may fail to detect such type errors, because the combination of all places where values are created and all places where a certain value is used must be taken into account.
A number of useful and common programming language features cannot be checked statically, such as downcasting. Thus, many languages will have both static and dynamic type checking; the static type checker verifies what it can, and dynamic checks verify the rest.
Many languages with static type checking provide a way to bypass the type checker. Some languages allow programmers to choose between static and dynamic type safety. For example, C# distinguishes between statically-typed and dynamically-typed variables. Uses of the former are checked statically, whereas uses of the latter are checked dynamically. Other languages allow writing code that is not type-safe. For example, in C, programmers can freely cast a value between any two types that have the same size.
For a list of languages with static type checking, see the category for statically typed languages.
Dynamic type checking is the process of verifying the type safety of a program at runtime. Implementations of dynamically type-checked languages generally associate each runtime object with a type tag (i.e., a reference to a type) containing its type information. This runtime type information (RTTI) can also be used to implement dynamic dispatch, late binding, downcasting, reflection, and similar features.
Most type-safe languages include some form of dynamic type checking, even if they also have a static type checker.[citation needed] The reason for this is that many useful features or properties are difficult or impossible to verify statically. For example, suppose that a program defines two types, A and B, where B is a subtype of A. If the program tries to convert a value of type A to type B, which is known as downcasting, then the operation is legal only if the value being converted is actually a value of type B. Thus, a dynamic check is needed to verify that the operation is safe. This requirement is one of the criticisms of downcasting.
By definition, dynamic type checking may cause a program to fail at runtime. In some programming languages, it is possible to anticipate and recover from these failures. In others, type-checking errors are considered fatal.
Programming languages that include dynamic type checking but not static type checking are often called ""dynamically typed programming languages"". For a list of such languages, see the category for dynamically typed programming languages.
Some languages allow both static and dynamic typing (type checking), sometimes called soft typing. For example, Java and some other ostensibly statically typed languages support downcasting types to their subtypes, querying an object to discover its dynamic type, and other type operations that depend on runtime type information. More generally, most programming languages include mechanisms for dispatching over different 'kinds' of data, such as disjoint unions, subtype polymorphism, and variant types. Even when not interacting with type annotations or type checking, such mechanisms are materially similar to dynamic typing implementations. See programming language for more discussion of the interactions between static and dynamic typing.
Objects in object-oriented languages are usually accessed by a reference whose static target type (or manifest type) is equal to either the object's run-time type (its latent type) or a supertype thereof. This is conformant with the Liskov substitution principle, which states that all operations performed on an instance of a given type can also be performed on an instance of a subtype. This concept is also known as subsumption. In some languages subtypes may also possess covariant or contravariant return types and argument types respectively.
Certain languages, for example Clojure, Common Lisp, or Cython are dynamically type-checked by default, but allow programs to opt into static type checking by providing optional annotations. One reason to use such hints would be to optimize the performance of critical sections of a program. This is formalized by gradual typing. The programming environment DrRacket, a  pedagogic environment based on Lisp, and a precursor of the language Racket was also soft-typed.
Conversely, as of version 4.0, the C# language provides a way to indicate that a variable should not be statically type-checked. A variable whose type is dynamic will not be subject to static type checking. Instead, the program relies on runtime type information to determine how the variable may be used.
The choice between static and dynamic typing requires certain trade-offs.
Static typing can find type errors reliably at compile time, which should increase the reliability of the delivered program. However, programmers disagree over how commonly type errors occur, resulting in further disagreements over the proportion of those bugs that are coded that would be caught by appropriately representing the designed types in code. Static typing advocates[who?] believe programs are more reliable when they have been well type-checked, whereas dynamic-typing advocates[who?] point to distributed code that has proven reliable and to small bug databases.[citation needed] The value of static typing, then, presumably[vague] increases as the strength of the type system is increased. Advocates of dependent typing,[who?] implemented in languages such as Dependent ML and Epigram, have suggested that almost all bugs can be considered type errors, if the types used in a program are properly declared by the programmer or correctly inferred by the compiler.
Static typing usually results in compiled code that executes faster. When the compiler knows the exact data types that are in use (which is necessary for static verification, either through declaration or inference) it can produce optimized machine code. Some dynamically typed languages such as Common Lisp allow optional type declarations for optimization for this reason. 
By contrast, dynamic typing may allow compilers to run faster and interpreters to dynamically load new code, because changes to source code in dynamically typed languages may result in less checking to perform and less code to revisit.[clarification needed] This too may reduce the edit-compile-test-debug cycle.
Statically typed languages that lack type inference (such as C and Java) require that programmers declare the types that a method or function must use. This can serve as added program documentation, that is active and dynamic, instead of static. This allows a compiler to prevent it from drifting out of synchrony, and from being ignored by programmers. However, a language can be statically typed without requiring type declarations (examples include Haskell, Scala, OCaml, F#, and to a lesser extent C# and C++), so explicit type declaration is not a necessary requirement for static typing in all languages.
Dynamic typing allows constructs that some static type checking would reject as illegal. For example, eval functions, which execute arbitrary data as code, become possible. An eval function is possible with static typing, but requires advanced uses of algebraic data types. Further, dynamic typing better accommodates transitional code and prototyping, such as allowing a placeholder data structure (mock object) to be transparently used in place of a full data structure (usually for the purposes of experimentation and testing).
Dynamic typing typically allows duck typing (which enables easier code reuse). Many[specify] languages with static typing also feature duck typing or other mechanisms like generic programming that also enable easier code reuse.
Dynamic typing typically makes metaprogramming easier to use. For example, C++ templates are typically more cumbersome to write than the equivalent Ruby or Python code since C++ has stronger rules regarding type definitions (for both functions and variables). This forces a developer to write more boilerplate code for a template than a Python developer would need to. More advanced run-time constructs such as metaclasses and introspection are often harder to use in statically typed languages. In some languages, such features may also be used e.g. to generate new types and behaviors on the fly, based on run-time data. Such advanced constructs are often provided by dynamic programming languages; many of these are dynamically typed, although dynamic typing need not be related to dynamic programming languages.
Languages are often colloquially referred to as strongly typed or weakly typed. In fact, there is no universally accepted definition of what these terms mean. In general, there are more precise terms to represent the differences between type systems that lead people to call them ""strong"" or ""weak"".
A third way of categorizing the type system of a programming language uses the safety of typed operations and conversions. Computer scientists consider a language ""type-safe"" if it does not allow operations or conversions that violate the rules of the type system.
Some observers use the term memory-safe language (or just safe language) to describe languages that do not allow programs to access memory that has not been assigned for their use. For example, a memory-safe language will check array bounds, or else statically guarantee (i.e., at compile time before execution) that array accesses out of the array boundaries will cause compile-time and perhaps runtime errors.
Consider the following program of a language that is both type-safe and memory-safe:
In this example, the variable z will have the value 42. Although this may not be what the programmer anticipated, it is a well-defined result. If y were a different string, one that could not be converted to a number (e.g. ""Hello World""), the result would be well-defined as well. Note that a program can be type-safe or memory-safe and still crash on an invalid operation; in fact, if a program encounters an operation that is not type-safe, terminating the program is often the only option.
Now consider a similar example in C:
In this example z will point to a memory address five characters beyond y, equivalent to three characters after the terminating zero character of the string pointed to by y. This is memory that the program is not expected to access. It may contain garbage data, and it certainly doesn't contain anything useful. As this example shows, C is neither a memory-safe nor a type-safe language.
In general, type-safety and memory-safety go hand in hand. For example, a language that supports pointer arithmetic and number-to-pointer conversions (like C) is neither memory-safe nor type-safe, because it allows arbitrary memory to be accessed as if it were valid memory of any type.
For more information, see memory safety.
Some languages allow different levels of checking to apply to different regions of code. Examples include:
Additional tools such as lint and IBM Rational Purify can also be used to achieve a higher level of strictness.
It has been proposed, chiefly by Gilad Bracha, that the choice of type system be made independent of choice of language; that a type system should be a module that can be plugged into a language as needed. He believes this is advantageous, because what he calls mandatory type systems make languages less expressive and code more fragile. The requirement that types do not affect the semantics of the language is difficult to fulfill.
Optional typing is related to gradual typing, but still distinct from it.[better source needed]
The term polymorphism refers to the ability of code (especially, functions or classes) to act on values of multiple types, or to the ability of different instances of the same data structure to contain elements of different types. Type systems that allow polymorphism generally do so in order to improve the potential for code re-use: in a language with polymorphism, programmers need only implement a data structure such as a list or an associative array once, rather than once for each type of element with which they plan to use it. For this reason computer scientists sometimes call the use of certain forms of polymorphism generic programming. The type-theoretic foundations of polymorphism are closely related to those of abstraction, modularity and (in some cases) subtyping.
In duck typing, a statement calling a method m on an object does not rely on the declared type of the object; only that the object, of whatever type, must supply an implementation of the method called, when called, at run-time.
Duck typing differs from structural typing in that, if the part (of the whole module structure) needed for a given local computation is present at runtime, the duck type system is satisfied in its type identity analysis. On the other hand, a structural type system would require the analysis of the whole module structure at compile time to determine type identity or type dependence.
Duck typing differs from a nominative type system in a number of aspects. The most prominent ones are that for duck typing, type information is determined at runtime (as contrasted to compile time), and the name of the type is irrelevant to determine type identity or type dependence; only partial structure information is required for that for a given point in the program execution.
Duck typing uses the premise that (referring to a value) ""if it walks like a duck, and quacks like a duck, then it is a duck"" (this is a reference to the duck test that is attributed to James Whitcomb Riley). The term may have been coined [citation needed] by Alex Martelli in a 2000 message to the comp.lang.python newsgroup (see Python).
While one controlled experiment showed an increase in developer productivity for duck typing in single developer projects, other controlled experiments on API usability show the opposite.
Many type systems have been created that are specialized for use in certain environments with certain types of data, or for out-of-band static program analysis. Frequently, these are based on ideas from formal type theory and are only available as part of prototype research systems.
Dependent types are based on the idea of using scalars or values to more precisely describe the type of some other value. For example, matrix(3,3){\displaystyle \mathrm {matrix} (3,3)} might be the type of a 3×3{\displaystyle 3\times 3} matrix. We can then define typing rules such as the following rule for matrix multiplication:
matrixmultiply:matrix(k,m)×matrix(m,n)→matrix(k,n){\displaystyle \mathrm {matrix} _{\mathrm {multiply} }:\mathrm {matrix} (k,m)\times \mathrm {matrix} (m,n)\to \mathrm {matrix} (k,n)}
where k{\displaystyle k}, m{\displaystyle m}, n{\displaystyle n} are arbitrary positive integer values. A variant of ML called Dependent ML has been created based on this type system, but because type checking for conventional dependent types is undecidable, not all programs using them can be type-checked without some kind of limits. Dependent ML limits the sort of equality it can decide to Presburger arithmetic.
Other languages such as Epigram make the value of all expressions in the language decidable so that type checking can be decidable. However, in general proof of decidability is undecidable, so many programs require hand-written annotations that may be very non-trivial. As this impedes the development process, many language implementations provide an easy way out in the form of an option to disable this condition. This, however, comes at the cost of making the type-checker run in an infinite loop when fed programs that do not type-check, causing the compilation to fail.
Linear types, based on the theory of linear logic, and closely related to uniqueness types, are types assigned to values having the property that they have one and only one reference to them at all times. These are valuable for describing large immutable values such as files, strings, and so on, because any operation that simultaneously destroys a linear object and creates a similar object (such as 'str= str + ""a""') can be optimized ""under the hood"" into an in-place mutation. Normally this is not possible, as such mutations could cause side effects on parts of the program holding other references to the object, violating referential transparency. They are also used in the prototype operating system Singularity for interprocess communication, statically ensuring that processes cannot share objects in shared memory in order to prevent race conditions. The Clean language (a Haskell-like language) uses this type system in order to gain a lot of speed (compared to performing a deep copy) while remaining safe.
Intersection types are types describing values that belong to both of two other given types with overlapping value sets. For example, in most implementations of C the signed char has range -128 to 127 and the unsigned char has range 0 to 255, so the intersection type of these two types would have range 0 to 127. Such an intersection type could be safely passed into functions expecting either signed or unsigned chars, because it is compatible with both types.
Intersection types are useful for describing overloaded function types: For example, if ""int → int"" is the type of functions taking an integer argument and returning an integer, and ""float → float"" is the type of functions taking a float argument and returning a float, then the intersection of these two types can be used to describe functions that do one or the other, based on what type of input they are given. Such a function could be passed into another function expecting an ""int → int"" function safely; it simply would not use the ""float → float"" functionality.
In a subclassing hierarchy, the intersection of a type and an ancestor type (such as its parent) is the most derived type. The intersection of sibling types is empty.
The Forsythe language includes a general implementation of intersection types. A restricted form is refinement types.
Union types are types describing values that belong to either of two types. For example, in C, the signed char has a -128 to 127 range, and the unsigned char has a 0 to 255 range, so the union of these two types would have an overall ""virtual"" range of -128 to 255 that may be used partially depending on which union member is accessed. Any function handling this union type would have to deal with integers in this complete range. More generally, the only valid operations on a union type are operations that are valid on both types being unioned. C's ""union"" concept is similar to union types, but is not typesafe, as it permits operations that are valid on either type, rather than both. Union types are important in program analysis, where they are used to represent symbolic values whose exact nature (e.g., value or type) is not known.
In a subclassing hierarchy, the union of a type and an ancestor type (such as its parent) is the ancestor type. The union of sibling types is a subtype of their common ancestor (that is, all operations permitted on their common ancestor are permitted on the union type, but they may also have other valid operations in common).
Existential types are frequently used in connection with record types to represent modules and abstract data types, due to their ability to separate implementation from interface. For example, the type ""T = ∃X { a: X; f: (X → int); }"" describes a module interface that has a data member named a of type X and a function named f that takes a parameter of the same type X and returns an integer. This could be implemented in different ways; for example:
These types are both subtypes of the more general existential type T and correspond to concrete implementation types, so any value of one of these types is a value of type T. Given a value ""t"" of type ""T"", we know that ""t.f(t.a)"" is well-typed, regardless of what the abstract type X is. This gives flexibility for choosing types suited to a particular implementation while clients that use only values of the interface type—the existential type—are isolated from these choices.
In general it's impossible for the typechecker to infer which existential type a given module belongs to. In the above example intT { a: int; f: (int → int); } could also have the type ∃X { a: X; f: (int → int); }. The simplest solution is to annotate every module with its intended type, e.g.:
Although abstract data types and modules had been implemented in programming languages for quite some time, it wasn't until 1988 that John C. Mitchell and Gordon Plotkin established the formal theory under the slogan: ""Abstract [data] types have existential type"". The theory is a second-order typed lambda calculus similar to System F, but with existential instead of universal quantification.
Gradual typing is a type system in which variables may be typed either at compile-time (which is static typing) or at run-time (which is dynamic typing), allowing software developers to choose either type paradigm as appropriate, from within a single language.  In particular, gradual typing uses a special type named dynamic to represent statically-unknown types, and gradual typing replaces the notion of type equality with a new relation called consistency that relates the dynamic type to every other type. The consistency relation is symmetric but not transitive.
Many static type systems, such as those of C and Java, require type declarations: The programmer must explicitly associate each variable with a specific type. Others, such as Haskell's, perform type inference: The compiler draws conclusions about the types of variables based on how programmers use those variables. For example, given a function f(x, y)
 that adds x
 and y
 together, the compiler can infer that x
 and y
 must be numbers – since addition is only defined for numbers. Thus, any call to f
 elsewhere in the program that specifies a non-numeric type (such as a string or list) as an argument would signal an error.
Numerical and string constants and expressions in code can and often do imply type in a particular context. For example, an expression 3.14
 might imply a type of floating-point, while [1, 2, 3]
 might imply a list of integers – typically an array.
Type inference is in general possible, if it is decidable in the type theory in question. Moreover, even if inference is undecidable in general for a given type theory, inference is often possible for a large subset of real-world programs. Haskell's type system, a version of Hindley–Milner, is a restriction of System Fω to so-called rank-1 polymorphic types, in which type inference is decidable. Most Haskell compilers allow arbitrary-rank polymorphism as an extension, but this makes type inference undecidable. (Type checking is decidable, however, and rank-1 programs still have type inference; higher rank polymorphic programs are rejected unless given explicit type annotations.)
Some languages like Perl 6 or C# have a unified type system. This means that all C# types including primitive types inherit from a single root object. Every type in C# inherits from the Object class. Java has several primitive types that are not objects. Java provides wrapper object types that exist together with the primitive types so developers can use either the wrapper object types or the simpler non-object primitive types.
A type-checker for a statically typed language must verify that the type of any expression is consistent with the type expected by the context in which that expression appears. For example, in an assignment statement of the form x := e,
the inferred type of the expression e must be consistent with the declared or inferred type of the variable x. This notion of consistency, called compatibility, is specific to each programming language.
If the type of e and the type of x are the same, and assignment is allowed for that type, then this is a valid expression. Thus, in the simplest type systems, the question of whether two types are compatible reduces to that of whether they are equal (or equivalent). Different languages, however, have different criteria for when two type expressions are understood to denote the same type. These different equational theories of types vary widely, two extreme cases being structural type systems, in which any two types that describe values with the same structure are equivalent, and nominative type systems, in which no two syntactically distinct type expressions denote the same type (i.e., types must have the same ""name"" in order to be equal).
In languages with subtyping, the compatibility relation is more complex. In particular, if A is a subtype of B, then a value of type A can be used in a context where one of type B is expected, even if the reverse is not true. Like equivalence, the subtype relation is defined differently for each programming language, with many variations possible. The presence of parametric or ad hoc polymorphism in a language may also have implications for type compatibility."
"34","In programming language theory, subtyping (also subtype polymorphism or inclusion polymorphism) is a form of type polymorphism in which a subtype is a datatype that is related to another datatype (the supertype) by some notion of substitutability, meaning that program elements, typically subroutines or functions, written to operate on elements of the supertype can also operate on elements of the subtype. If S is a subtype of T, the subtyping relation is often written S <: T, to mean that any term of type S can be safely used in a context where a term of type T is expected. The precise semantics of subtyping crucially depends on the particulars of what ""safely used in a context where"" means in a given programming language. The type system of a programming language essentially defines its own subtyping relation, which may well be trivial [clarification needed].
Due to the subtyping relation, a term may belong to more than one type.  Subtyping is therefore a form of type polymorphism. In object-oriented programming the term 'polymorphism' is commonly used to refer solely to this subtype polymorphism,   while the techniques of parametric polymorphism would be considered generic programming.
Functional programming languages often allow the subtyping of records. Consequently, simply typed lambda calculus extended with record types is perhaps the simplest theoretical setting in which a useful notion of subtyping may be defined and studied [citation needed]. Because the resulting calculus allows terms to have more than one type, it is no longer a ""simple"" type theory. Since functional programming languages, by definition, support function literals, which can also be stored in records, records types with subtyping provide some of the features of object-oriented programming. Typically, functional programming languages also provide some, usually restricted, form of parametric polymorphism. In a theoretical setting, it is desirable to study the interaction of the two features; a common theoretical setting is system F<:. Various calculi that attempt to capture the theoretical properties of object-oriented programming may be derived from system F<:.
The concept of subtyping is related to the linguistic notions of hyponymy and holonymy. It is also related to the concept of bounded quantification in mathematical logic. Subtyping should not be confused with the notion of (class or object) inheritance from object-oriented languages; subtyping is a relation between types (interfaces in object-oriented parlance) whereas inheritance is a relation between implementations stemming from a language feature that allows new objects to be created from existing ones. In a number of object-oriented languages, subtyping is called interface inheritance, with inheritance referred to as implementation inheritance.
The notion of subtyping in programming languages dates back to the 1960s; it was introduced in Simula derivatives. The first formal treatments of subtyping were given by John C. Reynolds in 1980 who used category theory to formalize implicit conversions, and Luca Cardelli (1985).
The concept of subtyping has gained visibility (and synonymy with polymorphism in some circles) with the mainstream adoption of object-oriented programming. In this context, the principle of safe substitution is often called the Liskov substitution principle, after Barbara Liskov who popularized it in a keynote address at a conference on object-oriented programming in 1987. Because it must consider mutable objects, the ideal notion of subtyping defined by Liskov and Jeannette Wing, called behavioral subtyping is considerably stronger than what can be implemented in a type checker. (See Function types below for details.)
A simple practical example of subtypes is shown in the diagram, right. The type ""bird"" has three subtypes ""duck"", ""cuckoo"" and ""ostrich"". Conceptually, each of these is a variety of the basic ""bird"" that inherits many ""bird"" characteristics but has some specific differences. The UML notation is used in this diagram, with open-headed arrows showing the direction and type of the relationship between the supertype and its subtypes.
As a more practical example, a language might allow integer values to be used wherever floating point values are expected (Integer <: Float), or it might define a generic type Number as a common supertype of integers and the reals. In this second case, we only have Integer <: Number and Float <: Number, but Integer and Float are not subtypes of each other.
Programmers may take advantage of subtyping to write code in a more abstract manner than would be possible without it. Consider the following example:
If integer and real are both subtypes of Number, and an operator of comparison with an arbitrary Number is defined for both types, then values of either type can be passed to this function. However, the very possibility of implementing such an operator highly constrains the Number type (for example, one can't compare an integer with a complex number), and actually only comparing integers with integers and reals with reals makes sense. Rewriting this function so that it would only accept 'x' and 'y' of the same type requires bounded polymorphism.
Subtyping in type theory is characterized by the fact that any expression of type A may also be given type B if A<:B; the formal typing rule that codifies this is known as the subsumption rule.
Type theorists make a distinction between nominal subtyping, in which only types declared in a certain way may be subtypes of each other, and structural subtyping, in which the structure of two types determines whether or not one is a subtype of the other.  The class-based object-oriented subtyping described above is nominal; a structural subtyping rule for an object-oriented language might say that if objects of type A can handle all of the messages that objects of type B can handle (that is, if they define all the same methods), then A is a subtype of B regardless of whether either inherits from the other.  This so-called duck typing is common in dynamically typed object-oriented languages.  Sound structural subtyping rules for types other than object types are also well known.[citation needed]
Implementations of programming languages with subtyping fall into two general classes: inclusive implementations, in which the representation of any value of type A also represents the same value at type B if A<:B, and coercive implementations, in which a value of type A can be automatically converted into one of type B.  The subtyping induced by subclassing in an object-oriented language is usually inclusive; subtyping relations that relate integers and floating-point numbers, which are represented differently, are usually coercive.
In almost all type systems that define a subtyping relation, it is reflexive (meaning A<:A for any type A) and transitive (meaning that if A<:B and B<:C then A<:C).  This makes it a preorder on types.
Types of records give rise to the concepts of width and depth subtyping. These express two different ways of obtaining a new type of record that allows the same operations as the original record type.
Recall that a record is a collection of (named) fields. Since a subtype is a type which allows all operations allowed on the original type, a record subtype should support the same operations on the fields as the original type supported.
One kind of way to achieve such support, called width subtyping, adds more fields to the record. More formally, every (named) field appearing in the width supertype will appear in the width subtype. Thus, any operation feasible on the supertype will be supported by the subtype.
The second method, called depth subtyping, replaces the various fields with their subtypes. That is, the fields of the subtype are subtypes of the fields of the supertype. Since any operation supported for a field in the supertype is supported for its subtype, any operation feasible on the record supertype is supported by the record subtype. Depth subtyping only makes sense for immutable records: for example, you can assign 1.5 to the 'x' field of a real point (a record with two real fields), but you can't do the same to the 'x' field of an integer point (which, however, is a deep subtype of the real point type) because 1.5 is not an integer (see Variance).
Subtyping of records can be defined in System F<:, which combines parametric polymorphism with subtyping of record types and is a theoretical basis for many functional programming languages that support both features.
Some systems also support subtyping of labeled disjoint union types (such as algebraic data types).  The rule for width subtyping is reversed: every tag appearing in the width subtype must appear in the width supertype.
If T1 → T2 is a function type, then a subtype of it is any function S1 → S2 with the property that T1 <: S1 and S2 <: T2. The argument type of S1 → S2 is said to be contravariant because the subtyping relation is reversed for it, whereas the return type is covariant. Informally, this reversal occurs because the refined type is ""more liberal"" in the types it accepts and ""more conservative"" in the type it returns. This is what exactly works in Scala: a n-ary function is internally a class that inherits the FunctionN(-A1, -A2, …, -An, +B) trait (which can be seen as a general interface in Java-like languages), where A1, A2, … An are the parameter types, and B is its return type; ""-"" before the type means the type is contravariant while ""+"" means covariant.
In languages that allow side effects, like most object-oriented languages, subtyping is generally not sufficient to guarantee that a function can be safely used in the context of another. Liskov's work in this area focused on behavioral subtyping, which besides the type system safety discussed in this article also requires that subtypes preserve all invariants guaranteed by the supertypes in some contract. This definition of subtyping is generally undecidable, so it cannot be verified by a type checker.
The subtyping of mutable references is similar to the treatment of function arguments and return values.  Write-only references (or sinks) are contravariant, like function arguments; read-only references (or sources) are covariant, like return values.  Mutable references which act as both sources and sinks are invariant.
In coercive subtyping systems, subtypes are defined by implicit type conversion functions from subtype to supertype.  For each subtyping relationship (S <: T), a coercion function coerce: S → T is provided, and any object s of type S is regarded as the object coerceS → T(s) of type T.  A coercion function may be defined by composition: if S <: T and T <: U then s may be regarded as an object of type u under the compound coercion (coerceT → U ∘ coerceS → T).  The type coercion from a type to itself coerceT → T is the identity function idT
Coercion functions for records and disjoint union subtypes may be defined componentwise; in the case of width-extended records, type coercion simply discards any components which are not defined in the supertype.  The type coercion for function types may be given by f'(s) = coerceS2 → T2(f(coerceT1 → S1(t))), reflecting the contravariance of function arguments and covariance of return values.
The coercion function is uniquely determined given the subtype and supertype.  Thus, when multiple subtyping relationships are defined, one must be careful to guarantee that all type coercions are coherent.  For instance, if an integer such as 2 : int can be coerced to a floating point number (say, 2.0 : float), then it is not admissible to coerce 2.1 : float to 2 : int, because the compound coercion coercefloat → float given by coerceint → float ∘ coercefloat → int would then be distinct from the identity coercion idfloat.
Textbooks
Papers"
"35","In programming languages, ad-hoc polymorphism is a kind of polymorphism in which polymorphic functions can be applied to arguments of different types, because a polymorphic function can denote a number of distinct and potentially heterogeneous implementations depending on the type of argument(s) to which it is applied. It is also known as function overloading or operator overloading. The term ad hoc in this context is not intended to be pejorative; it refers simply to the fact that this type of polymorphism is not a fundamental feature of the type system. This is in contrast to parametric polymorphism, in which polymorphic functions are written without mention of any specific type, and can thus apply a single abstract implementation to any number of types in a transparent way. This classification was introduced by Christopher Strachey in 1967.
Ad hoc polymorphism is a dispatch mechanism: control moving through one named function is dispatched to various other functions without having to specify the exact function being called. Overloading allows multiple functions taking different types to be defined with the same name; the compiler or interpreter automatically ensures that the right function is called. This way, functions appending lists of integers, lists of strings, lists of real numbers, and so on could be written, and all be called append—and the right append function would be called based on the type of lists being appended. This differs from parametric polymorphism, in which the function would need to be written generically, to work with any kind of list. Using overloading, it is possible to have a function perform two completely different things based on the type of input passed to it; this is not possible with parametric polymorphism. Another way to look at overloading is that a routine is uniquely identified not by its name, but by the combination of its name and the number, order and types of its parameters.
This type of polymorphism is common in object-oriented programming languages, many of which allow operators to be overloaded in a manner similar to functions (see operator overloading). Some languages that are not dynamically typed and lack ad hoc  polymorphism (including type classes) have longer function names such as print_int, print_string, etc. This can be seen as advantage (more descriptive) or a disadvantage (overly verbose) depending on one's point of view.
An advantage that is sometimes gained from overloading is the appearance of specialization, e.g., a function with the same name can be implemented in multiple different ways, each optimized for the particular data types that it operates on. This can provide a convenient interface for code that needs to be specialized to multiple situations for performance reasons. The downside is that the type system cannot guarantee the consistency of the different implementations.
Since overloading is done at compile time, it is not a substitute for late binding as found in subtyping polymorphism.
The previous section notwithstanding, there are other ways in which ad hoc polymorphism can work out. Consider for example the Smalltalk language. In Smalltalk, the overloading is done at run time, as the methods (""function implementation"") for each overloaded message (""overloaded function"") are resolved when they are about to be executed. This happens at run time, after the program is compiled. Therefore, polymorphism is given by subtyping polymorphism as in other languages, and it is also extended in functionality by ad hoc polymorphism at run time.
A closer look will also reveal that Smalltalk provides a slightly different variety of ad hoc polymorphism. Since Smalltalk has a late bound execution model, and since it provides objects the ability to handle messages that are not understood, it is possible to go ahead and implement functionality using polymorphism without explicitly overloading a particular message. This may not be generally recommended practice for everyday programming, but it can be quite useful when implementing proxies.
Also, while in general terms common class method and constructor overloading is not considered polymorphism, there are more uniform languages in which classes are regular objects. In Smalltalk, for instance, classes are regular objects. In turn, this means messages sent to classes can be overloaded, and it is also possible to create objects that behave like classes without their classes inheriting from the hierarchy of classes. These are effective techniques which can be used to take advantage of Smalltalk's powerful reflection capabilities. Similar arrangements are also possible in languages such as Self and Newspeak.
Imagine an operator + that may be used in the following ways:
To handle these six function calls, four different pieces of code are needed—or three, if strings are considered to be lists of characters:
Thus, the name + actually refers to three or four completely different functions. This is an example of overloading.
(Note that string types used in the last case do not, by themselves, lend themselves to the programmer naturally assuming concatenation, rather than addition, is meant; consider ""123"" + ""456"", which might reasonably be expected to yield ""579"". Overloading can therefore provide different meaning, or semantics, for an operation, as well as differing implementations.)"
"36","In programming languages and type theory, parametric polymorphism is a way to make a language more expressive, while still maintaining full static type-safety. Using parametric polymorphism, a function or a data type can be written generically so that it can handle values identically without depending on their type. Such functions and data types are called generic functions and generic datatypes respectively and form the basis of generic programming.
For example, a function append that joins two lists can be constructed so that it does not care about the type of elements: it can append lists of integers, lists of real numbers, lists of strings, and so on. Let the type variable a denote the type of elements in the lists. Then append can be typed 
where [a] denotes the type of lists with elements of type a. We say that the type of append is parameterized by a for all values of a. (Note that since there is only one type variable, the function cannot be applied to just any pair of lists: the pair, as well as the result list, must consist of the same type of elements.) For each place where append is applied, a value is decided for a.
Following Christopher Strachey, parametric polymorphism may be contrasted with ad hoc polymorphism, in which a single polymorphic function can have a number of distinct and potentially heterogeneous implementations depending on the type of argument(s) to which it is applied. Thus, ad hoc polymorphism can generally only support a limited number of such distinct types, since a separate implementation has to be provided for each type.
Parametric polymorphism was first introduced to programming languages in ML in 1975. Today it exists in Standard ML, OCaml, F#, Ada, Haskell, Mercury, Visual Prolog, Scala, Julia, and others. Java, C#, Visual Basic .NET and Delphi have each introduced ""generics"" for parametric polymorphism. Some implementations of type polymorphism are superficially similar to parametric polymorphism while also introducing ad hoc aspects. One example is C++ template specialization.
The most general form of polymorphism is ""higher-rank impredicative polymorphism"". Two popular restrictions of this form are restricted rank polymorphism (for example, rank-1 or prenex polymorphism) and predicative polymorphism. Together, these restrictions give ""predicative prenex polymorphism"", which is essentially the form of polymorphism found in ML and early versions of Haskell.
In a prenex polymorphic system, type variables may not be instantiated with polymorphic types. This is very similar to what is called ""ML-style"" or ""Let-polymorphism"" (technically ML's Let-polymorphism has a few other syntactic restrictions).
This restriction makes the distinction between polymorphic and non-polymorphic types very important; thus in predicative systems polymorphic types are sometimes referred to as type schemas to distinguish them from ordinary (monomorphic) types, which are sometimes called monotypes. A consequence is that all types can be written in a form that places all quantifiers at the outermost (prenex) position.
For example, consider the append function described above, which has type 
In order to apply this function to a pair of lists, a type must be substituted for the variable a in the type of the function such that the type of the arguments matches up with the resulting function type. In an impredicative system, the type being substituted may be any type whatsoever, including a type that is itself polymorphic; thus append can be applied to pairs of lists with elements of any type—even to lists of polymorphic functions such as append itself.
Polymorphism in the language ML and its close relatives is predicative. This is because predicativity, together with other restrictions, makes the type system simple enough that type inference is possible. In languages where explicit type annotations are necessary when applying a polymorphic function, the predicativity restriction is less important; thus these languages are generally impredicative.
For some fixed value k, rank-k polymorphism is a system in which a quantifier may not appear to the left of k or more arrows (when the type is drawn as a tree).
Type inference for rank-2 polymorphism is decidable, but reconstruction for rank-3 and above is not.
Rank-n polymorphism is polymorphism in which quantifiers may appear to the left of arbitrarily many arrows.
In a predicative parametric polymorphic system, a type τ{\displaystyle \tau } containing a type variable α{\displaystyle \alpha } may not be used in such a way that α{\displaystyle \alpha } is instantiated to a polymorphic type. Predicative type theories include Martin-Löf Type Theory and NuPRL.
Impredicative polymorphism (also called first-class polymorphism) is the most powerful form of parametric polymorphism. A definition is said to be impredicative if it is self-referential; in type theory this allows the instantiation of a variable in a type τ{\displaystyle \tau } with any type, including polymorphic types, such as τ{\displaystyle \tau } itself. An example of this is the System F with the type variable X in the type T=∀X.X→X{\displaystyle T=\forall X.X\to X}, where X could even refer to T itself.
In type theory, the most frequently studied impredicative typed λ-calculi are based on those of the lambda cube, especially System F.
In 1985, Luca Cardelli and Peter Wegner recognized the advantages of allowing bounds on the type parameters. Many operations require some knowledge of the data types, but can otherwise work parametrically. For example, to check whether an item is included in a list, we need to compare the items for equality. In Standard ML, type parameters of the form ’’a are restricted so that the equality operation is available, thus the function would have the type ’’a × ’’a list → bool and ’’a can only be a type with defined equality. In Haskell, bounding is achieved by requiring types to belong to a type class; thus the same function has the type Eqα⇒α→[α]→Bool{\displaystyle {\scriptstyle Eq\,\alpha \,\Rightarrow \alpha \,\rightarrow \left[\alpha \right]\rightarrow Bool}} in Haskell. In most object-oriented programming languages that support parametric polymorphism, parameters can be constrained to be subtypes of a given type (see Subtype polymorphism and the article on Generic programming)."
"37","In type theory, bounded quantification (also bounded polymorphism or constrained genericity) refers to universal or existential quantifiers which are restricted (""bounded"") to range only over the subtypes of a particular type. Bounded quantification is an interaction of parametric polymorphism with subtyping. Bounded quantification has traditionally been studied in the functional setting of System F<:, but is available in modern object-oriented languages supporting parametric polymorphism (generics) such as Java, C# and Scala.
The purpose of bounded quantification is to allow for polymorphic functions to depend on some specific behaviour of objects instead of type inheritance. It assumes a record-based model for object classes, where every class member is a record element and all class members are named functions. Object attributes are represented as functions that take no argument and return an object. The specific behaviour is then some function name along with the types of the arguments and the return type. Bounded quantification allows to considers all objects with such a function. An example would be a polymorphic min function that considers all objects that are comparable to each other.
F-bounded quantification or recursively bounded quantification, introduced in 1989, allows for more precise typing of functions that are applied on recursive types. A recursive type is one that includes a function that uses it as a type for some argument or its return value.
This kind of type constraint can be expressed in Java with a generic interface. The following example demonstrates how to describe types that can be compared to each other and use this as typing information in polymorphic functions. The Test.min function uses simple bounded quantification and does not preserve the type of the assigned types, in contrast with the Test.Fmin function which uses F-bounded quantification.
In mathematical notation, the types of the two functions are
where"
"38","In computer programming, a parameter (often called formal parameter or formal argument) is a special kind of variable, used in a subroutine to refer to one of the pieces of data provided as input to the subroutine.[lower-alpha 1] These pieces of data are the values of the arguments (often called actual arguments or actual parameters) with which the subroutine is going to be called/invoked. An ordered list of parameters is usually included in the definition of a subroutine, so that, each time the subroutine is called, its arguments for that call are evaluated, and the resulting values can be assigned to the corresponding parameters.
Unlike argument in usual mathematical usage, the argument in computer science is thus the actual input expression passed/supplied to a function, procedure, or routine in the invokation/call statement, whereas the parameter is the variable inside the implementation of the subroutine. For example, if one defines the add subroutine as def add(x, y): return x + y, then x, y are parameters, while if this is called as add(2, 3), then 2, 3 are the arguments. Note that variables (and expressions thereof) from the calling context can be arguments: if the subroutine is called as a = 2; b = 3; add(a, b) then the variables a, b are the arguments, not the values 2, 3. See the Parameters and arguments section for more information.
In the most common case, call by value, a parameter acts within the subroutine as a new local variable initialized to the value of the argument (a local (isolated) copy of the argument if the argument is a variable), but in other cases, e.g. call by reference, the argument variable supplied by the caller can be affected by actions within the called subroutine (as discussed in evaluation strategy). 
The semantics for how parameters can be declared and how the (value of) arguments are passed to the parameters of subroutines are defined by the language, but the details of how this is represented in any particular computer system depend on the calling conventions of that system.
The following program in the C programming language defines a function that is named ""sales_tax"" and has one parameter named ""price"". The type of price is ""double"" (i.e. a double-precision floating point number). The function's return type is also a double.
After the function has been defined, it can be invoked as follows:
In this example, the function has been invoked with the argument 10.00. When this happens, 10.00 will be assigned to price, and the function begins calculating its result. The steps for producing the result are specified below, enclosed in {}. 0.05 * price indicates that the first thing to do is multiply 0.05 by the value of price, which gives 0.50. return means the function will produce the result of 0.05 * price. Therefore, the final result (ignoring possible round-off errors one encounters with representing decimal fractions in IEEE-754 format) is 0.50.
The terms parameter and argument may have different meanings in different programming languages. Sometimes they are used interchangeably, and the context is used to distinguish the meaning.  The term parameter (sometimes called formal parameter) is often used to refer to the variable as found in the function definition, while argument (sometimes called actual parameter) refers to the actual input supplied at function call. For example, if one defines a function as def f(x): ..., then x is the parameter, and if it is called by a = ...; f(a) then a is the argument. A parameter is an (unbound) variable, while the argument can be a value or variable or more complex expression involving values and variables. In case of call by value, what is passed to the function is the value of the argument – for example, f(2) and a = 2; f(a) are equivalent calls – while in call by reference, with a variable as argument, what is passed is a reference to that variable - even though the syntax for the function call could stay the same. The specification for pass-by-reference or pass-by-value would be made in the function declaration and/or definition.
Parameters appear in procedure definitions; arguments appear in procedure calls. In the function definition f(x) = x*x the variable x is a parameter; in the function call f(2) the value 2 is the argument of the function. Loosely, a parameter is a type, and an argument is an instance.
A parameter is an intrinsic property of the procedure, included in its definition. For example, in many languages, a procedure to add two supplied integers together and calculate the sum would need two parameters, one for each integer. In general, a procedure may be defined with any number of parameters, or no parameters at all. If a procedure has parameters, the part of its definition that specifies the parameters is called its parameter list.
By contrast, the arguments are the expressions supplied to the procedure when it is called, usually one expression matching one of the parameters. Unlike the parameters, which form an unchanging part of the procedure's definition, the arguments may vary from call to call. Each time a procedure is called, the part of the procedure call that specifies the arguments is called the argument list.
Although parameters are also commonly referred to as arguments, arguments are sometimes thought of as the actual values or references assigned to the parameter variables when the subroutine is called at run-time. When discussing code that is calling into a subroutine, any values or references passed into the subroutine are the arguments, and the place in the code where these values or references are given is the parameter list. When discussing the code inside the subroutine definition, the variables in the subroutine's parameter list are the parameters, while the values of the parameters at runtime are the arguments. For example, in C, when dealing with threads it's common to pass in an argument of type void* and cast it to an expected type:
To better understand the difference, consider the following function written in C:
The function sum has two parameters, named addend1 and addend2. It adds the values passed into the parameters, and returns the result to the subroutine's caller (using a technique automatically supplied by the C compiler).
The code which calls the sum function might look like this:
The variables value1 and value2 are initialized with values. value1 and value2 are both arguments to the sum function in this context.
At runtime, the values assigned to these variables are passed to the function sum as arguments. In the sum function, the parameters addend1 and addend2 are evaluated, yielding the arguments 40 and 2, respectively. The values of the arguments are added, and the result is returned to the caller, where it is assigned to the variable sumValue.
Because of the difference between parameters and arguments, it is possible to supply inappropriate arguments to a procedure. The call may supply too many or too few arguments; one or more of the arguments may be a wrong type; or arguments may be supplied in the wrong order. Any of these situations causes a mismatch between the parameter and argument lists, and the procedure will often return an unintended answer or generate a runtime error.
Within the Eiffel software development method and language, the terms argument and parameter have distinct uses established by convention. The term argument is used exclusively in reference to a routine's inputs, and the term parameter is used exclusively in type parameterization for generic classes.
Consider the following routine definition:
The routine sum takes two arguments addend1 and addend2, which are called the routine's formal arguments.  A call to sum specifies actual arguments, as shown below with value1 and value2.
Parameters are also thought of as either formal or actual. Formal generic parameters are used in the definition of generic classes. In the example below, the class HASH_TABLE  is declared as a generic class which has two formal generic parameters, G representing data of interest and K representing the hash key for the data:
When a class becomes a client to HASH_TABLE, the formal generic parameters are substituted with actual generic parameters in a generic derivation. In the following attribute declaration, my_dictionary is to be used as a character string based dictionary. As such, both data and key formal generic parameters are substituted with actual generic parameters of type STRING.
In strongly typed programming languages, each parameter's type must be specified in the procedure declaration.  Languages using type inference attempt to discover the types automatically from the function's body and usage. Dynamically typed programming languages defer type resolution until run-time. Weakly typed languages perform little to no type resolution, relying instead on the programmer for correctness.
Some languages use a special keyword (e.g. void) to indicate that the subroutine has no parameters; in formal type theory, such functions take an empty parameter list (whose type is not void, but rather unit).
The exact mechanism for assigning arguments to parameters, called argument passing, depends upon the evaluation strategy used for that parameter (typically call by value), which may be specified using keywords.
Some programming languages such as Ada, C++, Clojure, Common Lisp, Fortran 90, Python, Ruby, Tcl, and Windows PowerShell allow for a default argument to be explicitly or implicitly given in a subroutine's declaration. This allows the caller to omit that argument when calling the subroutine. If the default argument is explicitly given, then that value is used if it is not provided by the caller. If the default argument is implicit (sometimes by using a keyword such as Optional) then the language provides a well-known value (such as null, Empty, zero, an empty string, etc.) if a value is not provided by the caller.
PowerShell example:
Default arguments can be seen as a special case of the variable-length argument list.
Some languages allow subroutines to be defined to accept a variable number of arguments. For such languages, the subroutines must iterate through the list of arguments.
PowerShell example:
Some programming languages—such as Ada and Windows PowerShell—allow subroutines to have named parameters. This allows the calling code to be more self-documenting. It also provides more flexibility to the caller, often allowing the order of the arguments to be changed, or for arguments to be omitted as needed.
PowerShell example:
In lambda calculus, each function has exactly one parameter. What is thought of as functions with multiple parameters is usually represented in lambda calculus as a function which takes the first argument, and returns a function which takes the rest of the arguments; this is a transformation known as currying. Some programming languages, like ML and Haskell, follow this scheme. In these languages, every function has exactly one parameter, and what may look like the definition of a function of multiple parameters, is actually syntactic sugar for the definition of a function that returns a function, etc. Function application is left-associative in these languages as well as in lambda calculus, so what looks like an application of a function to multiple arguments is correctly evaluated as the function applied to the first argument, then the resulting function applied to the second argument, etc.
An output parameter, also known as an out parameter or return parameter, is a parameter used for output, rather than the more usual use for input. Using call by reference parameters, or call by value parameters where the value is a reference, as output parameters is an idiom in some languages, notably C and C++,[lower-alpha 2] while other languages have built-in support for output parameters. Languages with built-in support for output parameters include Ada (see Ada subprograms), Fortran (since Fortran 90; see Fortran ""intent""), various procedural extensions to SQL, such as PL/SQL (see PL/SQL functions) and Transact-SQL, C# and the .NET Framework, and the scripting language TScript (see TScript function declarations).
More precisely, one may distinguish three types of parameters or parameter modes: input parameters, output parameters, and input/output parameters; these are often denoted in, out, and in out or inout. An input argument (the argument to an input parameter) must be a value, such as an initialized variable or literal, and must not be redefined or assigned to; an output argument must be an assignable variable, but it need not be initialized, any existing value is not accessible, and must be assigned a value; and an input/output argument must be an initialized, assignable variable, and can optionally be assigned a value. The exact requirements and enforcement vary between languages – for example, in Ada 83 output parameters can only be assigned to, not read, even after assignment (this was removed in Ada 95 to remove the need for an auxiliary accumulator variable). These are analogous to the notion of a value in an expression being an r-value (has a value), an l-value (can be assigned), or an r-value/l-value (has a value and can be assigned), respectively, though these terms have specialized meanings in C.
In some cases only input and input/output are distinguished, with output being considered a specific use of input/output, and in other cases only input and output (but not input/output) are supported. The default mode varies between languages: in Fortran 90 input/output is default, while in C# and SQL extensions input is default, and in TScript each parameter is explicitly specified as input or output.
Syntactically, parameter mode is generally indicated with a keyword in the function declaration, such as void f(out int x) in C#. Conventionally output parameters are often put at the end of the parameter list to clearly distinguish them, though this is not always followed. TScript uses a different approach, where in the function declaration input parameters are listed, then output parameters, separated by a colon (:) and there is no return type to the function itself, as in this function, which computes the size of a text fragment:
Parameter modes are a form of denotational semantics, stating the programmer's intent and allowing compilers to catch errors and apply optimizations – they do not necessarily imply operational semantics (how the parameter passing actually occurs). Notably, while input parameters can be implemented by call by value, and output and input/output parameters by call by reference – and this is a straightforward way to implement these modes in languages without built-in support – this is not always how they are implemented. This distinction is discussed in detail in the Ada '83 Rationale, which emphasizes that the parameter mode is abstracted from which parameter passing mechanism (by reference or by copy) is actually implemented. For instance, while in C# input parameters (default, no keyword) are passed by value, and output and input/output parameters (out and ref) are passed by reference, in PL/SQL input parameters (IN) are passed by reference, and output and input/output parameters (OUT and IN OUT) are by default passed by value and the result copied back, but can be passed by reference by using the NOCOPY compiler hint.
A syntactically similar construction to output parameters is to assign the return value to a variable with the same name as the function. This is found in Pascal and Fortran 66 and Fortran 77, as in this Pascal example:
This is semantically different in that when called, the function is simply evaluated – it is not passed a variable from the calling scope to store the output in.
The primary use of output parameters is to return multiple values from a function, while the use of input/output parameters is to modify state using parameter passing (rather than by shared environment, as in global variables). An important use of returning multiple values is to solve the semipredicate problem of returning both a value and an error status – see Semipredicate problem: Multivalued return.
For example, to return two variables from a function in C, one may write:
where x is an input parameter and width and height are output parameters, passed by reference.
A common use case in C and related languages is for exception handling, where a function places the return value in an output variable, and returns a boolean corresponding to whether the function succeeded or not. An archetypal example is the TryParse method in .NET, especially C#, which parses a string into an integer, returning true on success and false on failure. This has the following signature:
and may be used as follows:
Similar considerations apply to returning a value of one of several possible types, where the return value can specify the type and then value is stored in one of several output variables.
Another use is as a micro-optimization, to avoid assigning a local variable in a function and then needing to copy it when returning. This can be done when output parameters are implemented by call by reference. For example, in C++, instead of the more usual:
one might instead write:
so the function f does not need to assign space for the object or copy it on returning.
Output parameters are often discouraged in modern programming, essentially as being awkward, confusing, and too low-level – commonplace return values are considerably easier to understand and work with. Notably, output parameters involve functions with side effects (modifying the output parameter) and are semantically similar to references, which are more confusing than pure functions and values, and the distinction between output parameters and input/output parameters can be subtle. Further, since in common programming styles most parameters are simply input parameters, output parameters and input/output parameters are unusual and hence susceptible to misunderstanding.
Output and input/output parameters prevent function composition, since the output is stored in variables, rather than in the value of an expression. Thus one must initially declare a variable, and then each step of a chain of functions must be a separate statement. For example, in C++ the following function composition:
when written with output and input/output parameters instead becomes (for f it is an output parameter, for g an input/output parameter):
In the special case of a function with a single output or input/output parameter and no return value, function composition is possible if the output or input/output parameter (or in C/C++, its address) is also returned by the function, in which case the above becomes:
There are various alternatives to the use cases of output parameters.
For returning multiple values from a function, an alternative is to return a tuple. Syntactically this is clearer if automatic sequence unpacking and parallel assignment can be used, as in Go or Python, such as:
For returning a value of one of several types, a tagged union can be used instead; the most common cases are nullable types (option types), where the return value can be null to indicate failure. For exception handling, one can return a nullable type, or raise an exception. For example, in Python one might have either:
or, more idiomatically:
The micro-optimization of not requiring a local variable and copying the return when using output variables can also be applied to conventional functions and return values by sufficiently sophisticated compilers.
The usual alternative to output parameters in C and related languages is to return a single data structure containing all return values. For example, given a structure encapsulating width and height, one can write:
In object-oriented languages, instead of using input/output parameters, one can often use call by sharing, passing a reference to an object and then mutating the object, though not changing which object the variable refers to."
"39"," (Learn how and when to remove this template message)
C++11 is a version of the standard for the programming language C++. It was approved by International Organization for Standardization (ISO) on 12 August 2011, replacing C++03, superseded by C++14 on 18 August 2014 and later, by C++17. The name follows the tradition of naming language versions by the publication year of the specification, though it was formerly named C++0x because it was expected to be published before 2010.
Although one of the design goals was to prefer changes to the libraries over changes to the core language,  C++11 does make several additions to the core language. Areas of the core language that were significantly improved include multithreading support, generic programming support, uniform initialization, and performance. Significant changes were also made to the C++ Standard Library, incorporating most of the C++ Technical Report 1 (TR1) libraries, except the library of mathematical special functions.
C++11 was published as ISO/IEC 14882:2011 in September 2011 and is available for a fee. The working draft most similar to the published C++11 standard is N3337, dated 16 January 2012; it has only editorial corrections from the C++11 standard.
The design committee attempted to stick to a number of goals in designing C++11:
Attention to beginners is considered important, because most computer programmers will always be such, and because many beginners never widen their knowledge, limiting themselves to work in aspects of the language in which they specialize.
One function of the C++ committee is the development of the language core. Areas of the core language that were significantly improved include multithreading support, generic programming support, uniform initialization, and performance.
These language features primarily exist to provide some kind of performance benefit, either of memory or of computational speed.[citation needed]
In C++03 (and before), temporaries (termed ""rvalues"", as they often lie on the right side of an assignment) were intended to never be modifiable — just as in C — and were considered to be indistinguishable from const T& types; nevertheless, in some cases, temporaries could have been modified, a behavior that was even considered to be a useful loophole. C++11 adds a new non-const reference type called an rvalue reference, identified by T&&. This refers to temporaries that are permitted to be modified after they are initialized, for the purpose of allowing ""move semantics"".
A chronic performance problem with C++03 is the costly and unneeded deep copies that can happen implicitly when objects are passed by value. To illustrate the issue, consider that an std::vector<T> is, internally, a wrapper around a C-style array with a size.  If an std::vector<T> temporary is created or returned from a function, it can be stored only by creating a new std::vector<T> and copying all the rvalue's data into it.  Then the temporary and all its memory is destroyed. (For simplicity, this discussion neglects the return value optimization.)
In C++11, a move constructor of std::vector<T> that takes an rvalue reference to an std::vector<T> can copy the pointer to the internal C-style array out of the rvalue into the new std::vector<T>, then set the pointer inside the rvalue to null.  Since the temporary will never again be used, no code will try to access the null pointer, and because the pointer is null, its memory is not deleted when it goes out of scope.  Hence, the operation not only forgoes the expense of a deep copy, but is safe and invisible.
Rvalue references can provide performance benefits to existing code without needing to make any changes outside the standard library.  The type of the returned value of a function returning an std::vector<T> temporary does not need to be changed explicitly to std::vector<T> && to invoke the move constructor, as temporaries are considered rvalues automatically.  (However, if std::vector<T> is a C++03 version without a move constructor, then the copy constructor will be invoked with an const std::vector<T>&, incurring a significant memory allocation.)
For safety reasons, some restrictions are imposed.  A named variable will never be considered to be an rvalue even if it is declared as such. To get an rvalue, the function template std::move() should be used.  Rvalue references can also be modified only under certain circumstances, being intended to be used primarily with move constructors.
Due to the nature of the wording of rvalue references, and to some modification to the wording for lvalue references (regular references), rvalue references allow developers to provide perfect function forwarding. When combined with variadic templates, this ability allows for function templates that can perfectly forward arguments to another function that takes those particular arguments. This is most useful for forwarding constructor parameters, to create factory functions that will automatically call the correct constructor for those particular arguments. This is seen in the emplace_back set of the C++ standard library methods.
C++ has always had the concept of constant expressions. These are expressions such as 3+4 that will always yield the same results, at compile time and at run time. Constant expressions are optimization opportunities for compilers, and compilers frequently execute them at compile time and hardcode the results in the program. Also, in several places, the C++ specification requires using constant expressions. Defining an array requires a constant expression, and enumerator values must be constant expressions.
However, a constant expression has never been allowed to contain a function call or object constructor. So a piece of code as simple as this is invalid:
This was not valid in C++03, because get_five() + 7 is not a constant expression. A C++03 compiler has no way of knowing if get_five() actually is constant at runtime. In theory, this function could affect a global variable, call other non-runtime constant functions, etc.
C++11 introduced the keyword constexpr, which allows the user to guarantee that a function or object constructor is a compile-time constant. The above example can be rewritten as follows:
This allows the compiler to understand, and verify, that get_five() is a compile-time constant.
Using constexpr on a function imposes some limits on what that function can do. First, the function must have a non-void return type. Second, the function body cannot declare variables or define new types. Third, the body may contain only declarations, null statements and a single return statement. There must exist argument values such that, after argument substitution, the expression in the return statement produces a constant expression.
Before C++11, the values of variables could be used in constant expressions only if the variables are declared const, have an initializer which is a constant expression, and are of integral or enumeration type. C++11 removes the restriction that the variables must be of integral or enumeration type if they are defined with the constexpr keyword:
Such data variables are implicitly const, and must have an initializer which must be a constant expression.
To construct constant expression data values from user-defined types, constructors can also be declared with constexpr. A constexpr constructor's function body can contain only declarations and null statements, and cannot declare variables or define types, as with a constexpr function. There must exist argument values such that, after argument substitution, it initializes the class's members with constant expressions. The destructors for such types must be trivial.
The copy constructor for a type with any constexpr constructors should usually also be defined as a constexpr constructor, to allow objects of the type to be returned by value from a constexpr function. Any member function of a class, such as copy constructors, operator overloads, etc., can be declared as constexpr, so long as they meet the requirements for constexpr functions. This allows the compiler to copy objects at compile time, perform operations on them, etc.
If a constexpr function or constructor is called with arguments which aren't constant expressions, the call behaves as if the function were not constexpr, and the resulting value is not a constant expression. Likewise, if the expression in the return statement of a constexpr function does not evaluate to a constant expression for a given invocation, the result is not a constant expression.
In C++03, a class or struct must follow a number of rules for it to be considered a plain old data (POD) type. Types that fit this definition produce object layouts that are compatible with C, and they could also be initialized statically. The C++03 standard has restrictions on what types are compatible with C or can be statically initialized despite there being no technical reason a compiler couldn't accept the program; if someone were to create a C++03 POD type and add a non-virtual member function, this type would no longer be a POD type, could not be statically initialized, and would be incompatible with C despite no change to the memory layout.
C++11 relaxed several of the POD rules, by dividing the POD concept into two separate concepts: trivial and standard-layout.
A type that is trivial can be statically initialized. It also means that it is valid to copy data around via memcpy, rather than having to use a copy constructor. The lifetime of a trivial type begins when its storage is defined, not when a constructor completes.
A trivial class or struct is defined as one that:
Constructors are trivial only if there are no virtual member functions of the class and no virtual base classes. Copy/move operations also require all non-static data members to be trivial.
A type that is standard-layout means that it orders and packs its members in a way that is compatible with C. A class or struct is standard-layout, by definition, provided:
A class/struct/union is considered POD if it is trivial, standard-layout, and all of its non-static data members and base classes are PODs.
By separating these concepts, it becomes possible to give up one without losing the other. A class with complex move and copy constructors may not be trivial, but it could be standard-layout and thus interoperate with C. Similarly, a class with public and private non-static data members would not be standard-layout, but it could be trivial and thus memcpy-able.
In C++03, the compiler must instantiate a template whenever a fully specified template is encountered in a translation unit. If the template is instantiated with the same types in many translation units, this can dramatically increase compile times.  There is no way to prevent this in C++03, so C++11 introduced extern template declarations, analogous to extern data declarations.
C++03 has this syntax to oblige the compiler to instantiate a template:
C++11 now provides this syntax:
which tells the compiler not to instantiate the template in this translation unit.
These features exist for the primary purpose of making the language easier to use. These can improve type safety, minimize code repetition, make erroneous code less likely, etc.
C++03 inherited the initializer-list feature from C. A struct or array is given a list of arguments in braces, in the order of the members' definitions in the struct. These initializer-lists are recursive, so an array of structs or struct containing other structs can use them.
This is very useful for static lists, or initializing a struct to some value. C++ also provides constructors to initialize an object, but they are often not as convenient as the initializer list. However, C++03 allows initializer-lists only on structs and classes that conform to the Plain Old Data (POD) definition; C++11 extends initializer-lists, so they can be used for all classes including standard containers like std::vector.
C++11 binds the concept to a template, called std::initializer_list. This allows constructors and other functions to take initializer-lists as parameters. For example:
This allows SequenceClass to be constructed from a sequence of integers, such as:
This constructor is a special kind of constructor, called an initializer-list-constructor. Classes with such a constructor are treated specially during uniform initialization (see below)
The class std::initializer_list<> is a first-class C++11 standard library type. However, they can be initially constructed statically by the C++11 compiler only via use of the {} syntax. The list can be copied once constructed, though this is only a copy-by-reference. An initializer list is constant; its members cannot be changed once the initializer list is created, nor can the data in those members be changed.
Because initializer_list is a real type, it can be used in other places besides class constructors. Regular functions can take typed initializer lists as arguments. For example:
Standard containers can also be initialized in these ways:
C++03 has a number of problems with initializing types. Several ways to do this exist, and some produce different results when interchanged. The traditional constructor syntax, for example, can look like a function declaration, and steps must be taken to ensure that the compiler's most vexing parse rule will not mistake it for such. Only aggregates and POD types can be initialized with aggregate initializers (using SomeType var = {/*stuff*/};).
C++11 provides a syntax that allows for fully uniform type initialization that works on any object. It expands on the initializer list syntax:
The initialization of var1 behaves exactly as though it were aggregate-initialization. That is, each data member of an object, in turn, will be copy-initialized with the corresponding value from the initializer-list. Implicit type conversion will be used where needed. If no conversion exists, or only a narrowing conversion exists, the program is ill-formed. The initialization of var2 invokes the constructor.
One can also do this:
Uniform initialization does not replace constructor syntax, which is still needed at times. If a class has an initializer list constructor (TypeName(initializer_list<SomeType>);), then it takes priority over other forms of construction, provided that the initializer list conforms to the sequence constructor's type. The C++11 version of std::vector has an initializer list constructor for its template type. Thus this code:
will call the initializer list constructor, not the constructor of std::vector that takes a single size parameter and creates the vector with that size. To access the latter constructor, the user will need to use the standard constructor syntax directly.
In C++03 (and C), to use a variable, its type must be specified explicitly. However, with the advent of template types and template metaprogramming techniques, the type of something, particularly the well-defined return value of a function, may not be easily expressed. Thus, storing intermediates in variables is difficult, possibly needing knowledge of the internals of a given metaprogramming library.
C++11 allows this to be mitigated in two ways. First, the definition of a variable with an explicit initialization can use the auto keyword. This creates a variable of the specific type of the initializer:
The type of some_strange_callable_type is simply whatever the particular template function override of std::bind returns for those particular arguments. This type is easily determined procedurally by the compiler as part of its semantic analysis duties, but is not easy for the user to determine upon inspection.
The type of other_variable is also well-defined, but it is easier for the user to determine. It is an int, which is the same type as the integer literal.
Further, the keyword decltype can be used to determine the type of expression at compile-time. For example:
This is more useful in conjunction with auto, since the type of auto variable is known only to the compiler. However, decltype can also be very useful for expressions in code that makes heavy use of operator overloading and specialized types.
auto is also useful for reducing the verbosity of the code. For instance, instead of writing
the programmer can use the shorter
which can be further compacted since ""myvec"" implements begin/end iterators:
This difference grows as the programmer begins to nest containers, though in such cases typedefs are a good way to decrease the amount of code.
The type denoted by decltype can be different from the type deduced by auto.
C++11 extends the syntax of the for statement to allow for easy iteration over a range of elements:
This form of for, called the “range-based for”, will iterate over each element in the list. It will work for C-style arrays, initializer lists, and any type that has begin() and end() functions defined for it that return iterators. All the standard library containers that have begin/end pairs will work with the range-based for statement.
C++11 provides the ability to create anonymous functions, called lambda functions.
These are defined as follows:
The return type (-> int in this example) can be omitted as long as all return expressions return the same type.
A lambda can optionally be a closure.
Standard C function declaration syntax was perfectly adequate for the feature set of the C language. As C++ evolved from C, it kept the basic syntax and extended it where needed. However, as C++ grew more complex, it exposed several limits, especially regarding template function declarations. For example, in C++03 this is disallowed:
The type Ret is whatever the addition of types Lhs and Rhs will produce. Even with the aforementioned C++11 functionality of decltype, this is not possible:
This is not valid C++ because lhs and rhs have not yet been defined; they will not be valid identifiers until after the parser has parsed the rest of the function prototype.
To work around this, C++11 introduced a new function declaration syntax, with a trailing-return-type:
This syntax can be used for more mundane function declarations and definitions:
Use of the keyword “auto” in this case is only part of the syntax and does not perform automatic type deduction.
In C++03, constructors of a class are not allowed to call other constructors in an initializer list of that class. Each constructor must construct all of its class members itself or call a common member function, as follows:
Constructors for base classes cannot be directly exposed to derived classes; each derived class must implement constructors even if a base class constructor would be appropriate. Non-constant data members of classes cannot be initialized at the site of the declaration of those members. They can be initialized only in a constructor.
C++11 provides solutions to all of these problems.
C++11 allows constructors to call other peer constructors (termed delegation).  This allows constructors to utilize another constructor's behavior with a minimum of added code. Delegation has been used in other languages e.g., Java, Objective-C.
This syntax is as follows:
Notice that, in this case, the same effect could have been achieved by making new_number a defaulting parameter.  The new syntax, however, allows the default value (42) to be expressed in the implementation rather than the interface — a benefit to maintainers of library code since default values for function parameters are “baked in” to call sites, whereas constructor delegation allows the value to be changed without recompilation of the code using the library.
This comes with a caveat: C++03 considers an object to be constructed when its constructor finishes executing, but C++11 considers an object constructed once any constructor finishes execution. Since multiple constructors will be allowed to execute, this will mean that each delegating constructor will be executing on a fully constructed object of its own type. Derived class constructors will execute after all delegation in their base classes is complete.
For base-class constructors, C++11 allows a class to specify that base class constructors will be inherited. Thus, the C++11 compiler will generate code to perform the inheritance and the forwarding of the derived class to the base class. This is an all-or-nothing feature: either all of that base class's constructors are forwarded or none of them are. Also, an inherited constructor will be shadowed if it matches the signature of a constructor of the derived class, and restrictions exist for multiple inheritance: class constructors cannot be inherited from two classes that use constructors with the same signature.
The syntax is as follows:
For member initialization, C++11 allows this syntax:
Any constructor of the class will initialize value with 5, if the constructor does not override the initialization with its own. So the above empty constructor will initialize value as the class definition states, but the constructor that takes an int will initialize it to the given parameter.
It can also use constructor or uniform initialization, instead of the assignment initialization shown above.
In C++03, it is possible to accidentally create a new virtual function, when one intended to override a base class function. For example:
Suppose the Derived::some_func is intended to replace the base class version. But instead, because it has a different signature, it creates a second virtual function. This is a common problem, particularly when a user goes to modify the base class.
C++11 provides syntax to solve this problem.
The override special identifier means that the compiler will check the base class(es) to see if there is a virtual function with this exact signature. And if there is not, the compiler will indicate an error.
C++11 also adds the ability to prevent inheriting from classes or simply preventing overriding methods in derived classes. This is done with the special identifier final. For example:
In this example, the virtual void f() final; statement declares a new virtual function, but it also prevents derived classes from overriding it. It also has the effect of preventing derived classes from using that particular function name and parameter combination.
Note that neither override nor final are language keywords. They are technically identifiers for declarator attributes:
For the purposes of this section and this section alone, every occurrence of “0” is meant as “a constant expression which evaluates to 0, which is of type int”. In reality, the constant expression can be of any integral type.
Since the dawn of C in 1972, the constant 0 has had the double role of constant integer and null pointer constant. The ambiguity inherent in the double meaning of 0 was dealt with in C by using the preprocessor macro NULL, which commonly expands to either ((void*)0) or 0. C++ forbids implicit conversion from void * to other pointer types, thus removing the benefit of casting 0 to void *. As a consequence, only 0 is allowed as a null pointer constant. This interacts poorly with function overloading:
If NULL is defined as 0 (which is usually the case in C++), the statement foo(NULL); will call foo(int), which is almost certainly not what the programmer intended, and not what a superficial reading of the code suggests.
C++11 corrects this by introducing a new keyword to serve as a distinguished null pointer constant: nullptr. It is of type nullptr_t, which is implicitly convertible and comparable to any pointer type or pointer-to-member type. It is not implicitly convertible or comparable to integral types, except for bool. While the original proposal specified that an rvalue of type nullptr_t should not be convertible to bool, the core language working group decided that such a conversion would be desirable, for consistency with regular pointer types. The proposed wording changes were unanimously voted into the Working Paper in June 2008.
For backwards compatibility reasons, 0 remains a valid null pointer constant.
In C++03, enumerations are not type-safe. They are effectively integers, even when the enumeration types are distinct. This allows the comparison between two enum values of different enumeration types. The only safety that C++03 provides is that an integer or a value of one enum type does not convert implicitly to another enum type. Further, the underlying integral type is implementation-defined; code that depends on the size of the enumeration is thus non-portable. Lastly, enumeration values are scoped to the enclosing scope. Thus, it is not possible for two separate enumerations in the same scope to have matching member names.
C++11 allows a special classification of enumeration that has none of these issues. This is expressed using the enum class (enum struct is also accepted as a synonym) declaration:
This enumeration is type-safe. Enum class values are not implicitly converted to integers. Thus, they cannot be compared to integers either (the expression Enumeration::Val4 == 101 gives a compile error).
The underlying type of enum classes is always known. The default type is int; this can be overridden to a different integral type as can be seen in this example:
With old-style enumerations the values are placed in the outer scope. With new-style enumerations they are placed within the scope of the enum class name. So in the above example, Val1 is undefined, but Enum2::Val1 is defined.
There is also a transitional syntax to allow old-style enumerations to provide explicit scoping, and the definition of the underlying type:
In this case the enumerator names are defined in the enumeration's scope (Enum3::Val1), but for backwards compatibility they are also placed in the enclosing scope.
Forward-declaring enums are also possible in C++11. Formerly, enum types could not be forward-declared because the size of the enumeration depends on the definition of its members. As long as the size of the enumeration is specified either implicitly or explicitly, it can be forward-declared:
C++03's parser defines “>>” as the right shift operator or stream extraction operator in all cases. However, with nested template declarations, there is a tendency for the programmer to neglect to place a space between the two right angle brackets, thus causing a compiler syntax error.
C++11 improves the specification of the parser so that multiple right angle brackets will be interpreted as closing the template argument list where it is reasonable. This can be overridden by using parentheses around parameter expressions using the “>”, “>=” or “>>” binary operators:
C++98 added the explicit keyword as a modifier on constructors to prevent single-argument constructors from being used as implicit type conversion operators. However, this does nothing for actual conversion operators. For example, a smart pointer class may have an operator bool() to allow it to act more like a primitive pointer: if it includes this conversion, it can be tested with if (smart_ptr_variable) (which would be true if the pointer was non-null and false otherwise). However, this allows other, unintended conversions as well. Because C++ bool is defined as an arithmetic type, it can be implicitly converted to integral or even floating-point types, which allows for mathematical operations that are not intended by the user.
In C++11, the explicit keyword can now be applied to conversion operators. As with constructors, it prevents using those conversion functions in implicit conversions. However, language contexts that specifically need a boolean value (the conditions of if-statements and loops, and operands to the logical operators) count as explicit conversions and can thus use a bool conversion operator.
For example, this feature solves cleanly the safe bool issue.
In C++03, it is possible to define a typedef only as a synonym for another type, including a synonym for a template specialization with all actual template arguments specified. It is not possible to create a typedef template. For example:
This will not compile.
C++11 adds this ability with this syntax:
The using syntax can be also used as type aliasing in C++11:
In C++03, there are restrictions on what types of objects can be members of a union. For example, unions cannot contain any objects that define a non-trivial constructor or destructor. C++11 lifts some of these restrictions.
If a union member has a non trivial special member function, the compiler will not generate the equivalent member function for the union and it must be manually defined.
This is a simple example of a union permitted in C++11:
The changes will not break any existing code since they only relax current rules.
These features allow the language to do things that were formerly impossible, exceedingly verbose, or needed non-portable libraries.
In C++11, templates can take variable numbers of template parameters. This also allows the definition of type-safe variadic functions.
C++03 offers two kinds of string literals. The first kind, contained within double quotes, produces a null-terminated array of type const char. The second kind, defined as L"""", produces a null-terminated array of type const wchar_t, where wchar_t is a wide-character of undefined size and semantics. Neither literal type offers support for string literals with UTF-8, UTF-16, or any other kind of Unicode encodings.
The definition of the type char has been modified to explicitly express that it's at least the size needed to store an eight-bit coding of UTF-8, and large enough to contain any member of the compiler's basic execution character set. It was formerly defined as only the latter in the C++ standard itself, then relying on the C standard to guarantee at least 8 bits.
C++11 supports three Unicode encodings: UTF-8, UTF-16, and UTF-32. Along with the formerly noted changes to the definition of char, C++11 adds two new character types: char16_t and char32_t. These are designed to store UTF-16 and UTF-32 respectively.
Creating string literals for each of these encodings can be done thusly:
The type of the first string is the usual const char[]. The type of the second string is const char16_t[] (note lower case 'u' prefix). The type of the third string is const char32_t[] (upper case 'U' prefix).
When building Unicode string literals, it is often useful to insert Unicode codepoints directly into the string. To do this, C++11 allows this syntax:
The number after the \u is a hexadecimal number; it does not need the usual 0x prefix. The identifier \u represents a 16-bit Unicode codepoint; to enter a 32-bit codepoint, use \U and a 32-bit hexadecimal number. Only valid Unicode codepoints can be entered. For example, codepoints on the range U+D800–U+DFFF are forbidden, as they are reserved for surrogate pairs in UTF-16 encodings.
It is also sometimes useful to avoid escaping strings manually, particularly for using literals of XML files, scripting languages, or regular expressions. C++11 provides a raw string literal:
In the first case, everything between the ""( and the )"" is part of the string. The "" and \ characters do not need to be escaped. In the second case, the ""delimiter( starts the string, and it ends only when )delimiter"" is reached. The string delimiter can be any string up to 16 characters in length, including the empty string. This string cannot contain spaces, control characters, (, ), or the \ character. Using this delimiter string allows the user to have ) characters within raw string literals. For example, R""delimiter((a-z))delimiter"" is equivalent to ""(a-z)"".
Raw string literals can be combined with the wide literal or any of the Unicode literal prefixes:
C++03 provides a number of literals. The characters 12.5 are a literal that is resolved by the compiler as a type double with the value of 12.5. However, the addition of the suffix f, as in 12.5f, creates a value of type float that contains the value 12.5. The suffix modifiers for literals are fixed by the C++ specification, and C++03 code cannot create new literal modifiers.
By contrast, C++11 enables the user to define new kinds of literal modifiers that will construct objects based on the string of characters that the literal modifies.
Transformation of literals is redefined into two distinct phases: raw and cooked. A raw literal is a sequence of characters of some specific type, while the cooked literal is of a separate type. The C++ literal 1234, as a raw literal, is this sequence of characters '1', '2', '3', '4'. As a cooked literal, it is the integer 1234. The C++ literal 0xA in raw form is '0', 'x', 'A', while in cooked form it is the integer 10.
Literals can be extended in both raw and cooked forms, with the exception of string literals, which can be processed only in cooked form. This exception is due to the fact that strings have prefixes that affect the specific meaning and type of the characters in question.
All user-defined literals are suffixes; defining prefix literals is not possible.  All suffixes starting with any character except underscore (_) are reserved by the standard. Thus, all user-defined literals must have suffixes starting with an underscore (_).
User-defined literals processing the raw form of the literal are defined via a literal operator, which is written as operator """". An example follows:
The assignment statement OutputType some_variable = 1234_mysuffix; executes the code defined by the user-defined literal function. This function is passed ""1234"" as a C-style string, so it has a null terminator.
An alternative mechanism for processing integer and floating point raw literals is via a variadic template:
This instantiates the literal processing function as operator """" _tuffix<'1', '2', '3', '4'>(). In this form, there is no null character terminating the string. The main purpose for doing this is to use C++11's constexpr keyword to ensure that the compiler will transform the literal entirely at compile time, assuming OutputType is a constexpr-constructible and copyable type, and the literal processing function is a constexpr function.
For numeric literals, the type of the cooked literal is either unsigned long long for integral literals or long double for floating point literals. (Note: There is no need for signed integral types because a sign-prefixed literal is parsed as an expression containing the sign as a unary prefix operator and the unsigned number.) There is no alternative template form:
In accord with the formerly mentioned new string prefixes, for string literals, these are used:
There is no alternative template form. Character literals are defined similarly.
C++11 standardizes support for multithreaded programming.
There are two parts involved: a memory model which allows multiple threads to co-exist in a program and library support for interaction between threads. (See this article's section on threading facilities.)
The memory model defines when multiple threads may access the same memory location, and specifies when updates by one thread become visible to other threads.
In a multi-threaded environment, it is common for every thread to have some unique variables. This already happens for the local variables of a function, but it does not happen for global and static variables.
A new thread-local storage duration (in addition to the existing static, dynamic and automatic) is indicated by the storage specifier thread_local.
Any object which could have static storage duration (i.e., lifetime spanning the entire execution of the program) may be given thread-local duration instead. The intent is that like any other static-duration variable, a thread-local object can be initialized using a constructor and destroyed using a destructor.
In C++03, the compiler provides, for classes that do not provide them for themselves, a default constructor, a copy constructor, a copy assignment operator (operator=), and a destructor. The programmer can override these defaults by defining custom versions. C++ also defines several global operators (such as operator new) that work on all classes, which the programmer can override.
However, there is very little control over creating these defaults. Making a class inherently non-copyable, for example, requires declaring a private copy constructor and copy assignment operator and not defining them. Attempting to use these functions is a violation of the One Definition Rule (ODR). While a diagnostic message is not required, violations may result in a linker error.
In the case of the default constructor, the compiler will not generate a default constructor if a class is defined with any constructors. This is useful in many cases, but it is also useful to be able to have both specialized constructors and the compiler-generated default.
C++11 allows the explicit defaulting and deleting of these special member functions. For example, this type explicitly declares that it is using the default constructor:
Alternatively, certain features can be explicitly disabled. For example, this type is non-copyable:
The = delete specifier can be used to prohibit calling any function, which can be used to disallow calling a member function with particular parameters. For example:
An attempt to call f() with an int will be rejected by the compiler, instead of performing a silent conversion to double. This can be generalized to disallow calling the function with any type other than double as follows:
In C++03, the largest integer type is long int. It is guaranteed to have at least as many usable bits as int. This resulted in long int having size of 64 bits on some popular implementations and 32 bits on others. C++11 adds a new integer type long long int to address this issue. It is guaranteed to be at least as large as a long int, and have no fewer than 64 bits. The type was originally introduced by C99 to the standard C, and most C++ compilers supported it as an extension already.
C++03 provides two methods to test assertions: the macro assert and the preprocessor directive #error. However, neither is appropriate for use in templates: the macro tests the assertion at execution-time, while the preprocessor directive tests the assertion during preprocessing, which happens before instantiation of templates. Neither is appropriate for testing properties that are dependent on template parameters.
The new utility introduces a new way to test assertions at compile-time, using the new keyword static_assert.
The declaration assumes this form:
Here are some examples of how static_assert can be used:
When the constant expression is false the compiler produces an error message. The first example is similar to the preprocessor directive #error, although the preprocessor does only support integral types. In contrast, in the second example the assertion is checked at every instantiation of the template class Check.
Static assertions are useful outside of templates also. For instance, a given implementation of an algorithm might depend on the size of a long long being larger than an int, something the standard does not guarantee. Such an assumption is valid on most systems and compilers, but not all.
In C++03, the sizeof operator can be used on types and objects. But it cannot be used to do this:
This should return the size of OtherType. C++03 disallows this, so it is a compile error. C++11 allows it. It is also allowed for the alignof operator introduced in C++11.
C++11 allows variable alignment to be queried and controlled with alignof and alignas.
The alignof operator takes the type and returns the power of 2 byte boundary on which the type instances must be allocated (as a std::size_t).  When given a reference type alignof returns the referenced type's alignment; for arrays it returns the element type's alignment.
The alignas specifier controls the memory alignment for a variable. The specifier takes a constant or a type; when supplied a type alignas(T) is shorthand for alignas(alignof(T)).  For example, to specify that a char array should be properly aligned to hold a float:
Prior C++ standards provided for programmer-driven garbage collection via set_new_handler, but gave no definition of object reachability for the purpose of automatic garbage collection. C++11 defines conditions under which pointer values are ""safely derived"" from other values. An implementation may specify that it operates under strict pointer safety, in which case pointers that are not derived according to these rules can become invalid.
C++11 provides a standardized syntax for compiler/tool extensions to the language. Such extensions were traditionally specified using #pragma directive or vendor-specific keywords (like __attribute__ for GNU and __declspec for Microsoft). With the new syntax, added information can be specified in a form of an attribute enclosed in double square brackets. An attribute can be applied to various elements of source code:
In the example above, attribute attr1 applies to the type of variable i, attr2 and attr3 apply to the variable itself, attr4 applies to the if statement and vendor::attr5 applies to the return statement. In general (but with some exceptions), an attribute specified for a named entity is placed after the name, and before the entity otherwise, as shown above, several attributes may be listed inside one pair of double square brackets, added arguments may be provided for an attribute, and attributes may be scoped by vendor-specific attribute namespaces.
It is recommended that attributes have no language semantic meaning and do not change the sense of a program when ignored. Attributes can be useful for providing information that, for example, helps the compiler to issue better diagnostics or optimize the generated code.
C++11 provides two standard attributes itself: noreturn to specify that a function does not return, and carries_dependency to help optimizing multi-threaded code by indicating that function arguments or return value carry a dependency.[clarification needed]
A number of new features were introduced in the C++11 standard library. Many of these could have been implemented under the old standard, but some rely (to a greater or lesser extent) on new C++11 core features.
A large part of the new libraries was defined in the document C++ Standards Committee's Library Technical Report (called TR1), which was published in 2005. Various full and partial implementations of TR1 are currently available using the namespace std::tr1. For C++11 they were moved to namespace std. However, as TR1 features were brought into the C++11 standard library, they were upgraded where appropriate with C++11 language features that were not available in the initial TR1 version. Also, they may have been enhanced with features that were possible under C++03, but were not part of the original TR1 specification.
C++11 offers a number of new language features that the currently existing standard library components can benefit from. For example, most standard library containers can benefit from Rvalue reference based move constructor support, both for quickly moving heavy containers around and for moving the contents of those containers to new memory locations. The standard library components were upgraded with new C++11 language features where appropriate. These include, but are not necessarily limited to:
Further, much time has passed since the prior C++ standard. Much code using the standard library has been written. This has revealed parts of the standard libraries that could use some improving. Among the many areas of improvement considered were standard library allocators. A new scope-based model of allocators was included in C++11 to supplement the prior model.
While the C++03 language provides a memory model that supports threading, the primary support for actually using threading comes with the C++11 standard library.
A thread class (std::thread) is provided, which takes a function object (and an optional series of arguments to pass to it) to run in the new thread. It is possible to cause a thread to halt until another executing thread completes, providing thread joining support via the std::thread::join() member function. Access is provided, where feasible, to the underlying native thread object(s) for platform-specific operations by the std::thread::native_handle() member function.
For synchronization between threads, appropriate mutexes (std::mutex, std::recursive_mutex, etc.) and condition variables (std::condition_variable and std::condition_variable_any) are added to the library. These are accessible via Resource Acquisition Is Initialization (RAII) locks (std::lock_guard and std::unique_lock) and locking algorithms for easy use.
For high-performance, low-level work, communicating between threads is sometimes needed without the overhead of mutexes. This is done using atomic operations on memory locations. These can optionally specify the minimum memory visibility constraints needed for an operation.  Explicit memory barriers may also be used for this purpose.
The C++11 thread library also includes futures and promises for passing asynchronous results between threads, and std::packaged_task for wrapping up a function call that can generate such an asynchronous result. The futures proposal was criticized because it lacks a way to combine futures and check for the completion of one promise inside a set of promises.
Further high-level threading facilities such as  thread pools have been remanded to a future C++ technical report. They are not part of C++11, but their eventual implementation is expected to be built entirely on top of the thread library features.
The new std::async facility provides a convenient method of running tasks and tying them to a std::future. The user can choose whether the task is to be run asynchronously on a separate thread or synchronously on a thread that waits for the value. By default, the implementation can choose, which provides an easy way to take advantage of hardware concurrency without oversubscription, and provides some of the advantages of a thread pool for simple usages.
Tuples are collections composed of heterogeneous objects of pre-arranged dimensions. A tuple can be considered a generalization of a struct's member variables.
The C++11 version of the TR1 tuple type benefited from C++11 features like variadic templates. To implement reasonably, the TR1 version required an implementation-defined maximum number of contained types, and substantial macro trickery. By contrast, the implementation of the C++11 version requires no explicit implementation-defined maximum number of types. Though compilers will have an internal maximum recursion depth for template instantiation (which is normal), the C++11 version of tuples will not expose this value to the user.
Using variadic templates, the declaration of the tuple class looks as follows:
An example of definition and use of the tuple type:
It’s possible to create the tuple proof without defining its contents, but only if the tuple elements' types possess default constructors.  Moreover, it’s possible to assign a tuple to another tuple: if the two tuples’ types are the same, each element type must possess a copy constructor; otherwise, each element type of the right-side tuple must be convertible to that of the corresponding element type of the left-side tuple or that the corresponding element type of the left-side tuple has a suitable constructor.
Just like std::make_pair for std::pair, there exists std::make_tuple to automatically create std::tuples using type deduction and auto helps to declare such a tuple. std::tie creates tuples of lvalue references to help unpack tuples. std::ignore also helps here. See the example:
Relational operators are available (among tuples with the same number of elements), and two expressions are available to check a tuple’s characteristics (only during compilation):
Including hash tables (unordered associative containers) in the C++ standard library is one of the most recurring requests. It was not adopted in C++03 due to time constraints only. Although hash tables are less efficient than a balanced tree in the worst case (in the presence of many collisions), they perform better in many real applications.
Collisions are managed only via linear chaining because the committee didn't consider it to be opportune to standardize solutions of open addressing that introduce quite a lot of intrinsic problems (above all when erasure of elements is admitted). To avoid name clashes with non-standard libraries that developed their own hash table implementations, the prefix “unordered” was used instead of “hash”.
The new library has four types of hash tables, differentiated by whether or not they accept elements with the same key (unique keys or equivalent keys), and whether they map each key to an associated value. They correspond to the four existing binary-search-tree-based associative containers, with an unordered_ prefix.
The new classes fulfill all the requirements of a container class, and have all the methods needed to access elements: insert, erase, begin, end.
This new feature didn't need any C++ language core extensions (though implementations will take advantage of various C++11 language features), only a small extension of the header <functional> and the introduction of headers <unordered_set> and <unordered_map>. No other changes to any existing standard classes were needed, and it doesn’t depend on any other extensions of the standard library.
The new library, defined in the new header <regex>, is made of a couple of new classes:
The function std::regex_search is used for searching, while for ‘search and replace’ the function std::regex_replace is used which returns a new string.
The algorithms std::regex_search and std::regex_replace take a regular expression and a string and write the occurrences found in the struct std::match_results.
Here is an example of the use of std::match_results:
Note the use of double backslashes, because C++ uses backslash as an escape character. The C++11 raw string feature could be used to avoid the problem.
The library <regex> requires neither alteration of any existing header (though it will use them where appropriate) nor an extension of the core language. In POSIX C, regular expressions are also available the C POSIX library#regex.h.
C++11 provides std::unique_ptr, and improvements to std::shared_ptr and std::weak_ptr from TR1. std::auto_ptr is deprecated.
The C standard library provides the ability to generate pseudorandom numbers via the function rand. However, the algorithm is delegated entirely to the library vendor. C++ inherited this functionality with no changes, but C++11 provides a new method for generating pseudorandom numbers.
C++11's random number functionality is split into two parts: a generator engine that contains the random number generator's state and produces the pseudorandom numbers; and a distribution, which determines the range and mathematical distribution of the outcome. These two are combined to form a random number generator object.
Unlike the C standard rand, the C++11 mechanism will come with three base generator engine algorithms:
C++11 also provides a number of standard distributions:
The generator and distributions are combined as in this example:
A wrapper reference is obtained from an instance of the template class reference_wrapper. Wrapper references are similar to normal references (‘&’) of the C++ language. To obtain a wrapper reference from any object the function template ref is used (for a constant reference cref is used).
Wrapper references are useful above all for function templates, where references to parameters rather than copies are needed:
This new utility was added to the existing  <utility> header and didn't need further extensions of the C++ language.
Polymorphic wrappers for function objects are similar to function pointers in semantics and syntax, but are less tightly bound and can indiscriminately refer to anything which can be called (function pointers, member function pointers, or functors) whose arguments are compatible with those of the wrapper.
An example can clarify its characteristics:
The template class function was defined inside the header <functional>, without needing any change to the C++ language.
Metaprogramming consists of creating a program that creates or modifies another program (or itself). This can happen during compilation or during execution. The C++ Standards Committee has decided to introduce a library that allows metaprogramming during compiling via templates.
Here is an example of a meta-program, using the C++03 standard: a recursion of template instances for calculating integer exponents:
Many algorithms can operate on different types of data; C++'s templates support generic programming and make code more compact and useful. Nevertheless, it is common for algorithms to need information on the data types being used. This information can be extracted during instantiation of a template class using type traits.
Type traits can identify the category of an object and all the characteristics of a class (or of a struct). They are defined in the new header <type_traits>.
In the next example there is the template function ‘elaborate’ that, depending on the given data types, will instantiate one of the two proposed algorithms (algorithm.do_it).
Via type traits, defined in header <type_traits>, it’s also possible to create type transformation operations (static_cast and const_cast are insufficient inside a template).
This type of programming produces elegant and concise code; however the weak point of these techniques is the debugging: uncomfortable during compilation and very difficult during program execution.
Determining the return type of a template function object at compile-time is not intuitive, particularly if the return value depends on the parameters of the function. As an example:
Instantiating the class template Calculus<Clear>, the function object of calculus will have always the same return type as the function object of Clear. However, given class Confused below:
Attempting to instantiate Calculus<Confused> will cause the return type of Calculus to not be the same as that of class Confused. The compiler may generate warnings about the conversion from int to double and vice versa.
TR1 introduces, and C++11 adopts, the template class std::result_of that allows one to determine and use the return type of a function object for every declaration. The object CalculusVer2 uses the std::result_of object to derive the return type of the function object:
In this way in instances of function object of CalculusVer2<Confused> there are no conversions, warnings, or errors.
The only change from the TR1 version of std::result_of is that the TR1 version allowed an implementation to fail to be able to determine the result type of a function call. Due to changes to C++ for supporting decltype, the C++11 version of std::result_of no longer needs these special cases; implementations are required to compute a type in all cases.
For compatibility with C, from C99, these were added:
Heading for a separate TR:
Postponed:
The term sequence point was removed, being replaced by specifying that either one operation is sequenced before another, or that two operations are unsequenced.
The former use of the keyword export was removed. The keyword itself remains, being reserved for potential future use.
Dynamic exception specifications are deprecated. Compile-time specification of non-exception-throwing functions is available with the noexcept keyword, which is useful for optimization.
std::auto_ptr is deprecated, having been superseded by std::unique_ptr.
Function object base classes (std::unary_function, std::binary_function), adapters to pointers to functions and adapters to pointers to members, and binder classes are all deprecated."
"40","In computer programming, variadic templates are templates that take a variable number of arguments.
Variadic templates are supported by C++ (since the C++11 standard), and the D programming language.
The variadic template feature of C++ was designed by Douglas Gregor and Jaakko Järvi  and was later standardized in C++11.
Prior to C++11, templates (classes and functions) could only take a fixed number of arguments, which had to be specified when a template was first declared. C++11 allows template definitions to take an arbitrary number of arguments of any type.
The above template class tuple will take any number of typenames as its template parameters.  Here, an instance of the above template class is instantiated with three type arguments:
The number of arguments can be zero, so tuple<> some_instance_name; will also work.
If the variadic template should only allow a positive number of arguments, then this definition can be used:
Variadic templates may also apply to functions, thus not only providing a type-safe add-on to variadic functions (such as printf), but also allowing a function called with printf-like syntax to process non-trivial objects.
The ellipsis (...) operator has two roles. When it occurs to the left of the name of a parameter, it declares a parameter pack. Using the parameter pack, the user can bind zero or more arguments to the variadic template parameters. Parameter packs can also be used for non-type parameters. By contrast, when the ellipsis operator occurs to the right of a template or function call argument, it unpacks the parameter packs into separate arguments, like the args... in the body of printf below. In practice, the use of an ellipsis operator in the code causes the whole expression that precedes the ellipsis to be repeated for every subsequent argument unpacked from the argument pack, with the expressions separated by commas.
The use of variadic templates is often recursive. The variadic parameters themselves are not readily available to the implementation of a function or class. Therefore, the typical mechanism for defining something like a C++11 variadic printf replacement would be as follows:
This is a recursive template. Notice that the variadic template version of printf calls itself, or (in the event that args... is empty) calls the base case.
There is no simple mechanism to iterate over the values of the variadic template. However, there are several ways to translate the argument pack into a single argument that can be evaluated separately for each parameter. Usually this will rely on function overloading, or — if the function can simply pick one argument at a time — using a dumb expansion marker:
which can be used as follows:
which will expand to something like:
The use of this ""pass"" function is necessary, since the expansion of the argument pack proceeds by separating the function call arguments by commas, which are not equivalent to the comma operator. Therefore, some_function(args)...; will never work. Moreover, the solution above will only work when the return type of some_function is not void. Furthermore, the some_function calls will be executed in an unspecified order, because the order of evaluation of function arguments is undefined. To avoid the unspecified order, brace-enclosed initializer lists can be used, which guarantee strict left-to-right order of evaluation. An initializer list requires a non-void return type, but the comma operator can be used to yield 1 for each expansion element.
Instead of executing a function, a lambda expression may be specified and executed in place, which allows executing arbitrary sequences of statements in-place.
However, in this particular example, a lambda function is not necessary. A more ordinary expression can be used instead:
Another way is to use overloading with ""termination versions"" of functions. This is more universal, but requires a bit more code and more effort to create. One function receives one argument of some type and the argument pack, whereas the other receives neither. (If both had the same list of initial parameters, the call would be ambiguous — a variadic parameter pack alone cannot disambiguate a call.) For example:
If args... contains at least one argument, it will redirect to the second version — a parameter pack can be empty, in which case it will simply redirect to the termination version, which will do nothing.
Variadic templates can also be used in an exception specification, a base class list, or the initialization list of a constructor. For example, a class can specify the following:
The unpack operator will replicate the types for the base classes of ClassName, such that this class will be derived from each of the types passed in. Also, the constructor must take a reference to each base class, so as to initialize the base classes of ClassName.
With regard to function templates, the variadic parameters can be forwarded. When combined with universal references (see above), this allows for perfect forwarding:
This unpacks the argument list into the constructor of TypeToConstruct. The std::forward<Args>(params) syntax perfectly forwards arguments as their proper types, even with regard to rvalue-ness, to the constructor. The unpack operator will propagate the forwarding syntax to each parameter. This particular factory function automatically wraps the allocated memory in a std::shared_ptr for a degree of safety with regard to memory leaks.
Additionally, the number of arguments in a template parameter pack can be determined as follows:
The expression SomeStruct<Type1, Type2>::size will yield 2, while SomeStruct<>::size will give 0.
The definition of variadic templates in D is similar to their C++ counterpart:
Likewise, any argument can precede the argument list:
Variadic arguments are very similar to constant array in their usage. They can be iterated upon, accessed by an index, have a length property, and can be sliced. Operations are interpreted at compile time, which means operands can't be runtime value (such as function parameters).
Anything which is known at compile time can be passed as a variadic arguments. It makes variadic arguments similar to template alias arguments, but more powerful, as they also accept basic types (char, short, int...).
Here is an example that print the string representation of the variadic parameters. StringOf and StringOf2 produce equal results.
Outputs:
Variadic templates are often used to create a sequence of aliases, named AliasSeq.
The definition of an AliasSeq is actually very straightforward:
This structure allows one to manipulate a list of variadic arguments that will auto-expand. The arguments must either be symbols or values known at compile time. This includes values, types, functions or even non-specialized templates. This allows any operation you would expect:
For articles on variadic constructs other than templates"
"41","In computer science, primitive data type is either of the following:[citation needed]
In most programming languages, all basic data types are built-in. In addition, many languages also provide a set of composite data types. Opinions vary as to whether a built-in type that is composite should be considered ""primitive"".[citation needed]
Depending on the language and its implementation, primitive data types may or may not have a one-to-one correspondence with objects in the computer's memory. However, one usually expects operations on basic primitive data types to be the fastest language constructs there are.[citation needed] Integer addition, for example, can be performed as a single machine instruction, and some processors offer specific instructions to process sequences of characters with a single instruction.[citation needed] In particular, the C standard mentions that ""a 'plain' int object has the natural size suggested by the architecture of the execution environment"". This means that int is likely to be 32 bits long on a 32-bit architecture. Basic primitive types are almost always value types.
Most languages do not allow the behavior or capabilities of primitive (either built-in or basic) data types to be modified by programs. Exceptions include Smalltalk, which permits all data types to be extended within a program, adding to the operations that can be performed on them or even redefining the built-in operations.[citation needed]
The actual range of primitive data types that is available is dependent upon the specific programming language that is being used. For example, in C#, strings are a composite but built-in data type, whereas in modern dialects of BASIC and in JavaScript, they are assimilated to a primitive data type that is both basic and built-in.
Classic basic primitive types may include:
More sophisticated types which can be built-in include:
An integer data type that represents some range of mathematical integers. Integers may be either signed (allowing negative values) or unsigned (non-negative integers only). Common ranges are:
Literals for integers can be written as regular Hindu-Arabic numerals, consisting of a sequence of digits and with negation indicated by a minus sign before the value. However, most programming languages disallow use of commas for digit grouping. Examples of integer literals are:
There are several alternate methods for writing integer literals in many programming languages:
A boolean type, typically denoted ""bool"" or ""boolean"", is typically a logical type that can be either ""true"" or ""false"". Although only one bit is necessary to accommodate the value set ""true"" and ""false"", programming languages typically implement boolean types as one or more bytes. 
Many languages (e.g. Java, Pascal and Ada) implement booleans adhering to the concept of boolean as a distinct logical type. Languages, though, may implicitly convert booleans to numeric types at times to give extended semantics to booleans and boolean expressions or to achieve backwards compatibility with earlier versions of the language. For example, ANSI C and its former standards did not have a dedicated boolean type. Instead, numeric values of zero are interpreted as ""false"", and any other value is interpreted as ""true"". C99 adds a distinct boolean type that can be included with stdbool.h, and C++ supports bool as a built-in type and ""true"" and ""false"" as reserved words.
A floating-point number represents a limited-precision rational number that may have a fractional part. These numbers are stored internally in a format equivalent to scientific notation, typically in binary but sometimes in decimal. Because floating-point numbers have limited precision, only a subset of real or rational numbers are exactly representable; other numbers can be represented only approximately.
Many languages have both a single precision (often called ""float"") and a double precision type.
Literals for floating point numbers include a decimal point, and typically use e or E to denote scientific notation. Examples of floating-point literals are:
Some languages (e.g., Fortran, Python, D) also have a complex number type comprising two floating-point numbers: a real part and an imaginary part.
A fixed-point number represents a limited-precision rational number that may have a fractional part. These numbers are stored internally in a scaled-integer form, typically in binary but sometimes in decimal. Because fixed-point numbers have limited precision, only a subset of real or rational numbers are exactly representable; other numbers can be represented only approximately. Fixed-point numbers also tend to have a more limited range of values than floating point, and so the programmer must be careful to avoid overflow in intermediate calculations as well as the final result.
A character type (typically called ""char"") may contain a single letter, digit, punctuation mark, symbol, formatting code, control code, or some other specialized code (e.g., a byte order mark). In C, char is defined as the smallest addressable unit of memory. On most systems, this is 8 bits; Several standards, such as POSIX, require it to be this size. Some languages have two or more character types, for example a single-byte type for ASCII characters and a multi-byte type for Unicode characters. The term ""character type"" is normally used even for types whose values more precisely represent code units, for example a UTF-16 code unit as in Java and JavaScript.
Characters may be combined into strings. The string data can include numbers and other numerical symbols but will be treated as text.
Strings are implemented in various ways, depending on the programming language. The simplest way to implement strings is to create them as an array of characters, followed by a delimiting character used to signal the end of the string, usually NUL. These are referred to as null-terminated strings, and are usually found in languages with a low amount of hardware abstraction, such as C and Assembly. While easy to implement, null terminated strings have been criticized for causing buffer overflows. Most high-level scripting languages, such as Python, Ruby, and many dialects of BASIC, have no separate character type; strings with a length of one are normally used to represent single characters. Some languages, such as C++ and Java, have the capability to use null-terminated strings (usually for backwards-compatibility measures), but additionally provide their own class for string handling (std::string and java.lang.String, respectively) in the standard library.
There is also a difference on whether or not strings are mutable or immutable in a language. Mutable strings may be altered after their creation, whereas immutable strings maintain a constant size and content. In the latter, the only way to alter strings are to create new ones. There are both advantages and disadvantages to each approach: although immutable strings are much less flexible, they are simpler and completely thread-safe. Some examples of languages that use mutable strings include C++, Perl, and Ruby, whereas languages that do not include JavaScript, Lua, and Go. A few languages, such as Objective-C, provide different types for mutable and immutable strings.
Literals for characters and strings are usually surrounded by quotation marks: sometimes, single quotes (') are used for characters and double quotes ("") are used for strings.
Examples of character literals in C syntax are:
Examples of string literals in C syntax are:
Each numeric data type has its maximum and minimum value known as the range. Attempting to store a number outside the range may lead to compiler/runtime errors, or to incorrect calculations (due to truncation) depending on the language being used.
The range of a variable is based on the number of bytes used to save the value, and an integer data type is usually able to store 2n values (where n is the number of bits that contribute to the value). For other data types (e.g. floating-point values) the range is more complicated and will vary depending on the method used to store it. There are also some types that do not use entire bytes, e.g. a boolean that requires a single bit, and represents a binary value (although in practice a byte is often used, with the remaining 7 bits being redundant). Some programming languages (such as Ada and Pascal) also allow the opposite direction, that is, the programmer defines the range and precision needed to solve a given problem and the compiler chooses the most appropriate integer or floating-point type automatically."
"42","{""type"":""https://mediawiki.org/wiki/HyperSwitch/errors/not_found"",""title"":""Not found."",""method"":""get"",""uri"":""/en.wikipedia.org/v1/page/html/""}"
"43","In computer science, the Boolean data type is a data type, having two values (usually denoted true and false), intended to represent the truth values of logic and Boolean algebra. It is named after George Boole, who first defined an algebraic system of logic in the mid 19th century. The Boolean data type is primarily associated with conditional statements, which allow different actions and change control flow depending on whether a programmer-specified Boolean condition evaluates to true or false. It is a special case of a more general logical data type; logic need not always be Boolean.
In programming languages with a built-in Boolean data type, such as Pascal and Java, the comparison operators such as > and ≠ are usually defined to return a Boolean value. Conditional and iterative commands may be defined to test Boolean-valued expressions.
Languages with no explicit Boolean data type, like C90 and Lisp, may still represent truth values by some other data type. Common Lisp uses an empty list for false, and any other value for true. The C programming language uses an integer type, where relational expressions like i > j and logical expressions connected by && and || are defined to have value 1 if true and 0 if false, whereas the test parts of if, while, for, etc., treat any non-zero value as true. Indeed, a Boolean variable may be regarded (and implemented) as a numerical variable with one binary digit (bit), which can store only two values. The implementation of Booleans in computers are most likely represented as a full word, rather than a bit; this is usually due to the ways computers transfer blocks of information.
Most programming languages, even those with no explicit Boolean type, have support for Boolean algebraic operations such as conjunction (AND, &, *), disjunction (OR, |, +), equivalence (EQV, =, ==), exclusive or/non-equivalence (XOR, NEQV, ^, !=), and negation (NOT, ~, !).
In some languages, like Ruby, Smalltalk, and Alice the true and false values belong to separate classes, i.e., True and False, respectively, so there is no one Boolean type.
In SQL, which uses a three-valued logic for explicit comparisons because of its special treatment of Nulls, the Boolean data type (introduced in SQL:1999) is also defined to include more than two truth values, so that SQL Booleans can store all logical values resulting from the evaluation of predicates in SQL. A column of Boolean type can also be restricted to just TRUE and FALSE though.
One of the earliest programming languages to provide an explicit boolean data type was ALGOL 60 (1960) with values true and false and logical operators denoted by symbols '∧{\displaystyle \wedge }' (and), '∨{\displaystyle \vee }' (or), '⊃{\displaystyle \supset }' (implies), '≡{\displaystyle \equiv }' (equivalence), and '¬{\displaystyle \neg }' (not).  Due to input device and character set limits on many computers of the time, however, most compilers used alternative representations for many of the operators, such as AND or 'AND'.
This approach with boolean as a built-in (either primitive or otherwise predefined) data type was adopted by many later programming languages, such as Simula 67 (1967), ALGOL 68 (1970),Pascal (1970), Ada (1980), Java (1995), and C# (2000), among others.
The first version of FORTRAN (1957) and its successor FORTRAN II (1958) had no logical values or operations; even the conditional IF statement took an arithmetic expression and branched to one of three locations according to its sign; see arithmetic IF.  FORTRAN IV (1962), however, followed the ALGOL 60 example by providing a Boolean data type (LOGICAL), truth literals (.TRUE. and .FALSE.), Boolean-valued numeric comparison operators (.EQ., .GT., etc.), and logical operators (.NOT., .AND., .OR.).  In FORMAT statements, a specific control character ('L') was provided for the parsing or formatting of logical values.
The language Lisp (1958) never had a built-in Boolean data type. Instead, conditional constructs like cond assume that the logical value false is represented by the empty list (), which is defined to be the same as the special atom nil or NIL; whereas any other s-expression is interpreted as true. For convenience, most modern dialects of Lisp predefine the atom t to have value t, so that t can be used as a mnemonic notation for true.
This approach (any value can be used as a Boolean value) was retained in most Lisp dialects (Common Lisp, Scheme, Emacs Lisp), and similar models were adopted by many scripting languages, even ones having a distinct Boolean type or Boolean values; although which values are interpreted as false and which are true vary from language to language. In Scheme, for example, the false value is an atom distinct from the empty list, so the latter is interpreted as true.
The language Pascal (1970) introduced the concept of programmer-defined enumerated types. A built-in Boolean data type was then provided as a predefined enumerated type with values FALSE and TRUE. By definition, all comparisons, logical operations, and conditional statements applied to and/or yielded Boolean values.  Otherwise, the Boolean type had all the facilities which were available for enumerated types in general, such as ordering and use as indices. In contrast, converting between Booleans and integers (or any other types) still required explicit tests or function calls, as in ALGOL 60. This approach (Boolean is an enumerated type) was adopted by most later languages which had enumerated types, such as Modula, Ada, and Haskell.
Initial implementations of the language C (1972) provided no Boolean type, and to this day Boolean values are commonly represented by integers (ints) in C programs. The comparison operators (>, ==, etc.) are defined to return a signed integer (int) result, either 0 (for false) or 1 (for true). Logical operators (&&, ||, !, etc.) and condition-testing statements (if, while) assume that zero is false and all other values are true.
After enumerated types (enums) were added to the American National Standards Institute version of C, ANSI C (1989), many C programmers got used to defining their own Boolean types as such, for readability reasons. However, enumerated types are equivalent to integers according to the language standards; so the effective identity between Booleans and integers is still valid for C programs.
Standard C (since C99) provides a boolean type, called _Bool. By including the header stdbool.h, one can use the more intuitive name bool and the constants true and false. The language guarantees that any two true values will compare equal (which was impossible to achieve before the introduction of the type). Boolean values still behave as integers, can be stored in integer variables, and used anywhere integers would be valid, including in indexing, arithmetic, parsing, and formatting. This approach (Boolean values are just integers) has been retained in all later versions of C.
C++ has a separate Boolean data type bool, but with automatic conversions from scalar and pointer values that are very similar to those of C. This approach was adopted also by many later languages, especially by some scripting languages such as AWK.
Objective-C also has a separate Boolean data type BOOL, with possible values being YES or NO, equivalents of true and false respectively. Also, in Objective-C compilers that support C99, C's _Bool type can be used, since Objective-C is a superset of C.
Perl has no boolean data type. Instead, any value can behave as boolean in boolean context (condition of if or while statement, argument of && or ||, etc.). The number 0, the strings ""0"" and """", the empty list (), and the special value undef evaluate to false. All else evaluates to true.
Lua has a boolean data type, but non-boolean value can also behave as boolean. The non-value nil evaluate to false, whereas every other data type always evaluates to true, regardless of value.
Python, from version 2.3 forward, has a bool type which is a subclass of int, the standard integer type. It has two possible values: True and False, which are special versions of 1 and 0 respectively and behave as such in arithmetic contexts. Also, a numeric value of zero (integer or fractional), the null value (None), the empty string, and empty containers (i.e. lists, sets, etc.) are considered Boolean false; all other values are considered Boolean true by default. Classes can define how their instances are treated in a Boolean context through the special method __nonzero__ (Python 2) or __bool__ (Python 3). For containers, __len__ (the special method for determining the length of containers) is used if the explicit Boolean conversion method is not defined.
In Ruby, in contrast, only nil (Ruby's null value) and a special false object are false, all else (including the integer 0 and empty arrays) is true.
In JavaScript, the empty string (""""), null, undefined, NaN, +0, −0 and false
are sometimes called falsy, and their complement, truthy, to distinguish between strictly type-checked and coerced Booleans. Languages such as PHP also use this approach.
The SQL:1999 standard introduced a BOOLEAN data type as an optional feature (T031). When restricted by a NOT NULL constraint, a SQL BOOLEAN behaves like Booleans in other languages. However, in SQL the BOOLEAN type is nullable by default like all other SQL data types, meaning it can have the special null value also. Although the SQL standard defines three literals for the BOOLEAN type – TRUE, FALSE, and UNKNOWN – it also says that the NULL BOOLEAN and UNKNOWN ""may be used interchangeably to mean exactly the same thing"". This has caused some controversy because the identification subjects UNKNOWN to the equality comparison rules for NULL. More precisely UNKNOWN = UNKNOWN is not TRUE but UNKNOWN/NULL. As of 2012 few major SQL systems implement the T031 feature.PostgreSQL is a notable exception, although it implements no UNKNOWN literal; NULL can be used instead."
"44","In computing, a null pointer has a value reserved for indicating that the pointer does not refer to a valid object. Programs routinely use null pointers to represent conditions such as the end of a list of unknown length or the failure to perform some action; this use of null pointers can be compared to nullable types and to the Nothing value in an option type.
A null pointer should not be confused with an uninitialized pointer: A null pointer is guaranteed to compare unequal to any pointer that points to a valid object. However, depending on the language and implementation, an uninitialized pointer may not have any such guarantee.  It might compare equal to other, valid pointers; or it might compare equal to null pointers.  It might do both at different times.
In C, two null pointers of any type are guaranteed to compare equal. The preprocessor macro NULL is defined as an implementation-defined null pointer constant, which in C99 can be portably expressed as the integer value 0 converted implicitly or explicitly to the type void* (pointer to void).
Dereferencing a null pointer typically results in an attempted read or write from memory that is not mapped, triggering a segmentation fault or memory access violation. This may manifest itself as a program crash, or be transformed into a software exception that can be caught by program code. There are, however, certain circumstances where this is not the case. For example, in x86 real mode, the address 0000:0000 is readable and also usually writable, hence dereferencing the null pointer is a perfectly valid but typically unwanted action that may lead to undefined but non-crashing behavior in the application. Note also that there are occasions when dereferencing the null pointer is intentional and well-defined; for example, BIOS code written in C for 16-bit real-mode x86 devices may write the IDT at physical address 0 of the machine by dereferencing a null pointer for writing. It is also possible for the compiler to optimize away the null pointer dereference, avoiding a segmentation fault but causing other undesired behavior.
In C++, while the NULL macro was inherited from C, the integer literal for zero has been traditionally preferred to represent a null pointer constant. However, C++11 has introduced an explicit nullptr constant to be used instead.
In some programming language environments (at least one proprietary Lisp implementation, for example),[citation needed] the value used as the null pointer (called nil in Lisp) may actually be a pointer to a block of internal data useful to the implementation (but not explicitly reachable from user programs), thus allowing the same register to be used as a useful constant and a quick way of accessing implementation internals.  This is known as the nil vector.
In languages with a tagged architecture, a possibly null pointer can be replaced with a tagged union which enforces explicit handling of the exceptional case; in fact, a possibly null pointer can be seen as a tagged pointer with a computed tag.
Programming languages use different names for a null pointer. In Pascal, for example, a null pointer it is called nil. In Eiffel, it is called a void reference. Object-oriented languages use the term null reference, while older third-generation languages use the term null pointer or even null address.
Because a null pointer does not point to a meaningful object, an attempt to dereference (ie. access the data stored at that memory location) a null pointer usually (but not always) causes a run-time error or immediate program crash.
In 2009 Tony Hoare (C.A.R. Hoare) stated
that he invented the null reference in 1965 as part of the ALGOL W language. In that 2009 reference Hoare describes his invention as a ""billion-dollar mistake"":"
"45","In computer programming, programming languages are often colloquially classified as to whether the language's type system makes it strongly typed or weakly typed (loosely typed). These terms do not have a precise definition, but in general, a strongly typed language has stricter typing rules and is more likely to generate an error or refuse to compile if the argument passed to a function does not closely match the expected type. On the other hand, a weakly typed language has looser typing rules and may produce unpredictable results or may perform implicit type conversion. A different but related concept is latent typing.
In 1974, Liskov and Zilles defined a strongly-typed language as one in which ""whenever an object is passed from a calling function to a called function, its type must be compatible with the type declared in the called function.""
In 1977, Jackson wrote, ""In a strongly typed language each data area will have a distinct type and each process will state its communication requirements in terms of these types.""
A number of different language design decisions have been referred to as evidence of ""strong"" or ""weak"" typing. In fact, many of these are more accurately understood as the presence or absence of type safety, memory safety, static type-checking, or dynamic type-checking.
""Strong typing"" generally refers to use of programming language types in order to both capture invariants of the code, and ensure its correctness, and definitely exclude certain classes of programming errors. Thus there are many ""strong typing"" disciplines used to achieve these goals.
Some programming languages make it easy to use a value of one type as if it were a value of another type. This is sometimes described as ""weak typing"".
For example, Aahz Maruch opines that ""Coercion occurs when you have a statically typed language and you use the syntactic features of the language to force the usage of one type as if it were a different type (consider the common use of void* in C). Coercion is usually a symptom of weak typing. Conversion, on the other hand, creates a brand-new object of the appropriate type.""
As another example, GCC describes this as type-punning and warns that it will break strict aliasing. Thiago Macieira discusses several problems that can arise when type-punning causes the compiler to make inappropriate optimizations.
There are many examples of languages that allow implicit type conversions, but in a type-safe manner. For example, both C++ and C# allow programs to define operators to convert a value from one type to another in a semantically meaningful way. When a C++ compiler encounters such a conversion, it treats the operation just like a function call. In contrast, converting a value to the C type void* is an unsafe operation that is invisible to the compiler.
Some programming languages expose pointers as if they were numeric values, and allow users to perform arithmetic on them. These languages are sometimes referred to as ""weakly typed"", since pointer arithmetic can be used to bypass the language's type system.
Some programming languages support untagged unions, which allow a value of one type to be viewed as if it were a value of another type.
In Luca Cardelli's article Typeful Programming, a ""strong type system"" is described as one in which there is no possibility of an unchecked runtime type error. In other writing, the absence of unchecked run-time errors is referred to as safety or type safety; Tony Hoare's early papers call this property security[citation needed].
Some programming languages do not have static type-checking. In many such languages, it is easy to write programs that would be rejected by most static type-checkers. For example, a variable might store either a number or the Boolean value ""false"".
Note that some of these definitions are contradictory, others are merely conceptually independent, and still others are special cases (with additional constraints) of other, more ""liberal"" (less strong) definitions. Because of the wide divergence among these definitions, it is possible to defend claims about most programming languages that they are either strongly or weakly typed. For instance:
For this reason, writers who wish to write unambiguously about type systems often eschew the term ""strong typing"" in favor of specific expressions such as ""type safety""."
"46","In class-based object-oriented programming, a constructor (abbreviation: ctor) is a special type of subroutine called to create an object. It prepares the new object for use, often accepting arguments that the constructor uses to set required member variables.
A constructor resembles an instance method, but it differs from a method in that it has no explicit return type, it is not implicitly inherited and it usually has different rules for scope modifiers. Constructors often have the same name as the declaring class. They have the task of initializing the object's data members and of establishing the invariant of the class, failing if the invariant is invalid. A properly written constructor leaves the resulting object in a valid state. Immutable objects must be initialized in a constructor.
Most languages allow overloading the constructor in that there can be more than one constructor for a class, with differing parameters. Some languages take consideration of some special types of constructors. Constructors, which concretely use a single class to create objects and return a new instance of the class, are abstracted by factories, which also create objects but can do so in various ways, using multiple classes or different allocation schemes such as an object pool.
Constructors that can take at least one argument are termed as parameterized constructors.
For example:
When an object is declared in a parameterized constructor, the initial values have to be passed as arguments to the constructor function. The normal way of object declaration may not work. The constructors can be called explicitly or implicitly. The method of calling the constructor implicitly is also called the shorthand method.
If the programmer does not supply a constructor for an instantiable class, most languages will provide a default constructor.
The behavior of the default constructor is language dependent. It may initialize data members to zero or other same values, or it may do nothing at all.
Some languages (Java, C#, VB .NET) will default construct arrays of class types to contain null references. Languages without null references may not allow default construction of arrays of non default constructible objects, or require explicit initialization at the time of the creation (C++):
Copy constructors define the actions performed by the compiler when copying class objects. A copy constructor has one formal parameter that is the type of the class (the parameter may be a reference to an object).
It is used to create a copy of an existing object of the same
class. Even though both classes are the same, it counts as a conversion
constructor.
While copy constructors are usually abbreviated copy ctor or cctor, they have nothing to do with class constructors used in .NET using the same abbreviation.
Conversion constructors provide a means for a compiler to implicitly create an object belonging to one class based on an object of a different type. These constructors are usually invoked implicitly to convert arguments or operands to an appropriate type, but they may also be called explicitly.
In C++, move constructors take a value reference to an object of the class, and are used to implement ownership transfer of the parameter object's resources.
In Java, C# and VB .NET the constructor creates objects in a special memory structure called
heap for reference types. Value types (such as int, double etc.), are created in a sequential structure called stack.
VB .NET and C# allow use of new to create objects of value types. However, in those languages even use of new for value types creates objects only on stack.
In C++, when constructor is invoked without new the objects are created on stack. When objects are created using new they are created on heap. They must be deleted implicitly by a destructor or explicitly by a call to operator delete.
In Java, constructors differ from other methods in that:
Java constructors perform the following tasks in the following order:
Java permit users to call one constructor in another constructor using this() keyword.
But this() must be first statement. 
Java provides access to the superclass's constructor through the super keyword.
A constructor taking zero number of arguments is called a ""no-arguments"" or ""no-arg"" constructor.
As of ES6, JavaScript has direct constructors like many other programming languages. They are written as such
This can be instantiated as such
The equivalent of this before ES6, was creating a function that instantiates an object as such
This is instantiated the same way as above. 
In Visual Basic .NET, constructors use a method declaration with the name ""New"".
Example C# constructor:
In C#, a static constructor is a static data initializer.  Static constructors are also called class constructors. Since the actual method generated has the name .cctor they are often also called ""cctors"".
Static constructors allow complex static variable initialization.
Static constructors are called implicitly when the class is first accessed. Any call to a class (static or constructor call), triggers the static constructor execution.
Static constructors are thread safe and implement a singleton pattern. When used in a generic programming class, static constructors are called at every new generic instantiation one per type. Static variables are instantiated as well.
In C++, the name of the constructor is the name of the class. It returns nothing. It can have parameters like any member function. Constructor functions are usually declared in the public section, but can also be declared in the protected and private sections, if the user wants to restrict access to them.
The constructor has two parts. First is the initializer list which follows the parameter list and before the method body. It starts with a colon and entries are comma-separated. The initializer list is not required, but offers the opportunity to provide values for data members and avoid separate assignment statements. The initializer list is required if you have const or reference type data members, or members that do not have parameterless constructor logic. Assignments occur according to the order in which data members are declared (even if the order in the initializer list is different). The second part is the body, which is a normal method body enclosed in curly brackets.
C++ allows more than one constructor. The other constructors must have different parameters. Additionally constructors which contain parameters which are given default values, must adhere to the restriction that not all parameters are given a default value. This is a situation which only matters if there is a default constructor. The constructor of a base class (or base classes) can also be called by a derived class. Constructor functions are not inherited and their addresses cannot be referenced. When memory allocation is required, the new and delete operators are called implicitly.
A copy constructor has a parameter of the same type passed as const reference, for example Vector(const Vector& rhs). If it is not provided explicitly, the compiler uses the copy constructor for each member variable or simply copies values in case of primitive types. The default implementation is not efficient if the class has dynamically allocated members (or handles to other resources), because it can lead to double calls to delete (or double release of resources) upon destruction.
Example invocations:
On returning objects from functions or passing objects by value, the objects copy constructor will be called implicitly, unless return value optimization applies.
C++ implicitly generates a default copy constructor which will call the copy constructors for all base classes and all member variables unless the programmer provides one, explicitly deletes the copy constructor (to prevent cloning) or one of the base classes or member variables copy constructor is deleted or not accessible (private). Most cases calling for a customized copy constructor (e.g. reference counting, deep copy of pointers) also require customizing the destructor and the copy assignment operator. This is commonly referred to as the Rule of three.
In F#, a constructor can include any let or do statements defined in a class. let statements define private fields and do statements execute code. Additional constructors can be defined using the new keyword.
In Eiffel, the routines which initialize new objects are called creation procedures. Creation procedures have the following traits:
Although object creation involves some subtleties, the creation of an attribute with a typical declaration x: T as expressed in a creation instruction create x.make consists of the following sequence of steps:
In the first snippet below, class POINT is defined. The procedure make is coded after the keyword feature.
The keyword create introduces a list of procedures which can be used to initialize instances. In this case the list includes default_create, a procedure with an empty implementation inherited from class ANY, and the make procedure coded within the class.
In the second snippet, a class which is a client to POINT has a declarations my_point_1 and my_point_2 of type POINT.
In procedural code, my_point_1 is created as the origin (0.0, 0.0). Because no creation procedure is specified, the procedure default_create inherited from class ANY is used. This line could have been coded create my_point_1.default_create .
Only procedures named as creation procedures can be used in an instruction with the create keyword.
Next is a creation instruction for my_point_2, providing initial values for the my_point_2's coordinates.
The third instruction makes an ordinary instance call to the make procedure to reinitialize the instance attached to my_point_2 with different values.
CFML uses a method named 'init' as a constructor method.
Cheese.cfc
Create instance of a cheese.
Since ColdFusion 10, CFML has also supported specifying the name of the constructor method:
In Object Pascal, the constructor is similar to a factory method. The only syntactic difference to regular methods is the keyword constructor in front of the name (instead of procedure or function). It can have any name, though the convention is to have Create as prefix, such as in CreateWithFormatting. Creating an instance of a class works like calling a static method of a class: TPerson.Create('Peter').
In Perl programming language version 5, by default, constructors are factory methods, that is, methods that create and return the object, concretely meaning create and return a blessed reference. A typical object is a reference to a hash, though rarely references to other types are used too.  By convention the only constructor is named new, though it is allowed to name it otherwise, or to have multiple constructors.  For example, a Person class may have a constructor named new as well as a constructor new_from_file which reads a file for Person attributes, and new_from_person which uses another Person object as a template.
With the Moose object system for Perl, most of this boilerplate can be left out, a default new is created, attributes can be specified, as well as whether they can be set, reset, or are required.  In addition, any extra constructor functionality can be included in a BUILD method which the Moose generated constructor will call, after it has checked the arguments.  A BUILDARGS method can be specified to handle constructor arguments not in hashref / key => value form.
In both cases the Person class is instiated like this:
In PHP version 5 and above, the constructor is a method named __construct() (notice that it's a double underscore), which the keyword new automatically calls after creating the object. It is usually used to automatically perform initializations such as property initializations. Constructors can also accept arguments, in which case, when the new statement is written, you also need to send the constructor arguments for the parameters.
In Python, constructors are defined by one or both of __new__ and __init__ methods. A new instance is created by calling the class as if it were a function, which calls the __new__ and __init__ methods. If a constructor method is not defined in the class, the next one found in the class's Method Resolution Order will be called.
In the typical case, only the __init__ method need be defined. (The most common exception is for immutable objects.)
Classes normally act as factories for new instances of themselves, that is, a class is a callable object (like a function), with the call being the constructor, and calling the class returns an instance of that class. However the __new__ method is permitted to return something other than an instance of the class for specialised purposes. In that case, the __init__ is not invoked.
In Ruby, constructors are created by defining a method called initialize. This method is executed to initialize each new instance."
"47","The syntax of the Java programming language is the set of rules defining how a Java program is written and interpreted.
The syntax is mostly derived from C and C++. Unlike C++, in Java there are no global functions or variables, but there are data members which are also regarded as global variables. All code belongs to classes and all values are objects. The only exception is the primitive types, which are not represented by a class instance for performance reasons (though can be automatically converted to objects and vice versa via autoboxing). Some features like operator overloading or unsigned integer types are omitted to simplify the language and to avoid possible programming mistakes.
The Java syntax has been gradually extended in the course of the eight major JDK releases support capabilities such as generic programming and function literals (called lambda expressions in Java).
An identifier is the name of an element in the code. There are certain standard naming conventions to follow when selecting names for elements. Identifiers in Java are case-sensitive.
An identifier can contain:
An identifier cannot:
Integer literals are of int type by default unless long type is specified by appending L or l suffix to the literal, e.g. 367L. Since Java SE 7, it is possible to include underscores between the digits of a number to increase readability; for example, a number 145608987 can be written as 145_608_987.
Variables are identifiers associated with values. They are declared by writing the variable's type and name, and are optionally initialized in the same statement by assigning a value.
Multiple variables of the same type can be declared and initialized in one statement using comma as a delimiter.
The separators { and } signify a code block and a new scope. Class members and the body of a method are examples of what can live inside these braces in various contexts.
Inside of method bodies, braces may be used to create new scopes, as follows:
Java has three kinds of comments: traditional comments, end-of-line comments and documentation comments.
Traditional comments, also known as block comments, start with /* and end with */, they may span across multiple lines. This type of comment was derived from C and C++.
End-of-line comments start with // and extend to the end of the current line. This comment type is also present in C++ and in modern C.
Documentation comments in the source files are processed by the Javadoc tool to generate documentation. This type of comment is identical to traditional comments, except it starts with /** and follows conventions defined by the Javadoc tool. Technically, these comments are a special kind of traditional comment and they are not specifically defined in the language specification.
Java applications consist of collections of classes. Classes exist in packages but can also be nested inside other classes.
Every Java application must have an entry point. This is true of both graphical interface applications and console applications. The entry point is the main method. There can be more than one class with a main method, but the main class is always defined externally (for example, in a manifest file). The method must be static and is passed command-line arguments as an array of strings. Unlike C++ or C#, it never returns a value and must return void.
Packages are a part of a class name and they are used to group and/or distinguish named entities from other ones. Another purpose of packages is to govern code access together with access modifiers. For example, java.io.InputStream is a fully qualified class name for the class InputStream which is located in the package java.io.
A package is declared at the start of the file with the package declaration:
Classes with the public modifier must be placed in the files with the same name and java extension and put into nested folders corresponding to the package name. The above class myapplication.mylibrary.MyClass will have the following path: ""myapplication/mylibrary/MyClass.java"".
A type import declaration allows a named type to be referred to by a simple name rather than the full name that includes the package. Import declarations can be single type import declarations or import-on-demand declarations. Import declarations must be placed at the top of a code file after the package declaration.
Import-on-demand declarations are mentioned in the code. A ""type import"" imports all the types of the package. A ""static import"" imports members of the package.
This type of declaration has been available since J2SE 5.0. Static import declarations allow access to static members defined in another class, interface, annotation, or enum; without specifying the class name:
Import-on-demand declarations allow to import all the fields of the type:
Enum constants may also be used with static import. For example, this enum is in the package called screen:
It is possible to use static import declarations in another class to retrieve the enum constants:
Operators in Java are similar to those in C++. However, there is no delete operator due to garbage collection mechanisms in Java, and there are no operations on pointers since Java does not support them. Another difference is that Java has an unsigned right shift operator (>>>), while C's right shift operator's signedness is type-dependent. Operators in Java cannot be overloaded.
if statements in Java are similar to those in C and use the same syntax:
if statement may include optional else block, in which case it becomes an if-then-else statement:
Like C, else-if construction does not involve any special keywords, it is formed as a sequence of separate if-then-else statements:
Also, note that the ?: operator can be used in place of simple if statement, for example
Switch statements in Java can use byte, short, char, and int (note: not long) primitive data types or their corresponding wrapper types. Starting with J2SE 5.0, it is possible to use enum types. Starting with Java SE 7, it is possible to use Strings. Other reference types cannot be used in switch statements.
Possible values are listed using case labels. These labels in Java may contain only constants (including enum constants and string constants). Execution will start after the label corresponding to the expression inside the brackets. An optional default label may be present to declare that the code following it will be executed if none of the case labels correspond to the expression.
Code for each label ends with the break keyword. It is possible to omit it causing the execution to proceed to the next label, however, a warning will usually be reported during compilation.
Iteration statements are statements that are repeatedly executed when a given condition is evaluated as true. Since J2SE 5.0, Java has four forms of such statements.
In the while loop, the test is done before each iteration.
In the do ... while loop, the test is done after each iteration. Consequently, the code is always executed at least once.
for loops in Java include an initializer, a condition and a counter expression. It is possible to include several expressions of the same kind using comma as delimiter (except in the condition). However, unlike C, the comma is just a delimiter and not an operator.
Like C, all three expressions are optional. The following loop is infinite:
Enhanced for loops have been available since J2SE 5.0. This type of loop uses built-in iterators over arrays and collections to return each item in the given collection. Every element is returned and reachable in the context of the code block. When the block is executed, the next item is returned until there are no items remaining. Unlike C#, this kind of loop does not involve a special keyword, but instead uses a different notation style.
Labels are given points in code used by break and continue statements. Note that the Java goto keyword cannot be used to jump to specific points in the code.
The break statement breaks out of the closest loop or switch statement. Execution continues in the statement after the terminated statement, if any.
It is possible to break out of the outer loop using labels:
The continue statement discontinues the current iteration of the current control statement and begins the next iteration. The following while loop in the code below reads characters by calling getChar(), skipping the statements in the body of the loop if the characters are spaces:
Labels can be specified in continue statements and break statements:
The return statement is used to end method execution and to return a value. A value returned by the method is written after the return keyword. If the method returns anything but void, it must use the return statement to return some value.
return statement ends execution immediately, except for one case: if the statement is encountered within a try block and it is complemented by a finally, control is passed to the finally block.
Exceptions are managed within try ... catch blocks.
The statements within the try block are executed, and if any of them throws an exception, execution of the block is discontinued and the exception is handled by the catch block. There may be multiple catch blocks, in which case the first block with an exception variable whose type matches the type of the thrown exception is executed.
Java SE 7 also introduced multi-catch clauses besides uni-catch clauses. This type of catch clauses allows Java to handle different types of exceptions in a single block provided they are not subclasses of each other.
If no catch block matches the type of the thrown exception, the execution of the outer block (or method) containing the try ... catch statement is discontinued, and the exception is passed up and outside the containing block (or method). The exception is propagated upwards through the call stack until a matching catch block is found within one of the currently active methods. If the exception propagates all the way up to the top-most main method without a matching catch block being found, a textual description of the exception is written to the standard output stream.
The statements within the finally block are always executed after the try and catch blocks, whether or not an exception was thrown and even if a return statement was reached. Such blocks are useful for providing clean-up code that is guaranteed to always be executed.
The catch and finally blocks are optional, but at least one or the other must be present following the try block.
try-with-resources statements are a special type of try-catch-finally statements introduced as an implementation of the dispose pattern in Java SE 7. In a try-with-resources statement the try keyword is followed by initialization of one or more resources that are released automatically when the try block execution is finished. Resources must implement java.lang.AutoCloseable. try-with-resources statements are not required to have a catch or finally block unlike normal try-catch-finally statements.
The throw statement is used to throw an exception and end the execution of the block or method. The thrown exception instance is written after the throw statement.
Java has built-in tools for multi-thread programming. For the purposes of thread synchronization the synchronized statement is included in Java language.
To make a code block synchronized, it is preceded by the synchronized keyword followed by the lock object inside the brackets. When the executing thread reaches the synchronized block, it acquires a mutual exclusion lock, executes the block, then releases the lock. No threads may enter this block until the lock is released. Any non-null reference type may be used as the lock.
assert statements have been available since J2SE 1.4. These types of statements are used to make assertions in the source code, which can be turned on and off during execution for specific classes or packages. To declare an assertion the assert keyword is used followed by a conditional expression. If it evaluates to false when the statement is executed, an exception is thrown. This statement can include a colon followed by another expression, which will act as the exception's detail message.
Primitive types in Java include integer types, floating-point numbers, UTF-16 code units and a boolean type. There are no unsigned types in Java except char type, which is used to represent UTF-16 code units. The lack of unsigned types is offset by introducing unsigned right shift operation (>>>), which is not present in C++. Nevertheless, criticisms have been levelled about the lack of compatibility with C and C++ this causes.
char does not necessarily correspond to a single character. It may represent a part of a surrogate pair, in which case Unicode code point is represented by a sequence of two char values.
This language feature was introduced in J2SE 5.0. Boxing is the operation of converting a value of a primitive type into a value of a corresponding reference type, which serves as a wrapper for this particular primitive type. Unboxing is the reverse operation of converting a value of a reference type (previously boxed) into a value of a corresponding primitive type. Neither operation requires an explicit conversion.
Example:
Reference types include class types, interface types, and array types. When the constructor is called, an object is created on the heap and a reference is assigned to the variable. When a variable of an object gets out of scope, the reference is broken and when there are no references left, the object gets marked as garbage. The garbage collector then collects and destroys it some time afterwards.
A reference variable is null when it does not reference any object.
Arrays in Java are created at runtime, just like class instances. Array length is defined at creation and cannot be changed.
In Java, multi-dimensional arrays are represented as arrays of arrays. Technically, they are represented by arrays of references to other arrays.
Due to the nature of the multi-dimensional arrays, sub-arrays can vary in length, so multi-dimensional arrays are not bound to be rectangular unlike C:
Classes are fundamentals of an object-oriented language such as Java. They contain members that store and manipulate data. Classes are divided into top-level and nested. Nested classes are classes placed inside another class that may access the private members of the enclosing class. Nested classes include member classes (which may be defined with the static modifier for simple nesting or without it for inner classes), local classes and anonymous classes.
Non-static members of a class define the types of the instance variables and methods, which are related to the objects created from that class. To create these objects, the class must be instantiated by using the new operator and calling the class constructor.
Members of both instances and static classes are accessed with the . operator.
Accessing an instance member
Instance members can be accessed through the name of a variable.
Accessing a static class member
Static members are accessed by using the name of the class or any other type. This does not require the creation of a class instance. Static members are declared using the static modifier.
Modifiers are keywords used to modify declarations of types and type members. Most notably there is a sub-group containing the access modifiers.
The access modifiers, or inheritance modifiers, set the accessibility of classes, methods, and other members. Members marked as public can be reached from anywhere. If a class or its member does not have any modifiers, default access is assumed.
The following table shows whether code within a class has access to the class or method depending on the accessing class location and the modifier for the accessed class or class member:
A constructor is a special method called when an object is initialized. Its purpose is to initialize the members of the object. The main differences between constructors and ordinary methods are that constructors are called only when an instance of the class is created and never return anything. Constructors are declared as common methods, but they are named after the class and no return type is specified:
Initializers are blocks of code that are executed when a class or an instance of a class is created. There are two kinds of initializers, static initializers and instance initializers.
Static initializers initialize static fields when the class is created. They are declared using the static keyword:
A class is created only once. Therefore, static initializers are not called more than once. On the contrary, instance initializers are automatically called before the call to a constructor every time an instance of the class is created. Unlike constructors instance initializers cannot take any arguments and generally they cannot throw any checked exceptions (except in several special cases). Instance initializers are declared in a block without any keywords:
Since Java has a garbage collection mechanism, there are no destructors. However, every object has a finalize() method called prior to garbage collection, which can be overridden to implement finalization.
All the statements in Java must reside within methods. Methods are similar to functions except they belong to classes. A method has a return value, a name and usually some parameters initialized when it is called with some arguments. Similar to C++, methods returning nothing have return type declared as void. Unlike in C++, methods in Java are not allowed to have default argument values and methods are usually overloaded instead.
A method is called using . notation on an object, or in the case of a static method, also on the name of a class.
The throws keyword indicates that a method throws an exception. All checked exceptions must be listed in a comma-separated list.
This language feature was introduced in J2SE 5.0. The last argument of the method may be declared as a variable arity parameter, in which case the method becomes a variable arity method (as opposed to fixed arity methods) or simply varargs method. This allows one to pass a variable number of values, of the declared type, to the method as parameters - including no parameters. These values will be available inside the method as an array.
Fields, or class variables, can be declared inside the class body to store data.
Fields can be initialized directly when declared.
Classes in Java can only inherit from one class. A class can be derived from any class that is not marked as final. Inheritance is declared using the extends keyword. A class can reference itself using the this keyword and its direct superclass using the super keyword.
If a class does not specify its superclass, it implicitly inherits from java.lang.Object class. Thus all classes in Java are subclasses of Object class.
If the superclass does not have specified a constructor without parameters the subclass must especify in its constructors what constructor of the superclass to use. For example:
Unlike C++, all non-final methods in Java are virtual and can be overridden by the inheriting classes.
An Abstract Class is a class that is incomplete, or to be considered incomplete.
Normal classes may have abstract methods, that is, methods that are declared but not yet implemented, only if they are abstract classes.
A class C has abstract methods if any of the following is true:
Output:
This language feature was introduced in J2SE 5.0. Technically enumerations are a kind of class containing enum constants in its body. Each enum constant defines an instance of the enum type. Enumeration classes cannot be instantiated anywhere except in the enumeration class itself.
Enum constants are allowed to have constructors, which are called when the class is loaded:
Enumerations can have class bodies, in which case they are treated like anonymous classes extending the enum class:
Interfaces are data structures that contain member definitions and not actual implementation. They are useful to define a contract between members in different types that have different implementations. Every interface is implicitly abstract. The only modifier allowed to use with interfaces apart from access modifiers is strictfp, which has the same effect as for classes.
An interface is implemented by a class using the implements keyword. It is allowed to implement more than one interface, in which case they are written after implements keyword in a comma-separated list. Class implementing an interface must override all its methods, otherwise it must be declared as abstract.
Interfaces can inherit from other interfaces just like classes. Unlike classes it is allowed to inherit from multiple interfaces. However, it is possible that several interfaces have a field with the same name, in which case it becomes a single ambiguous member, which cannot be accessed.
Annotations in Java are a way to embed metadata into code. This language feature was introduced in J2SE 5.0.
Java has a set of predefined annotation types, but it is allowed to define new ones. An annotation type declaration is a special type of an interface declaration. They are declared in the same way as the interfaces, except the interface keyword is preceded by the @ sign. All annotations are implicitly extended from java.lang.annotation.Annotation and cannot be extended from anything else.
Annotations may have the same declarations in the body as the common interfaces, in addition they are allowed to include enums and annotations. The main difference is that abstract method declarations must not have any parameters or throw any exceptions. Also they may have a default value, which is declared using the default keyword after the method name:
Annotations may be used in any kind of declaration, whether it is package, class (including enums), interface (including annotations), field, method, parameter, constructor, or local variable. Also they can be used with enum constants. Annotations are declared using the @ sign preceding annotation type name, after which element-value pairs are written inside brackets. All elements with no default value must be assigned a value.
Besides the generic form, there are two other forms to declare an annotation, which are shorthands. Marker annotation is a short form, it is used when no values are assigned to elements:
The other short form is called single element annotation. It is used with annotations types containing only one element or in the case when multiple elements are present, but only one elements lacks a default value. In single element annotation form the element name is omitted and only value is written instead:
Generics, or parameterized types, or parametric polymorphism is one of the major features introduced in J2SE 5.0. Before generics were introduced, it was required to declare all the types explicitly. With generics it became possible to work in a similar manner with different types without declaring the exact types. The main purpose of generics is to ensure type safety and to detect runtime errors during compilation. Unlike C#, information on the used parameters is not available at runtime due to type erasure.
Classes can be parameterized by adding a type variable inside angle brackets (< and >) following the class name. It makes possible the use of this type variable in class members instead of actual types. There can be more than one type variable, in which case they are declared in a comma-separated list.
It is possible to limit a type variable to a subtype of some specific class or declare an interface that must be implemented by the type. In this case the type variable is appended by the extends keyword followed by a name of the class or the interface. If the variable is constrained by both class and interface or if there are several interfaces, the class name is written first, followed by interface names with &  sign used as the delimiter.
When a variable of a parameterized type is declared or an instance is created, its type is written exactly in the same format as in the class header, except the actual type is written in the place of the type variable declaration.
Since Java SE 7, it is possible to use a diamond (<>) in place of type arguments, in which case the latter will be inferred. The following code in Java SE 7 is equivalent to the code in the previous example:
When declaring a variable for a parameterized type, it is possible to use wildcards instead of explicit type names. Wildcards are expressed by writing ? sign instead of the actual type. It is possible to limit possible types to the subclasses or superclasses of some specific class by writing the extends keyword or the super keyword correspondingly followed by the class name.
Usage of generics may be limited to some particular methods, this concept applies to constructors as well. To declare a parameterized method, type variables are written before the return type of the method in the same format as for the generic classes. In the case of constructor, type variables are declared before the constructor name.
Interfaces can be parameterized in the similar manner as the classes.
"
"48","
typedef is a reserved keyword in the C and C++ programming languages. It is used to create an alias name for another data type. As such, it is often used to simplify the syntax of declaring complex data structures consisting of struct and union types, but is just as common in providing specific descriptive type names for integer data types of varying lengths. The C standard library and POSIX reserve the suffix '_t', for example as in size_t and time_t.
The syntax for a creating a typedef is: typedef typedeclaration;
Some examples: 
 creates Length as a synonym for int. 
 creates PFI as a synonym for a pointer to a function of two char * arguments that returns an int.
A typedef declaration may be used to indicate the meaning of a variable within the programming context, e.g., it may be include the expression of a unit of measurement or counts. The generic declarations in the following code,
may be expressed by declaring context specific types:
Both sections of code execute identically. However, the use of typedef declarations in the second code block makes it clear that the two variables, while represented by the same data type int, represent different or incompatible data. The definition in congratulate() of your_score indicates to the programmer that current_speed (or any other variable not declared as a points) should not be passed as an argument. This would not be as apparent if both were declared as variables of int datatype.  However, the indication is for the programmer only; the C/C++ compiler considers both variables to be of type int and does not flag type mismatch warnings or errors for ""wrong"" argument types for congratulate(points your_score) in the code snippet below:
Although the compiler considers km_per_hour to be equivalent to int in the above code, the two cannot be used interchangeably when the type is changed via a prefix of unsigned, signed, or long.
A typedef may be used to simplify the declaration of a compound type (struct, union) or pointer type.  For example, in the following snippet:
the data type struct MyStruct is defined. To declare a variable of this type in C, the struct key word is required (in C++, it may be omitted):
A typedef may be used to eliminate the need for the keyword struct  in C.  For example, the following declaration
may be used to define an object of type newtype:
The structure declaration and definition may instead be combined into a single statement:
Or it may be used as follows:
In C++, in contrast to C, the keywords struct, class, and enum are optional in variable declarations that are separate from the definitions, as long as there is no ambiguity to another identifier:
As such, MyStruct can be used wherever newtype can be used.  However, the reverse is not true; for instance, the constructor methods for MyStruct cannot be named newtype.
A notorious example where even C++ needs the struct keyword is POSIX' stat system call that uses a struct of the same name in its arguments:
Here both C as well as C++ need the struct keyword in the parameter definition.
We can use the typedef to define a new pointer type. For example,
Above, intptr is a new alias with the pointer type int*. The definition, intptr ptr;, defines a variable ptr with the type int*. So, ptr is a pointer which can point to a memory with int type.
Using typedef to define a new pointer type may sometimes lead to confusion. For example:
Above, intptr cliff, allen; means defining 2 variables with int* type for both. This is because a type defined by typedef is a type, not an expansion. In other words, intptr, which is the int* type, decorates both cliff and allen. For intptr cliff2, *allen2;, the intptr type decorates the cliff2 and *allen2. So, intptr cliff2, *allen2; is equivalent to 2 separate definitions, intptr cliff2; and intptr *allen2. intptr *allen2 means that allen2 is a pointer pointing to a memory with int* type. Shortly, allen2 has the type, int**.
Typedefs can also simplify definitions or declarations for structure pointer types. Consider this:
Using typedef, the above code can be rewritten like this:
In C, one can declare multiple variables of the same type in a single statement, even mixing structure with pointer or non-pointers. However, one would need to prefix an asterisk to each variable to designate it as a pointer.  In the following, a programmer might assume that errptr was indeed a Node *, but a typographical error means that errptr is a Node. This can lead to subtle syntax errors.
By defining the typedef Node *, it is assured that all variables are structure pointer types, or say, that each variable is a pointer type pointing to a structure type.
Consider the following code, which does not use a typedef:
This code can be rewritten with a typedef as follows:
Here, MathFunc is the new alias for the type. A MathFunc is a pointer to a function that returns an integer and takes as arguments a float followed by an integer.
When a function returns a function pointer, it can be even more confusing without typedef. The following is the function prototype of signal(3) from FreeBSD:
The function declaration above is cryptic as it does not clearly show what the function accepts as arguments, or the type that it returns. A novice programmer may even assume that the function accepts a single int as its argument and returns nothing, but in reality it also needs a function pointer and returns another function pointer. It can be written more cleanly:
A typedef can also be used to simplify the definition of array types. For example,
Here, arrType is the new alias for the ""char"" type, which is an array type with 6 elements. For arrType *pArr;, pArr is a pointer pointing to the memory of the ""char"" type.
A typedef is created using type definition syntax but can be used as if it were created using type cast syntax. (Type casting changes a data type.)  For instance, in each line after the first line of:
funcptr is used on the left-hand side to declare a variable and is used on the right-hand side to cast a value.  Thus, the typedef can be used by programmers who do not wish to figure out how to convert definition syntax to type cast syntax.
Note that, without the typedef, it is generally not possible to use definition syntax and cast syntax interchangeably.  For example:
Some people[who?] are opposed to the extensive use of typedefs. Most arguments center on the idea that typedefs simply hide the actual data type of a variable. For example, Greg Kroah-Hartman, a Linux kernel hacker and documenter, discourages their use for anything except function prototype declarations. He argues that this practice not only unnecessarily obfuscates code, it can also cause programmers to accidentally misuse large structures thinking them to be simple types.
Others argue that the use of typedefs can make code easier to maintain.  K&R states that there are two reasons for using a typedef.  First, it provides a means to make a program more portable.  Instead of having to change a type everywhere it appears throughout the program's source files, only a single typedef statement needs to be changed.  Second, a typedef can make a complex definition or declaration easier to understand.
In C++ type names can be very complicated and typedef provides a mechanism to assign a simple name to the type. Consider:
and
C++03 does not provide templated typedefs.  For instance, to have stringpair<T> represent std::pair<std::string, T> for every type T one cannot use:
However, if one is willing to accept stringpair<T>::type in lieu of stringpair<T> then it is possible to achieve the desired result via a typedef within an otherwise unused templated class or struct:
In C++11, templated typedefs are added with the following syntax, which requires the using keyword rather than the typedef keyword. (See template aliases.)
In many statically typed functional languages, like Haskell, Miranda, OCaml, etc., one can define type synonyms, which are the same as typedefs in C. An example in Haskell:
This example has defined a type synonym PairOfInts as an integer type.
In Seed7 the definition of a constant type is used to introduce a synonym for a type:
In Swift, typedef is called typealias:
C# also contains a feature which is similar to the using syntax in C++.
In D the keyword alias allows to create type or partial type synonyms."
"49","Generics are a facility of generic programming that were added to the Java programming language in 2004 within version J2SE 5.0. They were designed to extend Java's type system to allow “a type or method to operate on objects of various types while providing compile-time type safety”. The aspect compile-time type safety was not fully achieved, since it was shown in 2016 that it is not guaranteed in all cases.
The Java collections framework supports generics to specify the type of objects stored in a collection instance.
In 1998, Gilad Bracha, Martin Odersky, David Stoutamire and Philip Wadler created Generic Java, an extension to the Java language to support generic types. Generic Java was incorporated in Java with the addition of wildcards.
According to Java Language Specification:
The following block of Java code illustrates a problem that exists when not using generics. First, it declares an ArrayList of type Object. Then, it adds a String to the ArrayList. Finally, it attempts to retrieve the added String and cast it to an Integer.
Although the code is compiled without error, it throws a runtime exception (java.lang.ClassCastException) when executing the third line of code. This type of problem can be avoided by using generics and is the primary motivation for using generics.
Using generics, the above code fragment can be rewritten as follows:
The type parameter String within the angle brackets declares the ArrayList to be constituted of String (a descendant of the ArrayList's generic Object constituents). With generics, it is no longer necessary to cast the third line to any particular type, because the result of v.get(0) is defined as String by the code generated by the compiler.
Compiling the third line of this fragment with J2SE 5.0 (or later) will yield a compile-time error because the compiler will detect that v.get(0) returns String instead of Integer. For a more elaborate example, see reference.
Here is a small excerpt from the definition of the interfaces List and Iterator in package java.util:
A type argument for a parameterized type is not limited to a concrete class or interface. Java allows the use of type wildcards to serve as type arguments for parameterized types. Wildcards are type arguments in the form ""<?>""; optionally with an upper or lower bound. Given that the exact type represented by a wildcard is unknown, restrictions are placed on the type of methods that may be called on an object that uses parameterized types.
Here is an example where the element type of a Collection<E> is parameterized by a wildcard:
Since we don’t know what the element type of c stands for, we cannot add objects to it. The add() method takes arguments of type E, the element type of the Collection<E> generic interface. When the actual type argument is ?, it stands for some unknown type. Any method argument value we pass to the add() method would have to be a subtype of this unknown type. Since we don't know what type that is, we cannot pass anything in. The sole exception is null; which is a member of every type.
To specify the upper bound of a type wildcard, the extends keyword is used to indicate that the type argument is a subtype of the bounding class. So List<? extends Number> means that the given list contains objects of some unknown type which extends the Number class. For example, the list could be List<Float> or List<Number>. Reading an element from the list will return a Number. Adding null elements is, again, also allowed.
The use of wildcards above adds flexibility since there is not any inheritance relationship between any two parameterized types with concrete type as type argument. Neither List<Number> nor List<Integer> is a subtype of the other; even though Integer is a subtype of Number. So, any method that takes List<Number> as a parameter does not accept an argument of List<Integer>. If it did, it would be possible to insert a Number that is not an Integer into it; which violates type safety. Here is an example that demonstrates how type safety would be violated if List<Integer> were a subtype of List<Number>:
The solution with wildcards works because it disallows operations that would violate type safety:
To specify the lower bounding class of a type wildcard, the super keyword is used. This keyword indicates that the type argument is a supertype of the bounding class. So, List<? super Number> could represent List<Number> or List<Object>. Reading from a list defined as List<? super Number> returns elements of type Object. Adding to such a list requires either elements of type Number, any subtype of Number or null (which is a member of every type).
The mnemonic PECS (Producer Extends, Consumer Super) from the book Effective Java by Joshua Bloch gives an easy way to remember when to use wildcards (corresponding to covariance and contravariance) in Java.
Here is an example of a generic Java class, which can be used to represent individual entries (key to value mappings) in a map:
This generic class could be used in the following ways, for example:
It outputs:
Java SE 7 and above allow the programmer to substitute an empty pair of angle brackets (<>, called the diamond operator) for a pair of angle brackets containing the one or more type parameters that a sufficiently-close context implies. Thus, the above code example using Entry can be rewritten as:
Here is an example of a generic method using the generic class above:
Note: If we remove the first <Type> in the above method, we will get compilation error (cannot find symbol 'Type') since it represents the declaration of the symbol.
In many cases the user of the method need not indicate the type parameters, as they can be inferred:
The parameters can be explicitly added if needed:
The use of primitive types is not allowed, and boxed versions must be used instead:
There is also the possibility to create generic methods based on given parameters.
In such cases you can't use primitive types either, e.g.:
Although exceptions themselves cannot be generic, generic parameters can appear in a throws clause:
Generics are checked at compile-time for type-correctness. The generic type information is then removed in a process called type erasure. For example, List<Integer> will be converted to the non-generic type List, which ordinarily contains arbitrary objects. The compile-time check guarantees that the resulting code is type-correct.
Because of type erasure, type parameters cannot be determined at run-time.  For example, when an ArrayList is examined at runtime, there is no general way to determine whether, before type erasure, it was an ArrayList<Integer> or an ArrayList<Float>. Many people are dissatisfied with this restriction. There are partial approaches. For example, individual elements may be examined to determine the type they belong to; for example, if an ArrayList contains an Integer, that ArrayList may have been parameterized with Integer (however, it may have been parameterized with any parent of Integer, such as Number or Object).
Demonstrating this point, the following code outputs ""Equal"":
Another effect of type erasure is that a generic class cannot extend the Throwable class in any way, directly or indirectly:
The reason why this is not supported is due to type erasure:
Due to type erasure, the runtime will not know which catch block to execute, so this is prohibited by the compiler.
Java generics differ from C++ templates. Java generics generate only one compiled version of a generic class or function regardless of the number of parameterizing types used. Furthermore, the Java run-time environment does not need to know which parameterized type is used because the type information is validated at compile-time and is not included in the compiled code. Consequently, instantiating a Java class of a parameterized type is impossible because instantiation requires a call to a constructor, which is unavailable if the type is unknown.
For example, the following code cannot be compiled:
Because there is only one copy per generic class at runtime, static variables are shared among all the instances of the class, regardless of their type parameter. Consequently, the type parameter cannot be used in the declaration of static variables or in static methods.
Project Valhalla is an experimental project to incubate improved Java generics and language features, for future versions potentially from Java 10 onwards. Potential enhancements include:
"
"50","In computer programming, lazy initialization is the tactic of delaying the creation of an object, the calculation of a value, or some other expensive process until the first time it is needed. It is a kind of lazy evaluation that refers specifically to the instantiation of objects or other resources.
This is typically accomplished by augmenting an accessor method (or property getter) to check whether a private member, acting as a cache, has already been initialized.  If it has, it is returned straight away. If not, a new instance is created, placed into the member variable, and returned to the caller just-in-time for its first use. 
If objects have properties that are rarely used, this can improve startup speed. Mean average program performance may be slightly worse in terms of memory (for the condition variables) and execution cycles (to check them), but the impact of object instantiation is spread in time (""amortized"") rather than concentrated in the startup phase of a system, and thus median response times can be greatly improved.
In multithreaded code, access to lazy-initialized objects/state must be synchronized to guard against race conditions.
In a software design pattern view, lazy initialization is often used together with a factory method pattern. This combines three ideas:
The following is an example of a class with Lazy initialization implemented in Actionscript:
Basic Usage:
In C, lazy evaluation would normally be implemented inside a single function, or a single source file, using static variables.
In a function:
Using a single source file instead allows the state to be shared between multiple functions, while still hiding it from non-related functions.
fruit.h:
fruit.c:
main.c:
In .NET 4.0 Microsoft has included a Lazy class that can be used to do lazy loading.
Below is some dummy code that does lazy loading of Class Fruit
Here is a dummy example in C#.
The Fruit class itself doesn't do anything here, The class variable _typesDictionary is a Dictionary/Map used to store Fruit instances by typeName.
A fairly straightforward 'fill-in-the-blanks' example of a Lazy Initialization design pattern, except that this uses an enumeration for the type
Here is an example in C++.
Here is an example in Java.
Output
Here is an example in JavaScript.
Output
Here is an example of lazy initialization in PHP 5:
Here is an example in Python.
Here is an example in Ruby, of lazily initializing an authentication token from a remote service like Google. The way that @auth_token is cached is also an example of memoization.
Here is an example in Smalltalk, of a typical accessor method to return the value of a variable using lazy initialization.
The 'non-lazy' alternative is to use an initialization method that is run when the object is created and then use a simpler accessor method to fetch the value.
Note that lazy initialization can also be used in non-object-oriented languages.
Scala has built-in support for lazy variable initiation.
Output:"
"51","In computer programming, an entry point is where control is transferred from the operating system to a computer program, at which place the processor enters a program or a code fragment and execution begins. In some operating systems or programming languages, the initial entry is not part of the program but of the runtime library, in which case the runtime library initializes the program and then the runtime library enters the program. In other cases, the program may call the runtime library before doing anything when it is entered for the first time, and, after the runtime library returns, the actual code of the program begins to execute. This marks the transition from load time (and dynamic link time, if present) to run time.
In simple layouts, programs begin their execution at the beginning, which is common in scripting languages, simple binary executable formats, and boot loaders. In other cases, the entry point is at some other fixed point, which is some memory address than can be an absolute address or relative address (offset).
Alternatively, execution of a program can begin at a named point, either with a conventional name defined by the programming language or operating system, or at a caller-specified name. In many programming languages, notably C, this named point is a function called main; as a result, the entry point is often called the main function.
Entry points apply both to source code and to executable files. However, in day-to-day software development, programmers specify the entry points only in source code, which makes them much better known. Entry points in executable files depend on the application binary interface (ABI) of the actual operating system, and are generated by the compiler or linker (if not fixed by the ABI). Non-executable object files may also have entry points, which are used later by the linker when generating entry points of an executable file.
In most of today's popular programming languages and operating systems, a computer program usually only has a single entry point.
In C, C++, D, Rust and Kotlin programs this is a function named main; in Java it is a static method named main (although the class must be specified at the invocation time), and in C# it is a static method named Main.
In major operating systems, the standard executable format has a single entry point. In the Executable and Linkable Format (ELF), used in Unix and Unix-like systems such as Linux, the entry point is specified in the e_entry field of the ELF header. In the GNU Compiler Collection (gcc), the entry point used by the linker is the _start symbol. Similarly, in the Portable Executable format, used in Microsoft Windows, the entry point is specified by the AddressOfEntryPoint field, which is inherited from COFF. In COM files, the entry point is at the fixed offset of 0100h.
One notable modern exception to the single-entry-point paradigm is Android.  Unlike applications on most other operating systems, Android applications do not have a single entry point –  there is no main function, for example.  Instead of a single entry point, they have essential components (which include activities and services) which the system can instantiate and run as needed.
An occasionally used technique is the fat binary, which consists of several executables for different targets packaged in a single binary. Most commonly, this is implemented by a single overall entry point, which is compatible with all targets and branches to the target-specific entry point. Alternative techniques include storing separate executables in separate forks, each with its own entry point, which is then selected by the operating system.
Historically, and in some contemporary legacy systems, such as VMS and OS/400, computer programs have a multitude of entry points, each corresponding to the different functionalities of the program. The usual way to denote entry points, as used system-wide in VMS and in PL/I and MACRO programs, is to append them at the end of the name of the executable image, delimited by a dollar sign ($), e.g. directory.exe$make.
The Apple I computer also used this to some degree. For example, an alternative entry point in Apple I's BASIC would keep the BASIC program useful when the reset button was accidentally pushed.[clarification needed]
In general, programs can exit at any time in an unstructured way, by returning to the operating system or crashing. Scripting languages typically end by reaching the end of the program, but for binaries the control must return to the operating system or it will simply run off the end of the process's memory, either executing whatever code is there or (in modern operating systems) resulting in a memory access violation and termination by the operating system.
Usually, there is not a single exit point specified in a program. However, in other cases runtimes ensure that programs always terminate in a structured way via a single exit point, which is guaranteed unless the runtime itself crashes; this allows cleanup code to be run, such as atexit handlers. This can be done by either requiring that programs terminate by returning from the main function, by calling a specific exit function, or by the runtime catching exceptions or operating system signals.
In many programming languages, the main function is where a program starts its execution. It enables high-level organization of the program's functionality, and typically has access to the command arguments given to the program when it was executed.
The main function is generally the first programmer-written function that runs when a program starts, and is invoked directly from the system-specific initialization contained in the runtime environment (crt0 or equivalent). However, some languages can execute user-written functions before main runs, such as the constructors of C++ global objects.
In other languages, notably scripting languages, execution simply begins at the start of the program.
A non-exhaustive list of programming languages follows, describing their way of defining the main entry point:
In APL, when a workspace is loaded, the contents of ""quad LX"" (latent expression) variable is interpreted as an APL expression and executed.
In C and C++, the function prototype of the main function looks like one of the following:
The parameters argc, argument count, and argv, argument vector, respectively give the number and values of the program's command-line arguments. The names of argc and argv may be any valid identifier in C, but it is common convention to use these names. In C++, the names are to be taken literally, and the ""void"" in the parameter list is to be omitted, if strict conformance is desired. Other platform-dependent formats are also allowed by the C and C++ standards, except that in C++ the return type must always be int; for example, Unix (though not POSIX.1) and Windows have a third argument giving the program's environment, otherwise accessible through getenv in stdlib.h:
Darwin-based operating systems, such as macOS, have a fourth parameter containing arbitrary OS-supplied information, such as the path to the executing binary:
The value returned from the main function becomes the exit status of the process, though the C standard only ascribes specific meaning to two values: EXIT_SUCCESS (traditionally 0) and EXIT_FAILURE. The meaning of other possible return values is implementation-defined. In case a return value is not defined by the programmer, an implicit return 0; at the end of the main() function is inserted by the compiler; this behavior is required by the C++ standard.
It is guaranteed that argc is non-negative and that argv[argc] is a null pointer. By convention, the command-line arguments specified by argc and argv include the name of the program as the first element if argc is greater than 0; if a user types a command of ""rm file"", the shell will initialise the rm process with argc = 2 and argv = {""rm"", ""file"", NULL}.  As argv is the name that processes appear under in ps, top etc., some programs, such as daemons or those running within an interpreter or virtual machine (where argv would be the name of the host executable), may choose to alter their argv to give a more descriptive argv, usually by means of the exec system call.
The main() function is special; normally every C and C++ program must define it exactly once.
If declared, main() must be declared as if it has external linkage; it cannot be declared static or inline.
In C++, main() must be in the global namespace (i.e. ::main), cannot be overloaded, and cannot be a member function, although the name is not otherwise reserved, and may be used for member functions, classes, enumerations, or non-member functions in other namespaces. In C++ (unlike C) main() cannot be called recursively and cannot have its address taken.
When executing a program written in C#, the CLR searches for a static method marked with the .entrypoint IL directive, which takes either no arguments, or a single argument of type string[], and has a return type of void or int, and executes it.
Command-line arguments are passed in args, similar to how it is done in Java. For versions of Main() returning an integer, similar to both C and C++, it is passed back to the environment as the exit status of the process.
Clean is a functional programming language based on graph rewriting. The initial node is called Start and is of type *World -> *World if it changes the world or some fixed type if the program only prints the result after reducing Start.
Or even simpler
One tells the compiler which option to use to generate the executable file.
ANSI Common Lisp does not define a main function; instead, the code is read and evaluated from top to bottom in a source file. However, the following code will emulate a main function.
In D, the function prototype of the main function looks like one of the following:
Command-line arguments are passed in args, similar to how it is done in C# or Java. For versions of main() returning an integer, similar to both C and C++, it is passed back to the environment as the exit status of the process.
FORTRAN does not have a main subroutine or function. Instead a PROGRAM statement as the first line can be used to specify that a program unit is a main program, as shown below. The PROGRAM statement cannot be used for recursive calls.
Some versions of Fortran, such as those on the IBM System/360 and successor mainframes, do not support the PROGRAM statement. Many compilers from other software manufacturers will allow a fortran program to be compiled without a PROGRAM statement. In these cases, whatever module that has any non-comment statement where no SUBROUTINE, FUNCTION or BLOCK DATA statement occurs, is considered to be the Main program.
Using GNAT, the programmer is not required to write a function called main; a source file containing a single subprogram can be compiled to an executable. The binder will however create a package ada_main, which will contain and export a C-style main function.
In Go programming language, program execution starts with the main function of the package main
There is no way to access arguments or a return code outside of the standard library in Go. These can be accessed via os.Args and os.Exit respectively, both of which are included in the ""os"" package.
A Haskell program must contain a name called main bound to a value of type IO t, for some type t; which is usually IO (). IO is a monad, which organizes side-effects in terms of purely functional code. The main value represents the side-effects-ful computation done by the program. The result of the computation represented by main is discarded; that is why main usually has type IO (), which indicates that the type of the result of the computation is (), the unit type, which contains no information.
Command line arguments are not given to main; they must be fetched using another IO action, such as System.Environment.getArgs.
Java programs start executing at the main method, which has the following method heading:
Command-line arguments are passed in args. As in C and C++, the name ""main()"" is special. Java's main methods do not return a value directly, but one can be passed by using the System.exit() method.
Unlike C, the name of the program is not included in args, because the name of the program is exactly the name of the class that contains the main method called, so it is already known. Also unlike C, the number of arguments need not be included, since arrays in Java have a field that keeps track of how many elements there are.
Another aspect unique to Java is that the main function must be included within a class, and then called manually by the runtime. This is because in Java everything has to be contained within a class. For instance, a hello world program in Java may look like so:
To run this program, one must call java HelloWorld in the directory where the compiled class file (which itself must be named HelloWorld.class) exists. Alternatively, executable JAR files use a manifest file to specify the entry point in a manner that is filesystem-independent from the user's perspective.
In FMSLogo, the procedures when loaded do not execute. To make them execute, it is necessary to use this code:
Note that the variable startup is used for the startup list of actions, but the convention is that this calls another procedure that runs the actions. That procedure may be of any name.
OCaml has no main function. Programs are evaluated from top to bottom.
Command-line arguments are available in an array named Sys.argv and the exit status is 0 by default.
Example:
In Pascal, the main procedure is the only unnamed procedure in the program. Because Pascal programs have the procedures and functions in a more rigorous top-down order than C, C++ or Java programs, the main procedure is usually the last procedure in the program. Pascal does not have a special meaning for the name ""main"" or any similar name.
Command-line arguments are counted in ParamCount and accessible as strings by ParamStr(n), with n between 0 and ParamCount.
Note that ""unit"" or ""module"" based versions of Pascal start the main module with the PROGRAM keyword, while other separately compiled modules start with UNIT (UCSD/Borland) or MODULE (ISO). The unnamed function in modules is often module initialization, and run before the main program starts.
In Perl, there is no main function. Statements are executed from top to bottom.
Command-line arguments are available in the special array @ARGV. Unlike C, @ARGV does not contain the name of the program, which is $0.
PHP does not have a ""main"" function. Starting from the first line of a PHP script, any code not encapsulated by a function header is executed as soon as it is seen.
In Pike syntax is similar to that of C and C++.  The execution begins at main.  The ""argc"" variable keeps the number of arguments passed to the program.  The ""argv"" variable holds the value associated with the arguments passed to the program.
Example:
Python programs are evaluated top-to-bottom, as is usual in scripting languages: the entry point is the start of the source code. Since definitions must precede use, programs are typically structured with definitions at the top and the code to execute at the bottom (unindented), similar to code for a one-pass compiler, such as in Pascal.
Alternatively, a program can be structured with an explicit main function containing the code to be executed when a program is executed directly, but which can also be invoked by importing the program as a module and calling the function. This can be done by the following idiom, which relies on the internal variable __name__ being set to __main__ when a program is executed, but not when it is imported as a module (in which case it is instead set to the module name); there are many variants of this structure:
In this idiom, the call to the named entry point main is explicit, and the interaction with the operating system (receiving the arguments, calling system exit) are done explicitly by library calls, which are ultimately handled by the Python runtime. This contrast with C, where these are done implicitly by the runtime, based on convention.
The QB64 language has no main function, the code that is not within a function, or subrutine is executed first, from top to bottom:
Command line arguments (if any) can be read using the COMMAND$ function:
In Ruby, there is no distinct main function. The code written without additional ""class .. end"", ""module .. end"" enclosures is executed directly, step by step, in context of special ""main"" object. This object can be referenced using:
and contain the following properties:
Methods defined without additional classes/modules are defined as private methods of the ""main"" object, and, consequently, as private methods of almost any other object in Ruby:
Number and values of command-line arguments can be determined using the single ARGV constant array:
Note that first element of ARGV, ARGV, contains the first command-line argument, not the name of program executed, as in C. The name of program is available using $0 or $PROGRAM_NAME.
Similar to Python, one could use:
In Visual Basic, when a project contains no forms, the startup object may be the Main() procedure. The Command$ function can be optionally used to access the argument portion of the command line used to launch the program:
In Xojo, there are two different project types, each with a different main entry point. Desktop (GUI) applications start with the App.Open event of the project's Application object. Console applications start with the App.Run event of the project's ConsoleApplication object. In both instances, the main function is automatically generated, and cannot be removed from the project."
"52","Ceylon is an object-oriented, strongly statically typed programming language with an emphasis on immutability, created by Red Hat. Ceylon programs run on the Java virtual machine (JVM), and can be compiled to JavaScript.
The language design focuses on source code readability, predictability, toolability, modularity, and metaprogrammability.
Important features of Ceylon include:
The name ""Ceylon"" is an oblique reference to Java, in that Java and Sri Lanka, formerly known as Ceylon, are islands known for growth and export of coffee and tea.
In August 2017, Ceylon was donated to the Eclipse Foundation.
Ceylon is heavily influenced by Java's syntax, but adds many new features.
One of the most novel aspects of Ceylon is its type system. Ceylon foregoes Java's primitive types and boxing in favor of a type system composed entirely of first-class objects. While this may cause boxing overhead in some situations, it makes the type system more uniform.
Ceylon allows for union and intersection types, in a similar fashion to TypeScript, Whiley and Flow.
Union types, written A|B, allow a variable to have more than one type. The following example shows a Ceylon function which may take either an integer or a string:
Intersection types, written A&B, are the theoretical foundation of flow-sensitive typing:
The condition is Integer input narrows the type of input to <Integer|String> & Integer,
which distributes to Integer&Integer | String&Integer,
which, as String and Integer are disjoint types, is equivalent to Integer&Integer | Nothing (Nothing is the empty bottom type),
which simplifies to just Integer.
Union and intersection types are used to provide null safety.
The top type of the Ceylon type hierarchy is the class Anything,
which has two subclasses: Object, the superclass of all normal classes and all interfaces, and Null, with the only instance null.
Since Object and Null are disjoint types, most regular types like Integer or List<String> are not nullable;
a nullable type is the union Integer|Null, abbreviated Integer?.
Intersection types can be used to get a non-optional type out of a possibly-optional type, such as a type parameter. For example, the signature of a function that removes null elements from a stream of values could be:
When removeNulls is called with a stream of Integer|Null elements, the result will be a stream of <Integer|Null> & Object elements, which simplifies to Integer.
Similarly to many modern languages, Ceylon supports first class functions and higher order functions, including function types and anonymous functions
Similar to Java and many other languages, and with a similar mechanism as algebraic types, Ceylon supports enumerated types, otherwise known as enums. This is implemented in Ceylon with a pattern of limiting the instances of an abstract class at declaration to a limited set of objects (in this case, singleton instances). Another way to implement this pattern is with the new constructor feature in Ceylon 1.2 where the objects are implemented as different named constructor declarations.
Ceylon is strongly and statically typed, but also has support for type inference.
The value keyword is used to infer the type of a variable,
and the function keyword is used to infer the type of a function.
The following two definition pairs are each equivalent:
However, to make single-pass type inference possible, type inference is only allowed for non-toplevel and unshared declarations.
By default the starter (ceylon run) runs the shared run() function of a module:
but any other shared function without parameters can be used as main calling the program with the—run parameter, like this:
ceylon run --compile=force --run hello default
Versions of Ceylon released:
All parts of Ceylon are available under open source licenses, mostly the Apache License. Part of the source code is licensed under LGPL."
"53","

Dashcode was a software application created by Apple Inc. that was included with Mac OS X Leopard and facilitates the development of widgets for Dashboard. It was first included on new MacBooks shipping around the time of May 24, 2006, as part of the Xcode developer tools.
Dashcode, Version 3.0 (328), was included as part of Apple's Xcode developer tools on the Mac OS X Snow Leopard DVD as an optional install.
The last iteration of Dashcode, Version 3.0.5 for Xcode 4, is still available to developer account holders as an optional install from Downloads for Apple Developers (Apple Developer ID required).
Steve Jobs mentioned Dashcode as a new feature to be included in Leopard during his 2006 WWDC keynote speech.  Although not installed by default as part of an Xcode installation, the DVDs handed out at the WWDC did contain a version of Dashcode.  Although the version number was in fact lower than that of the ""MacBook build"", the WWDC build of Dashcode contained several additional templates, as well as some interface and functionality improvements.  This WWDC build launched on both Mac OS X v10.4 and the WWDC build of Mac OS X 10.5 (""Leopard""), but was unusable on 10.4 (crashes soon after startup).
On December 20, 2006, Apple released a public beta of Dashcode.  When announcing this release, Apple stated the beta had been ""scaled back"" for compatibility with Mac OS X v10.4.  This beta expired on July 15, 2007.
Dashcode Version 2.0 (151) is included as part of Apple's iOS SDK. This allows for the creation of Web apps for the iOS version of Safari.
Dashcode Version 3.0.2 (336) is installed with Xcode on OS X Lion.
It is not known if this will allow for the local installation of Dashcode-created web apps, as such an ability will allow iOS to run a software layer akin to Mac OS X's Dashboard, which runs on a local installation. Currently, iOS maintains a separation between native code and web code, in that way native applications can access data from the Internet, web content can't be accessed by native applications save for Safari; likewise, web content (including web apps) can be run inside the Safari browser, but cannot have access to the filesystem or other internals of iOS and cannot be installed on the operating system in the same way as native code.
Native code software for iOS is currently developed using the Xcode suite, particularly an iPhone-centric version of Interface Builder packaged with the iOS SDK."
"54","

The iOS SDK (Software Development Kit) (formerly iPhone SDK) is a software development kit developed by Apple Inc. The kit allows for the development of mobile apps on Apple's iOS operating system.
While originally developing iPhone prior to its unveiling in 2007, Apple's then-CEO Steve Jobs did not intend to let third-party developers build native apps for iOS, instead directing them to make web applications for the Safari web browser. However, backlash from developers prompted the company to reconsider, with Jobs announcing in October 2007 that Apple would have a software development kit available for developers by February 2008. The SDK was released on March 6, 2008.
The SDK is a free download for users of Mac personal computers. It is not available for Microsoft Windows PCs. The SDK contains sets giving developers access to various functions and services of iOS devices, such as hardware and software attributes. It also contains an iPhone simulator to mimic the look and feel of the device on the computer while developing. New versions of the SDK accompany new versions of iOS. In order to test applications, get technical support, and distribute apps through App Store, developers are required to subscribe to the Apple Developer Program.
Combined with Xcode, the iOS SDK helps developers write iOS apps using officially-supported programming languages, including Swift and Objective-C. Other companies have also created tools that allow for the development of native iOS apps using their respective programming languages.
While originally developing iPhone prior to its unveiling in 2007, Apple's then-CEO Steve Jobs did not intend to let third-party developers build native apps for the iOS operating system, instead directing them to make web applications for the Safari web browser. However, backlash from developers prompted the company to reconsider, with Jobs announcing on October 17, 2007 that Apple would have a software development kit (SDK) available for developers by February 2008. The SDK was released on March 6, 2008.
The iOS SDK is a free download for Mac users. It is not available for Microsoft Windows personal computers. To test the application, get technical support, and distribute applications through App Store, developers are required to subscribe to the Apple Developer Program.
The SDK contents are separated into the following sets:
The SDK also contains an iPhone simulator, a program used to simulate the look and feel of iPhone on the developer's computer.
New SDK versions accompany new iOS versions.
The iOS SDK, combined with Xcode, helps developers write iOS applications using officially-supported programming languages, including Swift and Objective-C.
In 2008, Sun Microsystems announced plans to release a Java Virtual Machine (JVM) for iOS, based on the Java Platform, Micro Edition version of Java. This would enable Java applications to run on iPhone and iPod Touch. Soon after the announcement, developers familiar with the SDK's terms of agreement believed that by not allowing third-party applications to run in the background (answer a phone call and still run the application, for example), and not allowing an application to download code from another source, nor allowing an application to interact with a third-party application, Sun's development efforts could be hindered without Apple's cooperation. Sun also worked with a third-party company called Innaworks in attempts to get Java on iPhone. Despite the apparent lack of interest from Apple, a firmware leak of the 2007 iPhone release revealed an ARM chip with a processor with Jazelle support for embedded Java execution.
Novell announced in September 2009 that they had successfully developed MonoTouch, a software framework that let developers write native iPhone applications in the C# and .NET programming languages, while still maintaining compatibility with Apple's requirements.
iOS does not support Adobe Flash, and although Adobe has two versions of its software – Flash and Flash Lite – Apple views neither as suitable for the iPhone, claiming that full Flash is ""too slow to be useful"" and Flash Lite to be ""not capable of being used with the Web.""
In October 2009, Adobe announced that an upcoming update to its Creative Suite would feature a component to let developers build native iPhone apps using the company's Flash development tools. The software was officially released as part of the company's Creative Suite 5 collection of professional applications.
In April 2010, Apple made controversial changes to its iPhone Developer Agreement, requiring developers to use only ""approved"" programming languages in order to publish apps on App Store, and banning applications that used third-party development tools. After developer backlash and news of a potential antitrust investigation, Apple again revised its agreement in September, allowing the use of third-party development tools."
"55","An Intermediate representation (IR) is the data structure or code used internally by a compiler or virtual machine to represent source code. An IR is designed to be conducive for further processing, such as optimization and translation. A ""good"" IR must be accurate – capable of representing the source code without loss of information – and independent of any particular source or target language. An IR may take one of several forms: an in-memory data structure, or a special tuple- or stack-based code readable by the program. In the latter case it is also called an intermediate language.
A canonical example is found in most modern compilers, where the linear human-readable text representing a program is transformed into an intermediate graph structure that allows flow analysis and re-arrangement before creating a sequence of actual CPU instructions. Use of an intermediate representation such as this allows compiler systems like the GNU Compiler Collection and LLVM to be used by many different source languages to generate code for many different target architectures.
An intermediate language is the language of an abstract machine designed to aid in the analysis of computer programs. The term comes from their use in compilers, where the source code of a program is translated into a form more suitable for code-improving transformations before being used to generate object or machine code for a target machine. The design of an intermediate language typically differs from that of a practical machine language in three fundamental ways:
A popular format for intermediate languages is three-address code.
The term is also used to refer to languages used as intermediates by some high-level programming languages which do not output object or machine code themselves, but output the intermediate language only. This intermediate language is submitted to a compiler for such language, which then outputs finished object or machine code. This is usually done to ease the process of optimization or to increase portability by using an intermediate language that has compilers for many processors and operating systems, such as C. Languages used for this fall in complexity between high-level languages and low-level languages, such as assembly languages.
Though not explicitly designed as an intermediate language, C's nature as an abstraction of assembly and its ubiquity as the de facto system language in Unix-like and other operating systems has made it a popular intermediate language: Eiffel, Sather, Esterel, some dialects of Lisp (Lush, Gambit), Haskell (Glasgow Haskell Compiler), Squeak's Smalltalk-subset Slang, Cython, Seed7, SystemTap, Vala, and others make use of C as an intermediate language. Variants of C have been designed to provide C's features as a portable assembly language, including C-- and the C Intermediate Language.
Any language targeting a virtual machine or p-code machine can be considered an intermediate language:
The GNU Compiler Collection (GCC) uses several intermediate languages internally to simplify portability and cross-compilation. Among these languages are
The LLVM compiler framework is based on the LLVM IR intermediate language, of which the compact, binary serialized representation is also referred to as ""bitcode"" and has been productized by Apple.
The ILOC intermediate language is used in classes on compiler design as a simple target language.
Static analysis tools often use an intermediate representation. For instance, radare2 is a toolbox for binary files analysis and reverse-engineering. It uses the intermediate languages ESIL et REIL to analyze binary files."
"56","Blocks are a non-standard extension added by Apple Inc. to Clang's implementations of the C, C++, and Objective-C programming languages that uses a lambda expression-like syntax to create closures within these languages. Blocks are supported for programs developed for Mac OS X 10.6+ and iOS 4.0+, although third-party runtimes allow use on Mac OS X 10.5 and iOS 2.2+ and non-Apple systems.
Apple designed blocks with the explicit goal of making it easier to write programs for the Grand Central Dispatch threading architecture, although it is independent of that architecture and can be used in much the same way as closures in other languages. Apple has implemented blocks both in their own branch of the GNU Compiler Collection and in the upstream Clang LLVM compiler front end. Language runtime library support for blocks is also available as part of the LLVM project. The Khronos group uses blocks syntax to enqueue kernels from within kernels as of version 2.0 of OpenCL.
Like function definitions, blocks can take arguments, and declare their own variables internally. Unlike ordinary C function definitions, their value can capture state from their surrounding context. A block definition produces an opaque value which contains both a reference to the code within the block and a snapshot of the current state of local stack variables at the time of its definition. The block may be later invoked in the same manner as a function pointer. The block may be assigned to variables, passed to functions, and otherwise treated like a normal function pointer, although the application programmer (or the API) must mark the block with a special operator (Block_copy) if it's to be used outside the scope in which it was defined.
Given a block value, the code within the block can be executed at any later time by calling it, using the same syntax that would be used for calling a function.
A simple example capturing mutable state in the surrounding scope is an integer range iterator:
Blocks bear a superficial resemblance to GCC's extension of C to support lexically scoped nested functions. However, GCC's nested functions, unlike blocks, must not be called after the containing scope has exited, as that would result in undefined behavior.
GCC-style nested functions currently use dynamic creation of executable thunks on most architectures when taking the address of the nested function.  On most architectures (including X86), these thunks are created on the stack, which requires marking the stack executable.  Executable stacks  are generally considered to be a potential security hole.  Blocks do not require the use of executable thunks, so they do not share this weakness. On the other hand, blocks introduces a completely new type for the pointer, while pointers to nested functions in GCC are regular function pointers and can be used directly with existing code."
"57"," (Learn how and when to remove this template message)
This page is intended to list all current compilers, compiler generators, interpreters, translators, tool foundations, assemblers, automatable command line interfaces (shells), etc.
This list is incomplete. A more extensive list of source-to-source compilers can be found here.
Liogo NET Compiler
http://liogo.sourceforge.net/
The Real LOGO Compiler
http://lhogho.sourceforge.net/
HaskellWiki maintains a list of Haskell implementations. Many of them are compilers.
Production quality, open source compilers.
Research compilers are mostly not robust or complete enough to handle real, large applications. They are used mostly for fast prototyping new language features and new optimizations in research areas."
"58","


Code::Blocks is a free, open-source cross-platform IDE that supports multiple compilers including GCC, Clang and Visual C++. It is developed in C++ using wxWidgets as the GUI toolkit. Using a plugin architecture, its capabilities and features are defined by the provided plugins.
Currently, Code::Blocks is oriented towards C, C++, and Fortran. It has a custom build system and optional Make support.
Code::Blocks is being developed for Windows, Linux, and macOS and has been ported to FreeBSD,OpenBSD and Solaris.
After releasing two release candidate versions, 1.0rc1 on July 25, 2005 and 1.0rc2 on October 25, 2005, instead of making a final release, the project developers started adding many new features, with the final release being repeatedly postponed. Instead, there were nightly builds of the latest SVN version made available on a daily basis.[citation needed]
The first stable release was on February 28, 2008, with the version number changed to 8.02. The versioning scheme was changed to that of Ubuntu, with the major and minor number representing the year and month of the release. Version 17.12 is the latest stable release; however for the most up-to-date version the user can download the relatively stable nightly build or download the source code from SVN.
Jennic Limited distributes a version of Code::Blocks customized to work with its microcontrollers.
Code::Blocks supports multiple compilers, including GCC, MinGW, Digital Mars, Microsoft Visual C++, Borland C++, LLVM Clang, Watcom, LCC and the Intel C++ compiler. Although the IDE was designed for the C++ language, there is some support for other languages, including Fortran and D. A plug-in system is included to support other programming languages.
The IDE features syntax highlighting and code folding (through its Scintilla editor component), C++ code completion, class browser, a hex editor and many other utilities. Opened files are organized into tabs. The code editor supports font and font size selection and personalized syntax highlighting colours.
The Code::Blocks debugger has full breakpoint support. It also allows the user to debug their program by having access to the local function symbol and argument display, user-defined watches, call stack, disassembly, custom memory dump, thread switching, CPU registers and GNU Debugger Interface.
As of version 13.12 Code::Blocks comes with a GUI designer called wxSmith. It is a derivative port of wxWidgets version 2.9.4. To make a complete wxWidgets application, the appropriate wxWidgets SDK must be installed.
Some of Code::Blocks features are targeted at users migrating from other IDE's - these include Dev-C++, Microsoft Visual C++ project import (MSVC 7 & 10), and Dev-C++ Devpak support.
Code::Blocks uses a custom build system, which stores its information in XML-based project files. It can optionally use external makefiles, which simplifies interfacing with projects using the GNU or qmake build systems."
"59","

Qt Creator is a cross-platform C++, JavaScript and QML integrated development environment which is part of the SDK for the Qt GUI application development framework. It includes a visual debugger and an integrated GUI layout and forms designer. The editor's features include syntax highlighting and autocompletion. Qt Creator uses the C++ compiler from the GNU Compiler Collection on Linux and FreeBSD. On Windows it can use MinGW or MSVC with the default install and can also use Microsoft Console Debugger when compiled from source code. Clang is also supported.
Development of what would eventually become Qt Creator had begun by 2007 or earlier under transitional names Workbench and later Project Greenhouse. It debuted during the later part of the Qt 4 era, starting with the release of Qt Creator, version 1.0 in March 2009 and subsequently bundled with Qt 4.5 in SDK 2009.3.
This was at a time when the standalone Qt Designer application was still the widget layout tool of choice for developers. There is no indication that Creator had layout capability at this stage. The record is somewhat muddied on this point (perhaps due to changes in ownership or the emphasis on Qt Quick), but the integration of Qt Designer under Qt Creator is first mentioned at least as early as Qt 4.7 (ca. late 2011). Currently (in the Qt 5 era) it is simply stated that ""[Qt Designer's] functionality is now included as part of [sic] Qt Creator IDE.""
Qt Creator includes a project manager that can use a variety of project formats such as .pro, CMake, Autotools and others. A project file can contain information such as what files are included into the project, custom build steps and settings for running the applications.
Qt Creator includes a code editor and integrates Qt Designer for designing and building graphical user interfaces (GUIs) from Qt widgets.
The code editor in Qt Creator supports syntax highlighting for various languages. In addition to that, the code editor can parse code in C++ and QML languages and as a result code completion, context-sensitive help, semantic navigation are provided.
Qt Designer is a tool for designing and building graphical user interfaces (GUIs) from Qt widgets. It is possible to compose and customize the widgets or dialogs and test them using different styles and resolutions directly in the editor. Widgets and forms created with Qt Designer are integrated with programmed code, using the Qt signals and slots mechanism.
Qt Quick Designer is a tool for developing animations by using a declarative programming language QML.
Qt Creator provides support for building and running Qt applications for desktop environments (Windows, Linux, FreeBSD and Mac OS), mobile devices (Android, BlackBerry, iOS, Maemo, and MeeGo) and embedded Linux devices. Build settings allow to switch between build targets, different Qt versions and build configurations. For mobile device targets, Qt Creator can generate an installation package, install it to a mobile device that is attached to the development computer and run it there. Installation packages can be published on the Ovi Store.
Qt Creator is integrated with a set of tools, such as version control systems and Qt Simulator. The following version control systems are supported:
Qt Simulator is a tool for testing Qt applications that are intended for mobile devices in an environment similar to that of the device.
Qt Creator does not include a debugger for native code. It provides a debugger plugin that acts as an interface between the Qt Creator core and external native debuggers to debug the C++ language. Qt Creator displays the raw information provided by the native debuggers in a simplified manner. Debuggers supported are:
"
"60","
In computer science, reference counting is a technique of storing the number of references, pointers, or handles to a resource such as an object, block of memory, disk space or other resource.
It may also refer, more specifically, to a garbage collection algorithm that uses these reference counts to deallocate objects which are no longer referenced.[not verified in body]
As a collection algorithm, reference counting tracks, for each object, a count of the number of references to it held by other objects. If an object's reference count reaches zero, the object has become inaccessible, and can be destroyed.
When an object is destroyed, any objects referenced by that object also have their reference counts decreased. Because of this, removing a single reference can potentially lead to a large number of objects being freed. A common modification allows reference counting to be made incremental: instead of destroying an object as soon as its reference count becomes zero, it is added to a list of unreferenced objects, and periodically (or as needed) one or more items from this list are destroyed.
Simple reference counts require frequent updates. Whenever a reference is destroyed or overwritten, the reference count of the object it references is decremented, and whenever one is created or copied, the reference count of the object it references is incremented.
Reference counting is also used in disk operating systems and distributed systems, where full non-incremental tracing garbage collection is too time consuming because of the size of the object graph and slow access speed.[citation needed]
The main advantage of the reference counting over tracing garbage collection is that objects are reclaimed as soon as they can no longer be referenced, and in an incremental fashion, without long pauses for collection cycles and with clearly defined lifetime of every object. In real-time applications or systems with limited memory, this is important to maintain responsiveness. Reference counting is also among the simplest forms of memory management to implement. It also allows for effective management of non-memory resources such as operating system objects, which are often much scarcer than memory (tracing GC systems use finalizers for this[citation needed], but the delayed reclamation may cause problems).  Weighted reference counts are a good solution for garbage collecting a distributed system.
Tracing garbage collection cycles are triggered too often if the set of live objects fills most of the available memory[citation needed]; it requires extra space to be efficient[citation needed]. Reference counting performance does not deteriorate as the total amount of free space decreases.
Reference counts are also useful information to use as input to other runtime optimizations. For example, systems that depend heavily on immutable objects such as many functional programming languages can suffer an efficiency penalty due to frequent copies.[citation needed] However, if the compiler (or runtime system) knows that a particular object has only one reference (as most do in many systems), and that the reference is lost at the same time that a similar new object is created (as in the string append statement str ← str + ""a""), it can replace the operation with a mutation on the original object.
Reference counting in naive form has two main disadvantages over the tracing garbage collection, both of which require additional mechanisms to ameliorate:
In addition to these, if the memory is allocated from a free list, reference counting suffers from poor locality. Reference counting alone cannot move objects to improve cache performance, so high performance collectors implement a tracing garbage collector as well. Most implementations (such as the ones in PHP and Objective-C) suffer from poor cache performance since they do not implement copying objects.
When dealing with garbage collection schemes, it is often helpful to think of the reference graph, which is a directed graph where the vertices are objects and there is an edge from an object A to an object B if A holds a reference to B. We also have a special vertex or vertices representing the local variables and references held by the runtime system, and no edges ever go to these nodes, although edges can go from them to other nodes.
In this context, the simple reference count of an object is the in-degree of its vertex. Deleting a vertex is like collecting an object. It can only be done when the vertex has no incoming edges, so it does not affect the out-degree of any other vertices, but it can affect the in-degree of other vertices, causing their corresponding objects to be collected as well if their in-degree also becomes 0 as a result.
The connected component containing the special vertex contains the objects that can't be collected, while other connected components of the graph only contain garbage. If a reference-counting garbage collection algorithm is implemented, then each of these garbage components must contain at least one cycle; otherwise, they would have been collected as soon as their reference count (i.e., the number of incoming edges) dropped to zero.
Incrementing and decrementing reference counts every time a reference is created or destroyed can significantly impede performance. Not only do the operations take time, but they damage cache performance and can lead to pipeline bubbles. Even read-only operations like calculating the length of a list require a large number of reads and writes for reference updates with naive reference counting.
One simple technique is for the compiler to combine a number of nearby reference updates into one. This is especially effective for references which are created and quickly destroyed. Care must be taken, however, to put the combined update at the right position so that a premature free be avoided.
The Deutsch-Bobrow method of reference counting capitalizes on the fact that most reference count updates are in fact generated by references stored in local variables. It ignores these references, only counting references in data structures, but before an object with reference count zero can be deleted, the system must verify with a scan of the stack and registers that no other reference to it still exists.
Another technique devised by Henry Baker involves deferred increments, in which references which are stored in local variables do not immediately increment the corresponding reference count, but instead defer this until it is necessary. If such a reference is destroyed quickly, then there is no need to update the counter. This eliminates a large number of updates associated with short-lived references (such as the above list-length-counting example). However, if such a reference is copied into a data structure, then the deferred increment must be performed at that time. It is also critical to perform the deferred increment before the object's count drops to zero, resulting in a premature free.
A dramatic decrease in the overhead on counter updates was obtained by Levanoni and Petrank. They introduce the update coalescing method which coalesces many of the redundant reference count updates. Consider a pointer that in a given interval of the execution is updated several times. It first points to an object O1, then to an object O2, and so forth until at the end of the interval it points to some object On. A reference counting algorithm would typically execute rc(O1)--, rc(O2)++, rc(O2)--, rc(O3)++, rc(O3)--, ..., rc(On)++. But most of these updates are redundant. In order to have the reference count properly evaluated at the end of the interval it is enough to perform rc(O1)-- and rc(On)++. The rest of the updates are redundant.
Levanoni and Petrank showed in 2001 how to use such update coalescing in a reference counting collector. When using update coalescing with an appropriate treatment of new objects, more than 99% of the counter updates are eliminated for typical Java benchmarks. In addition, the need for atomic operations during pointer updates on parallel processors is eliminated. Finally, they presented an enhanced algorithm that may run concurrently with multithreaded applications employing only fine synchronization.
Blackburn and McKinley's ulterior reference counting method in 2003 combines deferred reference counting with a copying nursery, observing that the majority of pointer mutations occur in young objects. This algorithm achieves throughput comparable with the fastest generational copying collectors with the low bounded pause times of reference counting.
Perhaps the most obvious way to handle reference cycles is to design the system to avoid creating them. A system may explicitly forbid reference cycles; file systems with hard links often do this. Judicious use of ""weak"" (non-counted) references may also help avoid retain cycles; the Cocoa framework, for instance, recommends using ""strong"" references for parent-to-child relationships and ""weak"" references for child-to-parent relationships.
Systems may also be designed to tolerate or correct the cycles they create in some way. Developers may design code to explicitly ""tear down"" the references in a data structure when it is no longer needed, though this has the cost of requiring them to manually track that data structure's lifetime. This technique can be automated by creating an ""owner"" object that does the tearing-down when it is destroyed; for instance, a Graph object's destructor could delete the edges of its GraphNodes, breaking the reference cycles in the graph. Cycles may even be ignored in systems with short lives and a small amount of cyclic garbage, particularly when the system was developed using a methodology of avoiding cyclic data structures wherever possible, typically at the expense of efficiency.
Computer scientists have also discovered ways to detect and collect reference cycles automatically, without requiring changes in the data structure design. One simple solution is to periodically use a tracing garbage collector to reclaim cycles; since cycles typically constitute a relatively small amount of reclaimed space, the collector can be run much less often than with an ordinary tracing garbage collector.
Bacon describes a cycle-collection algorithm for reference counting with similarities to tracing collectors, including the same theoretical time bounds. It is based on the observation that a cycle can only be isolated when a reference count is decremented to a nonzero value. All objects which this occurs on are put on a roots list, and then periodically the program searches through the objects reachable from the roots for cycles. It knows it has found a cycle that can be collected when decrementing all the reference counts on a cycle of references brings them all down to zero. An enhanced version of this algorithm by Paz et al.
is able to run concurrently with other operations and improve its efficiency by using the update coalescing method of Levanoni and Petrank.
Although it is possible to augment simple reference counts in a variety of ways, often a better solution can be found by performing reference counting in a fundamentally different way. Here we describe some of the variants on reference counting and their benefits and drawbacks.
In weighted reference counting, each reference is assigned a weight, and each object tracks not the number of references referring to it, but the total weight of the references referring to it. The initial reference to a newly created object has a large weight, such as 216. Whenever this reference is copied, half of the weight goes to the new reference, and half of the weight stays with the old reference. Since the total weight does not change, the object's reference count does not need to be updated.
Destroying a reference decrements the total weight by the weight of that reference. When the total weight becomes zero, all references have been destroyed. If an attempt is made to copy a reference with a weight of 1, the reference has to ""get more weight"" by adding to the total weight and then adding this new weight to the reference, and then splitting it. An alternative in this situation is to create an indirection reference object, the initial reference to which is created with a large weight which can then be split.
The property of not needing to access a reference count when a reference is copied is particularly helpful when the object's reference count is expensive to access, for example because it is in another process, on disk, or even across a network. It can also help increase concurrency by avoiding many threads locking a reference count to increase it. Thus, weighted reference counting is most useful in parallel, multiprocess, database, or  distributed applications.
The primary problem with simple weighted reference counting is that destroying a reference still requires accessing the reference count, and if many references are destroyed this can cause the same bottlenecks we seek to avoid. Some adaptations of weighted reference counting seek to avoid this by attempting to give weight back from a dying reference to one which is still active.
Weighted reference counting was independently devised by Bevan and Watson & Watson. in 1987.
In indirect reference counting, it is necessary to keep track of whom the reference was obtained from.  This means that two references are kept to the object: a direct one which is used for invocations; and an indirect one which forms part of a diffusion tree, such as in the Dijkstra-Scholten algorithm, which allows a garbage collector to identify dead objects.  This approach prevents an object from being discarded prematurely.
Microsoft's Component Object Model (COM) and WinRT makes pervasive use of reference counting. In fact, two of the three methods that all COM objects must provide (in the IUnknown interface) increment or decrement the reference count.  Much of the Windows Shell and many Windows applications (including MS Internet Explorer, MS Office, and countless third-party products) are built on COM, demonstrating the viability of reference counting in large-scale systems.
One primary motivation for reference counting in COM is to enable interoperability across different programming languages and runtime systems. A client need only know how to invoke object methods in order to manage object life cycle; thus, the client is completely abstracted from whatever memory allocator the implementation of the COM object uses. As a typical example, a Visual Basic program using a COM object is agnostic towards whether that object was allocated (and must later be deallocated) by a C++ allocator or another Visual Basic component.
C++ does not perform reference-counting by default, fulfilling its philosophy of not adding functionality that might incur overheads where the user has not explicitly requested it. Objects that are shared but not owned can be accessed via a reference, raw pointer, or iterator (a conceptual generalisation of pointers).
However, by the same token, C++ provides native ways for users to opt-into such functionality: C++11 provides reference counted smart pointers, via the std::shared_ptr class, enabling automatic shared memory-management of dynamically allocated objects. Programmers can use this in conjunction with weak pointers (via std::weak_ptr) to break cyclic dependencies. Objects that are dynamically allocated but not intended to be shared can have their lifetime automatically managed using a std::unique_ptr. 
In addition, C++11's move semantics further reduce the extent to which reference counts need to be modified by removing the deep copy normally used when a function returns an object, as it allows for a simple copy of the pointer of said object.
Apple's Cocoa and Cocoa Touch frameworks (and related frameworks, such as Core Foundation) use manual reference counting, much like COM. Traditionally this was accomplished by the programmer manually sending retain and release messages to objects, but Automatic Reference Counting, a Clang compiler feature that automatically inserts these messages as needed, was added in iOS 5 and Mac OS X 10.7.Mac OS X 10.5 introduced a tracing garbage collector as an alternative to reference counting, but it was deprecated in OS X 10.8 and is expected to be removed in a future version. iOS has never supported a tracing garbage collector.
One language that uses reference counting for garbage collection is Delphi. Delphi is mostly not a garbage collected language, in that user-defined types must still be manually allocated and deallocated. It does provide automatic collection, however, for a few built-in types, such as strings, dynamic arrays, and interfaces, for ease of use and to simplify the generic database functionality. It is up to the programmer to decide whether to use the built-in types or not; Delphi programmers have complete access to low-level memory management like in C/C++. So all potential cost of Delphi's reference counting can, if desired, be easily circumvented.
Some of the reasons reference counting may have been preferred to other forms of garbage collection in Delphi include:
The GObject object-oriented programming framework implements reference counting on its base types, including weak references.  Reference incrementing and decrementing uses atomic operations for thread safety.  A significant amount of the work in writing bindings to GObject from high-level languages lies in adapting GObject reference counting to work with the language's own memory management system.
The Vala programming language uses GObject reference counting as its primary garbage collection system, along with copy-heavy string handling.
Perl also uses reference counting, without any special handling of circular references, although (as in Cocoa and C++ above), Perl does support weak references, which allows programmers to avoid creating a cycle.
PHP uses a reference counting mechanism for its internal variable management. Since PHP 5.3, it implements the algorithm from Bacon's above mentioned paper. PHP allows you to turn on and off the cycle collection with user-level functions. It also allows you to manually force the purging mechanism to be run.
Python also uses reference counting and offers cycle detection as well (and can reclaim them).
Squirrel also uses reference counting and offers cycle detection as well.
This tiny language is relatively unknown outside the video game industry; however, it is a concrete example of how reference counting can be practical and efficient (especially in realtime environments).[citation needed]
Tcl 8 uses reference counting for memory management of values (Tcl Obj structs[disambiguation needed]). Since Tcl's values are immutable, reference cycles are impossible to form and no cycle detection scheme is needed. Operations that would replace a value with a modified copy are generally optimized to instead modify the original when its reference count indicates it to be unshared. The references are counted at a data structure level, so the problems with very frequent updates discussed above do not arise.
Xojo also uses reference counting, without any special handling of circular references, although (as in Cocoa and C++ above), Xojo does support weak references, which allows programmers to avoid creating a cycle.
Many file systems maintain a count of the number of references to any particular block or file, for example the inode link count on Unix-style file systems. When the count falls to zero, the file can be safely deallocated. In addition, while references can still be made from directories, some Unixes allow that the referencing can be solely made by live processes, and there can be files that do not exist in the file system hierarchy.
This article is based on material taken from  the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later."
"61","
 (Learn how and when to remove this template message)
This is a comparison of the programming languages Java and C++.
The differences between the programming languages C++ and Java can be traced to their heritage, as they have different design goals.
C++ language was designed for systems and applications programming (a.k.a., infrastructure programming), extending the procedural programming language C, which was designed for efficient execution. To C, C++ added support for object-oriented programming, exception handling, lifetime-based resource management (RAII), generic programming, template metaprogramming, and the C++ Standard Library which includes generic containers and algorithms (STL), and many other general purpose facilities.
Java is a general-purpose, concurrent, class-based, object-oriented programming language that is designed to minimize implementation dependencies. It relies on a Java virtual machine to be secure and highly portable. It is bundled with an extensive library designed to provide a full abstraction of the underlying platform. Java is a statically typed object-oriented language that uses a syntax similar (but incompatible) to C++. It includes a documentation system called Javadoc.
The different goals in the development of C++ and Java resulted in different principles and design trade-offs between the languages. The differences are as follows:
It can also be done using the internal API sun.misc.Unsafe but that usage is highly discouraged and will be replaced by a public API in an upcoming Java version.
A rich amount of third-party libraries exist for GUI and other functions like: Adaptive Communication Environment (ACE), Crypto++, various XMPP Instant Messaging (IM) libraries,OpenLDAP, Qt, gtkmm.
Both C++ and Java provide facilities for generic programming, templates and generics, respectively. Although they were created to solve similar kinds of problems, and have similar syntax, they are quite different.
An example comparing C++ and Java exists in Wikibooks.
In addition to running a compiled Java program, computers running Java applications generally must also run the Java virtual machine (JVM), while compiled C++ programs can be run without external applications. Early versions of Java were significantly outperformed by statically compiled languages such as C++. This is because the program statements of these two closely related languages may compile to a few machine instructions with C++, while compiling into several byte codes involving several machine instructions each when interpreted by a JVM. For example:
Since performance optimizing is a very complex issue, it is very difficult to quantify the performance difference between C++ and Java in general terms, and most benchmarks are unreliable and biased. Given the very different natures of the languages, definitive qualitative differences are also difficult to draw. In a nutshell, there are inherent inefficiencies and hard limits on optimizing in Java, given that it heavily relies on flexible high-level abstractions, however, the use of a powerful JIT compiler (as in modern JVM implementations) can mitigate some issues. In any case, if the inefficiencies of Java are too great, compiled C or C++ code can be called from Java via the JNI.
Some inefficiencies that are inherent to the Java language include, mainly:
However, there are a number of benefits to Java's design, some realized, some only theorized:
Also, some performance problems occur in C++:
The C++ language is defined by ISO/IEC 14882, an ISO standard, which is published by the ISO/IEC JTC1/SC22/WG21 committee. The latest, post-standardization draft of C++11 is available as well.
The C++ language evolves via an open steering committee called the C++ Standards Committee. The committee is composed of the creator of C++ Bjarne Stroustrup, the convener Herb Sutter, and other prominent figures, including many representatives of industries and user-groups (i.e., the stake-holders). Being an open committee, anyone is free to join, participate, and contribute proposals for upcoming releases of the standard and technical specifications. The committee now aims to release a new standard every few years, although in the past strict review processes and discussions have meant longer delays between publication of new standards (1998, 2003, and 2011).
The Java language is defined by the Java Language Specification, a book which is published by Oracle.
The Java language continuously evolves via a process called the Java Community Process, and the world's programming community is represented by a group of people and organizations - the Java Community members—which is actively engaged into the enhancement of the language, by sending public requests - the Java Specification Requests - which must pass formal and public reviews before they get integrated into the language.
The lack of a firm standard for Java and the somewhat more volatile nature of its specifications have been a constant source of criticism by stake-holders wanting more stability and conservatism in the addition of new language and library features. In contrast, the C++ committee also receives constant criticism, for the opposite reason, i.e., being too strict and conservative, and taking too long to release new versions.
""C++"" is not a trademark of any company or organization and is not owned by any individual.
""Java"" is a trademark of Oracle Corporation.
"
"62","In computer programming, tracing garbage collection is a form of automatic memory management that consists of determining which objects should be deallocated (""garbage collected"") by tracing which objects are reachable by a chain of references from certain ""root"" objects, and considering the rest as ""garbage"" and collecting them. Tracing garbage collection is the most common type of garbage collection – so much so that ""garbage collection"" often refers to tracing garbage collection, rather than other methods such as reference counting – and there are a large number of algorithms used in implementation.
Informally, an object is reachable if it is referenced by at least one variable in the program, either directly or through references from other reachable objects. More precisely, objects can be reachable in only two ways:
The reachability definition of ""garbage"" is not optimal, insofar as the last time a program uses an object could be long before that object falls out of the environment scope. A distinction is sometimes drawn between syntactic garbage, those objects the program cannot possibly reach, and semantic garbage, those objects the program will in fact never again use. For example:
The problem of precisely identifying semantic garbage can easily be shown to be partially decidable: a program that allocates an object X, runs an arbitrary input program P, and uses X if and only if P finishes would require a semantic garbage collector to solve the halting problem. Although conservative heuristic methods for semantic garbage detection remain an active research area, essentially all practical garbage collectors focus on syntactic garbage.[citation needed]
Another complication with this approach is that, in languages with both reference types and unboxed value types, the garbage collector needs to somehow be able to distinguish which variables on the stack or fields in an object are regular values and which are references: in memory, an integer and a reference might look alike. The garbage collector then needs to know whether to treat the element as a reference and follow it, or whether it is a primitive value. One common solution is the use of tagged pointers.
The garbage collector can reclaim only objects that have no references pointing to them either directly or indirectly from the root set. However, some programs require weak references, which should be usable for as long as the object exists but should not prolong its lifetime. In discussions about weak references, ordinary references are sometimes called strong references. An object is eligible for garbage collection if there are no strong (i.e. ordinary) references to it, even though there still might be some weak references to it.
A weak reference is not merely just any pointer to the object that a garbage collector does not care about. The term is usually reserved for a properly managed category of special reference objects which are safe to use even after the object disappears because they lapse to a safe value. An unsafe reference that is not known to the garbage collector will simply remain dangling by continuing to refer to the address where the object previously resided. This is not a weak reference.
In some implementations, weak references are divided into subcategories. For example, the Java Virtual Machine provides three forms of weak references, namely soft references,phantom references, and regular weak references. A softly referenced object is only eligible for reclamation, if the garbage collector decides that the program is low on memory. Unlike a soft reference or a regular weak reference, a phantom reference does not provide access to the object that it references. Instead, a phantom reference is a mechanism that allows the garbage collector to notify the program when the referenced object has become phantom reachable. An object is phantom reachable, if it still resides in memory and it is referenced by a phantom reference, but its finalizer has already executed. Similarly, Microsoft.NET provides two subcategories of weak references, namely long weak references (tracks resurrection) and short weak references.
Data structures can also be devised which have weak tracking features. For instance, weak hash tables are useful. Like a regular hash table, a weak hash table maintains an association between pairs of objects, where each pair is understood to be a key and value. However, the hash table does not actually maintain a strong reference on these objects. A special behavior takes place when either the key or value or both become garbage: the hash table entry is spontaneously deleted. There exist further refinements such as hash tables which have only weak keys (value references are ordinary, strong references) or only weak values (key references are strong).
Weak hash tables are important for maintaining associations between objects, such that the objects engaged in the association can still become garbage if nothing in the program refers to them any longer (other than the associating hash table).
The use of a regular hash table for such a purpose could lead to a ""logical memory leak"": the accumulation of reachable data which the program does not need and will not use.
Tracing collectors are so called because they trace through the working set of memory. These garbage collectors perform collection in cycles. It is common for cycles to be triggered when there is not enough free memory for the memory manager to satisfy an allocation request. But cycles can often be requested by the mutator directly or run on a time schedule. The original method involves a naïve mark-and-sweep in which the entire memory set is touched several times.
In the naive mark-and-sweep method, each object in memory has a flag (typically a single bit) reserved for garbage collection use only. This flag is always cleared, except during the collection cycle. 
The first stage is the mark stage which does a tree traversal of the entire 'root set' and marks each object that is pointed to by a root as being 'in-use'. All objects that those objects point to, and so on, are marked as well, so that every object that is reachable via the root set is marked.
In the second stage, the sweep stage, all memory is scanned from start to finish, examining all free or used blocks; those not marked as being 'in-use' are not reachable by any roots, and their memory is freed. For objects which were marked in-use, the in-use flag is cleared, preparing for the next cycle.
This method has several disadvantages, the most notable being that the entire system must be suspended during collection; no mutation of the working set can be allowed. This can cause programs to 'freeze' periodically (and generally unpredictably), making some real-time and time-critical applications impossible. In addition, the entire working memory must be examined, much of it twice, potentially causing problems in paged memory systems.
Because of these performance problems, most modern tracing garbage collectors implement some variant of the tri-color marking abstraction, but simple collectors (such as the mark-and-sweep collector) often do not make this abstraction explicit.  Tri-color marking works as described below.
Three sets are created –  white, black and gray:
In many algorithms, initially the black set starts as empty, the gray set is the set of objects which are directly referenced from roots and the white set includes all other objects. Every object in memory is at all times in exactly one of the three sets. The algorithm proceeds as following:
When the gray set is empty, the scan is complete; the black objects are reachable from the roots, while the white objects are not and can be garbage-collected.
Since all objects not immediately reachable from the roots are added to the white set, and objects can only move from white to gray and from gray to black, the algorithm preserves an important invariant –  no black objects reference white objects. This ensures that the white objects can be freed once the gray set is empty. This is called the tri-color invariant. Some variations on the algorithm do not preserve this invariant but use a modified form for which all the important properties hold.
The tri-color method has an important advantage –  it can be performed ""on-the-fly"", without halting the system for significant periods of time. This is accomplished by marking objects as they are allocated and during mutation, maintaining the various sets. By monitoring the size of the sets, the system can perform garbage collection periodically, rather than as needed. Also, the need to touch the entire working set on each cycle is avoided.
Once the unreachable set has been determined, the garbage collector may simply release the unreachable objects and leave everything else as it is, or it may copy some or all of the reachable objects into a new area of memory, updating all references to those objects as needed. These are called ""non-moving"" and ""moving"" (or, alternatively, ""non-compacting"" and ""compacting"") garbage collectors, respectively.
At first, a moving algorithm may seem inefficient compared to a non-moving one, since much more work would appear to be required on each cycle. But the moving algorithm leads to several performance advantages, both during the garbage collection cycle itself and during program execution:
One disadvantage of a moving garbage collector is that it only allows access through references that are managed by the garbage collected environment, and does not allow pointer arithmetic. This is because any pointers to objects will be invalidated if the garbage collector moves those objects (they become dangling pointers). For interoperability with native code, the garbage collector must copy the object contents to a location outside of the garbage collected region of memory. An alternative approach is to pin the object in memory, preventing the garbage collector from moving it and allowing the memory to be directly shared with native pointers (and possibly allowing pointer arithmetic).
Not only do collectors differ in whether they are moving or non-moving, they can also be categorized by how they treat the white, gray and black object sets during a collection cycle.
The most straightforward approach is the semi-space collector, which dates to 1969. In this moving collector, memory is partitioned into an equally sized ""from space"" and ""to space"". Initially, objects are allocated in ""to space"" until it becomes full and a collection cycle is triggered. At the start of the cycle, the ""to space"" becomes the ""from space"", and vice versa. The objects reachable from the root set are copied from the ""from space"" to the ""to space"". These objects are scanned in turn, and all objects that they point to are copied into ""to space"", until all reachable objects have been copied into ""to space"". Once the program continues execution, new objects are once again allocated in the ""to space"" until it is once again full and the process is repeated. 
This approach is very simple, but since only one semi space is used for allocating objects, the memory usage is twice as high compared to other algorithms. The technique is also known as stop-and-copy. Cheney's algorithm is an improvement on the semi-space collector.
A mark and sweep garbage collector keeps a bit or two with each object to record if it is white or black. The grey set is kept as a separate list or using another bit. As the reference tree is traversed during a collection cycle (the ""mark"" phase), these bits are manipulated by the collector. A final ""sweep"" of the memory areas then frees white objects. The mark and sweep strategy has the advantage that, once the condemned set is determined, either a moving or non-moving collection strategy can be pursued. This choice of strategy can be made at runtime, as available memory permits. It has the disadvantage of ""bloating"" objects by a small amount.
A mark and don't sweep garbage collector, like the mark-and-sweep, keeps a bit with each object to record if it is white or black; the gray set is kept as a separate list or using another bit. There are two key differences here. First, black and white mean different things than they do in the mark and sweep collector. In a ""mark and don't sweep"" collector, all reachable objects are always black. An object is marked black at the time it is allocated, and it will stay black even if it becomes unreachable. A white object is unused memory and may be allocated. Second, the interpretation of the black/white bit can change. Initially, the black/white bit may have the sense of (0=white, 1=black). If an allocation operation ever fails to find any available (white) memory, that means all objects are marked used (black). The sense of the black/white bit is then inverted (for example, 0=black, 1=white). Everything becomes white. This momentarily breaks the invariant that reachable objects are black, but a full marking phase follows immediately, to mark them black again. Once this is done, all unreachable memory is white. No ""sweep"" phase is necessary.
It has been empirically observed that in many programs, the most recently created objects are also those most likely to become unreachable quickly (known as infant mortality or the generational hypothesis).  A generational GC (also known as ephemeral GC) divides objects into generations and, on most cycles, will place only the objects of a subset of generations into the initial white (condemned) set. Furthermore, the runtime system maintains knowledge of when references cross generations by observing the creation and overwriting of references. When the garbage collector runs, it may be able to use this knowledge to prove that some objects in the initial white set are unreachable without having to traverse the entire reference tree. If the generational hypothesis holds, this results in much faster collection cycles while still reclaiming most unreachable objects.
In order to implement this concept, many generational garbage collectors use separate memory regions for different ages of objects. When a region becomes full, the objects in it are traced, using the references from the older generation(s) as roots.  This usually results in most objects in the generation being collected (by the hypothesis), leaving it to be used to allocate new objects.  When a collection doesn't collect many objects (the hypothesis doesn't hold, for example because the program has computed a large collection of new objects it does want to retain), some or all of the surviving objects that are referenced from older memory regions are promoted to the next highest region, and the entire region can then be overwritten with fresh objects. This technique permits very fast incremental garbage collection, since the garbage collection of only one region at a time is all that is typically required.
Ungar's classic generation scavenger has two generations. It divides the youngest generation, called ""new space"", into a large ""eden"" in which new objects are created and two smaller ""survivor spaces"", past survivor space and future survivor space. The objects in the older generation that may reference objects in new space are kept in a ""remembered set"". On each scavenge, the objects in new space are traced from the roots in the remembered set and copied to future survivor space. If future survivor space fills up, the objects that do not fit are promoted to old space, a process called ""tenuring"". At the end of the scavenge, some objects reside in future survivor space, and eden and past survivor space are empty. Then future survivor space and past survivor space are exchanged and the program continues, allocating objects in eden. In Ungar's original system, eden is 5 times larger than each survivor space.
Generational garbage collection is a heuristic approach, and some unreachable objects may not be reclaimed on each cycle. It may therefore occasionally be necessary to perform a full mark and sweep or copying garbage collection to reclaim all available space. In fact, runtime systems for modern programming languages (such as Java and the .NET Framework) usually use some hybrid of the various strategies that have been described thus far; for example, most collection cycles might look only at a few generations, while occasionally a mark-and-sweep is performed, and even more rarely a full copying is performed to combat fragmentation. The terms ""minor cycle"" and ""major cycle"" are sometimes used to describe these different levels of collector aggression.
Simple stop-the-world garbage collectors completely halt execution of the program to run a collection cycle, thus guaranteeing that new objects are not allocated and objects do not suddenly become unreachable while the collector is running.
This has the obvious disadvantage that the program can perform no useful work while a collection cycle is running (sometimes called the ""embarrassing pause""). Stop-the-world garbage collection is therefore mainly suitable for non-interactive programs. Its advantage is that it is both simpler to implement and faster than incremental garbage collection.
Incremental and concurrent garbage collectors are designed to reduce this disruption by interleaving their work with activity from the main program. Incremental garbage collectors perform the garbage collection cycle in discrete phases, with program execution permitted between each phase (and sometimes during some phases). Concurrent garbage collectors do not stop program execution at all, except perhaps briefly when the program's execution stack is scanned. However, the sum of the incremental phases takes longer to complete than one batch garbage collection pass, so these garbage collectors may yield lower total throughput.
Careful design is necessary with these techniques to ensure that the main program does not interfere with the garbage collector and vice versa; for example, when the program needs to allocate a new object, the runtime system may either need to suspend it until the collection cycle is complete, or somehow notify the garbage collector that there exists a new, reachable object.
Some collectors can correctly identify all pointers (references) in an object; these are called precise (also exact or accurate) collectors, the opposite being a conservative or partly conservative collector. Conservative collectors assume that any bit pattern in memory could be a pointer if, interpreted as a pointer, it would point into an allocated object. Conservative collectors may produce false positives, where unused memory is not released because of improper pointer identification. This is not always a problem in practice unless the program handles a lot of data that could easily be misidentified as a pointer. False positives are generally less problematic on 64-bit systems than on 32-bit systems because the range of valid memory addresses tends to be a tiny fraction of the range of 64-bit values. Thus, an arbitrary 64-bit pattern is unlikely to mimic a valid pointer. A false negative can also happen if pointers are ""hidden"", for example using an XOR linked list. Whether a precise collector is practical usually depends on the type safety properties of the programming language in question. An example for which a conservative garbage collector would be needed is the C language, which allows typed (non-void) pointers to be type cast into untyped (void) pointers, and vice versa.
A related issue concerns internal pointers, or pointers to fields within an object. If the semantics of a language allow internal pointers, then there may be many different addresses that can refer to parts of the same object, which complicates determining whether an object is garbage or not. An example for this is the C++ language, in which multiple inheritance can cause pointers to base objects to have different addresses. In a tightly optimized program, the corresponding pointer to the object itself may have been overwritten in its register, so such internal pointers need to be scanned.
Performance of tracing garbage collectors – both latency and throughput – depends significantly on the implementation, workload, and environment. Naive implementations or use in very memory-constrained environments, notably embedded systems, can result in very poor performance compared with other methods, while sophisticated implementations and use in environments with ample memory can result in excellent performance.
In terms of throughput, tracing by its nature requires some implicit runtime overhead, though in some cases the amortized cost can be extremely low, in some cases even lower than one instruction per allocation or collection, outperforming stack allocation. Manual memory management requires overhead due to explicit freeing of memory, and reference counting has overhead from incrementing and decrementing reference counts, and checking if the count has overflowed or dropped to zero.
In terms of latency, simple stop-the-world garbage collectors pause program execution for garbage collection, which can happen at arbitrary times and take arbitrarily long, making them unusable for real-time computing, notably embedded systems, and a poor fit for interactive use, or any other situation where low latency is a priority. However, incremental garbage collectors can provide hard real-time guarantees, and on systems with frequent idle time and sufficient free memory, such as personal computers, garbage collection can be scheduled for idle times and have minimal impact on interactive performance. Manual memory management (as in C++) and reference counting have a similar issue of arbitrarily long pauses in case of deallocating a large data structure and all its children, though these only occur at fixed times, not depending on garbage collection.
It is difficult to compare the two cases directly, as their behavior depends on the situation. For example, in the best case for a garbage collecting system, allocation just increments a pointer, but in the best case for manual heap allocation, the allocator maintains freelists of specific sizes and allocation only requires following a pointer. However, this size segregation usually cause a large degree of external fragmentation, which can have an adverse impact on cache behaviour. Memory allocation in a garbage collected language may be implemented using heap allocation behind the scenes (rather than simply incrementing a pointer), so the performance advantages listed above don't necessarily apply in this case. In some situations, most notably embedded systems, it is possible to avoid both garbage collection and heap management overhead by preallocating pools of memory and using a custom, lightweight scheme for allocation/deallocation.
The overhead of write barriers is more likely to be noticeable in an imperative-style program which frequently writes pointers into existing data structures than in a functional-style program which constructs data only once and never changes them.
Some advances in garbage collection can be understood as reactions to performance issues. Early collectors were stop-the-world collectors, but the performance of this approach was distracting in interactive applications. Incremental collection avoided this disruption, but at the cost of decreased efficiency due to the need for barriers. Generational collection techniques are used with both stop-the-world and incremental collectors to increase performance; the trade-off is that some garbage is not detected as such for longer than normal.
While garbage collection is generally nondeterministic, it is possible to use it in hard real-time systems. A real-time garbage collector should guarantee that even in the worst case it will dedicate a certain number of computational resources to mutator threads. Constraints imposed on a real-time garbage collector are usually either work based or time based. A time based constraint would look like: within each time window of duration T, mutator threads should be allowed to run at least for Tm time. For work based analysis, MMU (minimal mutator utilization) is usually used as a real-time constraint for the garbage collection algorithm.
One of the first implementations of hard real-time garbage collection for the JVM was based on the Metronome algorithm, whose commercial implementation is available as part of the IBM WebSphere Real Time. Another hard real-time garbage collection algorithm is Staccato, available in the IBM's J9 JVM, which also provides scalability to large multiprocessor architectures, while bringing various advantages over Metronome and other algorithms which, on the contrary, require specialized hardware."
"63","In computer science, the Boehm–Demers–Weiser garbage collector, often simply known as Boehm GC, is a conservative garbage collector for C and C++.
Boehm GC is free software distributed under a permissive free software licence similar to the X11 license.
The developer describes the operation of the collector as follows:
Boehm GC can also run in leak detection mode in which memory management is still done manually, but the Boehm GC can check if it is done properly. In this way a programmer can find memory leaks and double deallocations.
Boehm GC is also distributed with a C string handling library called cords. This is similar to ropes in C++ (strings are trees of small arrays, and they never change), but instead of using reference counting for proper deallocation, it relies on garbage collection to free objects. Cords are good at handling very large texts, modifications to them in the middle, slicing, concatenating, and keeping history of changes (undo/redo functionality).
The garbage collector works with most unmodified C programs, simply by replacing malloc() with GC_MALLOC() calls, replacing realloc() with GC_REALLOC() calls, and removing free() calls. The code piece below shows how one can use Boehm instead of traditional malloc and free in C.
The Boehm GC is used by many projects that are implemented in C or C++ like Inkscape, as well as by runtime environments for a number of other languages, including the GNU Compiler for Java runtime environment, the Portable.NET project, Embeddable Common Lisp, GNU Guile,  the Mono implementation of the Microsoft .NET platform (also using precise compacting GC since version 2.8), and libgc-d (a binding to libgc for the D programming language, used primarily in the MCI). It supports numerous operating systems, including many Unix variants (such as Mac OS X) and Microsoft Windows, and provides a number of advanced features including incremental collection, parallel collection and a variety of finalizer semantics."
"64","In computer science, manual memory management refers to the usage of manual instructions by the programmer to identify and deallocate unused objects, or garbage. Up until the mid-1990s, the majority of programming languages used in industry supported manual memory management, though garbage collection has existed since 1959, when it was introduced with Lisp. Today, however, languages with garbage collection such as Java are increasingly popular and the languages Objective-C and Swift provide similar functionality through Automatic Reference Counting. The main manually managed languages still in widespread use today are C and C++ – see C dynamic memory allocation.
All programming languages use manual techniques to determine when to allocate a new object from the free store. C uses the malloc function; C++ and Java use the new operator; and many other languages (such as Python) allocate all objects from the free store. Determination of when an object ought to be created (object creation) is generally trivial and unproblematic, though techniques such as object pools mean an object may be created before immediate use. The fundamental issue is object destruction – determination of when an object is no longer needed (i.e. is garbage), and arranging for its underlying storage to be returned to the free store so that it may be re-used to satisfy future memory requests. In manual memory allocation, this is also specified manually by the programmer; via functions such as free() in C, or the delete operator in C++ – this contrasts with automatic destruction of objects held in automatic variables, notably (non-static) local variables of functions, which are destroyed at the end of their scope in C and C++.
Manual memory management is known to enable several major classes of bugs into a program when used incorrectly, notably violations of memory safety or memory leaks. These are a significant source of security bugs.
Languages which exclusively use garbage collection are known to avoid the last two classes of defects.  Memory leaks can still occur (and bounded leaks frequently occur with generational or conservative garbage collection), but are generally less severe than memory leaks in manual systems.
Manual memory management has one correctness advantage, which is that it allows automatic resource management via the Resource Acquisition Is Initialization (RAII) paradigm.
This arises when objects own scarce system resources (like graphics resources, file handles, or database connections) which must be relinquished when an object is destroyed – when the lifetime of the resource ownership should be tied to the lifetime of the object. Languages with manual management can arrange this by acquiring the resource during object initialization (in the constructor), and releasing during object destruction (in the destructor), which occurs at a precise time. This is known as Resource Acquisition Is Initialization.
This can also be used with deterministic reference counting. In C++, this ability is put to further use to automate memory deallocation within an otherwise-manual framework, use of the shared_ptr template in the language's standard library to perform memory management is a common paradigm. shared_ptr is not suitable for all object usage patterns, however.
This approach is not usable in most garbage collected languages – notably tracing garbage collectors or more advanced reference counting – due to finalization being non-deterministic, and sometimes not occurring at all. That is, it is difficult to define (or determine) when or if a finalizer method might be called; this is commonly known as the finalizer problem. Java and other GC'd languages frequently use manual management for scarce system resources besides memory via the dispose pattern: any object which manages resources is expected to implement the dispose() method, which releases any such resources and marks the object as inactive.  Programmers are expected to invoke dispose() manually as appropriate to prevent ""leaking"" of scarce graphics resources.  Depending on the finalize() method (how Java implements finalizers) to release graphics resources is widely viewed as poor programming practice among Java programmers, and similarly the analogous __del__() method in Python cannot be relied on for releasing resources. For stack resources (resources acquired and released within a single block of code), this can be automated by various language constructs, such as Python's with, C#'s using or Java's try-with-resources.
Many advocates of manual memory management argue that it affords superior performance when compared to automatic techniques such as garbage collection. Traditionally latency was the biggest advantage, but this is no longer the case. Manual allocation frequently has superior locality of reference.[citation needed]
Manual allocation is also known to be more appropriate for systems where memory is a scarce resource, due to faster reclamation. Memory systems can and do frequently ""thrash"" as the size of a program's working set approaches the size of available memory; unused objects in a garbage-collected system remain in an unreclaimed state for longer than in manually managed systems, because they are not immediately reclaimed, increasing the effective working set size.
Manual management has a number of documented performance disadvantages:
Latency is a debated point that has changed over time, with early garbage collectors and simple implementations performing very poorly compared to manual memory management, but sophisticated modern garbage collectors often performing as well or better than manual memory management.
Manual allocation does not suffer from the long ""pause"" times that occur in simple stop-the-world garbage collection, although modern garbage collectors have collection cycles which are often not noticeable.
Manual memory management and garbage collection both suffer from potentially unbounded deallocation times – manual memory management because deallocating a single object may require deallocating its members, and recursively its members' members, etc., while garbage collection may have long collection cycles. This is especially an issue in real time systems, where unbounded collection cycles are generally unacceptable; real-time garbage collection is possible by pausing the garbage collector, while real-time manual memory management requires avoiding large deallocations, or manually pausing deallocation."
"65","In computer science, a smart pointer is an abstract data type that simulates a pointer while providing added features, such as automatic memory management or bounds checking. Such features are intended to reduce bugs caused by the misuse of pointers, while retaining efficiency. Smart pointers typically keep track of the memory they point to, and may also be used to manage other resources, such as network connections and file handles. Smart pointers originated in the programming language C++.
Pointer misuse can be a major source of bugs. Smart pointers prevent most situations of memory leaks by making the memory deallocation automatic. More generally, they make object destruction automatic: an object controlled by a smart pointer is automatically destroyed (finalized and then deallocated) when the last (or only) owner of an object is destroyed, for example because the owner is a local variable, and execution leaves the variable's scope. Smart pointers also eliminate dangling pointers by postponing destruction until an object is no longer in use.
Several types of smart pointers exist. Some work with reference counting, others by assigning ownership of an object to one pointer. If a language supports automatic garbage collection (for example, Java or C#), then smart pointers are unneeded for the reclaiming and safety aspects of memory management, yet are useful for other purposes, such as cache data structure residence management and resource management of objects such as file handles or network sockets.
In C++, a smart pointer is implemented as a template class that mimics, by means of operator overloading, the behaviors of a traditional (raw) pointer, (e.g. dereferencing, assignment) while providing additional memory management features.
Smart pointers can facilitate intentional programming by expressing, in the type, how the memory of the referent of the pointer will be managed. For example, if a C++ function returns a pointer, there is no way to know whether the caller should delete the memory of the referent when the caller is finished with the information.
Traditionally, naming conventions have been used to resolve the ambiguity, which is an error-prone, labor-intensive approach. C++11 introduced a way to ensure correct memory management in this case by declaring the function to return a unique_ptr,
The declaration of the function return type as a unique_ptr makes explicit the fact that the caller takes ownership of the result, and the C++ runtime ensures that the memory for *some_type will be reclaimed automatically. Before C++11, unique_ptr can be replaced with auto_ptr.
To ease the allocation of a 
C++11 introduced:
and similarly 
Since C++14 one can use:
It is preferred, in almost all circumstances, to use these facilities over the new keyword:
C++11 introduces std::unique_ptr, defined in the header <memory>.
A unique_ptr is a container for a raw pointer, which the unique_ptr is said to own. A unique_ptr explicitly prevents copying of its contained pointer (as would happen with normal assignment), but the std::move function can be used to transfer ownership of the contained pointer to another unique_ptr. A unique_ptr cannot be copied because its copy constructor and assignment operators are explicitly deleted.
std::auto_ptr is deprecated under C++11 and completely removed from C++17. The copy constructor and assignment operators of auto_ptr do not actually copy the stored pointer. Instead, they transfer it, leaving the prior auto_ptr object empty. This was one way to implement strict ownership, so that only one auto_ptr object can own the pointer at any given time. This means that auto_ptr should not be used where copy semantics are needed.[citation needed] Since auto_ptr already existed with its copy semantics, it could not be upgraded to be a move-only pointer without breaking backward compatibility with existing code.
C++11 introduces std::shared_ptr and std::weak_ptr, defined in the header <memory>.
A shared_ptr is a container for a raw pointer. It maintains reference counting ownership of its contained pointer in cooperation with all copies of the shared_ptr. An object referenced by the contained raw pointer will be destroyed when and only when all copies of the shared_ptr have been destroyed.
A weak_ptr is a container for a raw pointer. It is created as a copy of a shared_ptr. The existence or destruction of weak_ptr copies of a shared_ptr have no effect on the shared_ptr or its other copies. After all copies of a shared_ptr have been destroyed, all weak_ptr copies become empty.
Because the implementation of shared_ptr uses reference counting, circular references are potentially a problem. A circular shared_ptr chain can be broken by changing the code so that one of the references is a weak_ptr.
Multiple threads can safely simultaneously access different shared_ptr and weak_ptr objects that point to the same object.
The referenced object must be protected separately to ensure thread safety.
shared_ptr and weak_ptr are based on versions used by the Boost libraries.[citation needed]C++ Technical Report 1 (TR1) first introduced them to the standard, as general utilities, but C++11 adds more functions, in line with the Boost version."
"66","
In computer science, unreachable memory is a block of memory allocated dynamically where the program that allocated the memory no longer has any reachable pointer that refers to it. Similarly, an unreachable object is a dynamically allocated object that has no reachable  reference to it. Informally, unreachable memory is dynamic memory that the program can not reach directly, nor get to by starting at an object it can reach directly, and then following a chain of pointer references.
In dynamic memory allocation implementations that employ a garbage collector, objects are reclaimed after they become unreachable. The garbage collector is able to determine if an object is reachable; any object that is determined to no longer be reachable can be deallocated. Many programming languages (for example, Java, C#, D,  Dylan) use automatic garbage collection.
In contrast, when memory becomes unreachable in dynamic memory allocation implementations that require explicit deallocation, the memory can no longer be explicitly deallocated. Unreachable memory in systems that use manual memory management results in a memory leak.
Some garbage collectors implement weak references. If an object is reachable only through either weak references or chains of references that include a weak reference, then the object is said to be weakly reachable. The garbage collector can treat a weakly reachable object graph as unreachable and deallocate it. (Conversely, references that prevent an object from being garbage collected are called strong references; a weakly reachable object is unreachable by any chain consisting only of strong references.) Some garbage-collected object-oriented languages, such as Java and Python, feature weak references.  The Java package java.lang.ref supports soft, weak and phantom references, resulting in the additional object reachability states softly reachable and phantom reachable.
Unreachable memory is often associated with software aging."
"67"," (Learn how and when to remove this template message)
In object-oriented programming, a metaclass is a class whose instances are classes. Just as an ordinary class defines the behavior of certain objects, a metaclass defines the behavior of certain classes and their instances. Not all object-oriented programming languages support metaclasses. Among those that do, the extent to which metaclasses can override any given aspect of class behavior varies. Metaclasses can be implemented by having classes be first-class citizen, in which case a metaclass is simply an object that constructs classes. Each language has its own metaobject protocol, a set of rules that govern how objects, classes, and metaclasses interact.
In Python, the builtin class type is a metaclass. Consider this simple Python class:
At run time, Car itself is an instance of type. The source code of the Car class, shown above, does not include such details as the size in bytes of Car objects, their binary layout in memory, how they are allocated, that the __init__ method is automatically called each time a Car is created, and so on. These details come into play not only when a new Car object is created, but also each time any attribute of a Car is accessed. In languages without metaclasses, these details are defined by the language specification and can't be overridden. In Python, the metaclass - type - controls these details of Car's behavior. They can be overridden by using a different metaclass instead of type.
The above example contains some redundant code to do with the four attributes make, model, year, and color. It is possible to eliminate some of this redundancy using a metaclass. In Python, a metaclass is most easily defined as a subclass of type.
This metaclass only overrides object creation.  All other aspects of class and object behavior are still handled by type.
Now the class Car can be rewritten to use this metaclass.  This is done in Python 2 by assigning to __metaclass__ within the class definition:
In Python 3 you provide a named argument, metaclass=M to the class definition instead:
Car objects can then be instantiated like this:
In Smalltalk, everything is an object. Additionally, Smalltalk is a class based system, which means that every object has a class that defines the structure of that object (i.e. the instance variables the object has) and the messages an object understands. Together this implies that a class in Smalltalk is an object and that therefore a class needs to be an instance of a class (called metaclass).
As an example, a car object c is an instance of the class Car. In turn, the class Car is again an object and as such an instance of the metaclass of Car called Car class. Note the blank in the name of the metaclass. The name of the metaclass is the Smalltalk expression that, when evaluated, results in the metaclass object. Thus evaluating Car class results in the metaclass object for Car whose name is Car class (one can confirm this by evaluating Car class name which returns the name of the metaclass of Car.)
Class methods actually belong to the metaclass, just as instance methods actually belong to the class. When a message is sent to the object 2, the search for the method starts in Integer. If it is not found it proceeds up the superclass chain, stopping at Object whether it is found or not.
When a message is sent to Integer the search for the method starts in Integer class and proceeds up the superclass chain to Object class. Note that, so far, the metaclass inheritance chain exactly follows that of the class inheritance chain. But the metaclass chain extends further because Object class is the subclass of Class. All metaclasses are subclasses of Class.
In early Smalltalks, there was only one metaclass called Class. This implied that the methods all classes have were the same, in particular the method to create new objects, i.e., new. To allow classes to have their own methods and their own instance variables (called class instance variables and should not be confused with class variables), Smalltalk-80 introduced for each class C their own metaclass C class. This means that each metaclass is effectively a singleton class.
Since there is no requirement that metaclasses behave differently from each other, all metaclasses are instances of only one class called Metaclass. The metaclass of Metaclass is called Metaclass class which again is an instance of class Metaclass.
In Smalltalk-80, every class (except Object) has a superclass. The abstract superclass of all metaclasses is Class, which describes the general nature of classes.
The superclass hierarchy for metaclasses parallels that for classes, except for class Object. ALL metaclasses are subclasses of Class, therefore: 
Like conjoined twins, classes and metaclasses are born together. Metaclass has an instance variable thisClass, which points to its conjoined class.
Note that the usual Smalltalk class browser does not show metaclasses as separate classes. Instead the class browser allows to edit the class together with its metaclass at the same time.
The names of classes in the metaclass hierarchy are easily confused with the concepts of the same name. For instance:
Four classes provide the facilities to describe new classes. Their inheritance hierarchy (from Object), and the main facilities they provide are:
Ruby purifies the Smalltalk-80 concept of metaclasses by introducing eigenclasses,
removing the Metaclass class,
and (un)redefining the class-of map.

The change can be schematized as follows:
Note in particular the correspondence between Smalltalk's implicit metaclasses and Ruby's eigenclasses of classes.
The Ruby eigenclass model makes the concept of implicit metaclasses fully uniform: every object x has its own meta-object, called the eigenclass of x, which is one meta-level higher than x. The ""higher order"" eigenclasses usually exist purely conceptually – they do not contain any methods or store any (other) data in most Ruby programs.
The following diagrams show a sample core structure of Smalltalk-80 and Ruby in comparison.
In both languages, the structure consists of a built-in part which contains the circular objects (i.e. objects that appear in a cycle formed by a combination of blue or green links) and a user-part which has four explicit objects: classes A and B
and terminal objects u and v.
Green links show the child→parent relation of inheritance (with the implicit upward direction), blue links show the complementary member→container relation of instantiation (a blue link from x points to the least actual container of x that is the start point for the method lookup when a method is invoked on x). Gray nodes display the eigenclasses (resp. implicit metaclasses in the case of Smalltalk-80).
The diagram on the right also provides a picture of lazy evaluation of eigenclasses in Ruby. The v object can have its eigenclass evaluated (allocated) as a consequence of adding singleton methods to v.
According to the Ruby's introspection method named class,
the class of every class (and of every eigenclass) is
constantly the Class class (denoted by c in the diagram).
Class, and Struct are the only classes that have classes as instances.[disputed  – discuss] Subclassing of Class is disallowed.

Following the standard definition of metaclasses we can conclude that Class and Struct are the only metaclasses in Ruby.
This seems to contradict the correspondence between Ruby and Smalltalk,
since in Smalltalk-80, every class has its own metaclass.
The discrepancy is based on the disagreement between
the class introspection method in Ruby and Smalltalk. While the map x ↦ x.class coincides on terminal objects, it differs in the restriction to classes. As already mentioned above, for a class x, the Ruby expression x.class evaluates constantly to Class. In Smalltalk-80, if x is a class then the expression x class corresponds
to the Ruby's x.singleton_class
– which evaluates to the eigenclass of x.
Metaclasses in Objective-C are almost the same as those in Smalltalk-80—not surprising since Objective-C borrows a lot from Smalltalk. Like Smalltalk, in Objective-C, the instance variables and methods are defined by an object's class. A class is an object, hence it is an instance of a metaclass.
Like Smalltalk, in Objective-C, class methods are simply methods called on the class object, hence a class's class methods must be defined as instance methods in its metaclass. Because different classes can have different sets of class methods, each class must have its own separate metaclass. Classes and metaclasses are always created as a pair: the runtime has functions objc_allocateClassPair() and objc_registerClassPair() to create and register class-metaclass pairs, respectively.
There are no names for the metaclasses; however, a pointer to any class object can be referred to with the generic type Class (similar to the type id being used for a pointer to any object).
Because class methods are inherited through inheritance, like Smalltalk, metaclasses must follow an inheritance scheme paralleling that of classes (e.g. if class A's parent class is class B, then A's metaclass's parent class is B's metaclass), except that of the root class.
Unlike Smalltalk, the metaclass of the root class inherits from the root class (usually NSObject using the Cocoa framework) itself. This ensures that all class objects are ultimately instances of the root class, so that you can use the instance methods of the root class, usually useful utility methods for objects, on class objects themselves.
Since metaclass objects do not behave differently (you cannot add class methods for a metaclass, so metaclass objects all have the same methods), they are all instances of the same class—the metaclass of the root class (unlike Smalltalk). Thus, the metaclass of the root class is an instance of itself. The reason for this is that all metaclasses inherit from root class; hence, they must inherit the class methods of the root class.
The following are some of the most prominent programming languages that support metaclasses.
Some less widespread languages that support metaclasses include OpenJava, OpenC++, OpenAda, CorbaScript, ObjVLisp, Object-Z, MODEL-K, XOTcl, and MELDC. Several of these languages date from the early 1990s and are of academic interest.
Logtalk, an object-oriented extension of Prolog, also supports metaclasses.
Resource Description Framework (RDF) and Unified Modeling Language (UML) both support metaclasses."
"68","RubyCocoa is a Mac OS X framework that provides a bridge between the Ruby and the Objective-C programming languages, allowing the user to manipulate Objective-C objects from Ruby, and vice versa. It makes it possible to write a Cocoa application completely in Ruby as well as to write an application that mixes Ruby and Objective-C code. An Apple project called MacRuby was under development to replace RubyCocoa in 2008. A proprietary spin-off called RubyMotion was subsequently released in 2012, available for iOS, OS X and Android.
Some useful applications of RubyCocoa are exploration of a Cocoa object's features with irb interactively, prototyping of a Cocoa application, writing a Cocoa application that combines the features of Ruby and Objective-C and wrapping Mac OS X's native GUI for a Ruby script.
RubyCocoa is free software, released under both the Ruby License and the LGPL.
RubyCocoa was started in 2001 by Hisakuni Fujimoto when he implemented a Ruby extension module to wrap NSObject and NSClassFromString function. Later it was integrated with Project Builder (which later became Xcode).
In 2002 the project was registered on SourceForge and the development team began to grow.
In 2006 the committers list was first joined by a developer from Apple, Laurent Sansonetti, and then a RubyCocoa presentation was made during WWDC. Apple stated that RubyCocoa will be included and supported in Mac OS X v10.5 “Leopard”.
In August 2008, Sansonetti confirmed that MacRuby ""is supposed to replace RubyCocoa."" in the future.
RubyCocoa is sometimes interpreted as a set of bindings to the Cocoa frameworks, which is false. RubyCocoa is a real bridge between the Objective-C and Ruby programming languages.
RubyCocoa will import the Objective-C classes into the Ruby world on demand. For example, when you access OSX::NSTableView for the very first time in your code, RubyCocoa will retrieve all the necessary information regarding this class from the Objective-C runtime and create a Ruby class of the same name that will act as a proxy. It will also import in the same way all the inherited classes.
As stated earlier, RubyCocoa creates special proxy objects. Every time you send a Ruby message to a proxy object, RubyCocoa will try to forward it to the embedded Objective-C instance, by translating the message name to an Objective-C selector and asking the Objective-C runtime to forward it.
If an exception is raised from the Objective-C world, RubyCocoa will convert it to a Ruby exception and forward it to you.
RubyCocoa uses the libffi library to call the Objective-C methods implementations.
RubyCocoa makes it easy to override an Objective-C method from Ruby, either in a subclass or directly to the class (as you would do in Objective-C using a category).
Once your method is inserted, RubyCocoa will retrieve the signature of the existing Objective-C method and inject a new one to the Objective-C runtime, of the same signature, but which now points to your code.
To accomplish this, RubyCocoa uses the libffi library to dynamically create a closure that will call the Ruby method, and just passes a pointer to that new closure to the Objective-C runtime.
Due to the nature of the Objective-C language, you can freely use C from Objective-C code. In order to bridge the relevant C parts of an Objective-C framework, such as C structures, functions, enumerations, constants and more, RubyCocoa relies on the BridgeSupport project.
RubyCocoa will interpret at runtime the BridgeSupport files (using the very fast libXML2's xmlTextReader) and accordingly handle their content. It will for instance construct the Ruby proxy classes for the C structures and also create the functions.
Note that the costly operations, such as localizing the symbols, are done on demand, and obviously only once.
RubyCocoa is able to detect APIs that use format strings, like NSLog or NSString.stringWithFormat, and appropriately convert the variable arguments to the types specified in the format string.
RubyCocoa allows you to pass Ruby Proc objects as function pointer arguments. It will then use the libffi library to dynamically create a closure and pass it to the underlying function/method.
When you install RubyCocoa, the corresponding Xcode templates are installed
automatically. So when you start a new project, select Cocoa-Ruby Application  project type and all necessary files will be generated.
To invoke an Objective-C method, you replace each colon in the method name except the last with an underscore. Thus, for example, the NSWindow instance method initWithContentRect:styleMask:backing:defer: becomes initWithContentRect_styleMask_backing_defer.
All Cocoa classes and functions belong to OSX module, so for example, the Objective-C code:
will become:
As you can see, this decreases the code readability by rendering Objective-C parameter naming useless. So, there is another convenient way to write the method calls — the objc_send method, which accepts Ruby symbols as parameter names. For example, the previous code can also be written as:"
"69","
The NeXT Computer (also called the NeXT Computer System) is a workstation computer developed, marketed, and sold by NeXT Inc. It runs the Mach- and BSD-derived, Unix-based NeXTSTEP operating system, with a proprietary GUI using a Display PostScript-based back end. The motherboard is square and fits into one of four identical slots in the enclosure. The NeXT Computer enclosure consists of a 1-foot (305 mm) die-cast magnesium cube-shaped, black case, which led to the machine being informally referred to as ""The Cube"". It was launched in 1988 at US$6,500 (equivalent to $13,400 in 2017).
The NeXT Computer was succeeded by the NeXTcube, an upgraded model, in 1990.
The NeXT Computer was launched in October 1988 at a lavish invitation-only event, ""NeXT Introduction – the Introduction to the NeXT Generation of Computers for Education"" at the War Memorial Opera House in San Francisco, California. The next day, selected educators and software developers were invited to attend—for a $100 registration fee—the first public technical overview of the NeXT computer at an event called ""The NeXT Day"" at the San Francisco Hilton. It gave those interested in developing NeXT software an insight into the system's software architecture and object-oriented programming. Steve Jobs was the luncheon's speaker.
In 1989, BYTE Magazine listed the NeXT Computer among the ""Excellence"" winners of the BYTE Awards, stating that it showed ""what can be done when a personal computer is designed as a system, and not a collection of hardware elements"". Citing as ""truly innovative"" the optical drive, DSP and object-oriented programming environment, it concluded that ""the NeXT Computer is worth every penny of its $6500 market price"". It was, however, not a significant commercial success, failing to reach the level of high volume sales like the Apple II, Commodore 64, the Macintosh, or Microsoft Windows PCs.  The workstations were sold to universities, financial institutions, and government agencies.[citation needed]
A NeXT Computer and its object oriented development tools and libraries were used by Tim Berners-Lee and Robert Cailliau at CERN to develop the world's first web server software, CERN httpd, and also used to write the first web browser, WorldWideWeb.
The NeXT Computer and the same  object oriented development tools and libraries were used by Jesse Tayler at Paget Press to develop the first electronic app store, the Electronic AppWrapper in the early 1990s. Issue #3 was first demonstrated to Steve Jobs at NeXTWorld Expo 1993."
"70","The family of Macintosh operating systems developed by Apple Inc. includes the graphical user interface-based operating systems it has designed for use with its Macintosh series of personal computers since 1984, as well as the related system software it once created for compatible third-party systems.
In 1984, Apple debuted the operating system that is now known as the ""Classic"" Mac OS with its release of the original Macintosh System Software. The system, rebranded ""Mac OS"" in 1996, was preinstalled on every Macintosh until 2002 and offered on Macintosh clones for a short time in the 1990s. Noted for its ease of use, it was also criticized for its lack of modern technologies compared to its competitors.
The current Mac operating system is macOS, originally named ""Mac OS X"" until 2012 and then ""OS X"" until 2016. Developed between 1997 and 2001 after Apple's purchase of NeXT, Mac OS X brought an entirely new architecture based on NeXTSTEP, a Unix system, that eliminated many of the technical challenges that the classic Mac OS faced. The current macOS is preinstalled with every Mac and is updated annually. It is the basis of Apple's current system software for its other devices, iOS, watchOS, and tvOS.
Prior to the introduction of Mac OS X, Apple experimented with several other concepts, releasing different products designed to bring the Macintosh interface or applications to Unix-like systems or vice versa, A/UX, MAE, and MkLinux. Apple's effort to expand upon and develop a replacement for its classic Mac OS in the 1990s led to a few cancelled projects, code named Star Trek, Taligent, and Copland.
Although they have different architectures, the Macintosh operating systems share a common set of GUI principles, including a menu bar across the top of the screen; the Finder shell, featuring a desktop metaphor that represents files and applications using icons and relates concepts like directories and file deletion to real-world objects like folders and a trash can; and overlapping windows for multitasking.
The ""classic"" Mac OS is the original Macintosh operating system that was introduced in 1984 alongside the first Macintosh and remained in primary use on Macs through 2001.
Apple released the original Macintosh on January 24, 1984; its early system software was partially based on the Lisa OS and the Xerox PARC Alto computer, which former Apple CEO Steve Jobs previewed. It was originally named ""System Software"", or simply ""System""; Apple rebranded it as ""Mac OS"" in 1996 due in part to its Macintosh clone program that ended a year later.
Mac OS is characterized by its monolithic system. It was noted as easy to use and featured cooperative multitasking for most of its history, but it was criticized for its limited memory management, lack of protected memory and access controls, and susceptibility to conflicts among extensions.
Nine major versions of the classic Mac OS were released. The name ""Classic"" that now signifies the system as a whole is a reference to a compatibility layer that helped ease the transition to Mac OS X.
macOS (originally named ""Mac OS X"" until 2012 and then ""OS X"" until 2016)
is the current Mac operating system that officially succeeded the classic Mac OS in 2001.
Although the system was originally marketed as simply ""version 10"" of Mac OS, it has a history that's largely independent of the classic Mac OS. It is a Unix-based operating system built on NeXTSTEP and other technology developed at NeXT from the late 1980s until early 1997, when Apple purchased the company and its CEO Steve Jobs returned to Apple. Precursors to the original release of Mac OS X include OpenStep, Apple's Rhapsody project, and the Mac OS X Public Beta.
macOS makes use of the BSD codebase and the XNU kernel, and its core set of components is based upon Apple's open source Darwin operating system.
The first desktop version of the system was released on March 24, 2001, supporting the Aqua user interface. Since then, several more versions adding newer features and technologies have been released. Since 2011, new releases have been offered on an annual basis.
An early server computing version of the system was released in 1999 as a technology preview. It was followed by several more official server-based releases. Server functionality has instead been offered as an add-on for the desktop system since 2011.
The Apple Real-time Operating System Environment (A/ROSE) was a small embedded operating system which ran on the Macintosh Coprocessor Platform, an expansion card for the Macintosh. The idea was to offer a single ""overdesigned"" hardware platform on which third-party vendors could build practically any product, reducing the otherwise heavy workload of developing a NuBus-based expansion card. The first version of the system was ready for use in February 1988.
In 1988, Apple released its first Unix-based OS, A/UX, which was a Unix operating system with the Mac OS look and feel. It was not very competitive for its time, due in part to the crowded Unix market and Macintosh hardware lacking high-end design features present on workstation-class computers. A/UX had most of its success in sales to the U.S. government, where POSIX compliance was a requirement that Mac OS could not meet.
The Macintosh Application Environment (MAE) was a software package introduced by Apple in 1994 that allowed users of certain Unix-based computer workstations to run Apple Macintosh application software. MAE used the X Window System to emulate a Macintosh Finder-style graphical user interface. The last version, MAE 3.0, was compatible with System 7.5.3. MAE was available for Sun Microsystems SPARCstation and Hewlett-Packard systems. It was discontinued on May 14, 1998.
Announced at the 1996 Worldwide Developers Conference (WWDC), MkLinux is an open source operating system that was started by the OSF Research Institute and Apple in February 1996 to port Linux to the PowerPC platform, and thus Macintosh computers. In the summer of 1998, the community-led MkLinux Developers Association took over development of the operating system. MkLinux is short for ""Microkernel Linux,"" which refers to the project's adaptation of the Linux kernel to run as a server hosted atop the Mach microkernel. MkLinux is based on version 3.0 of Mach.
Star Trek (as in ""to boldly go where no Mac has gone before"") was a relatively unknown secret prototype beginning in 1992, whose goal was to create a version of the classic Mac OS that would run on Intel-compatible x86 personal computers. In partnership with Apple and with support from Intel, the project was instigated by Novell, which was looking to integrate its DR-DOS with the Mac OS GUI as a mutual response to the monopoly of Microsoft's Windows 3.0 and MS-DOS. A team consisting of four from Apple and four from Novell was able to get the Macintosh Finder and some basic applications such as QuickTime, running smoothly on the x86 architecture. The project was canceled a year later in early 1993, but some of the code was reused later when porting the Mac OS to PowerPC.
Taligent (a portmanteau of ""talent"" and ""intelligent"") was the name of an object-oriented operating system and the company dedicated to producing it. Started as a project within Apple to provide a replacement for the classic Mac OS, it was later spun off into a joint venture with IBM as part of the AIM alliance, with the purpose of building a competing platform to Microsoft Cairo and NeXTSTEP. The development process never worked, and Taligent is often cited as an example of a project death march. Apple pulled out of the project in 1995 before the code had been delivered.
Copland was a project at Apple to create an updated version of the classic Mac OS. It was to have introduced protected memory, preemptive multitasking and a number of new underlying operating system features, yet still be compatible with existing Mac software. As originally planned, a follow-up release known as ""Gershwin"" would add multithreading and other advanced features. New features were added more rapidly than they could be completed, and the completion date slipped into the future with no sign of a release. In 1996, Apple decided to cancel the project outright and find a suitable third-party system to replace it. Copland development ended in August 1996, and in December 1996, Apple announced that it was buying NeXT for its NeXTSTEP operating system.
Before the arrival of the Macintosh in 1984, Apple's history of operating systems began with its Apple II series computers in 1977, which ran Apple DOS, ProDOS, and later GS/OS; the Apple III in 1980, which ran Apple SOS; and the Apple Lisa in 1983, which ran Lisa OS and later MacWorks XL, a Macintosh emulator. Apple also developed the Newton OS for its Newton personal digital assistant from 1993 to 1997.
In recent years, Apple has also launched several new operating systems based on the core of macOS, including iOS in 2007 for its iPhone, iPad, and iPod Touch mobile devices; watchOS in 2015 for the Apple Watch; and tvOS in 2015 for the Apple TV set-top box."
"71"," (Learn how and when to remove this template message)

Andrew ""Andy"" C. Stone is an American computer programmer best known for his iOS app Twittelator, which to date has sold over a million units for the iPhone and the iPad. The founder, director, and principal programmer for Stone Design Corporation, Albuquerque, New Mexico. In his 25 plus year career as a programmer, he has published over 35 software titles for Hypercard, the NeXT workstation, Mac OS X, and currently for iOS iPhones and iPads.
Andrew Stone was a contributing author to the Waite Group’s Tricks of the HyperTalk Masters 
Stone developed software for Sandia National Laboratories called ProtoTymer which allowed physical interfaces to be trial tested in a software version.
Fascinated by Steve Jobs’ vision for the personal computer, Stone was the first independent developer for the NeXT Computer to ship a shrink-wrap product, TextArt in October 1989. TextArt allowed designers to manipulate PostScript text with virtual knobs, dials and sliders. By 1990, TextArt had evolved into Create, a drawing program which shipped in 1991. At the same time, Stone Design developed a multimedia database manager called DataPhile.
Stone Design was a leader in electronic software distribution on NeXT and was a constant advertising presence on the first ever App Store which was also invented using NeXT tools, The Electronic AppWrapper. According to an employee at the Paget Press (the startup responsible for the first App Store) it was originally AppWrapper #3 that was first demonstrated to Steve Jobs and showcased Stone Design Apps. where applications like Create and DataPhile were selling along with 3D Reality and other Stone Design Apps. Stone Design Apps can still be found on the iOS App Store today, making Stone Design perhaps the longest running developer actively using electronic distribution via any App Store service.
Besides Stone's notoriety in the NeXT World as the first independent software developer to ship shrinkwrap product for the NeXT Computer, were the legendary raves he and John Perry Barlow threw over 3 years, the first being held at the Exploratorium in October 1992. These parties are still being discussed today because of the mixing of LSD and the NSA together in the same space.
Stone's participation with the first government sanctioned Dimethyltryptamine research with Dr. Rick Strassman in Albuquerque in the early 1990s led to a collaboration in the underwriting of DMT: The Spirit Molecule: A Doctor's Revolutionary Research into the Biology of Near-Death and Mystical Experiences,  documenting the research. Andrew Stone was a featured DMT volunteer in the documentary film DMT - The Spirit Molecule. Andrew Stone serves on the board of the Cottonwood Research Foundation with Dr. Strassman, which provides scientific research into the nature of consciousness.
Between that time and Apple’s purchase of the NeXT Corporation in December 1996, Stone Design developed a number of other products for the NeXT, including 3DReality, a 3D modeling and rendering package and CheckSum, a personal finance application.
When NeXT became part of Apple on December 20, 1996, Andrew Stone was asked to help introduce NeXTStep to Mac users and developers. He demoed Create in the keynote presentations at both MacWorld Boston and WWDC in 1997. He was a contributing editor for Mactech for several years 
Stone Design began to develop for the pre-OS X Macintosh, turning out a healthy number of products for a small independent company, including PhotoToWeb, a slideshow & photo application for the Web; SliceAndDice, a tool for making javascript navigation bars; PStill, a conversion utility for turning Post Script and EPS files into .PDF files; TimeEqualsMoney, a time/expense tracking and invoicing application; PackUpAndGo, a cross-platform archiving tool; and GIFfun for making animated .gif files. All of these applications were eventually bundled together as Stone Studio, but the company continued to develop software, 16 applications in all, which eventually found their way into a single package called Stone Works, which included all the titles above plus eight additional applications including: FontSight, GlobalWarmth, iMaginator, Stone Studio widget, PreferenceCommander, VideationNation, StarMores, and Xaos – Videator Enabled.
In 2008, Stone began to release apps for the iPhone. His first product, Twittelator, became one of the best selling apps for the micro-blogging service Twitter. Other products included iGraffiti, TalkingPics, Gesture, MobileMix, Soundbite, Pulsar, iCreated, TweetTV and Intentionizer. He has contributed to other apps such as 140Characters, The Daily, Wine.com for iPad, WeGetIn, Trekaroo and Bandojo.
When he’s not programming, Stone spends his time working on his organic farm, doing yoga, reading and hiking. He's married to KUNM public radio producer Katie Stone, and has four children and one grandchild. He’s written an extensive number of articles on programming with the Cocoa code base. He also spearheads a group called “the Cocoa Conspiracy,” a loose knit ad hoc professional organization for iOS app developers based in New Mexico."
"72","
Core Foundation (also called CF) is a C application programming interface (API) in macOS & iOS, and is a mix of low-level routines and wrapper functions. Apple releases most of it as an open source project called CFLite that can be used to write cross-platform applications for macOS, Linux, and Windows; a third-party open-source implementation called OpenCFLite also exists. Most Core Foundation routines follow a certain naming convention that deal with opaque objects, for example CFDictionaryRef for functions whose names begin with CFDictionary, and these objects are often reference counted (manually) through CFRetain and CFRelease. Internally, Core Foundation forms the base of the types in the Objective-C standard library as well.
The most prevalent use of Core Foundation is for passing its own primitive types for data, including raw bytes, Unicode strings, numbers, calendar dates, and UUIDs, as well as collections such as arrays, sets, and dictionaries, to numerous macOS C routines, primarily those that are GUI-related. At the operating system level Core Foundation also provides standardized application preferences management  through CFPropertyList, bundle handling, run loops, interprocess communication through CFMachPort and CFNotificationCenter, and a basic graphical user interface message dialog through CFUserNotification.
Other parts of the API include utility routines and wrappers around existing APIs for ease of use. Utility routines perform such actions as file system and network I/O through CFReadStream, CFWriteStream, and CFURL and endianness translation (Byte Order Utilities). Some examples of wrapper routines include those for Core Foundation's wrapper routines for Unix sockets, the CFSocket API.
Some types in Core Foundation are ""toll-free bridged"", or interchangeable with a simple cast, with those of their Foundation Kit counterparts. For example, one could create a CFDictionaryRef Core Foundation type, and then later simply use a standard C cast to convert it to its Objective-C counterpart, NSDictionary *, and then use the desired Objective-C methods on that object as one normally would.
Core Foundation has a plug-in model (CFPlugin) that is based on the Microsoft Component Object Model."
"73","
The Dock is a prominent feature of the graphical user interface of the macOS operating system. It is used to launch applications and to switch between running applications. The Dock is also a prominent feature of macOS's predecessor NeXTSTEP and OpenStep operating systems. The earliest known implementations of a dock are found in operating systems such as RISC OS and NeXTSTEP. iOS has its own version of the Dock for iPhone, iPod Touch and iPad.
Apple applied for a US patent for the design of the Dock in 1999 and was granted the patent in October 2008, nine years later. Applications can be added to and removed from the Dock by drag and drop, except for the Finder, which is a permanent fixture as the leftmost item (or topmost if the Dock is configured to be vertical). The Trash icon is also a permanent fixture at the right end (or bottom if the Dock is repositioned). Part of the macOS Core Services, Dock.app is located at /System/Library/CoreServices/.
In NeXTSTEP and OpenStep, the Dock is an application launcher that holds icons for frequently used programs. The icon for the Workspace Manager and the Recycler are always visible. The Dock indicates if a program is not running by showing an ellipsis below its icon. If the program is running, there isn't an ellipsis on the icon. In macOS, running applications have been variously identified by a small black triangle (Mac OS X 10.0-10.4) a blue-tinted luminous dot (Mac OS X 10.5-10.7), a horizontal light bar (OS X 10.8 and 10.9), and a simple black or white dot (OS X 10.10-macOS 10.13).
In macOS, however, the Dock is used as a repository for any program or file in the operating system. It can hold any number of items and resizes them dynamically to fit while using magnification to clarify smaller resized items. By default, it appears on the bottom edge of the screen, but it can also instead be placed on the left or right edges of the screen if the user wishes. Applications that do not normally keep icons in the Dock will still appear there when running and remain until they are quit. These features are unlike those of the dock in the NeXT operating systems where the capacity of the Dock is dependent on display resolution. This may be an attempt to recover some Shelf functionality since macOS inherits no other such technology from NeXTSTEP. (Minimal Shelf functionality has been implemented in the Finder.)
The changes to the dock bring its functionality also close to that of Apple's Newton OS Button Bar, as found in the MessagePad 2x00 series and the likes. Applications could be dragged in and out of the Extras Drawer, a Finder-like app, onto the bar. Also, when the screen was put into landscape mode, the user could choose to position the Button Bar at the right or left  side of the screen, just like the Dock in macOS.
The macOS Dock also has extended menus that control applications without making them visible on screen. On most applications it has simple options such as Quit, Keep In Dock, Remove From Dock, and other options, though some applications use these menus for other purposes, such as iTunes, which uses this menu as a way for a user to control certain playback options. Other Applications include changing the status of an online alias (MSN, AIM/iChat etc.) or automatically saving the changes that have been made in a document (There is no current application with this feature made available for macOS). Docklings (in Mac OS X 10.4 or earlier) can also be opened by using the right-mouse button, if the mouse has one, but most of the time either clicking and holding or control-click will bring the menu up.
In Mac OS X Leopard, docklings were replaced by Stacks.  Stacks ""stack"" files into a small organized folder on the Dock, and they can be opened by left-clicking.
Stacks could be shown in three ways: a ""fan"", a ""grid"", or a ""list"", which is similar to docklings.  In grid view, the folders in that stack can be opened directly in that stack without the need to open Finder.
In iOS, the dock is used to store applications and, since iOS 4, folders containing applications. Unlike the macOS dock, a maximum of 4 icons can be placed in the dock on the iPhone and the iPod Touch. The maximum for the iPad however is 6 icons. The size of the dock on iOS cannot be changed.
The original version of the dock, found in Mac OS X Public Beta to 10.0, presents a flat white translucent interface with the Aqua styled pinstripes. The dock found in Mac OS X 10.1 to Tiger removes the pinstripes, but otherwise is identical. Mac OS X Leopard to Lion presents a 3D glass-like perspective instead of the traditional flat one, resembling Sun Microsystems' Project Looking Glass application dock. OS X Mountain Lion and Mavericks changes the look to resemble frosted glass and has rounded corners. OS X Yosemite reverts to a 2D appearance, similar to Mac OS X Tiger, although more translucent and with a blur effect.
In iPhone OS 1 to 3, the dock used a metal look which looks similar to the front of the Mac Pro. iOS 4 adopted the dock design from Mac OS X Leopard to Lion which was used until iOS 7, which uses a similar dock from Mac OS X Tiger but with iOS 7 styled blur effects.[citation needed] In iOS 11, the dock for the iPad is redesigned to more resemble the macOS dock.
The classic Mac OS does has a dock-like application called Launcher, which was first introduced with Macintosh Performa models in 1993 and later included as part of System 7.5.1.  It performs the same basic function. Also, add-ons such as DragThing added a dock for users of earlier versions.
Microsoft implemented a simplified dock feature in Windows 98 with the Quick Launch toolbar and this feature remained until Windows Vista.
Various docks are also used in Linux and BSD. Some examples are Window Maker (which emulates the look and feel of the NeXTstep GUI), Docky, and Avant Window Navigator, KXDocker (amongst others) for KDE and various other gdesklet/adesklets docks, AfterStep's Wharf (a derivation from the NeXTstep UI),  iTask NG (a module used with some Enlightenment-based Linux distributions such as gOS) and Blackbox's Slit.
Bruce Tognazzini, a usability consultant who worked for Apple in the 1980s and 1990s before Mac OS X was developed, wrote an article in 2001 listing ten problems he saw with the Dock. This article was updated in 2004, removing two of the original criticisms and adding a new one. One of his concerns was that the Dock uses too much screen space. Another was that icons only show their labels when the pointer hovers over them, so similar-looking folders, files, and windows are difficult to distinguish. Tognazzini also criticized the fact that when icons are dragged out of the Dock, they vanish with no easy way to get them back; he called this behavior ""object annihilation"".
John Siracusa, writing for Ars Technica, also pointed out some issues with the Dock around the releases of Mac OS X Public Beta in 2000. He noted that because the Dock is centered, adding and removing icons changes the location of the other icons. In a review of Mac OS X v10.0 the following year, he also noted that the Dock does far too many tasks than it should for optimum ease-of-use, including launching apps, switching apps, opening files, and holding minimized windows. Siracusa further criticised the Dock after the release of Mac OS X v10.5, noting that it was made less usable for the sake of eye-candy. Siracusa criticized the 3D look and reflections, the faint blue indicator for open applications, and less distinguishable files and folders.
Thom Holwerda, a managing editor OSNews, stated some concerns with the Dock, including the facts that it grows in both directions, holds the Trash icon, and has no persistent labels. Holwerda also criticized the revised Dock appearance in Mac OS X v10.5."
"74","A menu bar is a graphical control element which contains drop-down menus.
The menu bar's purpose is to supply a common housing for window- or application-specific menus which provide access to such functions as opening files, interacting with an application, or displaying help documentation or manuals. Menu bars are typically present in graphical user interfaces that display documents and representations of files in windows and windowing systems but menus can be used as well in command line interface programs like text editors or file managers where drop-down menu is activated with a shortcut or combination key.
Through the evolution of user interfaces, the menu bar has been implemented in different ways by different user interfaces and application programs.
In the Macintosh operating systems, the menu bar is a horizontal ""bar"" anchored to the top of the screen. In macOS, the left side contains the Apple menu, the Application menu (its name will match the name of the current application) and the currently focused application's menus (e.g. File, Edit, View, Window, Help). On the right side, it contains menu extras (for example the system clock, volume control, and the Fast user switching menu (if enabled) and the Spotlight icon. All of these menu extras (excluding Spotlight) can be moved horizontally by command-clicking and dragging left or right. If an icon is dragged and dropped vertically it will disappear with a puff of smoke, much like the icons in the dock. In the classic Mac OS (versions 7 through 9), the right side contains the application menu, allowing the user to switch between open applications. In Mac OS 8.5 and later, the menu can be dragged downwards, which would cause it to be represented on screen as a floating palette.
There is only one menu bar, so the application menus displayed are those of the application that is currently focused. Therefore, for example, if the System Preferences application is focused, its menus are in the menu bar, and if the user clicks on the Desktop which is a part of the Finder application, the menu bar will then display the Finder menus.
Apple experiments in GUI design for the Lisa project initially used multiple menu bars anchored to the bottom of windows, but this was quickly dropped in favor of the current arrangement, as it proved slower to use (in accordance with Fitts's law). The idea of separate menus in each window or document was later implemented in Microsoft Windows and is the default representation in most Linux desktop environments.
Even before the advent of the Macintosh, the universal graphical menu bar appeared in the Apple Lisa in 1983. It has been a feature of all versions of the classic Mac OS since the first Macintosh was released in 1984, and is still used in Mac OS X.
The menu bar in Microsoft Windows is usually anchored to the top of a window under the title bar; therefore, there can be many menu bars on screen at one time. Menus in the menu bar can be accessed through shortcuts involving the Alt key and the mnemonic letter that appears underlined in the menu title. Additionally, pressing Alt or F10 brings the focus on the first menu of the menu bar.
KDE and GNOME allow users to turn Macintosh-style and Windows-style menu bars on and off. KDE can have both types in use at the same time.
The standard GNOME desktop uses a menu bar at the top of the screen, but this menu bar only contains Applications and System menus and status information (such as the time of day); individual programs have their own menu bars as well. The Unity desktop shell shipped with Ubuntu Linux since version 11.04 uses a Macintosh-style menu bar; however, it is hidden unless the mouse pointer hovers over it, similar to the Commodore Amiga example below.
Other window managers and desktop environments use a similar scheme, where programs have their own menus, but clicking one or more of the mouse buttons on the root window brings up a menu containing, for example, commands to launch various applications or to log out.
Window manager menus in Linux are typically configurable either by editing text files, by using a desktop-environment-specific Control Panel applet, or both.
The Amiga used a menu-bar style similar to that of the Macintosh, with the exception that the machine's custom graphics chips allowed each program to have its own ""screen"", with its own resolution and colour settings, which could be dragged down to reveal the screens of other programs. The title/menu bar would typically sit at the top of the screen, and could be accessed by pressing the right mouse button, revealing the names of the various menus. When the right menu button was not pressed down, the menu/title bar would typically display the name of the program which owned the screen, and some other information such as the amount of memory used. When accessing menus with right mouse buttons pressed, one could select multiple menu entries by clicking the left mouse button, and when right mouse button was released, all actions selected in the menus would be performed in the order they were selected. This was known as multiselect.
The Workbench screen title bar would typically display the Workbench version and the amount of free Chip RAM and Fast RAM.  An unusual feature of the Amiga menu system was that the Workbench screen would display a ""Workbench"" menu instead of a ""File"" or ""Apple"" menu, while conforming applications would display ""Project"" and ""Tools"" menus (projects and tools being, respectively, the Amiga terms for what in other systems are called files or documents, and programs or applications).
Keyboard shortcuts could be accessed by pressing the ""right Amiga"" key along with a normal alphanumeric key.  (Some early keyboards had a Commodore key to the left of the spacebar instead of a ""left-Amiga"" key.) The filled-in and hollowed-out designs, respectively, of the left- and right-Amiga (or Commodore and Amiga) keys are similar to the closed-Apple and open-Apple keys of Apple II keyboards.
The NeXTstep OS for the NeXT machines would display a ""menu palette"", by default at the top left of the screen. Clicking on the entries in the menu list would display submenus of the commands in the menu. The contents of the menu change depending on whether the user is ""in"" the Workspace Manager or an application. The menus and the sub-menus can easily be torn off and moved around the screen as individual palette windows.
Power users would often switch off the always-on menu, leaving it to be displayed at the mouse pointer's location when the right mouse button was pressed. The same implementation is used by GNUstep and conforming apps, though applications written for the host operating system or another toolkit will use the menu scheme appropriate to that OS or toolkit.
The TOS operating system for the Atari ST would display menu bars at the top of the screen like Mac OS. Rather than being 'pulled-down' by holding the mouse button, the menu would appear as soon as the pointer was over its heading. This was done to get around an Apple patent on pull-down menus.
In RISC OS, clicking the middle button displays a menu list at the location of the mouse pointer. The RISC OS implementation of menus is similar to the context menus of other systems, except that menus will not close if the right mouse button is used to select a menu entry. This allows the user to implement or try out several settings before closing the menu.
In both Microsoft Windows and Apple Macintosh operating systems, in other similar desktop environments and in some applications, common functions are assigned keyboard shortcuts (e.g. Control-C or Command-C copies the current selection).
Microsoft-style menu bars are physically located in the same window as the content they are associated with. However, Bruce Tognazzini, former employee of Apple Inc. and Human–computer interaction professional, claims that the Mac OS's menu bars can be accessed up to five times faster due to Fitts's law: because the menu bar lies on a screen edge, it effectively has an infinite height — Mac users can just ""throw"" their mouse pointers toward the top of the screen with the assurance that it will never overshoot the menu bar and disappear.
This assumes that the desired menu is currently enabled, however.  If another application has ""focus"", the menu will belong to that application instead, requiring the user to check and see which menu is active before ""throwing"" the mouse, and often perform an extra step of focusing the desired application before using the menu, which is completely separate from the application it controls.  The effectiveness of this technique is also reduced on larger screens or with low mouse acceleration curves, especially due to the time required to travel back to a target in the window after using the menu. On systems with multiple displays, the menu bar may either be displayed on a single ""main"" display, or on all connected displays. The classic Mac OS, and versions of macOS prior to OS X Mavericks displayed only a single menu bar on the main display; Mavericks added the option to show the bar on all displays.
Some applications, e.g. Microsoft Office 2007, Internet Explorer 7 (by default), and Google Chrome and Mozilla Firefox 4 in Windows and Linux, have effectively removed the menu bar altogether by hiding it until a key is pressed (typically the ""alt"" key). These applications present options to the user contextually, typically using hyperlinks to select actions."
"75","Gorm or GORM may refer to:"
"76","GNUstep Renaissance is a development framework that reads XML descriptions of graphical user interfaces from an application bundle and converts them into native widgets and connections at runtime under either GNUstep or Mac OS X.
GNUstep Renaissance was written by Nicola Pero as an alternative to the NIB and gorm files used by Interface Builder and Gorm, respectively. Unlike the aforementioned formats, Renaissance can generate interfaces that can be run without modification on either GNUstep or Mac OS X. It also uses a feature called AutoLayout, which means that localized strings do not have to be manually resized.
As of January 2006, GNUstep Renaissance is beta software. A graphical frontend to Renaissance does not yet exist. A stable release has not been made since 2008, though the downloadable binary has not been rebuilt and is still the previous version.
A simple example of an interface specification:
Assuming this file is in the application bundle and named Sample.gsmarkup, it can be loaded with the following Objective-C code:
"
"77","
This article provides a list of widget toolkits  (also known as GUI frameworks), used to construct the graphical user interface (GUI) of programs, organized by their relationships with various operating systems.
Note that the X Window System was originally primarily for Unix-like operating systems, but it now runs on Microsoft Windows as well using, for example, Cygwin, so some or all of these toolkits can also be used under Windows.
General
RIAs
Full-stack framework
Resource-based
No longer developed
"
"78","
Core Data is an object graph and persistence framework provided by Apple in the macOS and iOS operating systems. It was introduced in Mac OS X 10.4 Tiger and iOS with iPhone SDK 3.0. It allows data organized by the relational entity–attribute model to be serialized into XML, binary, or SQLite stores. The data can be manipulated using higher level objects representing entities and their relationships. Core Data manages the serialized version, providing object lifecycle and object graph management, including persistence. Core Data interfaces directly with SQLite, insulating the developer from the underlying SQL.
Just as Cocoa Bindings handle many of the duties of the controller in a model–view–controller design, Core Data handles many of the duties of the data model. Among other tasks, it handles change management, serializing to disk, memory footprint minimization and queries against the data.
Core Data describes data with a high level data model expressed in terms of entities and their relationships plus fetch requests that retrieve entities meeting specific criteria. Code can retrieve and manipulate this data on a purely object level without having to worry about the details of storage and retrieval. The controller objects available in Interface Builder can retrieve and manipulate these entities directly. When combined with Cocoa bindings the UI can display many components of the data model without needing background code.
For example: a developer might be writing a program to handle vCards. In order to manage these, the author intends to read the vCards into objects, and then store them in a single larger XML file. Using Core Data the developer would drag their schema from the data designer in Xcode into an interface builder window to create a GUI for their schema. They could then write standard Objective-C or Swift code to read vCard files and put the data into Core Data managed entities. From that point on the author's code manipulates these Core Data objects, rather than the underlying vCards. Connecting the Save menu item to the appropriate method in the controller object will direct the controller to examine the object stack, determine which objects are dirty, and then re-write a Core Data document file with these changes.
Core Data is organized into a large hierarchy of classes, though interaction is only prevalent with a small set of them. 

Core Data can serialize objects into XML, Binary, or SQLite for storage. With the release of Mac OS X 10.5 Leopard, developers can also create their own custom atomic store types. Each method carries advantages and disadvantages, such as being human readable (XML) or more memory efficient (SQLite).  This portion of Core Data is similar to the original Enterprise Objects Framework (EOF) system, in that one can write fairly sophisticated queries.  Unlike EOF, it is not possible to write your own SQL. Recently, Core Data store for ODBC has been made available in ODBC framework. 
Core Data schemas are standardized.  If you have the Xcode Data Model file, you can read and write files in that format freely.  Unlike EOF, though, Core Data is not currently designed for multiuser or simultaneous access unless you use ODBC framework.Schema migration is also non-trivial, virtually always requiring code.  If other developers have access to and depend upon your data model, you may need to provide version translation code in addition to a new data model if your schema changes.
Core Data owes much of its design to an early NeXT product, Enterprise Objects Framework (EOF).
EOF was specifically aimed at object-relational mapping for high-end SQL database engines such as Microsoft SQL Server and Oracle. EOF's purpose was twofold: first, to connect to the database engine and hide the implementation details; second, to read the data out of the simple relational format and translate that into a set of objects. Developers typically interacted with the objects only, which dramatically simplifies development of complex programs, at the cost of some ""setup"". The EOF object model was deliberately designed to make the resulting programs ""document like"", in that the user could edit the data locally in memory, and then write out all changes with a single Save command.
Throughout its history, EOF ""contained"" a number of bits of extremely useful code that were not otherwise available under NeXTSTEP/OpenStep. For instance, EOF required the ability to track which objects were ""dirty"" so the system could later write them out. This was presented to the developer not only as a document-like system, but also in the form of an unlimited ""Undo"" command stack. Many developers complained that this state management code was far too useful to be isolated in EOF, and it was later moved into the Cocoa API during the transition to Mac OS X.
Oddly, what was not translated was EOF itself. EOF was used primarily along with another OpenStep-era product, WebObjects, which was an application server originally based on Objective-C. At the time, Apple was in the process of porting WebObjects to the Java programming language, and as part of this conversion, EOF became much more difficult to use from Cocoa. Enough developers complained about this that Apple apparently decided to do something about it.
One critical realization is that the object state management system in EOF did not really have anything to do with relational databases. The same code could be, and was, used by developers to manage graphs of other objects as well. In this role, the really useful parts of EOF were those that automatically built the object sets from the raw data, and then tracked them. It is this concept, and perhaps code, that forms the basis of Core Data."
"79","Carbon is one of Apple Inc.'s C-based application programming interfaces (APIs) for the Macintosh operating system. Carbon provided a good degree of backward compatibility for programs that ran on Mac OS 8 and 9. Developers could use the Carbon APIs to port their ""classic"" Mac software to the Mac OS X platform with far less effort than a port to the entirely different Cocoa system, which originated in OPENSTEP.
Carbon was an important part of Apple's strategy for bringing Mac OS X to market, offering a path for quick porting of existing software applications, as well as a means of shipping applications that would run on either Mac OS X or the classic Mac OS. As the market has increasingly moved to the Cocoa-based frameworks, especially after the release of iOS, the need for a porting library was diluted. Apple did not create a 64-bit version of Carbon while updating their other frameworks in the 2007 time-frame, and eventually deprecated the entire API in OS X 10.8, which was released on July 24, 2012.
The original Mac OS used Object Pascal as its primary development platform, and the APIs were heavily based on Pascal's call semantics. Much of the Macintosh Toolbox consisted of procedure calls, passing information back and forth between the API and program using a variety of data structures based on Pascal's variant record concept.
Over time, a number of object libraries evolved on the Mac, notably MacApp and the Think Class Library (TCL) in Pascal, and later versions of MacApp and CodeWarrior's PowerPlant in C++. By the mid-1990s, most Mac software was written in C++ using CodeWarrior.
With the purchase of NeXT in late 1996, Apple developed a new operating system strategy based largely on the existing OpenStep platform. The new Rhapsody was relatively simple; it retained most of OpenStep's existing object libraries under the name ""Yellow Box"", ported OpenStep's existing GUI and made it look more Mac-like, ported several major APIs from the Mac OS to Rhapsody's underlying Unix-like system (notably QuickTime and AppleSearch), and added an emulator known as the ""Blue Box"" that ran existing Mac OS software.
When this plan was unveiled at the Worldwide Developers Conference in 1997 there was some push-back from existing Mac OS developers, who were upset that their code bases would effectively be locked into an emulator that was unlikely to ever be updated. They took to calling the Blue Box the ""penalty box"".[citation needed] Larger developers like Microsoft and Adobe balked outright, and refused to consider porting to OpenStep, which was so different from the existing Mac OS that there was little or no compatibility.
Apple took these concerns to heart. When Steve Jobs announced this change in direction at the 1998 WWDC, he stated that ""what developers really wanted was a modern version of the Mac OS, and Apple [was] going to deliver it"". The statement was met with thunderous applause. The original Rhapsody concept was eventually released in 1999 as Mac OS X Server 1.0, the only release of its type.
In order to offer a real and well supported upgrade path for existing Mac OS code bases, Apple introduced the Carbon system. Carbon consists of many libraries and functions that offer a Mac-like API, but running on top of the underlying Unix-like OS, rather than a copy of the Mac OS running in emulation. The Carbon libraries are extensively cleaned up, modernized and better ""protected"". While the Mac OS was filled with APIs that shared memory to pass data, under Carbon all such access was re-implemented using accessor subroutines on opaque data types. This allowed Carbon to support true multitasking and memory protection, features Mac developers had been requesting for a decade. Other changes from the pre-existing API removed features which were conceptually incompatible with Mac OS X, or simply obsolete. For example, applications could no longer install interrupt handlers or device drivers.
In order to support Carbon, the entire Rhapsody model changed. Whereas Rhapsody would effectively be OpenStep with an emulator, under the new system both the OpenStep and Carbon API would, where possible, share common code. To do this, many of the useful bits of code from the lower-levels of the OpenStep system, written in Objective-C and known as Foundation, were re-implemented in pure C. This code became known as Core Foundation, or CF for short. A version of the Yellow Box ported to call CF became the new Cocoa API, and the Mac-like calls of Carbon also called the same functions. Under the new system, Carbon and Cocoa were peers. This conversion would normally have slowed the performance of Cocoa as the object methods called into the underlying C libraries, but Apple used a technique they called toll-free bridging to reduce this impact.
As part of this conversion, Apple also ported the graphics engine from the licence-encumbered Display PostScript to the licence-free Quartz (which has been called ""Display PDF""). Quartz provided native calls that could be used from either Carbon or Cocoa, as well as offering Java 2D-like interfaces as well. The underlying operating system itself was further isolated and released as Darwin.
Carbon was introduced in incomplete form in 2000, as a shared library backward-compatible with 1997's Mac OS 8.1. This version allowed developers to port their code to Carbon without losing the ability for those programs to run on existing Mac OS machines. Porting to Carbon became known as ""Carbonization"". Official Mac OS X support arrived in 2001 with the release of Mac OS X v10.0, the first public version of the new OS. Carbon was very widely used in early versions of Mac OS X by almost all major software houses, even by Apple. The Finder, for instance, remained a Carbon application for many years, only being ported to Cocoa with the release of Mac OS 10.6 in 2009.
The transition to 64-bit Macintosh applications beginning with Mac OS X v10.5, released October 26, 2007, brought the first major limitations to Carbon. Apple does not provide compatibility between the Macintosh graphical user interface and the C programming language in the 64-bit environment, instead requiring the use of the Objective-C dialect with the Cocoa API. Many commentaries took this to be the first sign of Carbon's eventual disappearance, a position that was re-enforced when Apple stated no new major additions would be added to the Carbon system.
Despite the purported advantages of Cocoa, the need to rewrite large amounts of legacy code slowed the transition of Carbon-based applications, famously with Adobe Photoshop, which was eventually updated to Cocoa in April 2010. This also extended to Apple's own flagship software packages, as iTunes and Final Cut Pro (as well as the features in the QuickTime engine that powers it) remained written in Carbon for many years. Both iTunes and Final Cut Pro X have since been released in Cocoa versions.
In 2012, with the release of OS X 10.8 Mountain Lion, most Carbon APIs were considered deprecated. The APIs are still accessible to developers and all Carbon applications will run, but the APIs will no longer be updated.
Carbon descends from the Toolbox, and as such, is composed of ""Managers"". Each Manager is a functionally related API, defining sets of data structures and functions to manipulate them. Managers are often interdependent or layered. Carbon consists of a broad set of functions for managing files, memory, data, the user interface, and other system services. It is implemented as any other API: in macOS, it is spread over several frameworks (each a structure built around a shared library), principally Carbon.framework, ApplicationServices.framework, and CoreServices.framework, and in classic Mac OS, it resides in a single shared library named CarbonLib.
As an umbrella term encompassing all C-language API procedures accessing Mac-specific functionality, Carbon is not designed as a discrete system. Rather, it opens nearly all the functionality of macOS to developers who do not know the Objective-C language required for the broadly equivalent Cocoa API.
Carbon is compatible with all of the several executable formats available for PowerPC Mac OS. Binary compatibility between Mac OS X and previous versions requires use of a Preferred Executable Format file, which Apple never supported in their Xcode IDE.
Newer parts of Carbon tend to be much more object-oriented in their conception, most of them based on Core Foundation. Some Managers, such as the HIView Manager (a superset of the Control Manager), are implemented in C++, but Carbon remains a C API.
Some examples of Carbon Managers:
The Mac Toolbox's Event Manager originally used a polling model for application design. The application's main event loop asks the Event Manager for an event using GetNextEvent. If there is an event in the queue, the Event Manager passes it back to the application, where it is handled, otherwise it returns immediately. This behavior is called  ""busy-waiting"", running the event loop unnecessarily. Busy-waiting reduces the amount of CPU time available for other applications and decreases battery power on laptops. The classic Event Manager dates from the original Mac OS in 1984, when whatever application was running was guaranteed to be the only application running, and where power management was not a concern.
With the advent of MultiFinder and the ability to run more than one application simultaneously came a new Event Manager call, WaitNextEvent, which allows an application to specify a sleep interval. One easy trick for legacy code to adopt a more efficient model without major changes to its source code is simply to set the sleep parameter passed to WaitNextEvent to a very large value—on macOS, this puts the thread to sleep whenever there is nothing to do, and only returns an event when there is one to process. In this way, the polling model is quickly inverted to become equivalent to the callback model, with the application performing its own event dispatching in the original manner. There are loopholes, though. For one, the legacy toolbox call ModalDialog, for example, calls the older GetNextEvent function internally, resulting in polling in a tight loop without blocking.
Carbon introduces a replacement system, called the Carbon Event Manager.  (The original Event Manager still exists for compatibility with legacy applications). Carbon Event Manager provides the event loop for the developer (based on Core Foundation's CFRunLoop in the current implementation); the developer sets up event handlers and enters the event loop in the main function, and waits for Carbon Event Manager to dispatch events to the application.
In the classic Mac OS, there was no operating system support for application level timers (the lower level Time Manager was available, but it executed timer callbacks at interrupt time, during which calls could not be safely made to most Toolbox routines). Timers were usually left to application developers to implement, and this was usually done by counting elapsed time during the idle event - that is, an event that was returned by WaitNextEvent when any other event wasn't available. In order for such timers to have reasonable resolution, developers could not afford WaitNextEvent to delay too long, and so low ""sleep"" parameters were usually set. This results in highly inefficient scheduling behavior, since the thread will not sleep for very long, instead repeatedly waking to return these idle events. Apple added timer support to Carbon to address this problem—the system can schedule timers with great efficiency.
"
"80","Copland is an unreleased operating system prototype for Apple Macintosh computers of the late 1990s, intended to be released as the modern System 8 successor to the aging but venerable System 7. It introduced protected memory, preemptive multitasking, and a number of new underlying operating system features, while retaining compatibility with existing Mac applications. Copland's planned successor, codenamed Gershwin, was intended to add advanced features such as application-level multithreading.
Across a protracted development period of several years, previews of Copland garnered much press that introduced the layperson Macintosh audience to basic concepts of modern operating system design such as object orientation, crash-proofing, and multitasking. The project was Apple's trigger to cofound several industry-wide standards and consortiums for next-generation operating system development, such as OpenDoc and Taligent.
Copland reached Developer Release beta testing status before its cancellation in August 1996. Instead, Apple released a much more legacy-oriented Mac OS 8 in 1997, followed by Mac OS 9's architectural improvements in 1999, and then Mac OS X became Apple's next generation operating system release in 2001. All of these releases bear some functional or cosmetic influence from Copland.
As a product of dysfunctional corporate personnel and project management, Copland is associated with empire-building, feature creep, and  project death march. In 2008, PC World named Copland on a list of the biggest project failures in IT history.
The pre-history of Copland begins with an understanding of the Mac OS legacy, and its architectural problems to be solved.
Launched in 1984, the Macintosh and its operating system were designed from the beginning as a single-user, single-tasking machine, which allowed the hardware development to be greatly simplified. As a side effect of this single application model, the original Mac developers were able to take advantage of a number of compromising simplifications that allowed great improvements in performance, running even faster than the much more expensive Lisa. But this design also led to several problems for future expansion.
By assuming only one program would be running at a time, the engineers were able to ignore the concept of reentrancy; the ability for a program (or library) to be stopped at any point, asked to do something else, and then return to the original task. In the case of QuickDraw for example, this means the system can store state information internally, like the current location of the window or the line style, knowing it would only change under control of the running program. Taking this one step further, the engineers left most of this state inside the application rather than in QuickDraw, thus eliminating the need to copy this data between the application and library.
The other main issue was that early Macs lack a memory management unit (MMU), which precludes the possibility of several fundamental modern features. An MMU would provide memory protection to ensure that programs cannot accidentally overwrite other program's memory, and it would provision shared memory. Lacking shared memory, the API was instead written so the operating system and application shares all memory, which is what allows QuickDraw to examine the application's memory for settings like the line drawing mode or color.
These limitations mean that supporting the multitasking of more than one program at a time would be difficult, without rewriting all of this operating system and application code. Yet doing so, would mean the system would run unacceptably slow on existing hardware. Instead, Apple adopted a system known as MultiFinder in 1987, which keeps the running application in control of the computer as before but allows an application to be rapidly switched to another, normally simply by clicking on its window. Programs that are not in the foreground are periodically given short bits of time to run, but as before, the entire process is controlled by the applications, not the operating system.
Because the operating system and applications all share a single memory space, it is possible for a bug in any one of them to corrupt the entire operating system, and crash the machine. This is not particularly annoying under a single-application model, in which case the application crashes anyway. But under MultiFinder, any crash will crash all the other running programs as well. The multitasking of multiple applications potentially increases the chances of a crash, making the system potentially more fragile.
Adding greatly to the severity of the problem, is the system used to add functionality to the operating system itself, which relies on a patching mechanism known as CDEVs and INITs, commonly known as Control Panels, and Extensions. Third party developers also make use of this mechanism to add features, including  screensavers and a hierarchal Apple menu, independently of Apple. Some of these third-party control panels became almost universal, like the popular After Dark screensaver package. Since there was no standard for use of these patches, it is not uncommon for several of these add-ons—including Apple's own additions to the OS—to use the same patches, and interfere with each other, leading to more crashing.
Copland was designed to consist of the Mac OS on top of a microkernel named Nukernel, which would handle basic tasks such as application startup and memory management, leaving all other tasks to a series of semi-special programs known as servers. For instance, networking and file services would not be provided by the kernel itself, but by servers that would be sent requests though interapplication communications. Copland consists of the combination of Nukernel, various servers, and a suite of application support libraries to provide implementations of the well-known classic Macintosh programming interface.
Application services are offered through a single program known officially as the Cooperative Macintosh Toolbox environment, but are universally referred to as the Blue Box. The Blue Box encapsulates an existing System 7 operating system inside a single process and address space. Mac programs run inside the Blue Box much as they do under System 7, as cooperative tasks that use the non-reentrant Toolbox calls. A worst-case scenario is that an application in the Blue Box crashes, taking down the entire Blue Box instance with it. This does not result in the system as a whole going down, however, and the Blue Box can be restarted.
New applications written with Copland in mind, are able to directly communicate with the system servers and thereby gain many advantages in terms of performance and scalability. They can also communicate with the kernel to launch separate applications or threads, which run as separate processes in protected memory, as in most modern operating systems. These separate applications cannot use non-reentrant calls like QuickDraw, however, and thus could have no user interface. Apple suggested that larger programs could place their user interface in a normal Macintosh application, which would then start ""worker threads"" externally.
Another key feature of Copland is that it is completely PowerPC native. System 7 had been ported to the PowerPC (PPC) with great success; large portions of the system run as PPC code—including both high-level functionality, such as the majority of the user interface ""toolbox"" managers, and low-level functionality, such as interrupt management. There is enough 68k code left in the system to be run in emulation, and especially user applications, however that the operating system must map some data between the two environments. In particular, every call into the Mac OS requires a mapping between the 68k's interrupt system and the PPC's. Removing these mappings would greatly improve general system performance; at WWDC 1996, engineers claimed that the performance of system calls would be as much as 50% faster.
Copland is also based on the newly defined Common Hardware Reference Platform, or CHRP, which standardized the Mac hardware to the point where it could be built by different companies and can run other operating systems (Solaris and AIX were two of many mentioned). This was a common theme at the time; many companies were forming groups to define standardized platforms to offer an alternative to the ""Wintel"" platform that was rapidly becoming dominant—examples include 88open, Advanced Computing Environment, and the AIM alliance.
The challenge in Copland would be getting all of this functionality to fit into an ordinary Mac. System 7.5 already uses up about 2.5 megabytes (MB) of RAM, and at the time this was a significant portion of the total RAM in most machines. Copland runs what was essentially a complete copy of System 7.5 (in the Blue Box) in addition to an entirely separate operating system running under it. Copland therefore uses a Mach-inspired memory management system and relies extensively upon shared libraries, with the goal being for Copland to be only some 50% larger than 7.5.
Copland's development began in 1994 and was underway in earnest by 1995 when the system started to be referred to as ""System 8"", and later, ""Mac OS 8"". It was the major topic of Apple's Apple Worldwide Developers Conference in 1996, where it was presented as the sole focus of the company's development efforts. As the project gathered momentum, however, a furious round of internal corporate empire building began. New features were added more rapidly than they could be completed, including most of the items originally slated for Gershwin, along with a wide variety of otherwise unrelated projects from within the company. The completion date for the beta continued to slip into the future and several announced dates passed with no sign of a release. The press became highly critical of the project, and of the company as a whole.
In 1996, Apple's newest CEO, Gil Amelio, poached Ellen Hancock from National Semiconductor and put her in charge of engineering in an effort to try to get development back on track. She decided it was best to cancel the project outright and try to find a suitable third-party system to replace it. Development of Copland officially ended in August 1996. After a short search, Apple announced its purchase of NeXT in order to use its NeXTSTEP operating system as the basis of a new Mac OS. Hancock also suggested that Apple should work on improving the existing System 7 while the new system matured. This was released as Mac OS 8 in 1997 and was followed by Mac OS 9 in 1999. The new operating system based on NeXTSTEP shipped in 2001 as Mac OS X.
In March 1988,[lower-alpha 1] technical middle managers at Apple held an offsite meeting to plan the future course of Mac OS development. Ideas were written on index cards; features that seemed simple enough to implement in the short term (like adding color to the user interface) were written on blue cards; longer-term goals—such as  preemptive multitasking—were on pink cards; and long-range ideas like an object-oriented file system were on red cards. Development of the ideas contained on the blue and pink cards was to proceed in parallel, and at first, the two projects were known simply as ""blue"" and ""pink"". Apple intended to have the ""blue"" team (which came to call themselves the ""Blue Meanies"" after characters in Yellow Submarine) release an updated version of the existing Macintosh operating system in the 1990–1991 timeframe, and the Pink team to release an entirely new OS around 1993.
The Blue team delivered what became known as System 7 on May 13, 1991, but the Pink team suffered from second-system effect and its release date continued to slip into the indefinite future. Some of the reason for this can be traced to problems that would become widespread at Apple, as time went on; as Pink became delayed and its engineers moved to Blue instead. This left the Pink team constantly struggling for staffing, and suffering from the problems associated with high employee turnover. Management ignored these sorts of technical development issues, leading to continual problems delivering working products.
At this same time, the recently released NeXTSTEP was generating intense interest in the developer world. Features that were originally part of Red, were folded into Pink, and the Red project (also known as ""Raptor"") was eventually canceled. This problem was also common at Apple during this period; in order to chase the ""next big thing"", middle managers would add new features to their projects with little oversight, leading to enormous problems with feature creep. In the case of Pink, development eventually slowed to the point the project appeared moribund.
On April 12, 1991, Apple CEO John Sculley performed a secret demonstration of Pink running on an PS/2 Model 70 to a delegation from IBM. Though the system was not fully functional, it resembled System 7 running on a PC. IBM was extremely interested, and over the next few months, the two companies formed an alliance to further development of the system. These efforts became public in early 1992, under the new name ""Taligent"". At the time, Sculley summed up his concerns with Apple's own ability to ship Pink when he stated ""We want to be a major player in the computer industry, not a niche player. The only way to do that is to work with another major player.""
Infighting at the new joint company was legendary, and the problems with Pink within Apple soon appeared to be minor in comparison. Apple employees made T-shirts, graphically displaying their prediction that the result would be an IBM-only project, a prediction that came true on December 19, 1995, when Apple officially pulled out of the project. IBM continued working with Taligent, and eventually released its application development portions under the new name ""CommonPoint"". This saw little interest and the project disappeared from IBM's catalogs within months.
While Taligent efforts continued, very little work addressing the structure of the original OS was carried out. Several new projects started during this time. Notably the Star Trek project is the code name of a port of System 7 and its basic applications, running on an Intel-compatible x86 machine, which reached demo quality. But as Taligent was still a going concern, it was difficult for new OS projects to gain any traction.
Instead, Apple's Blue team continued adding new features to the same basic OS. During the early 1990s Apple released a series of major new packages to the system; amongst them were QuickDraw GX, Open Transport, OpenDoc, PowerTalk, and many others. Most of these were larger than the original operating system. Problems with stability that had existed even with small patches, grew along with the size and requirements of these packages, and by the mid-1990s the Mac had a reputation for instability and constant crashing.
As the stability of the operating system collapsed, the ready answer was that Taligent would fix this—it was fully reentrant, truly multitasking, and made heavy use of protected memory. When the Taligent efforts collapsed, Apple was left with an aging OS and no designated solutions. By 1994 the press buzz surrounding the upcoming release of Windows 95 started to grow to a crescendo, often questioning Apple's ability to respond to the challenge it presented. The press turned on the company, often introducing Apple's new projects as failures in the making.
Given this pressure, the collapse of Taligent, the growing problems with the existing operating system, and the release of System 7.5 in late 1994, Apple management decided that the decade-old operating system had run its course. A new system that did not have these problems was needed, and soon. Since so much of the existing system would be difficult to rewrite, Apple developed a two-stage approach to the problem.
In the first stage, the existing system would be moved on top of a new kernel-based OS with built-in support for multitasking and protected memory. The existing libraries, like QuickDraw, would take too long to be rewritten for the new system and would not be converted to be reentrant. Instead, a single paravirtualized operating system, the ""Blue Box"", keeps applications and older code like QuickDraw in a single memory block so they continue to run as they had in the past. The Blue Box operating system itself runs in a separate memory space, so crashing applications or extensions within Blue Box can not crash the entire machine.
Once the new kernel was in place and this basic upgrade was released, development would move on to rewriting the older libraries into new forms that could run directly over the new kernel. At that point, applications would gain some additional modern features.
As System 7.5 was code-named ""Mozart"", the next-generation operating system was intended to address the looming architectural issues, was dubbed ""Copland"" after composer Aaron Copland. The intended successor system, ""Gershwin"", would complete the process of moving the entire system to the new platform.
The Copland project was first announced in March 1995. Parts of Copland, most notably an early version of the new file system, were demonstrated at Apple's Worldwide Developers Conference in May 1995. Apple also promised that a beta release of Copland would be ready by the end of the year, for final commercial release in early 1996. Gershwin would follow the next year. Throughout the year, Apple released a number of mock-ups to various magazines showing what the new system would look like, and commented continually that the company was fully committed to this project. By the end of the year, however, the Developer Release had not been produced.
As had happened in the past during the development of Pink, developers within Apple soon started abandoning their own projects in order to work on the new system. Middle management and project leaders fought back by claiming that their project was vital to the success of the system, and moving it into the Copland development stream. Thus, it could not be canceled along with their employees being removed to work on some other part of Copland anyway. This process took on momentum across the next year.
In mid-1996, information was leaked that Copland would have the ability to run applications written for other operating systems including Windows NT. This feature had supposedly been in development for more than 3 years. One user claimed to have been told about these plans by members of the Copland development team. Some analysts projected that this ability would increase Apple's penetration into the enterprise market, others said it was ""game over"" and was only a sign of the Mac platform's irrelevancy.
Soon the project looked less like a new operating system and more like a huge collection of new technologies; QuickDraw GX, SOM, and OpenDoc became core components of the system, while completely unrelated technologies like a new file management dialog box (the ""open dialog"") and ""themes"" support appeared as well. The feature list grew much faster than the features could be completed, a classic case of creeping featuritis.
An industry executive noted that ""The game is to cut it down to the three or four most compelling features as opposed to having hundreds of nice-to-haves, I'm not sure that's happening.""
As the ""package"" grew, testing it became increasingly difficult and engineers were commenting as early as 1995 that Apple's announced 1996 release date was hopelessly optimistic: ""There's no way in hell Copland ships next year. I just hope it ships in 1997.""
At WWDC 1996, Apple's new CEO, Gil Amelio, used the keynote to talk almost exclusively about Copland, now known as System 8. He repeatedly stated that it was the only focus of Apple engineering and that it would ship to developers at the end of summer with a full release planned for late fall. Very few, if any, demos of the running system were shown at the conference. Instead, various pieces of the technology and user interface that would go into the package (such as a new file management dialog) were demonstrated. Little of the core system's technology was demonstrated and the new file system that had been shown a year earlier was absent.
There was one way to actually use the new operating system, by signing up for time in the developer labs. This did not go well:
Also, it was incredibly fragile and crashed repeatedly, often corrupting system files on the disk in the process. The demo staff reformatted and rebuilt the hard disks at regular intervals. It was incredible that they even let us see the beast.
After a number of people at the show complained about the microkernel's lack of sophistication, notably the lack of symmetric multiprocessing—a feature that would be exceedingly difficult to add to a system due to ship in a few months—Amelio came back on stage and announced that they would be adding that to the feature list.
In August 1996, ""Developer Release 0"" was sent to a small number of selected partners. Far from demonstrating improved stability, it often crashed after doing nothing at all, and was completely unusable for development. In October, Apple moved the target delivery date to ""sometime"", hinting that it might be 1997. One of the groups most surprised by the announcement was Apple's own hardware team, who had been waiting for Copland to allow the PowerPC be truly represented, unburdened of software legacy. Members of Apple's software QA team suggested, jokingly, that given current resources and the number of bugs in the system they could clear the program for shipping sometime around 2030.
Later that summer, the situation was no better. Amelio complained that Copland was ""just a collection of separate pieces, each being worked on by a different team ... that were expected to magically come together somehow."" Hoping to salvage the situation, Amelio hired Ellen Hancock away from National Semiconductor to take over engineering and get Copland development back on track.
After a few months on the job, Hancock came to the conclusion that the situation was hopeless; given current development and engineering, she felt Copland would never ship. Instead, she suggested that the various user-facing technologies in Copland be rolled out in a series of staged releases, instead of a single big release. To address the aging infrastructure below these technologies, Amelio suggested looking outside the company for an entirely new operating system. Candidates considered were Sun's Solaris and Windows NT. Hancock reportedly was in favor of going with Solaris, while Amelio preferred Windows. Amelio even reportedly called Bill Gates to discuss the idea, and Gates promised to put Microsoft engineers to work porting QuickDraw to NT.
Apple officially canceled Copland in August 1996. While the CD envelopes for the developer's release had been printed, the discs themselves had not been mastered.
After lengthy discussions with Be and rumors of a merger with Sun Microsystems, many were surprised at Apple's December 1996 announcement that they were purchasing NeXT and bringing Steve Jobs on in an advisory role. Amelio quipped that they ""choose Plan A instead of Plan Be."" The project to port OpenStep to the Macintosh platform was named Rhapsody and was to be the core of Apple's cross-platform operating system strategy. This would inherit OpenStep's existing support for Power PC, Intel x86, and DEC Alpha CPU architectures, as well as an implementation of the OPENSTEP libraries running on Windows NT. This would in effect open the Windows application market to Macintosh developers as they could license the library from Apple for distribution with their product, or depend upon a preexisting installation.
Following Hancock's plan, development of System 7.5 continued, with a number of technologies originally slated for Copland being incorporated into the base OS. Apple embarked on a buying campaign, acquiring the rights to various third-party system enhancements and integrating them into the OS. The Extensions Manager, hierarchical Apple menu, collapsing windows, the menu bar clock, and sticky notes—all were developed outside of Apple. Stability and performance were improved by Mac OS 7.6, which dropped the ""System"" moniker in favor of ""Mac OS"". Eventually, many features developed for Copland, including the new multithreaded Finder and support for themes (the default Platinum was the only theme included) were rolled into the unreleased beta of Mac OS 7.7, which was instead rebranded and launched as Mac OS 8.
With the return of Jobs, this rebranding to version 8 also allowed Apple to exploit a legal loophole to terminate third-party manufacturers' licenses to System 7 and effectively shut down the Macintosh clone market. Later, Mac OS 8.1 finally added the new filesystem and Mac OS 8.6 updated the nanokernel to handle limited support for preemptive tasks. Its interface is Multiprocessing Services 2.x and later, but there is no process separation and the system still uses cooperative multitasking between processes. Even a process that is Multiprocessing Services-aware still has a portion that runs in the Blue Box—a task that also runs all single-threaded programs and the only task that can run 68k code.
The Rhapsody project was cancelled after several Developer Preview releases, support for running on non-Macintosh platforms was dropped, and it was eventually released as Mac OS X Server 1.0. In 2001 this foundation was coupled to the Carbon library and Aqua user interface to form the modern Mac OS X product. Versions of Mac OS X prior to the Intel release of Mac OS X 10.4 (Tiger), also use the rootless Blue Box concept in the form of Classic to run applications written for older versions of Mac OS. A number of features originally seen in Copland demos, including its advanced Find command, built-in Internet browser, ""piles"" of folders, and support for video-conferencing, have reappeared in subsequent releases of Mac OS X as Spotlight, Safari, Stacks, and iChat AV, respectively, although the implementation and user interface for each feature is completely different.
According to the documentation included in the Developer Release, Copland supports the following hardware configurations:"
"81","
A monolithic kernel is an operating system architecture where the entire operating system is working in kernel space and is alone in supervisor mode. The monolithic model differs from other operating system architectures (such as the microkernel architecture) in that it alone defines a high-level virtual interface over computer hardware. A set of primitives or system calls implement all operating system services such as process management, concurrency, and memory management. Device drivers can be added to the kernel as modules.
Modular operating systems such as OS-9 and most modern monolithic operating systems such as OpenVMS, Linux, BSD, and UNIX variants such as  SunOS, and AIX, in addition to MULTICS, can dynamically load (and unload) executable modules at runtime. 
This modularity of the operating system is at the binary (image) level and not at the architecture level. Modular monolithic operating systems are not to be confused with the architectural level of modularity inherent in server-client operating systems (and its derivatives sometimes marketed as hybrid kernel) which use microkernels and servers (not to be mistaken for modules or daemons). 
Practically speaking, dynamically loading modules is simply a more flexible way of handling the operating system image at runtime — as opposed to rebooting with a different operating system image. The modules allow easy extension of the operating systems' capabilities as required. Dynamically loadable modules incur a small overhead when compared to building the module into the operating system image. 
However, in some cases, loading modules dynamically (as-needed) helps to keep the amount of code running in kernel space to a minimum; for example, to minimize operating system footprint for embedded devices or those with limited hardware resources. Namely, an unloaded module need not be stored in scarce random access memory."
"82"," (Learn how and when to remove this template message)
Mach (/mʌk/) is an operating system kernel developed at Carnegie Mellon University to support operating system research, primarily distributed and parallel computing. Mach is often mentioned as one of the earliest examples of a microkernel. However, not all versions of Mach are microkernels. Mach's derivatives are the basis of the modern operating system kernels in GNU Hurd and Apple's operating systems macOS, iOS, tvOS, and watchOS.
The project at Carnegie Mellon ran from 1985 to 1994, ending with Mach 3.0, which is a true microkernel. Mach was developed as a replacement for the kernel in the BSD version of Unix, so no new operating system would have to be designed around it. Experimental research on Mach appears to have ended, although Mach and its derivatives exist within a number of commercial operating systems. These include NeXTSTEP and OpenStep, upon which macOS is based—all using the XNU operating system kernel which incorporates an earlier, non-microkernel, Mach as a major component. The Mach virtual memory management system was also adopted in 4.4BSD by the BSD developers at CSRG, and appears in modern BSD-derived Unix systems, such as FreeBSD.
Mach is the logical successor to Carnegie Mellon's Accent kernel. The lead developer on the Mach project, Richard Rashid, has been working at Microsoft since 1991 in various top-level positions revolving around the Microsoft Research division. Another of the original Mach developers, Avie Tevanian, was formerly head of software at NeXT, then Chief Software Technology Officer at Apple Inc. until March 2006.
Mach's name Mach evolved in a euphemization spiral: While the developers, once during the naming phase, had to bike to lunch through rainy Pittsburgh's mud puddles, Tevanian joked the word muck could serve as a backronym for their Multi-User [or Multiprocessor Universal] Communication Kernel. Italian CMU engineer Dario Giuse later asked project leader Rick Rashid about the project's current title and received ""MUCK"" as the answer, though not spelled out but just pronounced as IPA: [mʌk] which he, according to the Italian alphabet, wrote as Mach. Rashid liked Giuse's spelling ""Mach"" so much that it prevailed.
A key concept in the original Unix operating system was the idea of a pipe. A pipe was an abstraction that allowed data to be moved as an unstructured stream of bytes from program to program. Using pipes, users (or programmers) could link together multiple programs to complete tasks, feeding data through several small programs in turn. This contrasted with typical operating systems of the era, which required a single large program that could handle the entire task, or alternately, used files to pass data, which was resource expensive and time consuming.
Pipes were built on the underlying input/output system. This system was, in turn, based on a model where drivers were expected to periodically ""block"" while they waited for tasks to complete. For instance, a printer driver might send a line of text to a line printer and then have nothing to do until the printer completed printing that line. In this case, the driver would indicate that it was blocked, and the operating system would allow some other program to run until the printer indicated it was ready for more data. In the pipes system the limited resource was memory, and when one program filled the memory assigned to the pipe, it would naturally block. Normally this would cause the consuming program to run, emptying the pipe again. In contrast to a file, where the entire file has to be read or written before the next program can use it, pipes made the movement of data across multiple programs occur in a piecemeal fashion without any programmer intervention.
However, the implementation of pipes as memory buffers meant data was being copied from program to program, a time consuming and resource intensive operation. This made the pipe concept unsuitable for tasks where quick turnaround or low latency was needed, as is the case in most device drivers. The operating system kernel and most core functionality was instead written as a single large program. As the operating system added new functionality (computer networking, for instance), the size and complexity of the kernel grew, too.
Unix pipes offered a conceptual system that could be used to build arbitrarily complex solutions out of small interacting programs. Being smaller, these programs were easy to program and maintain, and had well defined interfaces that simplified programming and debugging. These qualities are even more valuable for device drivers, where small size and bug-free performance are extremely important. There was a strong desire to model the kernel itself on the same basis of small interacting programs.
One of the first systems to use a pipe-like system as the basis for the operating system was the Aleph kernel developed at the University of Rochester. This introduced the concept of ports, which were essentially a shared memory implementation. In Aleph, the kernel itself was reduced to providing access to the hardware, including memory and the ports, while conventional programs using the ports system implemented all behavior, from device drivers to user programs. This concept greatly reduced the size of the kernel, and allowed users to experiment with different drivers simply by loading them and connecting them together at runtime. This greatly eased the problems when developing new operating system code, which otherwise generally required the machine to be restarted. The general concept of a small kernel and external drivers became known as a microkernel.
Aleph was implemented on Data General Eclipse minicomputers and was tightly bound to them. This machine was far from ideal, as it required memory to be copied between programs, which involved a considerable performance overhead. It was also quite expensive. Nevertheless, Aleph proved that the basis system was sound, and went on to demonstrate computer clustering by copying the memory over an early Ethernet interface.
Around this time a new generation of central processors (CPUs) were coming to market, offering 32-bit address spaces and (initially optional) support for a memory management unit (MMU). The MMU handled the instructions needed to implement a virtual memory (VM) system by keeping track of which pages of memory were in use by various programs. This offered a new solution to the port concept, using the copy on write mechanism used by VM. Instead of copying data between programs, all that had to be sent was the data needed to instruct the MMU to provide access to the same memory. This system would implement the interprocess communications system with dramatically higher performance.
This concept was picked up at Carnegie-Mellon, who adapted Aleph for the PERQ workstation and implemented it using copy-on-write. The port was successful, but the resulting Accent kernel was of limited practical use because it did not run existing software. Moreover, Accent was as tightly tied to PERQ as Aleph was to the Eclipse.
The major change between these experimental kernels and Mach was the decision to make a version of the existing 4.2BSD kernel re-implemented on the Accent message-passing concepts. Such a kernel would be binary compatible with existing BSD software, making the system immediately useful for everyday use while still being a useful experimental platform. Additionally, the new kernel would be designed from the start to support multiple processor architectures, even allowing heterogeneous clusters to be constructed. In order to bring the system up as quickly as possible, the system would be implemented by starting with the existing BSD code, and re-implementing it bit by bit as inter-process communication-based (IPC-based) programs. Thus Mach would begin as a monolithic system similar to existing UNIX systems, and evolve more towards the microkernel concept over time.
Mach started largely as an effort to produce a cleanly defined, UNIX-based, highly portable Accent. The result was a short list of generic concepts:
Mach developed on Accent's IPC concepts, but made the system much more UNIX-like in nature, even able to run UNIX programs with little or no modification. To do this, Mach introduced the concept of a port, representing each endpoint of a two-way IPC. Ports had security and rights like files under UNIX, allowing a very UNIX-like model of protection to be applied to them. Additionally, Mach allowed any program to handle privileges that would normally be given to the operating system only, in order to allow user space programs to handle things like interacting with hardware.
Under Mach, and like UNIX, the operating system again becomes primarily a collection of utilities. As with UNIX, Mach keeps the concept of a driver for handling the hardware. Therefore, all the drivers for the present hardware have to be included in the microkernel. Other architectures based on Hardware Abstraction Layer or exokernels could move the drivers out of the microkernel.
The main difference with UNIX is that instead of utilities handling files, they can handle any ""task"". More operating system code was moved out of the kernel and into user space, resulting in a much smaller kernel and the rise of the term microkernel. Unlike traditional systems, under Mach a process, or ""task"", can consist of a number of threads. While this is common in modern systems, Mach was the first system to define tasks and threads in this way. The kernel's job was reduced from essentially being the operating system to maintaining the ""utilities"" and scheduling their access to hardware.
The existence of ports and the use of IPC is perhaps the most fundamental difference between Mach and traditional kernels. Under UNIX, calling the kernel consists of an operation known as a system call or trap. The program uses a library to place data in a well known location in memory and then causes a fault, a type of error. When the system is first started the kernel is set up to be the ""handler"" of all faults, so when the program causes a fault the kernel takes over, examines the information passed to it, and then carries out the instructions.
Under Mach, the IPC system was used for this role instead. In order to call system functionality, a program would ask the kernel for access to a port, then use the IPC system to send messages to that port. Although the messages were triggered by system calls as they would be on other kernels, under Mach that was pretty much all the kernel did—handling the actual request would be up to some other program.
Thread and concurrency support benefited by message passing with IPC mechanisms since tasks now consisted of multiple code threads which, Mach could freeze and unfreeze during message handling. This allowed the system to be distributed over multiple processors, either using shared memory directly as in most Mach messages, or by adding code to copy the message to another processor if needed. In a traditional kernel this is difficult to implement; the system has to be sure that different programs don't try to write to the same memory from different processors. However, Mach ports, its process for memory access, make this well defined and easy to implement, and were made a first-class citizen in that system.
The IPC system initially had performance problems, so a few strategies were developed to minimize the impact. Like its predecessor, Accent, Mach used a single shared-memory mechanism for physically passing the message from one program to another. Physically copying the message would be too slow, so Mach relies on the machine's memory management unit (MMU) to quickly map the data from one program to another. Only if the data is written to would it have to be physically copied, a process called ""copy-on-write"".
Messages were also checked for validity by the kernel, to avoid bad data crashing one of the many programs making up the system. Ports were deliberately modeled on the UNIX file system concepts. This allowed the user to find ports using existing file system navigation concepts, as well as assigning rights and permissions as they would on the file system.
Development under such a system would be easier. Not only would the code being worked on exist in a traditional program that could be built using existing tools, it could also be started, debugged and killed off using the same tools. With a monokernel a bug in new code would take down the entire machine and require a reboot, whereas under Mach this would require only that the program be restarted. Additionally the user could tailor the system to include, or exclude, whatever features they required. Since the operating system was simply a collection of programs, they could add or remove parts by simply running or killing them as they would any other program.
Finally, under Mach, all of these features were deliberately designed to be extremely platform neutral. To quote one text on Mach:
There are a number of disadvantages, however. A relatively mundane one is that it is not clear how to find ports. Under UNIX this problem was solved over time as programmers agreed on a number of ""well known"" locations in the file system to serve various duties. While this same approach worked for Mach's ports as well, under Mach the operating system was assumed to be much more fluid, with ports appearing and disappearing all the time. Without some mechanism to find ports and the services they represented, much of this flexibility would be lost.
Mach was initially hosted as additional code written directly into the existing 4.2BSD kernel, allowing the team to work on the system long before it was complete. Work started with the already functional Accent IPC/port system, and moved on to the other key portions of the OS, tasks and threads and virtual memory. As portions were completed various parts of the BSD system were re-written to call into Mach, and a change to 4.3BSD was also made during this process.
By 1986 the system was complete to the point of being able to run on its own on the DEC VAX. Although doing little of practical value, the goal of making a microkernel was realized. This was soon followed by versions on the IBM PC/RT and for Sun Microsystems 68030-based workstations, proving the system's portability. By 1987 the list included the Encore Multimax and Sequent Balance machines, testing Mach's ability to run on multiprocessor systems. A public Release 1 was made that year, and Release 2 followed the next year.
Throughout this time the promise of a ""true"" microkernel was not yet being delivered. These early Mach versions included the majority of 4.3BSD in the kernel, a system known as POE Server, resulting in a kernel that was actually larger than the UNIX it was based on. The idea, however, was to move the UNIX layer out of the kernel into user-space, where it could be more easily worked on and even replaced outright. Unfortunately performance proved to be a major problem, and a number of architectural changes were made in order to solve this problem. Unwieldy UNIX licensing issues were also plaguing researchers, so this early effort to provide a non-licensed UNIX-like system environment continued to find use, well into the further development of Mach.
The resulting Mach 3 was released in 1990, and generated intense interest. A small team had built Mach and ported it to a number of platforms, including complex multiprocessor systems which were causing serious problems for older-style kernels. This generated considerable interest in the commercial market, where a number of companies were in the midst of considering changing hardware platforms. If the existing system could be ported to run on Mach, it would seem it would then be easy to change the platform underneath.
Mach received a major boost in visibility when the Open Software Foundation (OSF) announced they would be hosting future versions of OSF/1 on Mach 2.5, and were investigating Mach 3 as well. Mach 2.5 was also selected for the NeXTSTEP system and a number of commercial multiprocessor vendors. Mach 3 led to a number of efforts to port other operating systems parts for the microkernel, including IBM's Workplace OS and several efforts by Apple to build a cross-platform version of the classic Mac OS.[citation needed]
Mach was originally intended to be a replacement for classical monolithic UNIX, and for this reason contained many UNIX-like ideas. For instance, Mach used a permissioning and security system patterned on UNIX's file system. Since the kernel was privileged (running in kernel-space) over other OS servers and software, it was possible for malfunctioning or malicious programs to send it commands that would cause damage to the system, and for this reason the kernel checked every message for validity. Additionally most of the operating system functionality was to be located in user-space programs, so this meant there needed to be some way for the kernel to grant these programs additional privileges, to operate on hardware for instance.
Some of Mach's more esoteric features were also based on this same IPC mechanism. For instance, Mach was able to support multi-processor machines with ease. In a traditional kernel extensive work needs to be carried out to make it reentrant or interruptible, as programs running on different processors could call into the kernel at the same time. Under Mach, the bits of the operating system are isolated in servers, which are able to run, like any other program, on any processor. Although in theory the Mach kernel would also have to be reentrant, in practice this isn't an issue because its response times are so fast it can simply wait and serve requests in turn. Mach also included a server that could forward messages not just between programs, but even over the network, which was an area of intense development in the late 1980s and early 1990s.
Unfortunately, the use of IPC for almost all tasks turned out to have serious performance impact. Benchmarks on 1997 hardware showed that Mach 3.0-based UNIX single-server implementations were about 50% slower than native UNIX.
Studies showed the vast majority of this performance hit, 73% by one measure, was due to the overhead of the IPC.[citation needed] And this was measured on a system with a single large server providing the operating system; breaking the operating system down further into smaller servers would only make the problem worse. It appeared the goal of a collection-of-servers was simply not possible.
Many attempts were made to improve the performance of Mach and Mach-like microkernels, but by the mid-1990s much of the early intense interest had died. The concept of an operating system based on IPC appeared to be dead, the idea itself flawed.[citation needed]
In fact, further study of the exact nature of the performance problems turned up a number of interesting facts. One was that the IPC itself was not the problem: there was some overhead associated with the memory mapping needed to support it, but this added only a small amount of time to making a call. The rest, 80% of the time being spent, was due to additional tasks the kernel was running on the messages. Primary among these was the port rights checking and message validity. In benchmarks on an 486DX-50, a standard UNIX system call took an average of 21μs to complete, while the equivalent operation with Mach IPC averaged 114μs. Only 18μs of this was hardware related; the rest was the Mach kernel running various routines on the message. Given a syscall that does nothing, a full round-trip under BSD would require about 40μs, whereas on a user-space Mach system it would take just under 500μs.
When Mach was first being seriously used in the 2.x versions, performance was slower than traditional monolithic operating systems, perhaps as much as 25%.[citation needed] This cost was not considered particularly worrying, however, because the system was also offering multi-processor support and easy portability. Many felt this was an expected and acceptable cost to pay. When Mach 3 attempted to move most of the operating system into user-space, the overhead became higher still: benchmarks between Mach and Ultrix on a MIPS R3000 showed a performance hit as great as 67% on some workloads.
For example, getting the system time involves an IPC call to the user-space server maintaining system clock. The caller first traps into the kernel, causing a context switch and memory mapping. The kernel then checks that the caller has required access rights and that the message is valid. If it does, there is another context switch and memory mapping to complete the call into the user-space server. The process must then be repeated to return the results, adding up to a total of four context switches and memory mappings, plus two message verifications. This overhead rapidly compounds with more complex services, where there are often code paths passing through many servers.
This was not the only source of performance problems. Another centered on the problems of trying to handle memory properly when physical memory ran low and paging had to occur. In the traditional monolithic operating systems the authors had direct experience with which parts of the kernel called which others, allowing them to fine-tune their pager to avoid paging out code that was about to be used. Under Mach this wasn't possible because the kernel had no real idea what the operating system consisted of. Instead they had to use a single one-size-fits-all solution that added to the performance problems. Mach 3 attempted to address this problem by providing a simple pager, relying on user-space pagers for better specialization. But this turned out to have little effect. In practice, any benefits it had were wiped out by the expensive IPC needed to call it in.
Other performance problems were related to Mach's support for multiprocessor systems. From the mid-1980s to the early 1990s, commodity CPUs grew in performance at a rate of about 60% a year, but the speed of memory access grew at only 7% a year. This meant that the cost of accessing memory grew tremendously over this period, and since Mach was based on mapping memory around between programs, any ""cache miss"" made IPC calls slow.
Regardless of the advantages of the Mach approach, these sorts of real-world performance hits were simply not acceptable. As other teams found the same sorts of results, the early Mach enthusiasm quickly disappeared. After a short time a few in the development community seemed to conclude that the entire concept of using IPC as the basis of an operating system was inherently flawed.[citation needed]
IPC overhead is a major issue for Mach 3 systems. However, the concept of a multi-server operating system is still promising, though it still requires some research. The developers have to be careful to isolate code into modules that do not call from server to server. For instance, the majority of the networking code would be placed in a single server, thereby minimizing IPC for normal networking tasks.
Most developers instead stuck with the original POE concept of a single large server providing the operating system functionality. In order to ease development, they allowed the operating system server to run either in user-space or kernel-space. This allowed them to develop in user-space and have all the advantages of the original Mach idea, and then move the debugged server into kernel-space in order to get better performance. Several operating systems have since been constructed using this method, known as co-location, among them Lites, MkLinux, OSF/1, and NeXTSTEP/OPENSTEP/macOS. The Chorus microkernel made this a feature of the basic system, allowing servers to be raised into the kernel space using built-in mechanisms.
Mach 4 attempted to address these problems, this time with a more radical set of upgrades. In particular, it was found that program code was typically not writable, so potential hits due to copy-on-write were rare. Thus it made sense to not map the memory between programs for IPC, but instead migrate the program code being used into the local space of the program. This led to the concept of ""shuttles"" and it seemed performance had improved, but the developers moved on with the system in a semi-usable state. Mach 4 also introduced built-in co-location primitives, making it a part of the kernel itself.
By the mid-1990s, work on microkernel systems was largely stagnant, although the market had generally believed that all modern operating systems would be microkernel based by the 1990s. The primary remaining widespread uses of the Mach kernel are Apple's macOS and its sibling iOS, which run atop a heavily modified hybrid Open Software Foundation Mach Kernel (OSFMK 7.3) called ""XNU"" also used in OSF/1. In XNU, the file systems, networking stacks, and process and memory management functions are implemented in the kernel; and file system, networking, and some process and memory management functions are invoked from user mode via ordinary system calls rather than message passing; XNU's Mach messages are used for communication between user-mode processes, and for some requests from user-mode code to the kernel and from the kernel to user-mode servers.
Further analysis demonstrated that the IPC performance problem was not as obvious as it seemed. Recall that a single-side of a syscall took 20μs under BSD[citation needed] and 114μs on Mach running on the same system.[citation needed] Of the 114, 11 were due to the context switch, identical to BSD.[citation needed] An additional 18 were used by the MMU to map the message between user-space and kernel space.[citation needed] This adds up to only 29μs, longer than a traditional syscall, but not by much.
The rest, the majority of the actual problem, was due to the kernel performing tasks such as checking the message for port access rights.[citation needed] While it would seem this is an important security concern, in fact, it only makes sense in a UNIX-like system. For instance, a single-user operating system running a cell phone or robot might not need any of these features, and this is exactly the sort of system where Mach's pick-and-choose operating system would be most valuable. Likewise Mach caused problems when memory had been moved by the operating system, another task that only really makes sense if the system has more than one address space. DOS and the early Mac OS have a single large address space shared by all programs, so under these systems the mapping does not provide any benefits.
These realizations led to a series of second generation microkernels, which further reduced the complexity of the system and placed almost all functionality in the user space. For instance, the L4 kernel (version 2) includes only seven system calls and uses 12k of memory,[citation needed] whereas Mach 3 includes about 140 functions and uses about 330k of memory.[citation needed] IPC calls under L4 on a 486DX-50 take only 5μs,[citation needed] faster than a UNIX syscall on the same system, and over 20 times as fast as Mach. Of course this ignores the fact that L4 is not handling permissioning or security; but by leaving this to the user-space programs, they can select as much or as little overhead as they require.
The potential performance gains of L4 are tempered by the fact that the user-space applications will often have to provide many of the functions formerly supported by the kernel. In order to test the end-to-end performance, MkLinux in co-located mode was compared with an L4 port running in user-space. L4 added about 5%–10% overhead, compared to Mach's 15%, all the more interesting considering the double context switches needed.[citation needed]
These newer microkernels have revitalized the industry as a whole, and projects such as the GNU Hurd have received new attention as a result.[citation needed]
The following is a list of operating system kernels derived from Mach and operating systems with kernels derived from Mach:"
"83","
XNU is the computer operating system kernel developed at Apple Inc. since December 1996 for use in the macOS operating system and released as free and open-source software as part of the Darwin operating system. It is also used as the kernel for the iOS, tvOS, and watchOS operating systems.  XNU is an abbreviation of X is Not Unix.
Originally developed by NeXT for the NeXTSTEP operating system, XNU was a hybrid kernel combining version 2.5 of the Mach kernel developed at Carnegie Mellon University with components from 4.3BSD and an Objective-C API for writing drivers called Driver Kit.[citation needed]
After Apple acquired NeXT, the Mach component was upgraded to OSFMK 7.3 from OSF, the BSD components were upgraded with code from the FreeBSD project, and the Driver Kit was replaced with a C++ API for writing drivers called I/O Kit.[citation needed]
XNU is a hybrid kernel, containing features of both monolithic kernels and microkernels, attempting to make the best use of both technologies, such as the message passing ability of microkernels enabling greater modularity and larger portions of the OS to benefit from memory protection, and retaining the speed of monolithic kernels for some critical tasks.
As of 2007, XNU runs on ARM,IA-32, and x86-64 processors, both one processor and symmetric multiprocessing (SMP) models. PowerPC support is removed as of version 10 (i.e., Mac OS X 10.6).
The basis of the XNU kernel is a heavily modified (hybrid) OSFMK 7.3 kernel. As such, it is able to run the core of an operating system as separated processes, which allows a great flexibility (it could run several operating systems in parallel above the Mach core), but this often reduces performance because of time consuming kernel/user mode context switches and overhead stemming from mapping or copying messages between the address spaces of the kernel and that of the service daemons. With Mac OS X, the designers have attempted to streamline some tasks and thus BSD functionality was built into the core with Mach. The result is a heavily modified (hybrid) OSFMK 7.3 kernel, Apple licensed OSFMK 7.3, which is a microkernel, from the OSF. (OSFMK 7.3 includes applicable code from the University of Utah Mach 4 kernel and applicable code from the many Mach 3.0 variants that sprouted off from the original Carnegie Mellon University Mach 3.0 microkernel.)
The Berkeley Software Distribution (BSD) part of the kernel provides the POSIX application programming interface (API, BSD system calls), the Unix process model atop Mach tasks, basic security policies, user and group ids, permissions, the networking protocols, the virtual file system code (including a filesystem independent journaling layer), several local file systems such as HFS/HFS+, the Network File System (NFS) client and server, cryptographic framework, UNIX System V inter-process communication (IPC), audit subsystem, mandatory access control, and some of the locking primitives. The BSD code present in XNU came from the FreeBSD kernel. Although much of it has been significantly modified, code sharing still occurs between Apple and the FreeBSD Project.
XNU in Mac OS X Snow Leopard, v10.6, (Darwin version 10) and later comes in two varieties, a 32-bit version called K32 and a 64-bit version called K64. K32 can run 64-bit applications in userland. What was new in Mac OS X 10.6 was the ability to run XNU in 64-bit kernel space. K32 was the default kernel for 10.6 Server when used on all machines except Mac Pro and Xserve models from 2008 onwards and can run 64-bit applications.  K64 has several benefits compared to K32:
Booting while holding down 6 and 4 forces the machine to boot K64 on machines supporting 64-bit kernels. K64 will run 32-bit applications but it will not run 32-bit kernel extensions (KEXTs) so these must be ported to K64 to be able to load.
I/O Kit is the device driver framework, written in a subset of C++ based on Embedded C++. Using its object-oriented design, features common to any class of driver are provided within the framework, helping device drivers be written in less time and code. The I/O Kit is multi-threaded, symmetric multiprocessing (SMP)-safe, and allows for hot-pluggable devices and automatic, dynamic device configuration.
Many drivers can be written to run from user space, which further enhances the stability of the system. If a user-space driver crashes, it will not crash the kernel. However, if a kernel-space driver crashes it will crash the kernel. Examples of kernel-space drivers include disk adapter and network adapter drivers, graphics drivers, drivers for Universal Serial Bus (USB) and FireWire host controllers, and drivers for virtual machine software such as VirtualBox, Parallels Desktop for Mac, and VMware Fusion.
To run safely on multiprocessor machines, access to shared resources (files, data structures etc.) must be serialized so that threads or processes do not attempt to modify the same resource at the same time. Atomic operations, spinlocks, critical sections, mutual exclusions (""mutexes""), and serializing tokens are all possible methods that can be used to prevent concurrent access. Like recent versions of Linux and FreeBSD, XNU, as of Mac OS X 10.4 and Darwin 8.0, employs a fine-grained mutex model to achieve higher performance on multiprocessor systems.[citation needed]"
"84","The architecture of Windows NT, a line of operating systems produced and sold by Microsoft, is a layered design that consists of two main components, user mode and kernel mode.  It is a preemptive, reentrant operating system, which has been designed to work with uniprocessor and symmetrical multi processor (SMP)-based computers. To process input/output (I/O) requests, they use packet-driven I/O, which utilizes I/O request packets (IRPs) and asynchronous I/O. Starting with Windows XP, Microsoft began making 64-bit versions of Windows available; before this, these operating systems only  existed in 32-bit versions.
Programs and subsystems in user mode are limited in terms of to what system resources they have access, while the kernel mode has unrestricted access to the system memory and external devices. The Windows NT kernel is known as a hybrid kernel. The architecture comprises a simple kernel, hardware abstraction layer (HAL), drivers, and a range of services (collectively named Executive), which all exist in kernel mode.
User mode in Windows NT is made of subsystems capable of passing I/O requests to the appropriate kernel mode device drivers by using the I/O manager.  The user mode layer of Windows NT is made up of the ""Environment subsystems,"" which run applications written for many different types of operating systems, and the ""Integral subsystem,"" which operates system specific functions on behalf of environment subsystems.  Kernel mode in Windows NT has full access to the hardware and system resources of the computer.  The kernel mode stops user mode services and applications from accessing critical areas of the operating system that they should not have access to.
The Executive interfaces, with all the user mode subsystems, deals with I/O, object management, security and process management.  The kernel sits between the Hardware Abstraction Layer and the Executive to provide multiprocessor synchronization, thread and interrupt scheduling and dispatching, and trap handling and exception dispatching.  The kernel is also responsible for initializing device drivers at bootup. Kernel mode drivers exist in three levels: highest level drivers, intermediate drivers and low level drivers.  Windows Driver Model (WDM) exists in the intermediate layer and was mainly designed to be binary and source compatible between Windows 98 and Windows 2000.  The lowest level drivers are either legacy Windows NT device drivers that control a device directly or can be a plug and play (PnP) hardware bus.
User mode is made up of various system-defined processes and DLLs. 
The interface between user mode applications and operating system kernel functions is called an ""environment subsystem."" Windows NT can have more than one of these, each implementing a different API set. 
This mechanism was designed to support applications written for many different types of operating systems.  None of the environment subsystems can directly access hardware; access to hardware functions is done by calling into kernel mode routines. [citation needed]
There are four main environment subsystems: the Win32 subsystem, an OS/2 subsystem, the Windows Subsystem for Linux and a POSIX subsystem.
The security subsystem deals with security tokens, grants or denies access to user accounts based on resource permissions, handles login requests and initiates login authentication, and determines which system resources need to be audited by Windows NT.[citation needed] It also looks after Active Directory.[citation needed]  The workstation service implements the network redirector, which is the client side of Windows file and print sharing; it implements local requests to remote files and printers by ""redirecting"" them to the appropriate servers on the network. Conversely, the server service allows other computers on the network to access file shares and shared printers offered by the local system.
Windows NT kernel mode has full access to the hardware and system resources of the computer and runs code in a protected memory area. It controls access to scheduling, thread prioritization, memory management and the interaction with hardware. The kernel mode stops user mode services and applications from accessing critical areas of the operating system that they should not have access to; user mode processes must ask the kernel mode to perform such operations on their behalf.
While the x86 architecture supports four different privilege levels (numbered 0 to 3), only the two extreme privilege levels are used. Usermode programs are run with CPL 3, and the kernel runs with CPL 0. These two levels are often referred to as ""ring 3"" and ""ring 0"", respectively. Such a design decision had been done to achieve code portability to RISC platforms that only support two privilege levels, though this breaks compatibility with OS/2 applications that contain I/O privilege segments that attempt to directly access hardware.
Kernel mode consists of executive services, which is itself made up of many modules that do specific tasks: kernel drivers, a kernel, and a Hardware Abstraction Layer (HAL).
The Windows Executive services make up the low-level kernel-mode portion, and are contained in the file NTOSKRNL.EXE. It deals with I/O, object management, security and process management. These are divided into several subsystems, among which are Cache Manager, Configuration Manager, I/O Manager, Local Procedure Call (LPC), Memory Manager, Object Manager, Process Structure and Security Reference Monitor (SRM). Grouped together, the components can be called Executive services (internal name Ex). System Services (internal name Nt), i.e., system calls, are implemented at this level, too, except very few that call directly into the kernel layer for better performance.[citation needed]
The term ""service"" in this context generally refers to a callable routine, or set of callable routines. This is distinct from the concept of a ""service process"", which is a user mode component somewhat analogous to a daemon in Unix-like operating systems.
The kernel sits between the HAL and the Executive and provides multiprocessor synchronization, thread and interrupt scheduling and dispatching, and trap handling and exception dispatching; it is also responsible for initializing device drivers at bootup that are necessary to get the operating system up and running. That is, the kernel performs almost all the tasks of a traditional microkernel; the strict distinction between Executive and Kernel is the most prominent remnant of the original microkernel design, and historical design documentation consistently refers to the kernel component as ""the microkernel"".
The kernel often interfaces with the process manager. The level of abstraction is such that the kernel never calls into the process manager, only the other way around (save for a handful of corner cases, still never to the point of a functional dependence).
Windows NT uses kernel-mode device drivers to enable it to interact with hardware devices. Each of the drivers has well defined system routines and internal routines that it exports to the rest of the operating system. All devices are seen by user mode code as a file object in the I/O manager, though to the I/O manager itself the devices are seen as device objects, which it defines as either file, device or driver objects. Kernel mode drivers exist in three levels: highest level drivers, intermediate drivers and low level drivers. The highest level drivers, such as file system drivers for FAT and NTFS, rely on intermediate drivers. Intermediate drivers consist of function drivers—or main driver for a device—that are optionally sandwiched between lower and higher level filter drivers. The function driver then relies on a bus driver—or a driver that services a bus controller, adapter, or bridge—which can have an optional bus filter driver that sits between itself and the function driver. Intermediate drivers rely on the lowest level drivers to function. The Windows Driver Model (WDM) exists in the intermediate layer. The lowest level drivers are either legacy Windows NT device drivers that control a device directly or can be a PnP hardware bus. These lower level drivers directly control hardware and do not rely on any other drivers.
The Windows NT hardware abstraction layer, or HAL, is a layer between the physical hardware of the computer and the rest of the operating system. It was designed to hide differences in hardware and provide a consistent platform on which the kernel is run. The HAL includes hardware-specific code that controls I/O interfaces, interrupt controllers and multiple processors.
However, despite its purpose and designated place within the architecture, the HAL isn't a layer that sits entirely below the kernel, the way the kernel sits below the Executive: All known HAL implementations depend in some measure on the kernel, or even the Executive. In practice, this means that kernel and HAL variants come in matching sets that are specifically constructed to work together.
In particular hardware abstraction does not involve abstracting the instruction set, which generally falls under the wider concept of portability. Abstracting the instruction set, when necessary (such as for handling the several revisions to the x86 instruction set, or emulating a missing math coprocessor), is performed by the kernel, or via hardware virtualization."
"85","
The kernel is a computer program that is the core of a computer's operating system, with complete control over everything in the system. On most systems, it is one of the first programs loaded on start-up (after the bootloader). It handles the rest of start-up as well as input/output requests from software, translating them into data-processing instructions for the central processing unit.  It handles memory and peripherals like keyboards, monitors, printers, and speakers.
The critical code of the kernel is usually loaded into a protected area of memory, which prevents it from being overwritten by applications or other, more minor parts of the operating system. The kernel performs its tasks, such as running processes and handling interrupts, in kernel space. In contrast, everything a user does is in user space: writing text in a text editor, running programs in a GUI, etc. This separation prevents user data and kernel data from interfering with each other and causing instability and slowness.
The kernel's interface is a low-level abstraction layer. When a process makes requests of the kernel, it is called a system call. Kernel designs differ in how they manage these system calls and resources. A monolithic kernel runs all the operating system instructions in the same address space for speed. A microkernel runs most processes in user space, for modularity.
The kernel's primary function is to mediate access to the computer's resources, including:
Key aspects necessary in resource management are the definition of an execution domain (address space) and the protection mechanism used to mediate the accesses to the resources within a domain.
Kernels also usually provide methods for synchronization and communication between processes called inter-process communication (IPC).
A kernel may implement these features itself, or rely on some of the processes it runs to provide the facilities to other processes, although in this case it must provide some means of IPC to allow processes to access the facilities provided by each other.
Finally, a kernel must provide running programs with a method to make requests to access these facilities.
The kernel has full access to the system's memory and must allow processes to safely access this memory as they require it. Often the first step in doing this is virtual addressing, usually achieved by paging and/or segmentation. Virtual addressing allows the kernel to make a given physical address appear to be another address, the virtual address. Virtual address spaces may be different for different processes; the memory that one process accesses at a particular (virtual) address may be different memory from what another process accesses at the same address. This allows every program to behave as if it is the only one (apart from the kernel) running and thus prevents applications from crashing each other.
On many systems, a program's virtual address may refer to data which is not currently in memory. The layer of indirection provided by virtual addressing allows the operating system to use other data stores, like a hard drive, to store what would otherwise have to remain in main memory (RAM). As a result, operating systems can allow programs to use more memory than the system has physically available. When a program needs data which is not currently in RAM, the CPU signals to the kernel that this has happened, and the kernel responds by writing the contents of an inactive memory block to disk (if necessary) and replacing it with the data requested by the program. The program can then be resumed from the point where it was stopped. This scheme is generally known as demand paging.
Virtual addressing also allows creation of virtual partitions of memory in two disjointed areas, one being reserved for the kernel (kernel space) and the other for the applications (user space). The applications are not permitted by the processor to address kernel memory, thus preventing an application from damaging the running kernel. This fundamental partition of memory space has contributed much to the current designs of actual general-purpose kernels and is almost universal in such systems, although some research kernels (e.g. Singularity) take other approaches.
To perform useful functions, processes need access to the peripherals connected to the computer, which are controlled by the kernel through device drivers. A device driver is a computer program that enables the operating system to interact with a hardware device. It provides the operating system with information of how to control and communicate with a certain piece of hardware. The driver is an important and vital piece to a program application. The design goal of a driver is abstraction; the function of the driver is to translate the OS-mandated function calls (programming calls) into device-specific calls. In theory, the device should work correctly with the suitable driver. Device drivers are used for such things as video cards, sound cards, printers, scanners, modems, and LAN cards. The common levels of abstraction of device drivers are:
1. On the hardware side:
2. On the software side:
For example, to show the user something on the screen, an application would make a request to the kernel, which would forward the request to its display driver, which is then responsible for actually plotting the character/pixel.
A kernel must maintain a list of available devices. This list may be known in advance (e.g. on an embedded system where the kernel will be rewritten if the available hardware changes), configured by the user (typical on older PCs and on systems that are not designed for personal use) or detected by the operating system at run time (normally called plug and play). In a plug and play system, a device manager first performs a scan on different hardware buses, such as Peripheral Component Interconnect (PCI) or Universal Serial Bus (USB), to detect installed devices, then searches for the appropriate drivers.
As device management is a very OS-specific topic, these drivers are handled differently by each kind of kernel design, but in every case, the kernel has to provide the I/O to allow drivers to physically access their devices through some port or memory location. Very important decisions have to be made when designing the device management system, as in some designs accesses may involve context switches, making the operation very CPU-intensive and easily causing a significant performance overhead.[citation needed]
In computing, a system call is how a process requests a service from an operating system's kernel that it does not normally have permission to run. System calls provide the interface between a process and the operating system. Most operations interacting with the system require permissions not available to a user level process, e.g. I/O performed with a device present on the system, or any form of communication with other processes requires the use of system calls.
A system call is a mechanism that is used by the application program to request a service from the operating system. They use a machine-code instruction that causes the processor to change mode. An example would be from supervisor mode to protected mode. This is where the operating system performs actions like accessing hardware devices or the memory management unit. Generally the operating system provides a library that sits between the operating system and normal programs. Usually it is a C library such as Glibc or Windows API. The library handles the low-level details of passing information to the kernel and switching to supervisor mode. System calls include close, open, read, wait and write.
To actually perform useful work, a process must be able to access the services provided by the kernel. This is implemented differently by each kernel, but most provide a C library or an API, which in turn invokes the related kernel functions.
The method of invoking the kernel function varies from kernel to kernel. If memory isolation is in use, it is impossible for a user process to call the kernel directly, because that would be a violation of the processor's access control rules. A few possibilities are:
An important consideration in the design of a kernel is the support it provides for protection from faults (fault tolerance) and from malicious behaviours (security). These two aspects are usually not clearly distinguished, and the adoption of this distinction in the kernel design leads to the rejection of a hierarchical structure for protection.
The mechanisms or policies provided by the kernel can be classified according to several criteria, including: static (enforced at compile time) or dynamic (enforced at run time); pre-emptive or post-detection; according to the protection principles they satisfy (e.g. Denning); whether they are hardware supported or language based;  whether they are more an open mechanism or a binding policy; and many more.
Support for hierarchical protection domains is typically implemented using CPU modes.
Many kernels provide implementation of ""capabilities"", i.e. objects that are provided to user code which allow limited access to an underlying object managed by the kernel.  A common example occurs in file handling: a file is a representation of information stored on a permanent storage device.  The kernel may be able to perform many different operations (e.g. read, write, delete or execute the file contents) but a user level application may only be permitted to perform some of these operations (e.g. it may only be allowed to read the file).  A common implementation of this is for the kernel to provide an object to the application (typically called a ""file handle"") which the application may then invoke operations on, the validity of which the kernel checks at the time the operation is requested. Such a system may be extended to cover all objects that the kernel manages, and indeed to objects provided by other user applications.
An efficient and simple way to provide hardware support of capabilities is to delegate the MMU the responsibility of checking access-rights for every memory access, a mechanism called capability-based addressing. Most commercial computer architectures lack such MMU support for capabilities.
An alternative approach is to simulate capabilities using commonly supported hierarchical domains; in this approach, each protected object must reside in an address space that the application does not have access to; the kernel also maintains a list of capabilities in such memory.  When an application needs to access an object protected by a capability, it performs a system call and the kernel then checks whether the application's capability grants it permission to perform the requested action, and if it is permitted performs the access for it (either directly, or by delegating the request to another user-level process).  The performance cost of address space switching limits the practicality of this approach in systems with complex interactions between objects, but it is used in current operating systems for objects that are not accessed frequently or which are not expected to perform quickly.
Approaches where protection mechanism are not firmware supported but are instead simulated at higher levels (e.g. simulating capabilities by manipulating page tables on hardware that does not have direct support), are possible, but there are performance implications. Lack of hardware support may not be an issue, however, for systems that choose to use language-based protection.
An important kernel design decision is the choice of the abstraction levels where the security mechanisms and policies should be implemented. Kernel security mechanisms play a critical role in supporting security at higher levels.
One approach is to use firmware and kernel support for fault tolerance (see above), and build the security policy for malicious behavior on top of that (adding features such as cryptography mechanisms where necessary), delegating some responsibility to the compiler. Approaches that delegate enforcement of security policy to the compiler and/or the application level are often called language-based security.
The lack of many critical security mechanisms in current mainstream operating systems impedes the implementation of adequate security policies at the application abstraction level. In fact, a common misconception in computer security is that any security policy can be implemented in an application regardless of kernel support.
Typical computer systems today use hardware-enforced rules about what programs are allowed to access what data. The processor monitors the execution and stops a program that violates a rule (e.g., a user process that is about to read or write to kernel memory, and so on). In systems that lack support for capabilities, processes are isolated from each other by using separate address spaces. Calls from user processes into the kernel are regulated by requiring them to use one of the above-described system call methods.
An alternative approach is to use language-based protection. In a language-based protection system, the kernel will only allow code to execute that has been produced by a trusted language compiler. The language may then be designed such that it is impossible for the programmer to instruct it to do something that will violate a security requirement.
Advantages of this approach include:
Disadvantages include:
Examples of systems with language-based protection include JX and Microsoft's Singularity.
Edsger Dijkstra proved that from a logical point of view, atomic lock and unlock operations operating on binary semaphores are sufficient primitives to express any functionality of process cooperation. However this approach is generally held to be lacking in terms of safety and efficiency, whereas a message passing approach is more flexible.  A number of other approaches (either lower- or higher-level) are available as well, with many modern kernels providing support for systems such as shared memory and remote procedure calls.
The idea of a kernel where I/O devices are handled uniformly with other processes, as parallel co-operating processes, was first proposed and implemented by Brinch Hansen (although similar ideas were suggested in 1967). In Hansen's description of this, the ""common"" processes are called internal processes, while the I/O devices are called external processes.
Similar to physical memory, allowing applications direct access to controller ports and registers can cause the controller to malfunction, or system to crash. With this, depending on the complexity of the device, some devices can get surprisingly complex to program, and use several different controllers. Because of this, providing a more abstract interface to manage the device is important. This interface is normally done by a Device Driver or Hardware Abstraction Layer. Frequently, applications will require access to these devices. The Kernel must maintain the list of these devices by querying the system for them in some way. This can be done through the BIOS, or through one of the various system buses (such as PCI/PCIE, or USB). When an application requests an operation on a device (Such as displaying a character), the kernel needs to send this request to the current active video driver. The video driver, in turn, needs to carry out this request. This is an example of Inter Process Communication (IPC).
Naturally, the above listed tasks and features can be provided in many ways that differ from each other in design and implementation.
The principle of separation of mechanism and policy is the substantial difference between the philosophy of micro and monolithic kernels. Here a mechanism is the support that allows the implementation of many different policies, while a policy is a particular ""mode of operation"".  For instance, a mechanism may provide for user log-in attempts to call an authorization server to determine whether access should be granted; a policy may be for the authorization server to request a password and check it against an encrypted password stored in a database.  Because the mechanism is generic, the policy could more easily be changed (e.g. by requiring the use of a security token) than if the mechanism and policy were integrated in the same module.
In minimal microkernel just some very basic policies are included, and its mechanisms allows what is running on top of the kernel (the remaining part of the operating system and the other applications) to decide which policies to adopt (as memory management, high level process scheduling, file system management, etc.). A monolithic kernel instead tends to include many policies, therefore restricting the rest of the system to rely on them.
Per Brinch Hansen presented arguments in favour of separation of mechanism and policy. The failure to properly fulfill this separation is one of the major causes of the lack of substantial innovation in existing operating systems, a problem common in computer architecture. The monolithic design is induced by the ""kernel mode""/""user mode"" architectural approach to protection (technically called hierarchical protection domains), which is common in conventional commercial systems; in fact, every module needing protection is therefore preferably included into the kernel. This link between monolithic design and ""privileged mode"" can be reconducted to the key issue of mechanism-policy separation; in fact the ""privileged mode"" architectural approach melts together the protection mechanism with the security policies, while the major alternative architectural approach, capability-based addressing, clearly distinguishes between the two, leading naturally to a microkernel design (see Separation of protection and security).
While monolithic kernels execute all of their code in the same address space (kernel space), microkernels try to run most of their services in user space, aiming to improve maintainability and modularity of the codebase. Most kernels do not fit exactly into one of these categories, but are rather found in between these two designs. These are called hybrid kernels. More exotic designs such as nanokernels and exokernels are available, but are seldom used for production systems. The Xen hypervisor, for example, is an exokernel.
In a monolithic kernel, all OS services run along with the main kernel thread, thus also residing in the same memory area. This approach provides rich and powerful hardware access. Some developers, such as UNIX developer Ken Thompson, maintain that it is ""easier to implement a monolithic kernel"" than microkernels. The main disadvantages of monolithic kernels are the dependencies between system components –  a bug in a device driver might crash the entire system –  and the fact that large kernels can become very difficult to maintain.
Monolithic kernels, which have traditionally been used by Unix-like operating systems, contain all the operating system core functions and the device drivers. This is the traditional design of UNIX systems. A monolithic kernel is one single program that contains all of the code necessary to perform every kernel related task. Every part which is to be accessed by most programs which cannot be put in a library is in the kernel space: Device drivers, Scheduler, Memory handling, File systems, Network stacks. Many system calls are provided to applications, to allow them to access all those services. A monolithic kernel, while initially loaded with subsystems that may not be needed, can be tuned to a point where it is as fast as or faster than the one that was specifically designed for the hardware, although more relevant in a general sense. Modern monolithic kernels, such as those of Linux and FreeBSD, both of which fall into the category of Unix-like operating systems, feature the ability to load modules at runtime, thereby allowing easy extension of the kernel's capabilities as required, while helping to minimize the amount of code running in kernel space. In the monolithic kernel, some advantages hinge on these points:
Most work in the monolithic kernel is done via system calls. These are interfaces, usually kept in a tabular structure, that access some subsystem within the kernel such as disk operations. Essentially calls are made within programs and a checked copy of the request is passed through the system call. Hence, not far to travel at all. The monolithic Linux kernel can be made extremely small not only because of its ability to dynamically load modules but also because of its ease of customization. In fact, there are some versions that are small enough to fit together with a large number of utilities and other programs on a
single floppy disk and still provide a fully functional operating system (one of the most popular of which is muLinux). This ability to miniaturize its kernel has also led to a rapid growth in the use of Linux in embedded systems.
These types of kernels consist of the core functions of the operating system and the device drivers with the ability to load modules at runtime. They provide rich and powerful abstractions of the underlying hardware. They provide a small set of simple hardware abstractions and use applications called servers to provide more functionality. This particular approach defines a high-level virtual interface over the hardware, with a set of system calls to implement operating system services such as process management, concurrency and memory management in several modules that run in supervisor mode.
This design has several flaws and limitations:
Microkernel (also abbreviated μK or uK) is the term describing an approach to operating system design by which the functionality of the system is moved out of the traditional ""kernel"", into a set of ""servers"" that communicate through a ""minimal"" kernel, leaving as little as possible in ""system space"" and as much as possible in ""user space"". A microkernel that is designed for a specific platform or device is only ever going to have what it needs to operate. The microkernel approach consists of defining a simple abstraction over the hardware, with a set of primitives or system calls to implement minimal OS services such as memory management, multitasking, and inter-process communication. Other services, including those normally provided by the kernel, such as networking, are implemented in user-space programs, referred to as servers. Microkernels are easier to maintain than monolithic kernels, but the large number of system calls and context switches might slow down the system because they typically generate more overhead than plain function calls.
Only parts which really require being in a privileged mode are in kernel space: IPC (Inter-Process Communication), basic scheduler, or scheduling primitives, basic memory handling, basic I/O primitives. Many critical parts are now running in user space: The complete scheduler, memory handling, file systems, and network stacks. Micro kernels were invented as a reaction to traditional ""monolithic"" kernel design, whereby all system functionality was put in a one static program running in a special ""system"" mode of the processor. In the microkernel, only the most fundamental of tasks are performed such as being able to access some (not necessarily all) of the hardware, manage memory and coordinate message passing between the processes. Some systems that use micro kernels are QNX and the HURD. In the case of QNX and Hurd user sessions can be entire snapshots of the system itself or views as it is referred to. The very essence of the microkernel architecture illustrates some of its advantages:
Most micro kernels use a message passing system of some sort to handle requests from one server to another. The message passing system generally operates on a port basis with the microkernel. As an example, if a request for more memory is sent, a port is opened with the microkernel and the request sent through. Once within the microkernel, the steps are similar to system calls. The rationale was that it would bring modularity in the system architecture, which would entail a cleaner system, easier to debug or dynamically modify, customizable to users' needs, and more performing. They are part of the operating systems like AIX, BeOS, Hurd, Mach, macOS, MINIX, QNX. Etc. Although micro kernels are very small by themselves, in combination with all their required auxiliary code they are, in fact, often larger than monolithic kernels. Advocates of monolithic kernels also point out that the two-tiered structure of microkernel systems, in which most of the operating system does not interact directly with the hardware, creates a not-insignificant cost in terms of system efficiency. These types of kernels normally provide only the minimal services such as defining memory address spaces, Inter-process communication (IPC) and the process management. The other functions such as running the hardware processes are not handled directly by micro kernels. Proponents of micro kernels point out those monolithic kernels have the disadvantage that an error in the kernel can cause the entire system to crash. However, with a microkernel, if a kernel process crashes, it is still possible to prevent a crash of the system as a whole by merely restarting the service that caused the error.
Other services provided by the kernel such as networking are implemented in user-space programs referred to as servers. Servers allow the operating system to be modified by simply starting and stopping programs. For a machine without networking support, for instance, the networking server is not started. The task of moving in and out of the kernel to move data between the various applications and servers creates overhead which is detrimental to the efficiency of micro kernels in comparison with monolithic kernels.
Disadvantages in the microkernel exist however. Some are:
The disadvantages for micro kernels are extremely context based. As an example, they work well for small single purpose (and critical) systems because if not many processes need to run, then the complications of process management are effectively mitigated.
A microkernel allows the implementation of the remaining part of the operating system as a normal application program written in a high-level language, and the use of different operating systems on top of the same unchanged kernel. It is also possible to dynamically switch among operating systems and to have more than one active simultaneously.
As the computer kernel grows, so grows the size and vulnerability of its trusted computing base; and, besides reducing security, there is the problem of enlarging the memory footprint. This is mitigated to some degree by perfecting the virtual memory system, but not all computer architectures have virtual memory support. To reduce the kernel's footprint, extensive editing has to be performed to carefully remove unneeded code, which can be very difficult with non-obvious interdependencies between parts of a kernel with millions of lines of code.
By the early 1990s, due to the various shortcomings of monolithic kernels versus microkernels, monolithic kernels were considered obsolete by virtually all operating system researchers.[citation needed] As a result, the design of Linux as a monolithic kernel rather than a microkernel was the topic of a famous debate between Linus Torvalds and Andrew Tanenbaum. There is merit on both sides of the argument presented in the Tanenbaum–Torvalds debate.
Monolithic kernels are designed to have all of their code in the same address space (kernel space), which some developers argue is necessary to increase the performance of the system. Some developers also maintain that monolithic systems are extremely efficient if well written. The monolithic model tends to be more efficient through the use of shared kernel memory, rather than the slower IPC system of microkernel designs, which is typically based on message passing.[citation needed]
The performance of microkernels was poor in both the 1980s and early 1990s. However, studies that empirically measured the performance of these microkernels did not analyze the reasons of such inefficiency. The explanations of this data were left to ""folklore"", with the assumption that they were due to the increased frequency of switches from ""kernel-mode"" to ""user-mode"", to the increased frequency of inter-process communication and to the increased frequency of context switches.
In fact, as guessed in 1995, the reasons for the poor performance of microkernels might as well have been: (1) an actual inefficiency of the whole microkernel approach, (2) the particular concepts implemented in those microkernels, and (3) the particular implementation of those concepts. Therefore it remained to be studied if the solution to build an efficient microkernel was, unlike previous attempts, to apply the correct construction techniques.
On the other end, the hierarchical protection domains architecture that leads to the design of a monolithic kernel has a significant performance drawback each time there's an interaction between different levels of protection (i.e. when a process has to manipulate a data structure both in 'user mode' and 'supervisor mode'), since this requires message copying by value.
By the mid-1990s, most researchers had abandoned the belief that careful tuning could reduce this overhead dramatically,[citation needed] but recently, newer microkernels, optimized for performance, such as L4 and K42 have addressed these problems.[verification needed]
Hybrid kernels are used in most commercial operating systems such as Microsoft Windows NT 3.1, NT 3.5, NT 3.51, NT 4.0, 2000, XP, Vista, 7, 8, 8.1 and 10. Apple Inc's own macOS uses a hybrid kernel called XNU which is based upon code from OSF/1's Mach kernel (OSFMK 7.3) and FreeBSD's monolithic kernel. They are similar to micro kernels, except they include some additional code in kernel-space to increase performance. These kernels represent a compromise that was implemented by some developers before it was demonstrated that pure micro kernels can provide high performance. These types of kernels are extensions of micro kernels with some properties of monolithic kernels. Unlike monolithic kernels, these types of kernels are unable to load modules at runtime on their own. Hybrid kernels are micro kernels that have some ""non-essential"" code in kernel-space in order for the code to run more quickly than it would were it to be in user-space. Hybrid kernels are a compromise between the monolithic and microkernel designs. This implies running some services (such as the network stack or the filesystem) in kernel space to reduce the performance overhead of a traditional microkernel, but still running kernel code (such as device drivers) as servers in user space.
Many traditionally monolithic kernels are now at least adding (if not actively exploiting) the module capability. The most well known of these kernels is the Linux kernel. The modular kernel essentially can have parts of it that are built into the core kernel binary or binaries that load into memory on demand. It is important to note that a code tainted module has the potential to destabilize a running kernel. Many people become confused on this point when discussing micro kernels. It is possible to write a driver for a microkernel in a completely separate memory space and test it before ""going"" live. When a kernel module is loaded, it accesses the monolithic portion's memory space by adding to it what it needs, therefore, opening the doorway to possible pollution. A few advantages to the modular (or) Hybrid kernel are:
Modules, generally, communicate with the kernel using a module interface of some sort. The interface is generalized (although particular to a given operating system) so it is not always possible to use modules. Often the device drivers may need more flexibility than the module interface affords. Essentially, it is two system calls and often the safety checks that only have to be done once in the monolithic kernel now may be done twice. Some of the disadvantages of the modular approach are:
A nanokernel delegates virtually all services –  including even the most basic ones like interrupt controllers or the timer –  to device drivers to make the kernel memory requirement even smaller than a traditional microkernel.
Exokernels are a still-experimental approach to operating system design. They differ from the other types of kernels in that their functionality is limited to the protection and multiplexing of the raw hardware, providing no hardware abstractions on top of which to develop applications. This separation of hardware protection from hardware management enables application developers to determine how to make the most efficient use of the available hardware for each specific program.
Exokernels in themselves are extremely small. However, they are accompanied by library operating systems (see also unikernel), providing application developers with the functionalities of a conventional operating system. A major advantage of exokernel-based systems is that they can incorporate multiple library operating systems, each exporting a different API, for example one for high level UI development and one for real-time control.
Strictly speaking, an operating system (and thus, a kernel) is not required to run a computer. Programs can be directly loaded and executed on the ""bare metal"" machine, provided that the authors of those programs are willing to work without any hardware abstraction or operating system support. Most early computers operated this way during the 1950s and early 1960s, which were reset and reloaded between the execution of different programs. Eventually, small ancillary programs such as program loaders and debuggers were left in memory between runs, or loaded from ROM. As these were developed, they formed the basis of what became early operating system kernels. The ""bare metal"" approach is still used today on some video game consoles and embedded systems, but in general, newer computers use modern operating systems and kernels.
In 1969, the RC 4000 Multiprogramming System introduced the system design philosophy of a small nucleus ""upon which operating systems for different purposes could be built in an orderly manner"", what would be called the microkernel approach.
In the decade preceding Unix, computers had grown enormously in power –  to the point where computer operators were looking for new ways to get people to use their spare time on their machines. One of the major developments during this era was time-sharing, whereby a number of users would get small slices of computer time, at a rate at which it appeared they were each connected to their own, slower, machine.
The development of time-sharing systems led to a number of problems. One was that users, particularly at universities where the systems were being developed, seemed to want to hack the system to get more CPU time. For this reason, security and access control became a major focus of the Multics project in 1965. Another ongoing issue was properly handling computing resources: users spent most of their time staring at the terminal and thinking about what to input instead of actually using the resources of the computer, and a time-sharing system should give the CPU time to an active user during these periods. Finally, the systems typically offered a memory hierarchy several layers deep, and partitioning this expensive resource led to major developments in virtual memory systems.
The Commodore Amiga was released in 1985, and was among the first –  and certainly most successful –  home computers to feature  an advanced kernel architecture. The AmigaOS kernel's executive component, exec.library, uses a microkernel message-passing design, but there are other kernel components, like graphics.library, that have direct access to the hardware. There is no memory protection, and the kernel is almost always running in user mode. Only special actions are executed in kernel mode, and user-mode applications can ask the operating system to execute their code in kernel mode.
During the design phase of Unix, programmers decided to model every high-level device as a file, because they believed the purpose of computation was data transformation.
For instance, printers were represented as a ""file"" at a known location –  when data was copied to the file, it printed out. Other systems, to provide a similar functionality, tended to virtualize devices at a lower level –  that is, both devices and files would be instances of some lower level concept. Virtualizing the system at the file level allowed users to manipulate the entire system using their existing file management utilities and concepts, dramatically simplifying operation. As an extension of the same paradigm, Unix allows programmers to manipulate files using a series of small programs, using the concept of pipes, which allowed users to complete operations in stages, feeding a file through a chain of single-purpose tools. Although the end result was the same, using smaller programs in this way dramatically increased flexibility as well as ease of development and use, allowing the user to modify their workflow by adding or removing a program from the chain.
In the Unix model, the operating system consists of two parts; first, the huge collection of utility programs that drive most operations, the other the kernel that runs the programs. Under Unix, from a programming standpoint, the distinction between the two is fairly thin; the kernel is a program, running in supervisor mode, that acts as a program loader and supervisor for the small utility programs making up the rest of the system, and to provide locking and I/O services for these programs; beyond that, the kernel didn't intervene at all in user space.
Over the years the computing model changed, and Unix's treatment of everything as a file or byte stream no longer was as universally applicable as it was before. Although a terminal could be treated as a file or a byte stream, which is printed to or read from, the same did not seem to be true for a graphical user interface. Networking posed another problem. Even if network communication can be compared to file access, the low-level packet-oriented architecture dealt with discrete chunks of data and not with whole files. As the capability of computers grew, Unix became increasingly cluttered with code. It is also because the modularity of the Unix kernel is extensively scalable. While kernels might have had 100,000 lines of code in the seventies and eighties, kernels of modern Unix successors like Linux have more than 13 million lines.
Modern Unix-derivatives are generally based on module-loading monolithic kernels. Examples of this are the Linux kernel in its many distributions as well as the Berkeley Software Distribution variant kernels such as FreeBSD, DragonflyBSD, OpenBSD, NetBSD,and macOS. Apart from these alternatives, amateur developers maintain an active operating system development community, populated by self-written hobby kernels which mostly end up sharing many features with Linux, FreeBSD, DragonflyBSD, OpenBSD or NetBSD kernels and/or being compatible with them.
Apple first launched its classic Mac OS in 1984, bundled with its Macintosh personal computer. Apple moved to a nanokernel design in Mac OS 8.6. Against this, the modern macOS (originally named Mac OS X) is based on Darwin, which uses a hybrid kernel called XNU, which was created combining the 4.3BSD kernel and the Mach kernel.
Microsoft Windows was first released in 1985 as an add-on to MS-DOS.  Because of its dependence on another operating system, initial releases of Windows, prior to Windows 95, were considered an operating environment (not to be confused with an operating system).  This product line continued to evolve through the 1980s and 1990s, with the Windows 9x series adding 32-bit addressing and pre-emptive multitasking; but ended with the release of Windows Me in 2000. 
Microsoft also developed Windows NT, an operating system with a very similar interface, but intended for high-end and business users. This line started with the release of Windows NT 3.1 in 1993, and was introduced to general users with the release of Windows XP in October 2001—replacing Windows 9x with a completely different, much more sophisticated operating system. This is the line that continues with Windows 10.
The architecture of Windows NT's kernel is considered a hybrid kernel because the kernel itself contains tasks such as the Window Manager and the IPC Managers, with a client/server layered subsystem model.
Supervisory program or supervisor is a computer program, usually part of an operating system, that controls the execution of other routines and regulates work scheduling, input/output operations, error actions, and similar functions and regulates the flow of work in a data processing system.
Historically, this term was essentially associated with IBM's line of mainframe operating systems starting with OS/360. In other operating systems, the supervisor is generally called the kernel.
In the 1970s, IBM further abstracted the supervisor state from the hardware, resulting in a hypervisor that enabled full virtualization, i.e. the capacity to run multiple operating systems on the same machine totally independently from each other. Hence the first such system was called Virtual Machine or VM.
Although Mach, developed at Carnegie Mellon University from 1985 to 1994, is the best-known general-purpose microkernel, other microkernels have been developed with more specific aims. The L4 microkernel family (mainly the L3 and the L4 kernel) was created to demonstrate that microkernels are not necessarily slow. Newer implementations such as Fiasco and Pistachio are able to run Linux next to other L4 processes in separate address spaces.
Additionally, QNX is a microkernel which is principally used in embedded systems, and the open-source software MINIX, while originally created for educational purposes, is now focussed on being a highly reliable and self-healing microkernel OS."
"86","In computer science, type safety is the extent to which a programming language discourages or prevents type errors. A type error is erroneous or undesirable program behaviour caused by a discrepancy between differing data types for the program's constants, variables, and methods (functions), e.g., treating an integer (int) as a floating-point number (float). Type safety is sometimes alternatively considered to be a property of a computer program rather than the language in which that program is written; that is, some languages have type-safe facilities that can be circumvented by programmers who adopt practices that exhibit poor type safety. The formal type-theoretic definition of type safety is considerably stronger than what is understood by most programmers.
Type enforcement can be static, catching potential errors at compile time, or dynamic, associating type information with values at run-time and consulting them as needed to detect imminent errors, or a combination of both.
The behaviors classified as type errors by a given programming language are usually those that result from attempts to perform operations on values that are not of the appropriate data type.  This classification is partly based on opinion; it may imply that any operation not leading to program crashes, security flaws or other obvious failures is legitimate and need not be considered an error, or it may imply that any contravention of the programmer's explicit intent (as communicated via typing annotations) to be erroneous and not ""type-safe"".
In the context of static (compile-time) type systems, type safety usually involves (among other things) a guarantee that the eventual value of any expression will be a legitimate member of that expression's static type. The precise requirement is more subtle than this — see, for example, subtype and polymorphism for complications.
Type safety is closely linked to memory safety, a restriction on the ability to copy arbitrary bit patterns from one memory location to another.  For instance, in an implementation of a language that has some type t{\displaystyle t}, such that some sequence of bits (of the appropriate length) does not represent a legitimate member of t{\displaystyle t}, if that language allows data to be copied into a variable of type t{\displaystyle t}, then it is not type-safe because such an operation might assign a non-t{\displaystyle t} value to that variable.  Conversely, if the language is type-unsafe to the extent of allowing an arbitrary integer to be used as a pointer, then it is not memory-safe.
Most statically typed languages provide a degree of type safety that is strictly stronger than memory safety, because their type systems enforce the proper use of abstract data types defined by programmers even when this is not strictly necessary for memory safety or for the prevention of any kind of catastrophic failure.
Type-safe code accesses only the memory locations it is authorized to access. (For this discussion, type safety specifically refers to memory type safety and should not be confused with type safety in a broader respect.) For example, type-safe code cannot read values from another object's private fields.
Robin Milner provided the following slogan to describe type safety:
The appropriate formalization of this slogan depends on the style of formal semantics used for a particular language.  In the context of denotational semantics, type safety means that the value of an expression that is well-typed, say with type τ, is a bona fide member of the set corresponding to τ.
In 1994, Andrew Wright and Matthias Felleisen formulated what is now the standard definition and proof technique for type safety in languages defined by operational semantics.  Under this approach, type safety is determined by two properties of the semantics of the programming language:
These properties do not exist in a vacuum; they are linked to the semantics of the programming language they describe, and there is a large space of varied languages that can fit these criteria, since the notion of ""well typed"" program is part of the static semantics of the programming language and the notion of ""getting stuck"" (or ""going wrong"") is a property of its dynamic semantics.
Vijay Saraswat provides the following definition:
Type safety is ultimately aimed at excluding other problems, e.g.:-
Type safety is usually a requirement for any toy language proposed in academic programming language research. Many languages, on the other hand, are too big for human-generated type safety proofs, as they often require checking thousands of cases. Nevertheless, some languages such as Standard ML, which has rigorously defined semantics, have been proved to meet one definition of type safety. Some other languages such as Haskell are believed[discuss] to meet some definition of type safety, provided certain ""escape"" features are not used (for example Haskell's unsafePerformIO, used to ""escape"" from the usual restricted environment in which I/O is possible, circumvents the type system and so can be used to break type safety.) Type punning is another example of such an ""escape"" feature. Regardless of the properties of the language definition, certain errors may occur at run-time due to bugs in the implementation, or in linked libraries written in other languages; such errors could render a given implementation type unsafe in certain circumstances. An early version of Sun's Java virtual machine was vulnerable to this sort of problem.
Programming languages are often colloquially classified as strongly typed or weakly typed (also loosely typed) to refer to certain aspects of type safety. In 1974, Liskov and Zilles defined a strongly-typed language as one in which ""whenever an object is passed from a calling function to a called function, its type must be compatible with the type declared in the called function.""
In 1977, Jackson wrote, ""In a strongly typed language each data area will have a distinct type and each process will state its communication requirements in terms of these types.""
In contrast, a weakly typed language may produce unpredictable results or may perform implicit type conversion.
In object oriented languages type safety is usually intrinsic in the fact that a type system is in place. This is expressed in terms of class definitions.
A class essentially defines the structure of the objects derived from it and an API as a contract for handling these objects.
Each time a new object is created it will comply with that contract.
Each function that exchanges objects derived from a specific class, or implementing a specific interface, will adhere to that contract: hence in that function the operations permitted on that object will be only those defined by the methods of the class the object implements.
This will guarantee that the object integrity will be preserved.
Exception to this are object oriented languages that allow dynamic modification of the object structure, or the use of reflection to modify the content of an object to overcome the constraints imposed by the class methods definitions.
Ada was designed to be suitable for embedded systems, device drivers and other forms of system programming, but also to encourage type-safe programming.  To resolve these conflicting goals, Ada confines type-unsafety to a certain set of special constructs whose names usually begin with the string Unchecked_. Unchecked_Deallocation can be effectively banned from a unit of Ada text by applying pragma Pure to this  unit. It is expected that programmers will use Unchecked_ constructs very carefully and only when necessary; programs that do not use them are type-safe.
The SPARK programming language is a subset of Ada eliminating all its potential ambiguities and insecurities while at the same time adding statically checked contracts to the language features available. SPARK avoids the issues with dangling pointers by disallowing allocation at run time entirely.
Ada2012 adds statically checked contracts to the language itself (in form of pre-, and post-conditions, as well as type invariants).
The C programming language is typesafe in limited contexts; for example, a compile-time error is generated when an attempt is made to convert a pointer to one type of structure to a pointer to another type of structure, unless an explicit cast is used.  However, a number of very common operations are non-typesafe; for example, the usual way to print an integer is something like printf(""%d"", 12), where the %d tells printf at run-time to expect an integer argument. (Something like printf(""%s"", 12), which erroneously tells the function to expect a pointer to a character-string, may be accepted by compilers, but will produce undefined results.) This is partially mitigated by some compilers (such as gcc) checking type correspondences between printf arguments and format strings.
In addition, C, like Ada, provides unspecified or undefined explicit conversions; and unlike in Ada, idioms that use these conversions are very common, and have helped to give C a type-unsafe reputation. For example, the standard way to allocate memory on the heap is to invoke a memory allocation function, such as malloc, with an argument indicating how many bytes are required. The function returns an untyped pointer (type void *), which the calling code must explicitly or implicitly cast to the appropriate pointer type. Pre-standardized implementations of C required an explicit cast to do so, therefore the code (struct foo *) malloc(sizeof(struct foo)) became the accepted practice. However, this practice is discouraged in ISO C as it can mask a failure to include the header file in which malloc is defined, resulting in downstream errors on machines where the int and pointer types are of different sizes, such as most common implementations of C for the now-ubiquitous x86 64 architecture. A conflict arises in code that is required to compile as C++, since the cast is necessary in that language.
Some features of C++ that promote more type-safe code:
C# is type-safe (but not statically type-safe). It has support for untyped pointers, but this must be accessed using the ""unsafe"" keyword which can be prohibited at the compiler level. It has inherent support for run-time cast validation. Casts can be validated by using the ""as"" keyword that will return a null reference if the cast is invalid, or by using a C-style cast that will throw an exception if the cast is invalid. See C Sharp conversion operators.
Undue reliance on the object type (from which all other types are derived) runs the risk of defeating the purpose of the C# type system. It is usually better practice to abandon object references in favour of generics, similar to templates in C++ and generics in Java.
The Java language is designed to enforce type safety. 
Anything in Java happens inside an object
and each object is an instance of a class.
To implement the type safety enforcement, each object, before usage, needs to be allocated.
Java allows usage of primitive types but only inside properly allocated objects.
Sometimes a part of the type safety is implemented indirectly: e.g. the class BigDecimal represents a floating point number of arbitrary precision, but handles only numbers that can be expressed with a finite representation.
The operation BigDecimal.divide() calculates a new object as the division of two numbers expressed as BigDecimal.
In this case if the division has no finite representation, as when one computes e.g. 1/3=0.33333..., the divide() method can raise an exception if no rounding mode is defined for the operation.
Hence the library, rather than the language, guarantees that the object respects the contract implicit in the class definition.
SML has rigorously defined semantics and is known to be type-safe.  However, some implementations of SML, including Standard ML of New Jersey (SML/NJ), its syntactic variant Mythryl and Mlton, provide libraries that offer certain unsafe operations.  These facilities are often used in conjunction with those implementations' foreign function interfaces to interact with non-ML code (such as C libraries) that may require data laid out in specific ways.  Another example is the SML/NJ interactive toplevel itself, which must use unsafe operations to execute ML code entered by the user.
Modula-2 is a strongly typed language with a design philosophy to require any unsafe facilities to be explicitly marked as unsafe. This is achieved by ""moving"" such facilities into a built-in pseudo-library called SYSTEM from where they must be imported before they can be used. The import thus makes it visible when such facilities are used. Unfortunately, this was not consequently implemented in the original language report and its implementation. There still remained unsafe facilities such as the type cast syntax and variant records (inherited from Pascal) that could be used without prior import. The difficulty in moving these facilities into the SYSTEM pseudo-module was the lack of any identifier for the facility that could then be imported since only identifiers can be imported, but not syntax.
The ISO Modula-2 standard corrected this for the type cast facility by changing the type cast syntax into a function called CAST which has to be imported from pseudo-module SYSTEM. However, other unsafe facilities such as variant records remained available without any import from pseudo-module SYSTEM.
A recent revision of the language applied the original design philosophy rigorously. First, pseudo-module SYSTEM was renamed to UNSAFE to make the unsafe nature of facilities imported from there more explicit. Then all remaining unsafe facilities where either removed altogether (for example variant records) or moved to pseudo-module UNSAFE. For facilities where there is no identifier that could be imported, enabling identifiers were introduced. In order to enable such a facility, its corresponding enabling identifier must be imported from pseudo-module UNSAFE. No unsafe facilities remain in the language that do not require import from UNSAFE.
Pascal has had a number of type safety requirements, some of which are kept in some compilers.  Where a Pascal compiler dictates ""strict typing"", two variables cannot be assigned to each other unless they are either compatible (such as conversion of integer to real) or assigned to the identical subtype.  For example, if you have the following code fragment:
Under strict typing, a variable defined as TwoTypes is not compatible with DualTypes (because they are not identical, even though the components of that user defined type are identical) and an assignment of  T1 := D2;  is illegal.  An assignment of T1 := T2;  would be legal because the subtypes they are defined to are identical.  However, an assignment such as T1.Q := D1.Q; would be legal.
In general, Common Lisp is a type-safe language. A Common Lisp compiler is responsible for inserting dynamic checks for operations whose type safety cannot be proven statically. However, a programmer may indicate that a program should be compiled with a lower level of dynamic type-checking. A program compiled in such a mode cannot be considered type-safe.
The following examples illustrates how C++ cast operators can break type safety when used incorrectly. The first example shows how basic data types can be incorrectly casted:
In this example, reinterpret_cast explicitly prevents the compiler from performing a safe conversion from integer to floating-point value. When the program runs it will output a garbage floating-point value. The problem could have been avoided by instead writing float fval = ival;
The next example shows how object references can be incorrectly downcasted:
The two child classes have members of different types. When downcasting a parent class pointer to a child class pointer, then the resulting pointer may not point to a valid object of correct type. In the example, this leads to garbage value being printed. The problem could have been avoided by replacing static_cast with dynamic_cast that throws an exception on invalid casts."
"87","In computer programming, a variable or scalar is a storage location (identified by a memory address) paired with an associated symbolic name (an identifier), which contains some known or unknown quantity of information referred to as a value. The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may thus change during the course of program execution.
Variables in programming may not directly correspond to the concept of variables in mathematics. The latter is abstract, having no reference to a physical object such as storage location. The value of a computing variable is not necessarily part of an equation or formula as in mathematics. Variables in computer programming are frequently given long names to make them relatively descriptive of their use, whereas variables in mathematics often have terse, one- or two-character names for brevity in transcription and manipulation.
A variable's storage location may be referred by several different identifiers, a situation known as aliasing. Assigning a value to the variable using one of the identifiers will change the value that can be accessed through the other identifiers.
Compilers have to replace variables' symbolic names with the actual locations of the data. While a variable's name, type, and location often remain fixed, the data stored in the location may be changed during program execution.
In imperative programming languages, values can generally be accessed or changed at any time. In pure functional and logic languages, variables are bound to expressions and keep a single value during their entire lifetime due to the requirements of referential transparency. In imperative languages, the same behavior is exhibited by (named) constants (symbolic constants), which are typically contrasted with (normal) variables.
Depending on the type system of a programming language, variables may only be able to store a specified datatype (e.g. integer or string). Alternatively, a datatype may be associated only with the current value, allowing a single variable to store anything supported by the programming language.
An identifier referencing a variable can be used to access the variable in order to read out the value, or alter the value, or edit other attributes of the variable, such as access permission, locks, semaphores, etc.
For instance, a variable might be referenced by the identifier ""total_count"" and the variable can contain the number 1956.  If the same variable is referenced by the identifier ""x"" as well, and if using this identifier ""x"", the value of the variable is altered to 2009, then reading the value using the identifier ""total_count"" will yield a result of 2009 and not 1956.
If a variable is only referenced by a single identifier that can simply be called the name of the variable. Otherwise, we can speak of one of the names of the variable.  For instance, in the previous example, the ""total_count"" is a name of the variable in question, and ""x"" is another name of the same variable.
The scope of a variable describes where in a program's text the variable may be used, while the extent (or lifetime) describes when in a program's execution a variable has a (meaningful) value. The scope of a variable is actually a property of the name of the variable, and the extent is a property of the variable itself. These should not be confused with context (also called environment), which is a property of the program, and varies by point in the source code or execution – see scope: overview. Further, object lifetime may coincide with variable lifetime, but in many cases is not tied to variable lifetime.
A variable name's scope affects its extent.
Scope is an important part of the name resolution of a variable. Most languages define a specific scope for each variable (as well as any other named entity), which may differ within a given program. The scope of a variable is the portion of the program code for which the variable's name has meaning and for which the variable is said to be ""visible"". Entrance into that scope typically begins a variable's lifetime (as it comes into context) and exit from that scope typically ends its lifetime (as it goes out of context). For instance, a variable with ""lexical scope"" is meaningful only within a certain function/subroutine, or more finely within a block of expressions/statements (accordingly with function scope or block scope); this is static resolution, performable at parse-time or compile-time. Alternatively, a variable with dynamic scope is resolved at run-time, based on a global binding stack that depends on the specific control flow. Variables only accessible within a certain functions are termed ""local variables"". A ""global variable"", or one with indefinite scope, may be referred to anywhere in the program.
Extent, on the other hand, is a runtime (dynamic) aspect of a variable. Each binding of a variable to a value can have its own extent at runtime. The extent of the binding is the portion of the program's execution time during which the variable continues to refer to the same value or memory location. A running program may enter and leave a given extent many times, as in the case of a closure.
Unless the programming language features garbage collection, a variable whose extent permanently outlasts its scope can result in a memory leak, whereby the memory allocated for the variable can never be freed since the variable which would be used to reference it for deallocation purposes is no longer accessible. However, it can be permissible for a variable binding to extend beyond its scope, as occurs in Lisp closures and C static local variables; when execution passes back into the variable's scope, the variable may once again be used. A variable whose scope begins before its extent does is said to be uninitialized and often has an undefined, arbitrary value if accessed (see wild pointer), since it has yet to be explicitly given a particular value. A variable whose extent ends before its scope may become a dangling pointer and deemed uninitialized once more since its value has been destroyed. Variables described by the previous two cases may be said to be out of extent or unbound. In many languages, it is an error to try to use the value of a variable when it is out of extent. In other languages, doing so may yield unpredictable results. Such a variable may, however, be assigned a new value, which gives it a new extent.
For space efficiency, a memory space needed for a variable may be allocated only when the variable is first used and freed when it is no longer needed. A variable is only needed when it is in scope, thus beginning each variable's lifetime when it enters scope may give space to unused variables. To avoid wasting such space, compilers often warn programmers if a variable is declared but not used.
It is considered good programming practice to make the scope of variables as narrow as feasible so that different parts of a program do not accidentally interact with each other by modifying each other's variables. Doing so also prevents action at a distance.  Common techniques for doing so are to have different sections of a program use different name spaces, or to make individual variables ""private"" through either dynamic variable scoping or lexical variable scoping.
Many programming languages employ a reserved value (often named null or nil) to indicate an invalid or uninitialized variable.
In statically typed languages such as Java or ML, a variable also has a type, meaning that only certain kinds of values can be stored in it. For example, a variable of type ""integer"" is prohibited from storing text values.
In dynamically typed languages such as Python, it is values, not variables, which carry type. In Common Lisp, both situations exist simultaneously: A variable is given a type (if undeclared, it is assumed to be T, the universal supertype) which exists at compile time. Values also have types, which can be checked and queried at runtime.
Typing of variables also allows polymorphisms to be resolved at compile time. However, this is different from the polymorphism used in object-oriented function calls (referred to as virtual functions in C++) which resolves the call based on the value type as opposed to the supertypes the variable is allowed to have.
Variables often store simple data, like integers and literal strings, but some programming languages allow a variable to store values of other datatypes as well. Such languages may also enable functions to be parametric polymorphic. These functions operate like variables to represent data of multiple types. For example, a function named length may determine the length of a list. Such a length function may be parametric polymorphic by including a type variable in its type signature, since the amount of elements in the list is independent of the elements' types.
The formal parameters (or formal arguments) of functions are also referred to as variables. For instance, in this Python code segment,
the variable named x is a parameter because it is given a value when the function is called.  The integer 5 is the argument which gives x its value. In most languages, function parameters have local scope. This specific variable named x can only be referred to within the addtwo function (though of course other functions can also have variables called x).
The specifics of variable allocation and the representation of their values vary widely, both among programming languages and among implementations of a given language. Many language implementations allocate space for local variables, whose extent lasts for a single function call on the call stack, and whose memory is automatically reclaimed when the function returns. More generally, in name binding, the name of a variable is bound to the address of some particular block (contiguous sequence) of bytes in memory, and operations on the variable manipulate that block. Referencing is more common for variables whose values have large or unknown sizes when the code is compiled. Such variables reference the location of the value instead of storing the value itself, which is allocated from a pool of memory called the heap.
Bound variables have values. A value, however, is an abstraction, an idea; in implementation, a value is represented by some data object, which is stored somewhere in computer memory. The program, or the runtime environment, must set aside memory for each data object and, since memory is finite, ensure that this memory is yielded for reuse when the object is no longer needed to represent some variable's value.
Objects allocated from the heap must be reclaimed—especially when the objects are no longer needed. In a garbage-collected language (such as C#, Java, Python, Golang and Lisp), the runtime environment automatically reclaims objects when extant variables can no longer refer to them. In non-garbage-collected languages, such as C, the program (and the programmer) must explicitly allocate memory, and then later free it, to reclaim its memory. Failure to do so leads to memory leaks, in which the heap is depleted as the program runs, risks eventual failure from exhausting available memory.
When a variable refers to a data structure created dynamically, some of its components may be only indirectly accessed through the variable. In such circumstances, garbage collectors (or analogous program features in languages that lack garbage collectors) must deal with a case where only a portion of the memory reachable from the variable needs to be reclaimed.
Unlike their mathematical counterparts, programming variables and constants commonly take multiple-character names, e.g. COST or total. Single-character names are most commonly used only for auxiliary variables; for instance, i, j, k for array index variables.
Some naming conventions are enforced at the language level as part of the language syntax and involve the format of valid identifiers. In almost all languages, variable names cannot start with a digit (0–9) and cannot contain whitespace characters. Whether, which, and when punctuation marks are permitted in variable names varies from language to language; many languages only permit the underscore (""_"") in variable names and forbid all other punctuation. In some programming languages, specific (often punctuation) characters (known as sigils) are prefixed or appended to variable identifiers to indicate the variable's type.
Case-sensitivity of variable names also varies between languages and some languages require the use of a certain case in naming certain entities;[note 1]  Most modern languages are case-sensitive; some older languages are not. Some languages reserve certain forms of variable names for their own internal use; in many languages, names beginning with two underscores (""__"") often fall under this category.
However, beyond the basic restrictions imposed by a language, the naming of variables is largely a matter of style. At the machine code level, variable names are not used, so the exact names chosen do not matter to the computer. Thus names of variables identify them, for the rest they are just a tool for programmers to make programs easier to write and understand. Using poorly chosen variable names can make code more difficult to review than non-descriptive names, so names which are clear are often encouraged.
Programmers often create and adhere to code style guidelines which offer guidance on naming variables or impose a precise naming scheme. Shorter names are faster to type but are less descriptive; longer names often make programs easier to read and the purpose of variables easier to understand. However, extreme verbosity in variable names can also lead to less comprehensible code."
"88","In knowledge representation, object-oriented programming and design (see object-oriented program architecture), is-a (is_a or is a) is a subsumption relationship between abstractions (e.g. types, classes), wherein one class A is a subclass  of another class B (and so B is a superclass  of A).
In other words, type A is a subtype of type B when A’s specification implies B’s specification. That is, any object (or class) that satisfies A’s specification also satisfies B’s specification, because B’s specification is weaker.
The is-a relationship is to be contrasted with the has-a (has_a or has a) relationship between types (classes); confusing the relations has-a and is-a is a common error when designing a model (e.g., a computer program) of the real-world relationship between an object and its subordinate. The is-a relationship may also be contrasted with the instance-of relationship between objects (instances) and types (classes): see ""type-token distinction"" and ""type-token relations.""
To summarize the relations, we have
Subtyping enables a given type to be substituted for another type or abstraction. Subtyping is said to establish an is-a relationship between the subtype and some existing abstraction, either implicitly or explicitly, depending on language support. The relationship can be expressed explicitly via inheritance in languages that support inheritance as a subtyping mechanism.
The following C++ code establishes an explicit inheritance relationship between classes B and A, where B is both a subclass and a subtype of A, and can be used as an A wherever a B is specified (via a reference, a pointer or the object itself).

The following python code establishes an explicit inheritance relationship between classes B and A, where B is both a subclass and a subtype of A, and can be used as an A wherever a B is required.
The following example, type(a) is a ""regular"" type, and type(type(a)) is a metatype. While as distributed all types have the same metatype (PyType_Type, which is also its own metatype), this is not a requirement. The type of classic classes, known as types.ClassType, can also be considered a distinct metatype.
In Java, is-a relation between the type parameters of one class or interface and the type parameters of another are determined by the extends and implements clauses.
Using the Collections classes, ArrayList<E> implements List<E>, and List<E> extends Collection<E>. So ArrayList<String> is a subtype of List<String>, which is a subtype of Collection<String>. The subtyping relationship is preserved between the types automatically. When we define an interface, PayloadList, that associates an optional value of generic type P with each element. Its declaration might look like:
The following parameterizations of PayloadList are subtypes of List<String>:
Liskov substitution principle explains a property, ""If for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T,"". Following example shows a violation of LSP.
Obviously, the DrawShape function is badly formatted. It has to know about every derivative classes of Shape class. Also, it should be changed whenever new subclass of Shape are created. In object-oriented design, many[who?] view the structure of this as anathema.
Here is a more subtle example of violation of LSP:
This works well but when it comes to Square class, which inherits Rectangle class, it violates LSP even though the is-a relationship holds between Rectangle and Square. Because square is rectangular. The following example overrides two functions, Setwidth and SetHeight, to fix the problem. But fixing the code implies that the design is faulty.
The following example, function g just works for Rectangle class but not for Square, and so the open-closed principle has been violated.
"
"89","Substitutability is a principle in object-oriented programming stating that, in a computer program, if S is a subtype of T, then objects of type T may be replaced with objects of type S (i.e. an object of type T may be substituted with any object of a subtype S) without altering any of the desirable properties of T (correctness, task performed, etc.).  More formally, the Liskov substitution principle (LSP) is a particular definition of a subtyping relation, called (strong) behavioral subtyping, that was initially introduced by Barbara Liskov in a 1987 conference keynote address titled Data abstraction and hierarchy. It is a semantic rather than merely syntactic relation because it intends to guarantee semantic interoperability of types in a hierarchy, object types in particular. Barbara Liskov and Jeannette Wing formulated the principle succinctly in a 1994 paper as follows:
In the same paper, Liskov and Wing detailed their notion of behavioral subtyping in an extension of Hoare logic, which bears a certain resemblance to Bertrand Meyer's design by contract in that it considers the interaction of subtyping with preconditions, postconditions and invariants.
Liskov's notion of a behavioral subtype defines a notion of substitutability for objects; that is, if S is a subtype of T, then objects of type T in a program may be replaced with objects of type S without altering any of the desirable properties of that program (e.g. correctness).
Behavioral subtyping is a stronger notion than typical subtyping of functions defined in type theory, which relies only on the contravariance of argument types and covariance of the return type. Behavioral subtyping is provably undecidable in general: if q is the property ""method for x always terminates"", then it is impossible for a program (e.g. a compiler) to verify that it holds true for some subtype S of T, even if q does hold for T. Nonetheless, the principle is useful in reasoning about the design of class hierarchies.
Liskov's principle imposes some standard requirements on signatures that have been adopted in newer object-oriented programming languages (usually at the level of classes rather than types; see nominal vs. structural subtyping for the distinction):
In addition to the signature requirements, the subtype must meet a number of behavioral conditions. These are detailed in a terminology resembling that of design by contract methodology, leading to some restrictions on how contracts can interact with inheritance:
The rules on pre- and postconditions are identical to those introduced by Bertrand Meyer in his 1988 book Object-Oriented Software Construction. Both Meyer, and later Pierre America, who was the first to use the term behavioral subtyping, gave proof-theoretic definitions of some behavioral subtyping notions, but their definitions did not take into account aliasing that may occur in programming languages that support references or pointers. Taking aliasing into account was the major improvement made by Liskov and Wing (1994), and a key ingredient is the history constraint. Under the definitions of Meyer and America, a MutablePoint would be a behavioral subtype of ImmutablePoint, whereas LSP forbids this.
General references
Specific references"
"90","A structural type system (or property-based type system) is a major class of type system, in which type compatibility and equivalence are determined by the type's actual structure or definition, and not by other characteristics such as its name or place of declaration. Structural systems are used to determine if types are equivalent and whether a type is a subtype of another. It contrasts with nominative systems, where comparisons are based on the names of the types or explicit declarations, and duck typing, in which only the part of the structure accessed at runtime is checked for compatibility.
In structural typing, an element is considered to be compatible with another if, for each feature within the second element's type, a corresponding and identical feature exists in the first element's type.  Some languages may differ on the details, such as whether the features must match in name. This definition is not symmetric, and includes subtype compatibility. Two types are considered to be identical if each is compatible with the other.
For example, OCaml uses structural typing on methods for compatibility of object types. Go uses structural typing on methods to determine compatibility of a type with an interface. C++ template functions exhibit structural typing on type arguments. Haxe uses structural typing, but classes are not structurally subtyped.
In languages which support subtype polymorphism, a similar dichotomy can be formed based on how the subtype relationship is defined.  One type is a subtype of another if and only if it contains all the features of the base type, or subtypes thereof. The subtype may contain added features, such as members not present in the base type, or stronger invariants.
A distinction exists between structural substitution for inferred and non-inferred polymorphism. Some languages, such as Haskell, do not substitute structurally in the case where an expected type is declared (i.e., not inferred), e.g., only substitute for functions that are signature-based polymorphic via type inference. Then it is not possible to accidentally subtype a non-inferred type, although it may still be possible to provide an explicit conversion to a non-inferred type, which is invoked implicitly.
Structural subtyping is arguably more flexible than nominative subtyping, as it permits the creation of ad hoc types and protocols; in particular, it permits creation of a type which is a supertype of an existing type, without modifying the definition of the latter. However, this may not be desirable where the programmer wishes to create closed abstractions.
A pitfall of structural typing versus nominative typing is that two separately defined types intended for different purposes, but accidentally holding the same properties (e.g. both composed of a pair of integers), could be considered the same type by the type system, simply because they happen to have identical structure. One way this can be avoided is by creating one algebraic data type for each use.
In 1990, Cook, et al., proved that inheritance is not subtyping in structurally-typed OO languages.
Objects in OCaml are structurally typed by the names and types of their methods.
Objects can be created directly (immediate objects) without going through a nominative class. Classes only serve as functions for creating objects.
Here the OCaml interactive runtime prints out the inferred type of the object for convenience. Its type (< get_x : int; set_x : int -> unit >) is defined only by its methods. In other words, the type of x is defined by the method types ""get_x : int"" and ""set_x : int -> unit"" rather than by any name.
To define another object, which has the same methods and types of methods:
OCaml considers them the same type. For example, the equality operator is typed to only take two values of the same type:
So they must be the same type, or else this wouldn't even type-check. This shows that equivalence of types is structural.
One can define a function that invokes a method:
The inferred type for the first argument (< set_x : int -> 'a; .. >) is interesting. The .. means that the first argument can be any object which has a ""set_x"" method, which takes an int as argument.
So it can be used on object x:
Another object can be made that happens to have that method and method type; the other methods are irrelevant:
The ""set_to_10"" function also works on it:
This shows that compatibility for things like method invocation is determined by structure.
Let us define a type synonym for objects with only a ""get_x"" method and no other methods:
The object x is not of this type; but structurally, x is of a subtype of this type, since x contains a superset of its methods. So x can be coerced to this type:
But not object z, because it is not a structural subtype:
This shows that compatibility for widening coercions are structural."
"91","In programming, operator overloading, sometimes termed operator ad hoc polymorphism, is a specific case of polymorphism, where different operators have different implementations depending on their arguments. Operator overloading is generally defined by a programming language, a programmer, or both.
Operator overloading is syntactic sugar, and is used because it allows programming using notation nearer to the target domain and allows user-defined types a similar level of syntactic support as types built into a language. It is common, for example, in scientific computing, where it allows computing representations of mathematical objects to be manipulated with the same syntax as on paper.
Operator overloading does not change the expressive power of a language (with functions), as it can be emulated using function calls. For example, consider variables a, b, c of some user-defined type, such as matrices:
a + b * c
In a language that supports operator overloading, and with the usual assumption that the '*' operator has higher precedence than '+' operator, this is a concise way of writing:
add (a, multiply (b,c))
However, the former syntax reflects common mathematical usage.
In this case, the addition operator is overloaded to allow addition on a user-defined type ""Time"" (in C++):
Addition is a binary operation, which means it has two operands. In C++, the arguments being passed are the operands, and the temp object is the returned value.
The operation could also be defined as a class method, replacing lhs by the hidden this argument; however this forces the left operand to be of type Time:
Note that a unary operator defined as a class method would receive no apparent argument (it only works from this):
Less than(<) operator is often overloaded to sort a structure or class.
In the last example, operator overloading is done within the class which is the same as the previous examples. In C++, after overloading the less-than operator (<), standard sorting functions can be used to sort some classes.
Operator overloading has often been criticized because it allows programmers to reassign the semantics of operators depending on the types of their operands. For example, the use of the << in C++'s:
shifts the bits in the variable a left by 1 bit if a is of an integer type, but if a is an output stream then the above code will attempt to write a ""1"" to the stream. Because operator overloading allows the original programmer to change the usual semantics of an operator and to catch any subsequent programmers by surprise, it is considered good practice to use operator overloading with care (the creators of Java decided not to use this feature, although not necessarily for this reason).
Another, more subtle, issue with operators is that certain rules from mathematics can be wrongly expected or unintentionally assumed. For example, the commutativity of + (i.e. that a + b == b + a) does not always apply; an example of this occurs when the operands are strings, since + is commonly overloaded to perform a concatenation of strings (i.e. ""bird"" + ""song"" yields ""birdsong"", while ""song"" + ""bird"" yields ""songbird""). A typical counter[citation needed] to this argument comes directly from mathematics: While + is commutative on integers (and more generally any complex numbers), it is not commutative for other ""types"" of variable. In practice, + is not associative even with floating-point values, due to rounding errors. Another example: In mathematics, multiplication is commutative for real and complex numbers but not commutative in matrix multiplication.
A classification of some common programming languages is made according to whether their operators are overloadable by the programmer and whether the operators are limited to a predefined set.
The ALGOL 68 specification allowed operator overloading.
Extract from the ALGOL 68 language specification (page 177) where the overloaded operators ¬, =, ≠, and abs are defined:
Note that no special declaration is needed to overload an operator, and the programmer is free to create new operators.
Ada supports overloading of operators from its inception, with the publication of the Ada 83 language standard. However, the language designers chose to preclude the definition of new operators. Only extant operators in the language may be overloaded, by defining new functions with identifiers such as ""+"", ""*"", ""and"" etc. Subsequent revisions of the language (in 1995 and 2005) maintain the restriction to overloading of extant operators.
In C++, operator overloading is more refined than in ALGOL 68.
Java language designers at Sun Microsystems chose to omit overloading.
Ruby allows operator overloading as syntactic sugar for simple method calls.
Lua allows operator overloading as syntactic sugar for method calls with the added feature that if the first operand doesn't define that operator, the method for the second operand will be used.
Microsoft includes operator overloading for C# in 2001.
Scala treats all operators as methods and thus allows operator overloading by proxy.
In Perl 6, the definition of all operators is delegated to lexical functions, and so, using function definitions, operators can be overloaded or new operators added. For example, the function defined in the Rakudo source for incrementing a Date object with ""+"" is:
Since ""multi"" was used, the function gets added to the list of multidispatch candidates, and ""+"" is only overloaded for the case where the type constraints in the function signature are met.
While the capacity for overloading includes +, *, >=, the postfix and term i, and so on, it also allows for overloading various brace operators: ""[x, y]"", ""x[ y ]"", ""x{ y }"", and ""x( y )"".
"
"92","In computer science, dynamic dispatch is the process of selecting which implementation of a polymorphic operation (method or function) to call at run time. It is commonly employed in, and considered a prime characteristic of, object-oriented programming (OOP) languages and systems.
Object-oriented systems model a problem as a set of interacting objects that enact operations referred to by name. Polymorphism is the phenomenon wherein somewhat interchangeable objects each expose an operation of the same name but possibly differing in behavior. As an example, a File object and a Database object both have a StoreRecord method that can be used to write a personnel record to storage. Their implementations differ. A program holds a reference to an object which may be either a File object or a Database object. Which it is may have been determined by a run-time setting, and at this stage, the program may not know or care which. When the program calls StoreRecord on the object, something needs to decide which behavior gets enacted. If one thinks of OOP as sending messages to objects, then in this example the program sends a StoreRecord message to an object of unknown type, leaving it to the run-time support system to dispatch the message to the right object. The object enacts whichever behavior it implements.
Dynamic dispatch contrasts with static dispatch, in which the implementation of a polymorphic operation is selected at compile-time. The purpose of dynamic dispatch is to support cases where the appropriate implementation of a polymorphic operation cannot be determined at compile time because it depends on the runtime type of one or more actual parameters to the operation.
Dynamic dispatch is different from late binding (also known as dynamic binding). In the context of selecting an operation, binding associates a name to an operation. Dispatching chooses an implementation for the operation after you have decided which operation a name refers to. With dynamic dispatch, the name may be bound to a polymorphic operation at compile time, but the implementation not be chosen until run time. While dynamic dispatch does not imply late binding, late binding does imply dynamic dispatching since the binding is what determines the set of available dispatches.
The decision of which version of a method to call may be based either on a single object, or on a combination of objects. The former is called single dispatch and is directly supported by common object-oriented languages such as C++, Java, Smalltalk, Objective-C, Swift, JavaScript, and Python. In these and similar languages, one may call a method for division with syntax that resembles
where the parameters are optional. This is thought of as sending a message named divide with parameter divisor to dividend. An implementation will be chosen based only on dividend's type (perhaps rational, floating point, matrix), disregarding the type or value of divisor.
By contrast, some languages (such as Common Lisp, Dylan, Julia) dispatch methods or functions based on the combination of operands; in the division case, the types of the dividend and divisor together determine which divide operation will be performed. This is known as multiple dispatch.
A language may be implemented with different dynamic dispatch mechanisms. The choices of the dynamic dispatch mechanism offered by a language to a large extent alter the programming paradigms that are available or are most natural to use within a given language.
Normally, in a typed language, the dispatch mechanism will be performed based on the type of the arguments (most commonly based on the type of the receiver of a message). This might be dubbed 'per-type dynamic dispatch'. Languages with weak or no typing systems often carry a dispatch table as part of the object data for each object. This allows instance behaviour as each instance may map a given message to a separate method.
Some languages offer a hybrid approach.
Dynamic dispatch will always incur an overhead so some languages offer static dispatch for particular methods.
C++ uses early binding and offers both dynamic and static dispatch. The default form of dispatch is static. To get dynamic dispatch the programmer must declare a method as virtual.
C++ compilers typically implement dynamic dispatch with a data structure called a virtual table (vtable) that defines the message-to-method mapping for a given class (C++ as such has no notion of a vtable). Instances of that type will then store a pointer to this table as part of their instance data. This is complicated when multiple inheritance is used. Since C++ does not support late binding, the virtual table in a C++ object cannot be modified at run-time, which limits the potential set of dispatch targets to a finite set chosen at compile time.
Type overloading does not produce dynamic dispatch in C++ as the language considers the types of the message parameters part of the formal message name. This means that the message name the programmer sees is not the formal name used for binding.
In Go and Rust, a more versatile variation of early binding is used. 
Vtable pointers are carried with object references as 'fat pointers' ('interfaces' in go,  or 'trait objects' in Rust). 
This decouples the supported interfaces from the underlying data structures. Each compiled library needn't know the full range of interfaces supported in order to correctly use a type, just the specific vtable layout that they require. Code can pass around different interfaces to the same piece of data to different functions. This versatility comes at the expense of extra data with each object reference, which is problematic if many such references are stored persistently.
Smalltalk uses a type-based message dispatcher. Each instance has a single type whose definition contains the methods. When an instance receives a message, the dispatcher looks up the corresponding method in the message-to-method map for the type and then invokes the method.
Because a type can have a chain of base types, this look-up can be expensive. A naive implementation of Smalltalk's mechanism would seem to have a significantly higher overhead than that of C++ and this overhead would be incurred for each and every message that an object receives.
Real Smalltalk implementations often use a technique known as inline caching that makes method dispatch very fast. Inline caching basically stores the previous destination method address and object class of the call site (or multiple pairs for multi-way caching). The cached method is initialized with the most common target method (or just the cache miss handler), based on the method selector. When the method call site is reached during execution, it just calls the address in the cache. (In a dynamic code generator, this call is a direct call as the direct address is back patched by cache miss logic.) Prologue code in the called method then compares the cached class with the actual object class, and if they don't match, execution branches to a cache miss handler to find the correct method in the class. A fast implementation may have multiple cache entries and it often only takes a couple of instructions to get execution to the correct method on an initial cache miss. The common case will be a cached class match, and execution will just continue in the method.
Out-of-line caching can also be used in the method invocation logic, using the object class and method selector. In one design, the class and method selector are hashed, and used as an index into a method dispatch cache table.
As Smalltalk is a reflective language, many implementations allow mutating individual objects into objects with dynamically generated method lookup tables. This allows altering object behavior on a per object basis. A whole category of languages known as prototype based languages has grown from this, the most famous of which are Self and JavaScript. Careful design of the method dispatch caching allows even prototype based languages to have high performance method dispatch.
Many other dynamically typed languages, including Python, Ruby, Objective-C and Groovy use similar approaches."
"93","In computer science, a type class is a type system construct that supports ad hoc polymorphism. This is achieved by adding constraints to type variables in parametrically polymorphic types. Such a constraint typically involves a type class T and a type variable a, and means that a can only be instantiated to a type whose members support the overloaded operations associated with T.
Type classes first appeared in the Haskell programming language, and were originally conceived as a way of implementing overloaded arithmetic and equality operators in a principled fashion.
In contrast with the ""eqtypes"" of Standard ML, overloading the equality operator through the use of type classes in Haskell does not require extensive modification of the compiler frontend or the underlying type system.
Since their creation, many other applications of type classes have been discovered.
The programmer defines a type class by specifying a set of function or constant names, together with their respective types, that must exist for every type that belongs to the class. In Haskell, types can be parameterized; a type class Eq intended to contain types that admit equality would be declared in the following way:
The type variable a has kind ∗{\displaystyle *} (also known as Type in the latest GHC release), meaning that the kind of Eq is
The declaration may be read as stating a ""type a belongs to type class Eq if there are functions named (==), and (/=), of the appropriate types, defined on it."" A programmer could then define a function elem (which determines if an element is in a list) in the following way:
The function elem has the type a -> [a] -> Bool with the context Eq a, which constrains the types which a can range over to those a which belong to the Eq type class. (Note: Haskell  =>  can be called a 'class constraint'.)
A programmer can make any type t a member of a given type class C by using an instance declaration that defines implementations of all of C's methods for the particular type t.  For instance, if a programmer defines a new data type t, they may then make this new type an instance of Eq by providing an equality function over values of type t in whatever way they see fit.  Once they have done this, they may use the function elem on [t], that is, lists of elements of type t.
Note that type classes are different from classes in object-oriented programming languages. In particular, Eq is not a type: there is no such thing as a value of type Eq.
Type classes are closely related to parametric polymorphism. For example, note that the type of elem as specified above would be the parametrically polymorphic type a -> [a] -> Bool were it not for the type class constraint ""Eq a =>"".
A type class need not take a type variable of kind ∗{\displaystyle *} but can take one of any kind. These type classes with higher kinds are sometimes called constructor classes (the constructors referred to are type constructors such as Maybe, rather than data constructors such as Just). An example is the Monad class:
The fact that m is applied to a type variable indicates that it has kind Type -> Type, i.e. it takes a type and returns a type, the kind of Monad is thus:
Type classes permit multiple type parameters, and so type classes can be seen as relations on types. For example, in the GHC standard library, the class IArray expresses a general immutable array interface. In this class, the type class constraint IArray a e means that a is an array type that contains elements of type e. (This restriction on polymorphism is used to implement unboxed array types, for example.)
Like multimethods[citation needed], multi-parameter type classes support calling different implementations of a method depending on the types of multiple arguments, and indeed return types.  Multi-parameter type classes do not require searching for the method to call on every call at runtime;:minute 25:12 rather the method to call is first compiled and stored in the dictionary of the type class instance, just as with single-parameter type classes.
Haskell code that uses multi-parameter type classes is not portable, as this feature is not part of the Haskell 98 standard. The popular Haskell implementations, GHC and Hugs, support multi-parameter type classes.
In Haskell, type classes have been refined to allow the programmer to declare functional dependencies between type parameters—a concept inspired from relational database theory. That is, the programmer can assert that a given assignment of some subset of the type parameters uniquely determines the remaining type parameters. For example, general monads m which carry a state parameter of type s satisfy the type class constraint MonadState s m. In this constraint, there is a functional dependency m -> s. This means that for a given monad, the state type accessible from this interface is uniquely determined. This aids the compiler in type inference, as well as aiding the programmer in type-directed programming.
Simon Peyton-Jones has objected to the introduction of functional dependencies in Haskell on grounds of complexity.
Type classes and implicit parameters are very similar in nature, although not quite the same.  A polymorphic function with a type class constraint such as:
can be intuitively treated as a function that implicitly accepts an instance of Num:
The instance Num_ a is essentially a record that contains the instance definition of Num a.  (This is in fact how type classes are implemented under the hood by the Glasgow Haskell Compiler.)
However, there is a crucial difference: implicit parameters are more flexible – you can pass different instances of Num Int.  In contrast, type classes enforce the so-called coherence property, which requires that there should only be one unique choice of instance for any given type.  The coherence property makes type classes somewhat antimodular, which is why orphan instances (instances that are defined in a module that neither contains the class nor the type of interest) are strongly discouraged.  On the other hand, coherence adds an additional level of safety to the language, providing the programmer a guarantee that two disjoint parts of the same code will share the same instance.
As an example, an ordered set (of type Set a) requires a total ordering on the elements (of type a) in order to function.  This can be evidenced by a constraint Ord a, which defines a comparison operator on the elements.  However, there can be numerous ways to impose a total order.  Since set algorithms are generally intolerant of changes in the ordering once a set has been constructed, passing an incompatible instance of Ord a to functions that operate on the set may lead to incorrect results (or crashes).  Thus, enforcing coherence of Ord a in this particular scenario is crucial.
Instances (or ""dictionaries"") in Scala type classes are just ordinary values in the language, rather than a completely separate kind of entity. While these instances are by default supplied by finding appropriate instances in scope to be used as the implicit actual parameters for explicitly-declared implicit formal parameters, the fact that they are ordinary values means that they can be supplied explicitly, to resolve ambiguity.  As a result, Scala type classes do not satisfy the coherence property and are effectively a syntactic sugar for implicit parameters.
This is an example taken from the Cats  documentation:
Coq (version 8.2 onwards) also supports type classes by inferring the appropriate instances. Recent versions of Agda 2 also provide a similar feature, called ""instance arguments"".
In Standard ML, the mechanism of ""equality types"" corresponds roughly to Haskell's built-in type class Eq, but all equality operators are derived automatically by the compiler.  The programmer's control of the process is limited to designating which type components in a structure are equality types and which type variables in a polymorphic type range over equality types.
SML's and OCaml's modules and functors can play a role similar to that of Haskell's type classes, the principal difference being the role of type inference, which makes type classes suitable for ad hoc polymorphism.
The object oriented subset of OCaml is yet another approach which is somewhat comparable to the one of type classes.
An analogous notion for overloaded data (implemented in GHC) is that of type family.
Rust supports traits, which are a limited form of type classes with coherence.
Mercury has typeclasses, although they are not exactly the same as in Haskell.
In Scala, type classes are a programming idiom which can be implemented with existing language features such as implicit parameters, not a separate language feature per se. Because of the way they are implemented in Scala, it is possible to explicitly specify which type class instance to use for a type at a particular place in the code, in case of ambiguity. However, this is not necessarily a benefit as ambiguous type class instances can be error-prone.
The proof assistant Coq has also supported type classes in recent versions. Unlike in ordinary programming languages, in Coq, any laws of a type class (such as the monad laws) that are stated within the type class definition, must be mathematically proved of each type class instance before using them."
"94","System F, also known as the (Girard–Reynolds) polymorphic lambda calculus or the second-order lambda calculus, is a typed lambda calculus that differs from the simply typed lambda calculus by the introduction of a mechanism of universal quantification over types. System F thus formalizes the notion of parametric polymorphism in programming languages, and forms a theoretical basis for languages such as Haskell and ML. System F was discovered independently by logician Jean-Yves Girard (1972) and computer scientist John C. Reynolds (1974).
Whereas simply typed lambda calculus has variables ranging over functions, and binders for them, System F additionally has variables ranging over types, and binders for them. As an example, the fact that the identity function can have any type of the form A→ A would be formalized in System F as the judgment
where α{\displaystyle \alpha } is a type variable.  The upper-case Λ{\displaystyle \Lambda } is traditionally used to denote type-level functions, as opposed to the lower-case λ{\displaystyle \lambda } which is used for value-level functions. (The superscripted α{\displaystyle \alpha } means that the bound x is of type α{\displaystyle \alpha }; the expression after the colon is the type of the lambda expression preceding it.)
As a term rewriting system, System F is strongly normalizing. However, type inference in System F (without explicit type annotations) is undecidable. Under the Curry–Howard isomorphism, System F corresponds to the fragment of second-order intuitionistic logic that uses only universal quantification. System F can be seen as part of the lambda cube, together with even more expressive typed lambda calculi, including those with dependent types.
The Boolean{\displaystyle \scriptstyle {\mathsf {Boolean}}} type is defined as:
∀α.α→α→α{\displaystyle \scriptstyle \forall \alpha .\alpha \to \alpha \to \alpha }, where α{\displaystyle \scriptstyle \alpha } is a type variable. This means: Boolean{\displaystyle \scriptstyle {\mathsf {Boolean}}} is the type of all functions which take as input a type α and two expressions of type α, and produce as output an expression of type α (note that we consider →{\displaystyle \to } to be right-associative.)
The following two definitions for the boolean values T{\displaystyle \scriptstyle \mathbf {T} } and F{\displaystyle \scriptstyle \mathbf {F} } are used, extending the definition of Church booleans:
(Note that the above two functions require three — not two — parameters. The latter two should be lambda expressions, but the first one should be a type. This fact is reflected in the fact that the type of these expressions is ∀α.α→α→α{\displaystyle \scriptstyle \forall \alpha .\alpha \to \alpha \to \alpha }; the universal quantifier binding the α corresponds to the Λ binding the alpha in the lambda expression itself. Also, note that Boolean{\displaystyle \scriptstyle {\mathsf {Boolean}}} is a convenient shorthand for ∀α.α→α→α{\displaystyle \scriptstyle \forall \alpha .\alpha \to \alpha \to \alpha }, but it is not a symbol of System F itself, but rather a ""meta-symbol"". Likewise, T{\displaystyle \scriptstyle \mathbf {T} } and F{\displaystyle \scriptstyle \mathbf {F} } are also ""meta-symbols"", convenient shorthands, of System F ""assemblies"" (in the Bourbaki sense); otherwise, if such functions could be named (within System F), then there would be no need for the lambda-expressive apparatus capable of defining functions anonymously.)
Then, with these two λ{\displaystyle \scriptstyle \lambda }-terms, we can define some logic operators (which are of type Boolean→Boolean→Boolean{\displaystyle \scriptstyle {\mathsf {Boolean}}\rightarrow {\mathsf {Boolean}}\rightarrow {\mathsf {Boolean}}}):
As in Church encodings, there is no need for an IFTHENELSE function as one can just use raw Boolean{\displaystyle \scriptstyle {\mathsf {Boolean}}}-typed terms as decision functions. However, if one is requested:
will do.
A predicate is a function which returns a Boolean{\displaystyle \scriptstyle {\mathsf {Boolean}}}-typed value. The most fundamental predicate is ISZERO which returns T{\displaystyle \scriptstyle \mathbf {T} } if and only if its argument is the Church numeral  0:
System F allows recursive constructions to be embedded in a natural manner, related to that in Martin-Löf's type theory. Abstract structures (S) are created using constructors. These are functions typed as:
Recursivity is manifested when S{\displaystyle S} itself appears within one of the types Ki{\displaystyle K_{i}}. If you have m{\displaystyle m} of these constructors, you can define the type of S{\displaystyle S} as:
For instance, the natural numbers can be defined as an inductive datatype N{\displaystyle N} with constructors
The System F type corresponding to this structure is
∀α.α→(α→α)→α{\displaystyle \forall \alpha .\alpha \to (\alpha \to \alpha )\to \alpha }.  The terms of this type comprise a typed version of the Church numerals, the first few of which are:
If we reverse the order of the curried arguments (i.e., ∀α.(α→α)→α→α{\displaystyle \forall \alpha .(\alpha \rightarrow \alpha )\rightarrow \alpha \rightarrow \alpha }), then the Church numeral for n{\displaystyle n} is a function that takes a function f as argument and returns the n{\displaystyle n}th power of f. That is to say, a Church numeral is a higher-order function – it takes a single-argument function f, and returns another single-argument function.
The version of System F used in this article is as an explicitly typed, or Church-style, calculus.  The typing information contained in λ-terms makes type-checking straightforward.  Joe Wells (1994) settled an ""embarrassing open problem"" by proving that type checking is undecidable for a Curry-style variant of System F, that is, one that lacks explicit typing annotations.
Wells' result implies that type inference for System F is impossible.
A restriction of System F known as ""Hindley–Milner"", or simply ""HM"", does have an easy type inference algorithm and is used for many statically typed functional programming languages such as Haskell 98 and ML. Over time, as the restrictions of HM-style type systems have become apparent, languages have steadily moved to more expressive logics for their type systems. As of 2008, GHC, a Haskell compiler, goes beyond HM, and now uses System F extended with non-syntactic type equality.F-Sharp is designed from scratch with System F in mind.[citation needed]
While System F corresponds to the first axis of the Barendregt's lambda cube, System Fω or the higher-order polymorphic lambda calculus combines the first axis (polymorphism) with the second axis (type operators); it is a different, more complex system.
System Fω can be defined inductively on a family of systems, where induction is based on the kinds permitted in each system:
In the limit, we can define system Fω{\displaystyle F_{\omega }} to be
That is, Fω is the system which allows functions from types to types where the argument (and result) may be of any order.
Note that although Fω places no restrictions on the order of the arguments in these mappings, it does restrict the universe of the arguments for these mappings: they must be types rather than values.  System Fω does not permit mappings from values to types (Dependent types), though it does permit mappings from values to values (λ{\displaystyle \lambda } abstraction), mappings from types to values (Λ{\displaystyle \Lambda } abstraction, sometimes written ∀{\displaystyle \forall }) and mappings from types to types (λ{\displaystyle \lambda } abstraction at the level of types)"
"95","In the area of mathematical logic and computer science known as type theory, a kind is the type of a type constructor or, less commonly, the type of a higher-order type operator. A kind system is essentially a simply typed lambda calculus ""one level up"", endowed with a primitive type, denoted ∗{\displaystyle *} and called ""type"", which is the kind of any data type which does not need any type parameters.
A kind is sometimes confusingly described as the ""type of a (data) type"", but it is actually more of an arity specifier. Syntactically, it is natural to consider polymorphic types to be type constructors, thus non-polymorphic types to be nullary type constructors. But all nullary constructors, thus all monomorphic types, have the same, simplest kind; namely ∗{\displaystyle *}.
Since higher-order type operators are uncommon in programming languages, in most programming practice, kinds are used to distinguish between data types and the types of constructors which are used to implement parametric polymorphism. Kinds appear, either explicitly or implicitly, in languages whose type systems account for parametric polymorphism in a programatically accessible way, such as Haskell and Scala.
(Note: Haskell documentation uses the same arrow for both function types and kinds.)
Haskell's kind system has just two rules:[vague]
An inhabited type (as proper types are called in Haskell) is a type which has values. For instance, ignoring type classes which complicate the picture, 4 is a value of type Int, while [1, 2, 3] is a value of type [Int] (list of Ints). Therefore, Int and [Int] have kind ∗{\displaystyle *}, but so does any function type, for instance Int -> Bool or even Int -> Int -> Bool.
A type constructor takes one or more type arguments, and produces a data type when enough arguments are supplied, i.e. it supports partial application thanks to currying. This is how Haskell achieves parametric types. For instance, the type [] (list) is a type constructor - it takes a single argument to specify the type of the elements of the list. Hence, [Int] (list of Ints), [Float] (list of Floats) and even [[Int]] (list of lists of Ints) are valid applications of the [] type constructor. Therefore, [] is a type of kind ∗→∗{\displaystyle *\rightarrow *}. Because Int has kind ∗{\displaystyle *}, applying it to [] results in [Int], of kind ∗{\displaystyle *}. The 2-tuple constructor (,) has kind ∗→∗→∗{\displaystyle *\rightarrow *\rightarrow *}, the 3-tuple constructor (,,) has kind ∗→∗→∗→∗{\displaystyle *\rightarrow *\rightarrow *\rightarrow *} and so on.
Standard Haskell does not allow polymorphic kinds. This is in contrast to parametric polymorphism on types, which is supported in Haskell.  For instance, in the following example:
the kind of z could be anything, including ∗{\displaystyle *}, but also ∗→∗{\displaystyle *\rightarrow *} etc. Haskell by default will always infer kinds to be ∗{\displaystyle *}, unless the type explicitly indicates otherwise (see below). Therefore the type checker will reject the following use of Tree:
because the kind of [], ∗→∗{\displaystyle *\rightarrow *} does not match the expected kind for z, which is always ∗{\displaystyle *}.
Higher-order type operators are allowed however. For instance:
has kind (∗→∗)→∗→∗{\displaystyle (*\rightarrow *)\rightarrow *\rightarrow *}, i.e. unt is expected to be a unary data constructor, which gets applied to its argument, which must be a type, and returns another type.
GHC has the extension PolyKinds, which, together with KindSignatures, allows polymorphic kinds. For example:
In GHC 8.0.1, types and kinds can merged using the experimental compiler option -XTypeInType."
"96","In type theory and functional programming, Hindley–Milner (HM), also known as Damas–Milner or Damas–Hindley–Milner, is a classical type system for the lambda calculus with parametric polymorphism, first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.
Among HM's more notable properties are its completeness and its ability to deduce the most general type of a given program without the need of any type annotations or other hints supplied by the programmer. Algorithm W is a fast algorithm, performing type inference in almost linear time with respect to the size of the source, making it practically useful to type large programs.[note 1] HM is preferably used for functional languages. Its first implementation was part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably by type class constraints like those used in Haskell.
Organizing their original paper, Damas and Milner clearly separated two very different tasks. One is to describe what types an expression can have and another to present an algorithm actually computing a type. Keeping the two aspects separate allows one to focus separately on the logic (i.e. meaning) behind the algorithm, as well as to establish a benchmark for the algorithm's properties.
How expressions and types fit to each other is described by means of a deductive system. Like any proof system, it allows different ways to come to a conclusion and since one and the same expression arguably might have different types, dissimilar conclusions about an expression are possible. Contrary to this, the type inference method itself (Algorithm W) is defined as a deterministic step-by-step procedure, leaving no choice what to do next. Thus clearly, decisions not present in the logic might have been made constructing the algorithm, which demand a closer look and justifications but would perhaps remain non-obvious without the above differentiation.
Logic and algorithm share the notions of ""expression"" and ""type"", whose form is made precise by the syntax.
The expressions to be typed are exactly those of the lambda calculus, enhanced by a let-expression.
These are shown in the adjacent table.
For readers unfamiliar with the lambda calculus, here is a brief explanation:
The application e1e2{\displaystyle e_{1}e_{2}} represents
applying the function  e1{\displaystyle e_{1}} to the argument  e2{\displaystyle e_{2}}, often written e1(e2){\displaystyle e_{1}(e_{2})}.
The abstraction λ x . e{\displaystyle \lambda \ x\ .\ e} represents an anonymous function that maps the input x{\displaystyle x} to the output e{\displaystyle e}.  This is also called function literal, common in most contemporary programming languages, and sometimes written as function(x) return e end{\displaystyle {\mathtt {function}}\,(x)\ {\mathtt {return}}\ e\ {\mathtt {end}}}.
The let expression let x=e1 in e2{\displaystyle {\mathtt {let}}\ x=e_{1}\ {\mathtt {in}}\ e_{2}} represents the result of substituting every occurrence of x{\displaystyle x} in e2{\displaystyle e_{2}} with e1{\displaystyle e_{1}}.
Types as a whole are split into two groups, called mono- and polytypes.[note 2]
The reason why the let-expression appears in the syntax is to allow let polymorphism (see below).
Monotypes always designate a particular type, in the sense that a monotype is equal only to itself and different from all others. Monotypes τ{\displaystyle \tau } are syntactically represented as terms.
Examples of monotypes include type constants like int{\displaystyle {\mathtt {int}}} or string{\displaystyle {\mathtt {string}}}, and parametric
types like Map (Set string) int{\displaystyle {\mathtt {Map\ (Set\ string)\ int}}}.   These types are examples of applications of type functions, for example, from the set
{Map2, Set1, string0, int0}{\displaystyle \{{\mathtt {Map^{2},\ Set^{1},\ string^{0},\ int^{0}}}\}}, 
where the superscript indicates the number of type parameters.  The complete set of type functions D{\displaystyle D} is arbitrary in HM, except that it must contain at least →2{\displaystyle \rightarrow ^{2}}, the type of functions.  It is often written in infix notation for convenience.  For example, a function mapping integers to strings has type int→string{\displaystyle {\mathtt {int}}\rightarrow {\mathtt {string}}}; here, the type int→string{\displaystyle {\mathtt {int}}\rightarrow {\mathtt {string}}} is written in infix notation. In prefix notation, it would be (→) int string{\displaystyle (\rightarrow )\ {\mathtt {int}}\ {\mathtt {string}}}.
[note 3]
Type variables are monotypes. Standing alone, a type variable α{\displaystyle \alpha } is meant
to be as concrete as int{\displaystyle {\mathtt {int}}} or β{\displaystyle \beta }, and clearly different from both. Type variables occurring as monotypes behave as if they were type constants whose identity is unknown. Correspondingly, a function typed α→α{\displaystyle \alpha \rightarrow \alpha } only maps values of the particular type α{\displaystyle \alpha } on itself. Such a function can only be applied to values having type α{\displaystyle \alpha } and to no others.
Polytypes (or type schemes) are types containing variables bound by one or more for-all quantifiers, e.g. ∀α.α→α{\displaystyle \forall \alpha .\alpha \rightarrow \alpha }.
A function with polytype ∀α.α→α{\displaystyle \forall \alpha .\alpha \rightarrow \alpha } can map any value of the same type to itself,
and the identity function is a value for this type.
As another example ∀α.(Set α)→int{\displaystyle \forall \alpha .({\mathtt {Set}}\ \alpha )\rightarrow {\mathtt {int}}} is the type of a function mapping all finite sets to integers. The count of members is a value for this type.
Note that quantifiers can only appear top level, i.e. a type ∀α.α→∀α.α{\displaystyle \forall \alpha .\alpha \rightarrow \forall \alpha .\alpha } for instance, is excluded by the syntax of types. Note also that monotypes are included in the polytypes, thus a type has the general form ∀α1…∀αn.τ{\displaystyle \forall \alpha _{1}\dots \forall \alpha _{n}.\tau }, where τ{\displaystyle \tau } is a monotype.
In a type ∀α1…∀αn.τ{\displaystyle \forall \alpha _{1}\dots \forall \alpha _{n}.\tau }, the symbol ∀{\displaystyle \forall } is the quantifier binding the type variables αi{\displaystyle \alpha _{i}} in the monotype τ{\displaystyle \tau }. The variables αi{\displaystyle \alpha _{i}} are called quantified and any occurrence of a quantified type variable in τ{\displaystyle \tau } is called bound and all unbound type variables in τ{\displaystyle \tau } are called free. Like in the lambda calculus, the notion of free and bound variables is essential for the understanding of the meaning of types.
One does not have clauses with free variables in Prolog. In Haskell, in the absence of the ScopedTypeVariables language extension, all type variables implicitly occur quantified, i.e. a Haskell type a -> a means ∀α.α→α{\displaystyle \forall \alpha .\alpha \rightarrow \alpha } here.
Consider foo{\displaystyle {\mathit {foo}}} in Example 1, with type annotations in brackets.
Its parameter y{\displaystyle y} is not used in the body, but the variable x{\displaystyle x} bound in the outer context of foo{\displaystyle {\mathit {foo}}} is.
As a consequence, foo{\displaystyle {\mathit {foo}}} accepts every value as argument, while returning a value bound outside and with it its type. bar{\displaystyle {\mathit {bar}}} to the contrary has type ∀α.∀β.α→(β→α){\displaystyle \forall \alpha .\forall \beta .\alpha \rightarrow (\beta \rightarrow \alpha )}, in which all occurring type variables are bound.
Evaluating, for instance bar 1{\displaystyle {\mathit {bar}}\ 1}, results in a function of type ∀β.β→ int{\displaystyle \forall \beta .\beta \rightarrow \ {\mathit {int}}}, perfectly reflecting that foo's monotype α{\displaystyle \alpha } in ∀β.β→α{\displaystyle \forall \beta .\beta \rightarrow \alpha } has been refined by this call.
In this example, the free monotype variable α{\displaystyle \alpha } in foo's type becomes meaningful by being quantified in the outer scope, namely in bar's type.
I.e. in context of the example, the same type variable α{\displaystyle \alpha } appears both bound and free in different types. As a consequence, a free type variable cannot be interpreted better than stating it is a monotype without knowing the context. Turning the statement around, in general, a typing is not meaningful without a context.
Consequently, to get the yet disjoint parts of the syntax, expressions and types together meaningfully, a third
part, the context is needed. Syntactically, it is a list of pairs x:σ{\displaystyle x:\sigma }, called 
assignments or assumptions, stating for each value variable xi{\displaystyle x_{i}}
therein a type σi{\displaystyle \sigma _{i}}. All three parts combined gives a typing judgment of the form Γ ⊢ e:σ{\displaystyle \Gamma \ \vdash \ e:\sigma },
stating, that under assumptions Γ{\displaystyle \Gamma }, the expression e{\displaystyle e} has type σ{\displaystyle \sigma }.
Now having the complete syntax at hand, one can finally make a meaningful statement about the type of foo{\displaystyle {\mathit {foo}}} in example 1, above,
namely x:α⊢λ y.x:∀β.β→α{\displaystyle x:\alpha \vdash \lambda \ y.x:\forall \beta .\beta \rightarrow \alpha }. Contrary to the above formulations, the monotype
variable α{\displaystyle \alpha } no longer appears unbound, i.e. meaningless, but bound in the context as the type of the value variable
x{\displaystyle x}. The circumstance whether a type variable is bound or free in the context apparently plays a significant
role for a type as part of a typing, so free( Γ ){\displaystyle {\text{free}}(\ \Gamma \ )} it is made precise in the side box.
While the equality of monotypes is purely syntactical, polytypes offer a richer structure by being related to other types through a specialization relation σ⊑σ′{\displaystyle \sigma \sqsubseteq \sigma '} expressing that σ′{\displaystyle \sigma '} is more special than σ{\displaystyle \sigma }.
When being applied to a value a polymorphic function has to change its shape specializing to deal with this particular type of values. During this process, it also changes its type to match that of the parameter. If for instance the identity function having type ∀α.α→α{\displaystyle \forall \alpha .\alpha \rightarrow \alpha } is to be applied on a number having type int{\displaystyle int}, both simply cannot work together, because all the types are different and nothing fits. What is needed is a function of type int→int{\displaystyle int\rightarrow int}. Thus, during application, the polymorphic identity is specialized to a monomorphic version of itself. In terms of the specialization relation, one writes ∀α.α→α⊑ int→int{\displaystyle \forall \alpha .\alpha \rightarrow \alpha \sqsubseteq \ int\rightarrow int}
Now the shape shifting of polymorphic values is not fully arbitrary but rather limited by their pristine polytype. Following what has happened in the example one could paraphrase the rule of specialization, saying, a polymorphic type ∀α.τ{\displaystyle \forall \alpha .\tau } is specialized by consistently replacing each occurrence of α{\displaystyle \alpha } in τ{\displaystyle \tau } and dropping the quantifier. While this rule works well for any monotype used as replacement, it fails when a polytype, say ∀β.β{\displaystyle \forall \beta .\beta } is tried as a replacement, resulting in the non-syntactical type ∀β.β→∀β.β{\displaystyle \forall \beta .\beta \rightarrow \forall \beta .\beta }.
The syntactic restriction to allow quantification only top-level is imposed to prevent generalization while specializing. Instead of ∀β.β→∀β.β{\displaystyle \forall \beta .\beta \rightarrow \forall \beta .\beta }, the more special type ∀β.β→β{\displaystyle \forall \beta .\beta \rightarrow \beta } must be produced in this case.
One could undo the former specialization by specializing on some value of type ∀α.α{\displaystyle \forall \alpha .\alpha } again. In terms of the relation
one gains ∀α.α→α⊑∀β.β→β⊑∀α.α→α{\displaystyle \forall \alpha .\alpha \rightarrow \alpha \sqsubseteq \forall \beta .\beta \rightarrow \beta \sqsubseteq \forall \alpha .\alpha \rightarrow \alpha } as a summary, meaning that syntactically different polytypes are equal with respect to renaming their quantified variables.
Now focusing only on the question whether a type is more special than another and no longer what the specialized type is used for, one could summarize the specialization as in the box above. Paraphrasing it clockwise, a type ∀α1…∀αn.τ{\displaystyle \forall \alpha _{1}\dots \forall \alpha _{n}.\tau } is specialized by consistently replacing any of the quantified variables αi{\displaystyle \alpha _{i}} by arbitrary monotypes τi{\displaystyle \tau _{i}} gaining a monotype τ′{\displaystyle \tau '}. Finally, type variables in τ′{\displaystyle \tau '} not occurring free in the pristine type can optionally be quantified.
Thus the specialization rules makes sure that no free variable, i.e. monotype in the pristine type becomes unintentionally bound by a quantifier, but originally quantified variable can be replaced with whatever, even with types introducing new quantified or unquantified type variables.
Starting with a polytype ∀α.α{\displaystyle \forall \alpha .\alpha }, the specialization could either replace the body by another quantified variable, actually a rename or by some type constant (including the function type) which may or may not have parameters filled either with monotypes or quantified type variables. Once a quantified variable is replaced by a type application, this specialization cannot be undone through another substitution as it was possible for quantified variables. Thus the type application is there to stay. Only if it contains another quantified type variable, the specialization could continue further replacing for it.
So the specialization introduces no further equivalence on polytype beside the already known renaming. Polytypes are syntactically equal up to renaming their quantified variables. The equality of types is a reflexive, antisymmetric and transitive relation and the remaining specializations of polytypes are transitive and with this the relation ⊑{\displaystyle \sqsubseteq } is an order.
The syntax of HM is carried forward to the syntax of the inference rules that form the body of the formal system, by using the typings as judgments. Each of the rules define what conclusion could be drawn from what premises. Additionally to the judgments, some extra conditions introduced above might be used as premises, too.
A proof using the rules is a sequence of judgments such that all premises are listed before a conclusion. Please see the Examples 2 and 3 below for a possible format of proofs. From left to right, each line shows the conclusion, the [Name]{\displaystyle [{\mathtt {Name}}]} of the rule applied and the premises, either by referring to an earlier line (number) if the premise is a judgment or by making the predicate explicit.
The side box shows the deduction rules of the HM type system. Remember that σ{\displaystyle \sigma } and τ{\displaystyle \tau } denote poly- and monotypes respectively, so the premises Γ,x:τ⊢De:τ′{\displaystyle \Gamma ,\;x:\tau \vdash _{D}e:\tau '} of [Abs]{\displaystyle [{\mathtt {Abs}}]} and Γ,x:σ⊢De1:τ{\displaystyle \Gamma ,\,x:\sigma \vdash _{D}e_{1}:\tau } of [Let]{\displaystyle [{\mathtt {Let}}]}, for example, are distinct. One can roughly divide the rules into two groups:
The first four rules [Var]{\displaystyle [{\mathtt {Var}}]} (variable or function access), [App]{\displaystyle [{\mathtt {App}}]} (application, i.e. function call with one parameter), [Abs]{\displaystyle [{\mathtt {Abs}}]} (abstraction, i.e. function declaration) and [Let]{\displaystyle [{\mathtt {Let}}]} (variable declaration) are centered around the syntax, presenting one rule for each of the expression forms. Their meaning is pretty obvious at the first glance, as they decompose each expression, prove their sub-expressions and finally combine the individual types found in the premises to the type in the conclusion.
The second group is formed by the  remaining two rules [Inst]{\displaystyle [{\mathtt {Inst}}]} and [Gen]{\displaystyle [{\mathtt {Gen}}]}.
They handle specialization and generalization of types. While the rule [Inst]{\displaystyle [{\mathtt {Inst}}]} should be clear from the section on specialization above, [Gen]{\displaystyle [{\mathtt {Gen}}]} complements the former, working in the opposite direction. It allows generalization, i.e. to quantify monotype variables that are not bound in the context. The necessity of this restriction α∉free( Γ ){\displaystyle \alpha \not \in free(\ \Gamma \ )} is introduced in the section on free type variables.
The following two examples exercise the rule system in action
Example 2: A proof for Γ⊢Did(n):int{\displaystyle \Gamma \vdash _{D}id(n):int} where Γ=id:∀α.α→α, n:int{\displaystyle \Gamma =id:\forall \alpha .\alpha \rightarrow \alpha ,\ n:int},
could be written
Example 3: To demonstrate generalization,
⊢D letid=λx.x in id:∀α.α→α{\displaystyle \vdash _{D}\ {\textbf {let}}\,id=\lambda x.x\ {\textbf {in}}\ id\,:\,\forall \alpha .\alpha \rightarrow \alpha }
is shown below:
As mentioned in the introduction, the rules allow one to deduce different types for one and the same expression. See for instance, Example 2, steps 1,2 and Example 3, steps 2,3 for three different typings of the same expression. Clearly, the different results are not fully unrelated, but connected by the type order. It is an important property of the rule system and this order that whenever more than one type can be deduced for an expression, among them is (modulo alpha-renaming of the type variables) a unique most general type in the sense, that all others are specialization of it. Though the rule system must allow to derive specialized types, a type inference algorithm should deliver this most general or principal type as its result.
Not visible immediately, the rule set encodes a regulation under which circumstances a type might be generalized or not by a slightly varying use of mono- and polytypes in the rules [Abs]{\displaystyle [{\mathtt {Abs}}]} and [Let]{\displaystyle [{\mathtt {Let}}]}.
In rule [Abs]{\displaystyle [{\mathtt {Abs}}]}, the value variable of the parameter of the function λx.e{\displaystyle \lambda x.e} is added to the context with a monomorphic type through the premise Γ, x:τ⊢De:τ′{\displaystyle \Gamma ,\ x:\tau \vdash _{D}e:\tau '}, while in the rule  [Let]{\displaystyle [{\mathtt {Let}}]}, the variable enters the environment in polymorphic form Γ, x:σ⊢De1:τ{\displaystyle \Gamma ,\ x:\sigma \vdash _{D}e_{1}:\tau }. Though in both cases the presence of x{\displaystyle x} in the context prevents the use of the generalisation rule for any monotype variable in the assignment. This regulation forces the parameter x{\displaystyle x} in a λ{\displaystyle \lambda }-expression to remain monomorphic, while in a let-expression, the variable could already be introduced polymorphic, making specializations possible.
As a consequence of this regulation, no type can be inferred for λf.(ftrue,f0){\displaystyle \lambda f.(f\,{\textrm {true}},f\,{\textrm {0}})}
since the parameter f{\displaystyle f} is in a monomorphic position, while let f=λx.xin(ftrue,f0){\displaystyle {\textbf {let}}\ f=\lambda x.x\,{\textbf {in}}\,(f\,{\textrm {true}},f\,{\textrm {0}})} yields a type (bool,int){\displaystyle (bool,int)}, because f{\displaystyle f} has been introduced in a let-expression and is treated polymorphic therefore.
Note that this behaviour is in strong contrast to the usual definition let x=e1 in e2 ::=(λ x.e2) e1{\displaystyle {\textbf {let}}\ x=e_{1}\ {\textbf {in}}\ e_{2}\ ::=(\lambda \ x.e_{2})\ e_{1}} and the reason why the let-expression appears in the syntax at all. This distinction is called let-polymorphism or let generalization and is a conception owed to HM.
Now that the deduction system of HM is at hand, one could present an algorithm and validate it with respect to the rules.
Alternatively, it might be possible to derive it by taking a closer look on how the rules interact and proof are
formed. This is done in the remainder of this article focusing on the possible decisions one can make while proving a typing.
Isolating the points in a proof, where no decision is possible at all,
the first group of rules centered around the syntax leaves no choice since
to each syntactical rule corresponds a unique typing rule, which determines
a part of the proof, while between the conclusion and the premises of these
fixed parts chains of [Inst]{\displaystyle [{\mathtt {Inst}}]} and [Gen]{\displaystyle [{\mathtt {Gen}}]}
could occur. Such a chain could also exist between the conclusion of the
proof and the rule for topmost expression. All proofs must have
the so sketched shape.
Because the only choice in a proof with respect of rule selection are the
[Inst]{\displaystyle [{\mathtt {Inst}}]} and [Gen]{\displaystyle [{\mathtt {Gen}}]} chains, the
form of the proof suggests the question whether it can be made more precise,
where these chains might be needed. This is in fact possible and leads to a
variant of the rules system with no such rules.
A contemporary treatment of HM uses a purely syntax-directed rule system due to
Clement
as an intermediate step. In this system, the specialization is located directly after the original [Var]{\displaystyle [{\mathtt {Var}}]} rule
and merged into it, while the generalization becomes part of the [Let]{\displaystyle [{\mathtt {Let}}]} rule. There the generalization is
also determined to always produce the most general type by introducing the function Γ¯(τ){\displaystyle {\bar {\Gamma }}(\tau )}, which quantifies
all monotype variables not bound in Γ{\displaystyle \Gamma }.
Formally, to validate, that this new rule system ⊢S{\displaystyle \vdash _{S}} is equivalent to the original ⊢D{\displaystyle \vdash _{D}}, one has
to show that Γ⊢D e:σ⇔Γ⊢S e:σ{\displaystyle \Gamma \vdash _{D}\ e:\sigma \Leftrightarrow \Gamma \vdash _{S}\ e:\sigma }, which falls apart into two sub-proofs:
While consistency can be seen by decomposing the rules [Let]{\displaystyle [{\mathtt {Let}}]} and [Var]{\displaystyle [{\mathtt {Var}}]}
of ⊢S{\displaystyle \vdash _{S}} into proofs in ⊢D{\displaystyle \vdash _{D}}, it is likely visible that ⊢S{\displaystyle \vdash _{S}} is incomplete, as
one cannot show λ x.x:∀α.α→α{\displaystyle \lambda \ x.x:\forall \alpha .\alpha \rightarrow \alpha } in ⊢S{\displaystyle \vdash _{S}}, for instance, but only
λ x.x:α→α{\displaystyle \lambda \ x.x:\alpha \rightarrow \alpha }.  An only slightly weaker version of completeness is provable
 though, namely
implying, one can derive the principal type for an expression in ⊢S{\displaystyle \vdash _{S}} allowing to generalize the proof in the end.
Comparing ⊢D{\displaystyle \vdash _{D}} and ⊢S{\displaystyle \vdash _{S}} note that now only monotypes appear in the judgments of all rules.
Within the rules themselves, assuming a given expression, one is free to pick
the instances for (rule) variables not occurring in this expression. These are
the instances for the type variable in the rules. Working towards finding the
most general type, this choice can be limited to picking suitable types for
τ{\displaystyle \tau } in [Var]{\displaystyle [{\mathtt {Var}}]} and [Abs]{\displaystyle [{\mathtt {Abs}}]}.
The decision of a suitable choice cannot be made locally, but its quality becomes apparent
in the premises of [App]{\displaystyle [{\mathtt {App}}]}, the only rule, in which
two different types, namely the function's formal and actual parameter type have
to come together as one.
Therefore, the general strategy for finding a proof would be to make the most
general assumption (α∉free(Γ){\displaystyle \alpha \not \in free(\Gamma )}) for τ{\displaystyle \tau }
in [Abs]{\displaystyle [{\mathtt {Abs}}]} and to refine this and the choice to be made in
[Var]{\displaystyle [{\mathtt {Var}}]} until all side conditions imposed by the
[App]{\displaystyle [{\mathtt {App}}]} rules are finally met. Fortunately, no trial and
error is needed, since an effective method is known to compute all the choices,
Robinson's Unification
in combination with the so-called Union-Find algorithm.
To briefly summarize the union-find algorithm, given the set of all types in a proof, it allows one
to group them together into equivalence classes by means of a union{\displaystyle {\mathtt {union}}}
procedure and to pick a representative for each such class using a find{\displaystyle {\mathtt {find}}}
procedure. Emphasizing on the word procedure in the sense of side effect,
we're clearly leaving the realm of logic to prepare an effective algorithm.
The representative of a union(a,b){\displaystyle {\mathtt {union}}(a,b)} is determined such, that if both a{\displaystyle a} and b{\displaystyle b} are type variables
the representative is arbitrarily one of them, while uniting a variable and a term, the term becomes the representative. Assuming an implementation of union-find at hand, one can formulate the unification of two monotypes as follows:
The presentation of Algorithm W as shown in the side box does not only deviate significantly from the original but is also a gross abuse of the notation of logical rules, since it includes side effects. It is legitimized here, for allowing a direct comparison with ⊢S{\displaystyle \vdash _{S}} while expressing an efficient implementation at the same time. The rules now specify a procedure with parameters Γ,e{\displaystyle \Gamma ,e} yielding τ{\displaystyle \tau } in the conclusion where the execution of the premises proceeds from left to right. Alternatively to a procedure, it could be viewed as an attributation of the expression.
The procedure inst(σ){\displaystyle inst(\sigma )} specializes the polytype σ{\displaystyle \sigma } by copying the term and replacing the bound type variables consistently by new monotype variables. 'newvar{\displaystyle newvar}' produces a new monotype variable. Likely, Γ¯(τ){\displaystyle {\bar {\Gamma }}(\tau )} has to copy the type introducing new variables for the quantification to avoid unwanted captures. Overall, the algorithm now proceeds by always making the most general choice leaving the specialization to the unification, which by itself produces the most general result. As noted above, the final result τ{\displaystyle \tau } has to be generalized to Γ¯(τ){\displaystyle {\bar {\Gamma }}(\tau )} in the end, to gain the most general type for a given expression.
Because the procedures used in the algorithm have nearly O(1) cost, the overall cost of the algorithm is close to linear in the size of the expression for which a type is to be inferred. This is in strong contrast to many other attempts to derive type inference algorithms, which often came out to be NP-hard, if not undecidable with respect to termination. Thus the HM performs as well as the best fully informed type-checking algorithms can. Type-checking here means that an algorithm does not have to find a proof, but only to validate a given one.
Efficiency is slightly reduced because the binding of type variables in the context has to be maintained to allow computation of Γ¯(τ){\displaystyle {\bar {\Gamma }}(\tau )} and enable an occurs check to prevent the building of recursive types during union(α,τ){\displaystyle union(\alpha ,\tau )}.
An example of such a case is λ x.(x x){\displaystyle \lambda \ x.(x\ x)}, for which no type can be derived using HM.  Practically, types are only small terms and do not build up expanding structures.  Thus, in complexity analysis, one can treat comparing them as a constant, retaining O(1) costs.
In the original paper, the algorithm is presented more formally using a substitution style instead of side effects in the method above. In the latter form, the side effect invisibly takes care of all places where a type variable is used. Explicitly using substitutions not only makes the algorithm hard to read[dubious  – discuss], because the side effect occurs virtually everywhere, but also gives the false impression that the method might be costly. When implemented using purely functional means or for the purpose of proving the algorithm to be basically equivalent to the deduction system, full explicitness is of course needed and the original formulation a necessary refinement.
A central property of the lambda calculus is, that recursive definitions are non-elemental, but can instead be expressed by a fixed point combinator.
The original paper notes that recursion can be realized by this combinator's type
fix:∀α.(α→α)→α{\displaystyle {\mathit {fix}}:\forall \alpha .(\alpha \rightarrow \alpha )\rightarrow \alpha }. A possible recursive definitions could thus be formulated as
rec v=e1 in e2 ::=let v=fix(λv.e1) in e2{\displaystyle {\mathtt {rec}}\ v=e_{1}\ {\mathtt {in}}\ e_{2}\ ::={\mathtt {let}}\ v={\mathit {fix}}(\lambda v.e_{1})\ {\mathtt {in}}\ e_{2}}.
Alternatively an extension of the expression syntax and an extra typing rule is possible as:
where
basically merging [Abs]{\displaystyle [{\mathtt {Abs}}]} and [Let]{\displaystyle [{\mathtt {Let}}]} while including the recursively defined
variables in monotype positions where they occur left to the in{\displaystyle {\mathtt {in}}} but as polytypes right to it. This
formulation perhaps best summarizes the essence of let-polymorphism."
"97","Generic programming is a style of computer programming in which algorithms are written in terms of types to-be-specified-later that are then instantiated when needed for specific types provided as parameters. This approach, pioneered by ML in 1973, permits writing common functions or types that differ only in the set of types on which they operate when used, thus reducing duplication. Such software entities are known as generics in Ada, C, C#, Delphi, Eiffel, F#, Java, Objective-C, Rust, Swift, TypeScript and Visual Basic .NET. They are known as parametric polymorphism in ML, Scala, Haskell (the Haskell community also uses the term ""generic"" for a related but somewhat different concept) and Julia; templates in C++ and D; and parameterized types in the influential 1994 book Design Patterns. The authors of Design Patterns note that this technique, especially when combined with delegation, is very powerful, however, 
The term generic programming was originally coined by David Musser and Alexander Stepanov in a more specific sense than the above, to describe a programming paradigm whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, with generic functions implemented in terms of these concepts, typically using language genericity mechanisms as described above.
Generic programming is defined in Musser & Stepanov (1989) as follows,
Generic programming paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, analogously to the abstraction of algebraic theories in abstract algebra.  Early examples of this programming approach were implemented in Scheme and Ada, although the best known example is the Standard Template Library (STL), which developed a theory of iterators that is used to decouple sequence data structures and the algorithms operating on them.
For example, given N sequence data structures, e.g. singly linked list, vector etc., and M algorithms to operate on them, e.g. find, sort etc., a direct approach would implement each algorithm specifically for each data structure, giving N × M combinations to implement. However, in the generic programming approach, each data structure returns a model of an iterator concept (a simple value type that can be dereferenced to retrieve the current value, or changed to point to another value in the sequence) and each algorithm is instead written generically with arguments of such iterators, e.g. a pair of iterators pointing to the beginning and end of the subsequence to process. Thus, only N + M data structure-algorithm combinations need be implemented. Several iterator concepts are specified in the STL, each a refinement of more restrictive concepts e.g. forward iterators only provide movement to the next value in a sequence (e.g. suitable for a singly linked list or a stream of input data), whereas a random-access iterator also provides direct constant-time access to any element of the sequence (e.g. suitable for a vector). An important point is that a data structure will return a model of the most general concept that can be implemented efficiently—computational complexity requirements are explicitly part of the concept definition. This limits the data structures a given algorithm can be applied to and such complexity requirements are a major determinant of data structure choice. Generic programming similarly has been applied in other domains,  e.g. graph algorithms.
Note that although this approach often utilizes language features of compile-time genericity/templates, it is in fact independent of particular language-technical details. Generic programming pioneer Alexander Stepanov wrote,
Bjarne Stroustrup noted,
Other programming paradigms that have been described as generic programming include Datatype generic programming as described in  ""Generic Programming — an Introduction"". The  Scrap your boilerplate approach is a lightweight generic programming approach for Haskell.
In this article we distinguish the high-level programming paradigms of generic programming, above, from the lower-level programming language genericity mechanisms used to implement them (see Programming language support for genericity). For further discussion and comparison of generic programming paradigms, see.
Genericity facilities have existed in high-level languages since at least the 1970s in languages such as ML, CLU and Ada, and were subsequently adopted by many object-based and object-oriented languages, including BETA, C++, D, Eiffel, Java, and DEC's now defunct Trellis-Owl language.
Genericity is implemented and supported differently in various programming languages; the term ""generic"" has also been used differently in various programming contexts. For example, in Forth the compiler can execute code while compiling and one can create new compiler keywords and new implementations for those words on the fly. It has few words that expose the compiler behaviour and therefore naturally offers genericity capacities that, however, are not referred to as such in most Forth texts. Similarly, dynamically typed languages, especially interpreted ones, usually offer genericity by default as both passing values to functions and value assignment are type-indifferent and such behavior is often utilized for abstraction or code terseness, however this is not typically labeled genericity as it's a direct consequence of dynamic typing system employed by the language [citation needed]. The term has been used in functional programming, specifically in Haskell-like languages, which use a structural type system where types are always parametric and the actual code on those types is generic. These usages still serve a similar purpose of code-saving and the rendering of an abstraction.
Arrays and structs can be viewed as predefined generic types. Every usage of an array or struct type instantiates a new concrete type, or reuses a previous instantiated type. Array element types and struct element types are parameterized types, which are used to instantiate the corresponding generic type. All this is usually built-in in the compiler and the syntax differs from other generic constructs. Some extensible programming languages try to unify built-in and user defined generic types.
A broad survey of genericity mechanisms in programming languages follows. For a specific survey comparing suitability of mechanisms for generic programming, see.
When creating container classes in statically typed languages, it is inconvenient to write specific implementations for each datatype contained, especially if the code for each datatype is virtually identical. For example, in C++, this duplication of code can be circumvented by defining a class template:
Above, T is a placeholder for whatever type is specified when the list is created. These ""containers-of-type-T"", commonly called templates, allow a class to be reused with different datatypes as long as certain contracts such as subtypes and signature are kept. This genericity mechanism should not be confused with inclusion polymorphism, which is the algorithmic usage of exchangeable sub-classes: for instance, a list of objects of type Moving_Object containing objects of type Animal and Car. Templates can also be used for type-independent functions as in the Swap example below:
The C++ template construct used above is widely cited[citation needed] as the genericity construct that popularized the notion among programmers and language designers and supports many generic programming idioms. The D programming language also offers fully generic-capable templates based on the C++ precedent but with a simplified syntax. The Java programming language has provided genericity facilities syntactically based on C++'s since the introduction of J2SE 5.0.
C# 2.0, Oxygene 1.5 (also known as Chrome) and Visual Basic .NET 2005 have constructs that take advantage of the support for generics present in the Microsoft .NET Framework since version 2.0.
Ada has had generics since it was first designed in 1977–1980. The standard library uses generics to provide many services. Ada 2005 adds a comprehensive generic container library to the standard library, which was inspired by C++'s standard template library.
A generic unit is a package or a subprogram that takes one or more generic formal parameters.
A generic formal parameter is a value, a variable, a constant, a type, a subprogram, or even an instance of another, designated, generic unit. For generic formal types, the syntax distinguishes between discrete, floating-point, fixed-point, access (pointer) types, etc. Some formal parameters can have default values.
To instantiate a generic unit, the programmer passes actual parameters for each formal. The generic instance then behaves just like any other unit. It is possible to instantiate generic units at run-time, for example inside a loop.
The specification of a generic package:
Instantiating the generic package:
Using an instance of a generic package:
The language syntax allows precise specification of constraints on generic formal parameters. For example, it is possible to specify that a generic formal type will only accept a modular type as the actual. It is also possible to express constraints between generic formal parameters; for example:
In this example, Array_Type is constrained by both Index_Type and Element_Type. When instantiating the unit, the programmer must pass an actual array type that satisfies these constraints.
The disadvantage of this fine-grained control is a complicated syntax, but, because all generic formal parameters are completely defined in the specification, the compiler can instantiate generics without looking at the body of the generic.
Unlike C++, Ada does not allow specialised generic instances, and requires that all generics be instantiated explicitly. These rules have several consequences:
C++ uses templates to enable generic programming techniques. The C++ Standard Library includes the Standard Template Library or STL that provides a framework of templates for common data structures and algorithms. Templates in C++ may also be used for template metaprogramming, which is a way of pre-evaluating some of the code at compile-time rather than run-time. Using template specialization, C++ Templates are considered Turing complete.
There are two kinds of templates: function templates and class templates. A function template is a pattern for creating ordinary functions based upon the parameterizing types supplied when instantiated. For example, the C++ Standard Template Library contains the function template max(x, y) that creates functions that return either x or y, whichever is larger. max() could be defined like this:
Specializations of this function template, instantiations with specific types, can be called just like an ordinary function:
The compiler examines the arguments used to call max and determines that this is a call to max(int, int). It then instantiates a version of the function where the parameterizing type T is int, making the equivalent of the following function:
This works whether the arguments x and y are integers, strings, or any other type for which the expression x < y is sensible, or more specifically, for any type for which operator< is defined. Common inheritance is not needed for the set of types that can be used, and so it is very similar to duck typing. A program defining a custom data type can use operator overloading to define the meaning of < for that type, thus allowing its use with the max() function template. While this may seem a minor benefit in this isolated example, in the context of a comprehensive library like the STL it allows the programmer to get extensive functionality for a new data type, just by defining a few operators for it. Merely defining < allows a type to be used with the standard sort(), stable_sort(), and binary_search() algorithms or to be put inside data structures such as sets, heaps, and associative arrays.
C++ templates are completely type safe at compile time. As a demonstration, the standard type complex does not define the < operator, because there is no strict order on complex numbers. Therefore, max(x, y) will fail with a compile error, if x and y are complex values. Likewise, other templates that rely on < cannot be applied to complex data unless a comparison (in the form of a functor or function) is provided. E.g.: A complex cannot be used as key for a map unless a comparison is provided. Unfortunately, compilers historically generate somewhat esoteric, long, and unhelpful error messages for this sort of error. Ensuring that a certain object adheres to a method protocol can alleviate this issue. Languages which use compare instead of < can also use complex values as keys.
The second kind of template, a class template, extends the same concept to classes. A class template specialization is a class. Class templates are often used to make generic containers. For example, the STL has a linked list container. To make a linked list of integers, one writes list<int>. A list of strings is denoted list<string>. A list has a set of standard functions associated with it, that work for any compatible parameterizing types.
A powerful feature of C++'s templates is template specialization. This allows alternative implementations to be provided based on certain characteristics of the parameterized type that is being instantiated. Template specialization has two purposes: to allow certain forms of optimization, and to reduce code bloat.
For example, consider a sort() template function. One of the primary activities that such a function does is to swap or exchange the values in two of the container's positions. If the values are large (in terms of the number of bytes it takes to store each of them), then it is often quicker to first build a separate list of pointers to the objects, sort those pointers, and then build the final sorted sequence. If the values are quite small however it is usually fastest to just swap the values in-place as needed. Furthermore, if the parameterized type is already of some pointer-type, then there is no need to build a separate pointer array. Template specialization allows the template creator to write different implementations and to specify the characteristics that the parameterized type(s) must have for each implementation to be used.
Unlike function templates, class templates can be partially specialized. That means that an alternate version of the class template code can be provided when some of the template parameters are known, while leaving other template parameters generic. This can be used, for example, to create a default implementation (the primary specialization) that assumes that copying a parameterizing type is expensive and then create partial specializations for types that are cheap to copy, thus increasing overall efficiency. Clients of such a class template just use specializations of it without needing to know whether the compiler used the primary specialization or some partial specialization in each case. Class templates can also be fully specialized, which means that an alternate implementation can be provided when all of the parameterizing types are known.
Some uses of templates, such as the max() function, were previously filled by function-like preprocessor macros (a legacy of the C programming language). For example, here is a possible max() macro:
Macros are expanded by preprocessor, before compilation proper; templates are expanded at compile time. Macros are always expanded inline; templates can also be expanded as inline functions when the compiler deems it appropriate. Thus both function-like macros and function templates have no run-time overhead.
However, templates are generally considered an improvement over macros for these purposes. Templates are type-safe. Templates avoid some of the common errors found in code that makes heavy use of function-like macros, such as evaluating parameters with side effects twice. Perhaps most importantly, templates were designed to be applicable to much larger problems than macros.
There are three primary drawbacks to the use of templates: compiler support, poor error messages, and code bloat.
Many compilers historically have poor support for templates, thus the use of templates can make code somewhat less portable. Support may also be poor when a C++ compiler is being used with a linker that is not C++-aware, or when attempting to use templates across shared library boundaries. Most modern compilers however now have fairly robust and standard template support, and the new C++ standard, C++11, further addresses these issues.
Almost all compilers produce confusing, long, or sometimes unhelpful error messages when errors are detected in code that uses templates. This can make templates difficult to develop.
Finally, the use of templates requires the compiler to generate a separate instance of the templated class or function for every permutation of type parameters used with it. (This is necessary because types in C++ are not all the same size, and the sizes of data fields are important to how classes work.) So the indiscriminate use of templates can lead to code bloat, resulting in excessively large executables. However, judicious use of template specialization and derivation can dramatically reduce such code bloat in some cases:
In simple cases templates can be transformed into generics (not causing code bloat) by creating a class getting a parameter derived from a type in compile time and wrapping a template around this class. It is a nice approach for creating generic heap-based containers.
The extra instantiations generated by templates can also cause debuggers to have difficulty working gracefully with templates. For example, setting a debug breakpoint within a template from a source file may either miss setting the breakpoint in the actual instantiation desired or may set a breakpoint in every place the template is instantiated.
Also, because the compiler needs to perform macro-like expansions of templates and generate different instances of them at compile time, the implementation source code for the templated class or function must be available (e.g. included in a header) to the code using it. Templated classes or functions, including much of the Standard Template Library (STL), if not included in header files, cannot be compiled. (This is in contrast to non-templated code, which may be compiled to binary, providing only a declarations header file for code using it.) This may be a disadvantage by exposing the implementing code, which removes some abstractions, and could restrict its use in closed-source projects.[citation needed]
The D programming language supports templates based in design on C++.
Most C++ template idioms will carry over to D without alteration, but D adds some additional functionality:
Templates in D use a different syntax than in C++: whereas in C++ template parameters are wrapped in angular brackets (Template<param1, param2>),
D uses an exclamation sign and parentheses: Template!(param1, param2).
This avoids the C++ parsing difficulties due to ambiguity with comparison operators.
If there is only one parameter, the parentheses can be omitted.
Conventionally, D combines the above features to provide compile-time polymorphism using trait-based generic programming.
For example, an input range is defined as any type that satisfies the checks performed by isInputRange, which is defined as follows:
A function that accepts only input ranges can then use the above template in a template constraint:
In addition to template metaprogramming, D also provides several features to enable compile-time code generation:
Combining the above allows generating code based on existing declarations.
For example, D serialization frameworks can enumerate a type's members and generate specialized functions for each serialized type
to perform serialization and deserialization.
User-defined attributes could further indicate serialization rules.
The import expression and compile-time function execution also allow efficiently implementing domain-specific languages.
For example, given a function that takes a string containing an HTML template and returns equivalent D source code, it is possible to use it in the following way:
Generic classes have been a part of Eiffel since the original method and language design. The foundation publications of Eiffel, use the term genericity to describe the creation and use of generic classes.
Generic classes are declared with their class name and a list of one or more formal generic parameters. In the following code, class LIST has one formal generic parameter G
The formal generic parameters are placeholders for arbitrary class names that will be supplied when a declaration of the generic class is made, as shown in the two generic derivations below, where ACCOUNT and DEPOSIT are other class names. ACCOUNT and DEPOSIT are considered actual generic parameters as they provide real class names to substitute for G in actual use.
Within the Eiffel type system, although class LIST [G] is considered a class, it is not considered a type. However, a generic derivation of LIST [G] such as LIST [ACCOUNT] is considered a type.
For the list class shown above, an actual generic parameter substituting for G can be any other available class. To constrain the set of classes from which valid actual generic parameters can be chosen, a generic constraint can be specified. In the declaration of class SORTED_LIST below, the generic constraint dictates that any valid actual generic parameter will be a class that inherits from class COMPARABLE. The generic constraint ensures that elements of a SORTED_LIST can in fact be sorted.
Support for the generics, or ""containers-of-type-T"" was added to the Java programming language in 2004 as part of J2SE 5.0. In Java, generics are only checked at compile time for type correctness. The generic type information is then removed via a process called type erasure, to maintain compatibility with old JVM implementations, making it unavailable at runtime. For example, a List<String> is converted to the raw type List. The compiler inserts type casts to convert the elements to the String type when they are retrieved from the list, reducing performance compared to other implementations such as C++ templates.
Generics were added as part of .NET Framework 2.0 in November 2005, based on a research prototype from Microsoft Research started in 1999. Although similar to generics in Java, .NET generics do not apply type erasure, but implement generics as a first class mechanism in the runtime using reification. This design choice provides additional functionality, such as allowing reflection with preservation of generic types, as well as alleviating some of the limitations of erasure (such as being unable to create generic arrays). This also means that there is no performance hit from runtime casts and normally expensive boxing conversions. When primitive and value types are used as generic arguments, they get specialized implementations, allowing for efficient generic collections and methods. As in C++ and Java, nested generic types such as Dictionary<string, List<int>> are valid types, however are advised against for member signatures in code analysis design rules.
.NET allows six varieties of generic type constraints using the where keyword including restricting generic types to be value types, to be classes, to have constructors, and to implement interfaces. Below is an example with an interface constraint:
The MakeAtLeast() method allows operation on arrays, with elements of generic type T. The method's type constraint indicates that the method is applicable to any type T that implements the generic IComparable<T> interface. This ensures a compile time error, if the method is called if the type does not support comparison. The interface provides the generic method CompareTo(T).
The above method could also be written without generic types, simply using the non-generic Array type. However, since arrays are contravariant, the casting would not be type safe, and compiler may miss errors that would otherwise be caught while making use of the generic types. In addition, the method would need to access the array items as objects instead, and would require casting to compare two elements. (For value types like types such as int this requires a boxing conversion, although this can be worked around using the Comparer<T> class, as is done in the standard collection classes.)
A notable behavior of static members in a generic .NET class is static member instantiation per run-time type (see example below).
Delphi's Object Pascal dialect acquired generics in the Delphi 2007 release, initially only with the (now discontinued) .NET compiler before being added to the native code one in the Delphi 2009 release. The semantics and capabilities of Delphi generics are largely modelled on those had by generics in .NET 2.0, though the implementation is by necessity quite different. Here's a more or less direct translation of the first C# example shown above:
As with C#, methods as well as whole types can have one or more type parameters. In the example, TArray is a generic type (defined by the language) and MakeAtLeast a generic method. The available constraints are very similar to the available constraints in C#: any value type, any class, a specific class or interface, and a class with a parameterless constructor. Multiple constraints act as an additive union.
Free Pascal implemented generics before Delphi, and with different syntax and semantics. However, work is now underway to implement Delphi generics alongside native FPC ones (see Wiki). This allows Free Pascal programmers to use generics in whatever style they prefer.
Delphi and Free Pascal example:
The type class mechanism of Haskell supports generic programming.
Six of the predefined type classes in Haskell (including Eq, the types that can be compared for equality, and Show, the types whose values can be rendered as strings) have the special property of supporting derived instances. This means that a programmer defining a new type can state that this type is to be an instance of one of these special type classes, without providing implementations of the class methods as is usually necessary when declaring class instances. All the necessary methods will be ""derived"" – that is, constructed automatically – based on the structure of the type.
For instance, the following declaration of a type of binary trees states that it is to be an instance of the classes Eq and Show:
This results in an equality function (==) and a string representation function (show) being automatically defined for any type of the form BinTree T provided that T itself supports those operations.
The support for derived instances of Eq and Show makes their methods == and show generic in a qualitatively different way from para-metrically polymorphic functions: these ""functions"" (more accurately, type-indexed families of functions) can be applied to values of various types, and although they behave differently for every argument type, little work is needed to add support for a new type. Ralf Hinze (2004) has shown that a similar effect can be achieved for user-defined type classes by certain programming techniques. Other researchers have proposed approaches to this and other kinds of genericity in the context of Haskell and extensions to Haskell (discussed below).
PolyP was the first generic programming language extension to Haskell. In PolyP, generic functions are called polytypic. The language introduces a special construct in which such polytypic functions can be defined via structural induction over the structure of the pattern functor of a regular datatype. Regular datatypes in PolyP are a subset of Haskell datatypes. A regular datatype t must be of kind * → *, and if a is the formal type argument in the definition, then all recursive calls to t must have the form t a. These restrictions rule out higher-kinded datatypes as well as nested datatypes, where the recursive calls are of a different form.
The flatten function in PolyP is here provided as an example:
Generic Haskell is another extension to Haskell, developed at Utrecht University in the Netherlands. The extensions it provides are:
The resulting type-indexed value can be specialized to any type.
As an example, the equality function in Generic Haskell:
Clean offers generic programming based PolyP and the generic Haskell as supported by the GHC>=6.0. It parametrizes by kind as those but offers overloading.
The ML family of programming languages support generic programming through parametric polymorphism and generic modules called functors.
Both Standard ML and OCaml provide functors, which are similar to class templates and to Ada's generic packages. Scheme syntactic abstractions also have a connection to genericity – these are in fact a superset of templating à la C++.
A Verilog module may take one or more parameters, to which their actual values are assigned upon the instantiation of the module. One example is a generic register array where the array width is given via a parameter. Such the array, combined with a generic wire vector, can make a generic buffer or memory module with an arbitrary bit width out of a single module implementation.
VHDL, being derived from Ada, also have generic ability.
"
"98","Many programming language type systems support subtyping. For instance, if the type Cat is a subtype of Animal, then an expression of type Cat can be used wherever an expression of type Animal is used. Variance refers to how subtyping between more complex types relates to subtyping between their components. For example, how should a list of Cats relate to a list of Animals? Or how should a function returning Cat relate to a function returning Animal? Depending on the variance of the type constructor, the subtyping relation of the simple types may be either preserved, reversed, or ignored for the respective complex types. In the OCaml programming language, for example, ""list of Cat"" is a subtype of ""list of Animal"" because the list constructor is covariant. Meaning, the subtyping relation of the simple types are preserved for the complex types. While ""function from Animal to String"" is a subtype of ""function from Cat to String"" because the function type constructor is contravariant in the argument type. Here the subtyping relation of the simple types is reversed for the complex types.
A programming language designer will consider variance when devising typing rules for language features such as arrays, inheritance, and generic datatypes. By making type constructors covariant or contravariant instead of invariant, more programs will be accepted as well-typed. On the other hand, programmers often find contravariance unintuitive, and accurately tracking variance to avoid runtime type errors can lead to complex typing rules. In order to keep the type system simple and allow useful programs, a language may treat a type constructor as invariant even if it would be safe to consider it variant, or treat it as covariant even though that could violate type safety.
Within the type system of a programming language, a typing rule or a type constructor is:
The article considers how this applies to some common type constructors.
For example, in C#, if  Cat is a subtype of Animal, then:
The variance of a C# generic interface is declared by placing the out (covariant) or in (contravariant) attribute on (zero or more of) its type parameters. For each so-marked type parameter, the compiler conclusively verifies, with any violation being fatal, that such use is globally consistent. The above interfaces are declared as IEnumerable<out T>, Action<in T>, and IList<T>. Types with more than one type parameter may specify different variances on each type parameter. For example, the delegate type Func<in T, out TResult> represents a function with a contravariant input parameter of type T and a covariant return value of type TResult.
The typing rules for interface variance ensure type safety. For example, an Action<T> represents a first-class function expecting an argument of type T, and a function that can handle any type of animal can always be used instead of one that can only handle cats.
Read-only data types (sources) can be covariant; write-only data types (sinks) can be contravariant. Mutable data types which act as both sources and sinks should be invariant. To illustrate this general phenomenon, consider the array type.  For the type Animal
 we can make the type Animal[]
, which is an ""array of animals"".  For the purposes of this example, this array supports both reading and writing elements.
We have the option to treat this as either:
If we wish to avoid type errors, then only the third choice is safe. Clearly, not every Animal[]
 can be treated as if it were a Cat[]
, since a client reading from the array will expect a Cat
, but an Animal[]
 may contain e.g. a Dog
. So the contravariant rule is not safe.
Conversely, a Cat[]
 cannot be treated as an Animal[]
. It should always be possible to put a Dog
 into an Animal[]
. With covariant arrays this cannot be guaranteed to be safe, since the backing store might actually be an array of cats. So the covariant rule is also not safe—the array constructor should be invariant. Note that this is only an issue for mutable arrays; the covariant rule is safe for immutable (read-only) arrays.
Early versions of Java and C# did not include generics, also termed parametric polymorphism. In such a setting, making arrays invariant rules out useful polymorphic programs.
For example, consider writing a function to shuffle an array, or a function that tests two arrays for equality using the Object.equals
 method on the elements. The implementation does not depend on the exact type of element stored in the array, so it should be possible to write a single function that works on all types of arrays. It is easy to implement functions of type:
However, if array types were treated as invariant, it would only be possible to call these functions on an array of exactly the type Object[]
. One could not, for example, shuffle an array of strings.
Therefore, both Java and C# treat array types covariantly.
For instance, in Java String[]
 is a subtype of Object[]
, and in C# string[] is a subtype of object[].
As discussed above, covariant arrays lead to problems with writes into the array. Java and C# deal with this by marking each array object with a type when it is created. Each time a value is stored into an array, the execution environment will check that the run-time type of the value is equal to the run-time type of the array. If there is a mismatch, an ArrayStoreException
 (Java) or ArrayTypeMismatchException (C#) is thrown:
In the above example, one can read from the array (b) safely. It is only trying to write to the array that can lead to trouble.
One drawback to this approach is that it leaves the possibility of a run-time error that a stricter type system could have caught at compile-time. Also, it hurts performance because each write into an array requires an additional run-time check.
With the addition of generics, Java and C# now offer ways to write this kind of polymorphic function without relying on covariance. The array comparison and shuffling functions can be given the parameterized types
Alternatively, to enforce that a C# method accesses a collection in a read-only way, one can use the interface  IEnumerable<object> instead of passing it an array object[].
Languages with first-class functions have function types like ""a function expecting a Cat and returning an Animal"" (written Cat -> Animal in OCaml syntax or Func<Cat,Animal> in C# syntax).
Those languages also need to specify when one function type is a subtype of another—that is, when it is safe to use a function of one type in a context that expects a function of a different type. 
It is safe to substitute a function f for a function g if f accepts a more general type of arguments and returns a more specific type than g.
For example, both functions of type Cat -> Cat and Animal -> Animal can be used wherever a Cat -> Animal was expected. (One can compare this to the robustness principle of communication: ""be liberal in what you accept and conservative in what you produce."") The general rule is:
T1≤S1S2≤T2S1→S2≤T1→T2{\displaystyle T_{1}\leq S_{1}\quad S_{2}\leq T_{2} \over S_{1}\rightarrow S_{2}\leq T_{1}\rightarrow T_{2}}
In other words, the → type constructor is contravariant in the input type and covariant in the output type. This rule was first stated formally by John C. Reynolds, and further popularized in a paper by Luca Cardelli.
When dealing with functions that take functions as arguments, this rule can be applied several times. For example, by applying the rule twice, we see that (A'→B)→B ≤ (A→B)→B if A'≤A. In other words, the type (A→B)→B is covariant in the A position. For complicated types it can be confusing to mentally trace why a given type specialization is or isn't type-safe, but it is easy to calculate which positions are co- and contravariant: a position is covariant if it is on the left side of an even number of arrows applying to it.
When a subclass overrides a method in a superclass, the compiler must check that the overriding method has the right type. While some languages require that the type exactly matches the type in the superclass (invariance), it is also type safe to allow the overriding method to have a ""better"" type. By the usual subtyping rule for function types, this means that the overriding method should return a more specific type (return type covariance), and accept a more general argument (argument type contravariance). In UML notation, the possibilities are as follows:
For a concrete example, suppose we are writing a class to model an animal shelter. We assume that Cat
 is a subclass of Animal
, and that we have a base class (using Java syntax)
Now the question is: if we subclass AnimalShelter
, what types are we allowed to give to getAnimalForAdoption
 and putAnimal
?
In a language which allows covariant return types, a derived class can override the getAnimalForAdoption
 method to return a more specific type:
Among mainstream OO languages, Java and C++ support covariant return types, while C# does not. Adding the covariant return type was one of the first modifications of the C++ language approved by the standards committee in 1998.Scala and D  also support covariant return types.
Similarly, it is type safe to allow an overriding method to accept a more general argument than the method in the base class:
Not many object-oriented languages actually allow this. C++ and Java would interpret this as an unrelated method with an overloaded name.
However, Sather supported both covariance and contravariance. Calling convention for overridden methods are covariant with out arguments and return values, and contravariant with normal arguments (with the mode in).
Uniquely among mainstream languages, Eiffel allows the arguments of an overriding method to have a more specific type than the method in the superclass (argument type covariance). Thus, the Eiffel version of the following code would type check, with putAnimal
 overriding the method in the base class:
This is not type safe. By up-casting a CatShelter
 to an AnimalShelter
, one can try to place a dog in a cat shelter. That does not meet CatShelter
 argument restrictions, and will result in a runtime error. The lack of type safety (known as the ""catcall problem"" in the Eiffel community) has been a long-standing issue. Over the years, various combinations of global static analysis, local static analysis, and new language features  have been proposed to remedy it, and these have been implemented in some Eiffel compilers.
Despite the type safety problem, the Eiffel designers consider covariant argument types crucial for modeling real world requirements. The cat shelter illustrates a common phenomenon: it is a kind of animal shelter but has additional restrictions, and it seems reasonable to use inheritance and restricted argument types to model this. In proposing this use of inheritance, the Eiffel designers reject the Liskov substitution principle, which states that objects of subclasses should always be less restricted than objects of their superclass.
One other instance of a mainstream language allowing covariance in method arguments is PHP in regards to class constructors.  In the following example, the __construct() method is accepted, despite the method argument being covariant to the parent's method argument.  Were this method anything other than __construct(), an error would occur:
Another example where covariant arguments seem helpful is so-called binary methods, i.e. methods where the argument is expected to be of the same type as the object the method is called on. An example is the compareTo
 method: a.compareTo(b)
 checks whether a
 comes before or after b
 in some ordering, but the way to compare, say, two rational numbers will be different from the way to compare two strings. Other common examples of binary methods include equality tests, arithmetic operations, and set operations like subset and union.
In older versions of Java, the comparison method was specified as an interface Comparable
:
The drawback of this is that the method is specified to take an argument of type Object
. A typical implementation would first down-cast this argument (throwing an error if it is not of the expected type):
In a language with covariant arguments, the argument to compareTo
 could be directly given the desired type RationalNumber
, hiding the typecast. (Of course, this would still give a runtime error if compareTo
 was then called on e.g. a String
).
Other language features can provide the apparent benefits of covariant arguments while preserving Liskov substitutability.
In a language with generics (a.k.a. parametric polymorphism) and bounded quantification, the previous examples can be written in a type-safe way. Instead of defining AnimalShelter
, we define a parameterized class Shelter<T>
. (One drawback of this is that the implementer of the base class needs to foresee which types will need to be specialized in the subclasses).
Similarly, in recent versions of Java the Comparable
 interface has been parameterized, which allows the downcast to be omitted in a type-safe way:
Another language feature that can help is multiple dispatch. 
One reason that binary methods are awkward to write is that in a call like a.compareTo(b)
, selecting the correct implementation of compareTo
 really depends on the runtime type of both a
 and b
, but in a conventional OO language only the runtime type of a
 is taken into account. In a language with Common Lisp Object System (CLOS)-style multiple dispatch, the comparison method could be written as a generic function where both arguments are used for method selection.
Giuseppe Castagna observed that in a typed language with multiple dispatch, a generic function can have some arguments which control dispatch and some ""left-over"" arguments which do not. Because the method selection rule chooses the most specific applicable method, if a method overrides another method, then the overriding method will have more specific types for the controlling arguments. On the other hand, to ensure type safety the language still must require the left-over arguments to be at least as general. 
Using the previous terminology, types used for runtime method selection are covariant while types not used for runtime method selection of the method are contravariant. Conventional single-dispatch languages like Java also obey this rule: there only one argument is used for method selection (the receiver object, passed along to a method as the hidden argument this
), and indeed the type of this
 is more specialized inside overriding methods than in the superclass.
Castagna suggests that examples where covariant argument types are superior, particularly binary methods, should be handled using multiple dispatch which is naturally covariant.
However, most programming languages do not support multiple dispatch.
The following table summarizes the rules for overriding methods in the languages discussed above.
In programming languages that support generics (a.k.a. parametric polymorphism), the programmer can extend the type system with new constructors. For example, a C# interface like IList<T> makes it possible to construct new types like IList<Animal> or IList<Cat>. The question then arises what the variance of these type constructors should be.
There are two main approaches. In languages with declaration-site variance annotations (e.g., C#), the programmer annotates the definition of a generic type with the intended variance of its type parameters. With use-site variance annotations (e.g., Java), the programmer instead annotates the places where a generic type is instantiated.
The most popular languages with declaration-site variance annotations are C# (using the keywords out and in),
and  Scala and OCaml (using the keywords + and -).
C# only allows variance annotations for interface types, while Scala and OCaml allows them for both interface types and concrete data types.
In C#, each type parameter of a generic interface can be marked covariant (out), contravariant (in), or invariant (no annotation). For example, we can define an interface IEnumerator<T> of read-only iterators, and declare it to be covariant (out) in its type parameter.
With this declaration, IEnumerator will be treated as covariant in its type argument, e.g. IEnumerator<Cat> is a subtype of IEnumerator<Animal>.
The typechecker enforces that each method declaration in an interface only mentions the type parameters in a way consistent with the in/out annotations. That is, a parameter that was declared covariant must not occur in any contravariant positions (where a position is contravariant if it occurs under an odd number of contravariant type constructors). The precise rule is that the return types of all methods in the interface must be valid covariantly and all the method argument types must be valid contravariantly, where valid S-ly is defined as follows:
As an example of how these rules apply, consider the IList<T> interface. 
The argument type T of Insert must be valid contravariantly, i.e. the type parameter T must not be tagged out. Similarly, the result type IEnumerator<T> of GetEnumerator must be valid covariantly, i.e. (since IEnumerator is a covariant interface) the type T must be valid covariantly, i.e. the type parameter T must not be tagged in. This shows that the interface IList is not allowed to be marked either co- or contravariant.
In the common case of a generic data structure such as IList, these restrictions mean that an out parameter can only be used for methods getting data out of the structure, and an in parameter can only be used for methods putting data into the structure, hence the choice of keywords.
C# allows variance annotations on the parameters of interfaces, but not the parameters of classes. Because fields in C# classes are always mutable, variantly parameterized classes in C# would not be very useful. But languages which emphasize immutable data can make good use of covariant data types. For example, both in Scala and OCaml the immutable list type is covariant: List[Cat] is a subtype of List[Animal].
Scala's rules for checking variance annotations are essentially the same as C#'s. However, there are some idioms that apply to immutable datastructures in particular. They are illustrated by the following (excerpt from the) definition of the List[A] class.
First, class members that have a variant type must be immutable. Here, head has the type A, which was declared covariant (+), and indeed head was declared as a method (def). Trying to declare it as a mutable field (var) would be rejected as type error.
Second, even if a data structure is immutable, it will often have methods where the parameter type occurs contravariantly. For example, consider the method :: which adds an element to the front of a list. (The implementation works by creating a new object of the similarly-named class ::, the class of nonempty lists). The most obvious type to give it would be 
However, this would be a type error, because the covariant parameter A appears in a contravariant position (as a function argument). But there is a trick to get around this problem. We give :: a more general type, which allows adding an element of any type B 
as long as B is a supertype of A. Note that this relies on List being covariant, since 
this  has type List[A] and we treat it as having type List[B].  At first glance it may not be obvious that the generalized type is sound, but if the programmer starts out with the simpler type declaration, the type errors will point out the place that  needs to be generalized.
It is possible to design a type system where the compiler automatically infers the best possible variance annotations for all datatype parameters. However, the analysis can get complex for several reasons. First, the analysis is nonlocal since the variance of an interface I
 depends on the variance of all interfaces that I
 mentions.  Second, in order to get unique best solutions the type system must allow bivariant parameters (which are simultaneously co- and contravariant). And finally, the variance of type parameters should arguably be a deliberate choice by the designer of an interface, not something that just happens.
For these reasons most languages do very little variance inference. C# and Scala do not infer any variance annotations at all. OCaml can infer the variance of parameterized concrete datatypes, but the programmer must explicitly specify the variance of abstract types (interfaces).
For example, consider an OCaml datatype T which wraps a function
The compiler will automatically infer that T is contravariant in the first parameter, and covariant in the second. The programmer can also provide explicit annotations, which the compiler will check are satisfied. Thus the following declaration is equivalent to the previous one:
Explicit annotations in OCaml become useful when specifying interfaces. For example, the standard library interface Map.S for association tables include an annotation saying that the map type constructor is covariant in the result type.
This ensures that e.g. cat IntMap.t is a subtype of animal IntMap.t.
One drawback of the declaration-site approach is that many interface types must be made invariant. For example, we saw above that IList needed to be invariant, because it contained both Insert and GetEnumerator. In order to expose more variance, the API designer could provide additional interfaces which provide subsets of the available methods (e.g. an ""insert-only list"" which only provides Insert). However this quickly becomes unwieldy.
Use-site variance annotations aim to give users of a class more opportunities for subtyping without requiring the designer of the class to define multiple interfaces with different variance. Instead, each time a class or interface is used in a type declaration, the programmer can indicate that only a subset of the methods will be used. In effect, each definition of a class also makes available interfaces for the covariant and contravariant parts of that class. Therefore, the designer of the class no longer needs to take variance into account, increasing re-usability.
Java provides use-site variance annotations through wildcards, a restricted form of bounded existential types. A parameterized type can be instantiated by a wildcard ?
 together with an upper or lower bound, e.g. List<? extends Animal>
 or List<? super Animal>
. (A an unbounded wildcard like List<?>
 is equivalent to List<? extends Object>
). Such a type  represents List<X>
 for some unknown type X
 which satisfies the bound. For example, if l
 has type List<? extends Animal>
, then the typechecker will accept
because the type X
 is known to be a subtype of Animal
, but
will be rejected as a type error since an Animal
 is not necessarily an X
. In general, given some interface I<T>
, a reference to a I<? extends A>
 forbids using  methods from the interface where T
 occurs contravariantly in the type of the method. Conversely, if l
 had type List<? super Animal>
 one could call l.add
 but not l.get
.
While plain generic types in Java are invariant (e.g. there is no subtyping relationship between List<Cat>
 and List<Animal>
), wildcard types can be made more specific by specifying a tighter bound, for example List<? extends Cat>
 is a subtype of List<? extends Animal>
. This shows that wildcard types are covariant in their upper bounds (and also contravariant in their lower bounds). In total, given a wildcard type like C<? extends T>
, there are three ways to form a subtype: by specializing the class C
, by specifying a tighter bound T
, or by replacing the wildcard ?
 by a specific type (see figure).
By combining two steps of subtyping, it is therefore possible to e.g. pass an argument of type List<Cat>
 to a method expecting a List<? extends Animal>
. This is exactly the kind of programs that covariant interface types allow. The type List<? extends Animal>
 acts as an interface type containing only the covariant methods of List<T>
, but the implementer of List<T>
 did not have to define it ahead of time. This is use-site variance.
In the common case of a generic data structure IList, covariant parameters are used for methods getting data out of the structure, and contravariant parameters for methods putting data into the structure. The mnemonic for Producer Extends, Consumer Super (PECS), from the book Effective Java by Joshua Bloch gives an easy way to remember when to use covariance and contravariance.
Wildcards are flexible, but there is a drawback. While use-site variance means that API designers need not consider variance of type parameters to interfaces, they must often instead use more complicated method signatures. A common example involves the Comparable interface. Suppose we want to write a function that finds the biggest element in a collection. The elements need to implement the compareTo
 method, so a first try might be
However, this type is not general enough—one can find the max of a Collection<Calendar>
, but not a Collection<GregorianCalendar>
. The problem is that GregorianCalendar does not implement Comparable<GregorianCalendar>
, but instead the (better) interface Comparable<Calendar>
. In Java, unlike in C#, Comparable<Calendar>
 is not considered a subtype of Comparable<GregorianCalendar>
. Instead the type of max
 has to be modified:
The bounded wildcard ? super T
 conveys the information that max
 calls only contravariant methods from the Comparable
 interface. This particular example is frustrating because all the methods in Comparable
 are contravariant, so that condition is trivially true. A declaration-site system could handle this example with less clutter by annotating only the definition of Comparable
.
Use-site variance annotations provide additional flexibility, allowing more programs to type-check. However, they have been criticized for the complexity they add to the language, leading to complicated type signatures and error messages.
One way to assess whether the extra flexibility is useful is to see if it is used in existing programs. A survey of a large set of Java libraries found that 39% of wildcard annotations could have been directly replaced by a declaration-site annotations. Thus the remaining 61% is an indication on places where Java benefits from having the use-site system available.
In a declaration-site language, libraries must either expose less variance, or define more interfaces. For example, the Scala Collections library defines three separate interfaces for classes which employ covariance: a covariant base interface containing common methods, an invariant mutable version which adds side-effecting methods, and a covariant immutable version which may specialize the inherited implementations to exploit structural sharing. This design works well with declaration-site annotations, but the large number of interfaces carry a complexity cost for clients of the library. And modifying the library interface may not be an option—in particular, one goal when adding generics to Java was to maintain binary backwards compatibility.
On the other hand, Java wildcards are themselves complex. In a conference presentationJoshua Bloch criticized them as being too hard to understand and use, stating that when adding support for closures ""we simply cannot afford another wildcards"". Early versions of Scala used use-site variance annotations but programmers found them difficult to use in practice, while declaration-site annotations were found to be very helpful when designing classes. Later versions of Scala added Java-style existential types and wildcards; however, according to Martin Odersky, if there were no need for interoperability with Java then these would probably not have been included.
Ross Tate argues that part of the complexity of Java wildcards is due to the decision to encode use-site variance using a form of existential types. The original proposals used special-purpose syntax for variance annotations, writing List<+Animal>
 instead of Java's more verbose List<? extends Animal>
.
Since wildcards are a form of existential types they can be used for more things than just variance. A type like List<?>
 (""some type of list"") lets objects be passed to methods or stored in fields without exactly specifying their type parameters. This is particularly valuable for classes such as Class where most of the methods do not mention the type parameter.
However, type inference for existential types is a difficult problem. For the compiler implementer, Java wildcards raise issues with type checker termination, type argument inference, and ambiguous programs. (In general it is undecidable whether a Java program using generics is well-typed or not, so any type checker will have to go into an infinite loop or time out for some programs.) For the programmer, it leads to complicated type error messages. Java typechecks wildcard types by replacing the wildcards with fresh type variables (so-called capture conversion). This can make error messages harder to read, because they refer to type variables that the programmer did not directly write. For example, trying to add a Cat
 to a List<? extends Animal>
 will give an error like
Since both declaration-site and use-site annotations can be useful, some type systems provide both.
These terms come from the notion of covariant and contravariant functors in category theory. Consider the category C{\displaystyle C} whose objects are types and whose morphisms represent the subtype relationship  ≤. (This is an example of how any partially ordered set can be considered as a category.) Then for example the function type constructor takes two types p and r and creates a new type p → r; so it takes objects in C2{\displaystyle C^{2}} to objects in C{\displaystyle C}. By the subtyping rule for function types this operation reverses ≤ for the first argument and preserves it for the second, so it is a contravariant functor in the first argument and a covariant functor in the second."
"99","In mathematics and computer science, a higher-order function (also functional, functional form or functor) is a function that does at least one of the following:
All other functions are first-order functions.  In mathematics higher-order functions are also termed operators or functionals.  The differential operator in calculus is a common example, since it maps a function to its derivative, also a function. Higher-order functions should not be confused with other uses of the word ""functor"" throughout mathematics, see Functor (disambiguation).
In the untyped lambda calculus, all functions are higher-order; in a typed lambda calculus, from which most functional programming languages are derived, higher-order functions that take one function as argument are values with types of the form (τ1→τ2)→τ3{\displaystyle (\tau _{1}\to \tau _{2})\to \tau _{3}}.
The examples are not intended to compare and contrast programming languages, but to serve as examples of higher-order function syntax
In the following examples, the higher-order function twice takes a function, and applies the function to some value twice. If twice has to be applied several times for the same f it preferably should return a function rather than a value. This is in line with the ""don't repeat yourself"" principle.
Or more quickly:
In Clojure, ""#"" starts a lambda expression, and ""%"" refers to the next function argument.
In this Scheme example, the higher-order function (f x) is used to implement currying. It takes a single argument and returns a function. The evaluation of the expression ((f 3) 7) first returns a function after evaluating (f 3). The returned function is (lambda (y) (+ 3 y)). Then, it evaluates the returned function with 7 as the argument, returning 10. This is equivalent to the expression (add 3 7), since (f x) is equivalent to the curried form of (add x y).
In this Erlang example, the higher-order function or_else/2 takes a list of functions (Fs) and argument (X). It evaluates the function F with the argument X as argument. If the function F returns false then the next function in Fs will be evaluated. If the function F returns {false,Y} then the next function in Fs with argument Y will be evaluated. If the function F returns R the higher-order function or_else/2 will return R. Note that X, Y, and R can be functions. The example returns false.

In Elixir, you can mix module definitions and anonymous functions
Alternatively, we can also compose using pure anonymous functions.

Notice a function literal can be defined either with an identifier (twice) or anonymously (assigned to variable f). Run full program on Go Playground!
With generic lambdas provided by C++14:
Or, using std::function in C++11 :
In Perl 6, all code objects are closures and therefore can reference inner ""lexical"" variables from an outer scope because the lexical variable is ""closed"" inside of the function. Perl 6 also supports ""pointy block"" syntax for lambda expressions which can be assigned to a variable or invoked anonymously.
Tcl uses apply command to apply an anonymous function (since 8.6).
The XACML standard defines higher-order functions in the standard to apply a function to multiple values of attribute bags.
The list of higher-order functions is can be found here.
Function pointers in languages such as C and C++ allow programmers to pass around references to functions. The following C code computes an approximation of the integral of an arbitrary function:
The qsort function from the C standard library uses a function pointer to emulate the behavior of a higher-order function.
Macros can also be used to achieve some of the effects of higher order functions.  However, macros cannot easily avoid the problem of variable capture; they may also result in large amounts of duplicated code, which can be more difficult for a compiler to optimize.  Macros are generally not strongly typed, although they may produce strongly typed code.
In other imperative programming languages, it is possible to achieve some of the same algorithmic results as are obtained via higher-order functions by dynamically executing code (sometimes called Eval or Execute operations) in the scope of evaluation.  There can be significant drawbacks to this approach:
In object-oriented programming languages that do not support higher-order functions, objects can be an effective substitute. An object's methods act in essence like functions, and a method may accept objects as parameters and produce objects as return values. Objects often carry added run-time overhead compared to pure functions, however, and added boilerplate code for defining and instantiating an object and its method(s). Languages that permit stack-based (versus heap-based) objects or structs can provide more flexibility with this method.
An example of using a simple stack based record in Free Pascal with a function that returns a function:
The function a() takes a Txy record as input and returns the integer value of the sum of the record's x and y fields (3 + 7).
Defunctionalization can be used to implement higher-order functions in languages that lack  first-class functions:
In this case, different types are used to trigger different functions via function overloading. The overloaded function in this example has the signature auto apply."
"100","In computer science, a type signature or type annotation defines the inputs and outputs for a function, subroutine or method. A type signature includes the number of arguments, the types of arguments and the order of the arguments contained by a function. A type signature is typically used during overload resolution for choosing the correct definition of a function to be called among many overloaded forms.
In C and C++, the type signature is declared by what is commonly known as a function prototype. In C/C++, a function declaration reflects its use; for example, a function pointer that would be invoked as:
has the signature:
In Erlang, type signatures may be optionally declared, as:
For example:
A type signature in the Haskell programming language is generally written in the following format:
Notice that the type of the result can be regarded as everything past the first supplied argument. This is a consequence of currying, which is made possible by Haskell's support for first-class functions; this function requires two inputs where one argument supplied and the function is ""curried"" to produce a function for the argument not supplied. Thus calling f x, where f :: a -> b -> c, yields a new function f2 :: b -> c that can be called f2 b to produce c.
The actual type specifications can consist of an actual type, such as Integer, or a general type variable that is used in parametric polymorphic functions, such as a, or b, or anyType. So we can write something like:
functionName :: a -> a -> ... -> a
Since Haskell supports higher-order functions, functions can be passed as arguments. This is written as:
functionName :: (a -> a) -> a
This function takes in a function with type signature a -> a and returns data of type a out.
In the Java virtual machine, internal type signatures are used to identify methods and classes at the level of the virtual machine code.
Example: The method String String.substring(int, int)
 is represented in bytecode as Ljava/lang/String/substring(II)Ljava/lang/String;
. The signature of main() method looks like this:
And in the disassembled bytecode, it takes the form of Lsome/package/Main/main:([Ljava/lang/String;)V

The method signature for the main() method contains three modifiers:
A function signature consists of the function prototype.  It specifies the general information about a function like the name, scope and parameters.  Many programming languages use name mangling in order to pass along more semantic information from the compilers to the linkers.  In addition to mangling, there is an excess of information in a function signature (stored internally to most compilers) which is not readily available, but may be accessed.
Understanding the notion of a function signature is an important concept for all computer science studies.
The practice of multiple inheritance requires consideration of the function signatures to avoid unpredictable results.
Computer science theory, and the concept of polymorphism in particular, make much use of the concept of function signature.
In the C programming language signature is roughly equivalent to its prototype definition.
The term ""signature"" may carry other meanings in computer science. For example:
In computer programming, especially object-oriented programming, a method is commonly identified by its unique method signature, which usually includes the method name, and the number, types and order of its parameters. A method signature is the smallest type of a method.
In C/C++, the method signature is the method name and the number and type of its parameters, but it is possible to have a last parameter that consists of an array of values:
Manipulation of these parameters can be done by using the routines in the standard library header <stdarg.h>.
Similar to the C syntax, C# sees as the method signature its name and the number and type of its parameters, where the last parameter may be an array of values:
In the Java programming language, a method signature is the method name and the number, type and order of its parameters. Return types and thrown exceptions are not considered to be a part of the method signature. 
For example, the following two methods have distinct signatures:
The following three methods do have the same signatures and are considered the same, as only the return value differs. The name of the parameter is not part of the method signature and is ignored by the compiler for checking method uniqueness.
In the Objective-C programming language, method signatures for an object are declared in the interface header file. For example,
defines a method initWithInt that returns a general object (an id) and takes one integer argument. Objective-C only requires a type in a signature to be explicit when the type is not id; this signature is equivalent:"
"101","In computing, a procedural parameter is a parameter of a procedure that is itself a procedure.
This concept is an extremely powerful and versatile programming tool, because it allows programmers to modify certain steps of a library procedure in arbitrarily complicated ways, without having to understand or modify the code of that procedure.
This tool is particularly effective and convenient in languages that support local function definitions, such as Pascal and the modern GNU dialect of C. It is even more so when function closures are available.  The same functionality (and more) is provided by objects in object oriented programming languages, but at a significantly higher cost.
Procedural parameters are somewhat related to the concepts of first-class function and anonymous function, but is distinct from them.  These two concepts have more to do with how functions are defined, rather than how they are used.
In most languages that provide this feature, a procedural parameter f of a subroutine P can be called inside the body of P as if it were an ordinary procedure:
When calling the subroutine P, one must give it one argument, that must be some previously defined function compatible with the way P uses its parameter f.  For example, if we define
then we may call P (plus), and the result will be plus(6,3) * plus(2,1) =  (6 + 3)*(2 + 1) = 27. On the other hand, if we define
then the call P (quot) will return quot(6,3)*quot(2,1) = (6/3)*(2/1) = 4.  Finally, if we define
then the call P (evil) will not make much sense, and may be flagged as an error.
Some programming languages that have this feature may allow or require a complete type declaration for each procedural parameter f, including the number and type of its arguments, and the type of its result, if any.  For example, in the C programming language the example above could be written as
In principle, the actual function actf that is passed as argument when P is called must be type-compatible with the declared type of the procedure parameter f. This usually means that actf and f must return the same type of result, must have the same number of arguments, and corresponding arguments must have the same type. The names of the arguments need not be the same, however, as shown by the plus and quot examples above. However, some programming languages may be more restrictive or more liberal in this regard.
In languages that allow procedural parameters, the scoping rules are usually defined in such a way that procedural parameters are executed in their native scope.  More precisely, suppose that the function actf is passed as argument to P, as its procedural parameter f; and f is then called from inside the body of P. While actf is being executed, it sees the environment of its definition.[example  needed]
The implementation of these scoping rules is not trivial.  By the time that actf is finally executed, the activation records where its environment variables live may be arbitrarily deep in the stack.  This is the so-called downwards funarg problem.
The concept of procedural parameter is best explained by examples. A typical application is the following generic implementation of the insertion sort algorithm, that takes two integer parameters a,b and two procedural parameters prec, swap:
This procedure can be used to sort the elements x[a] through x[b] of some array x, of arbitrary type, in a user-specified order. The parameters prec and swap should be two functions, defined by the client, both taking two integers r, s between a and b. The prec function should return true if and only if the data stored in x[r] should precede the data stored in x[s], in the ordering defined by the client. The swap function should exchange the contents of x[r] and x[s], and return no result.
By the proper choice of the functions prec and swap, the same isort procedure can be used to reorder arrays of any data type, stored in any medium and organized in any data structure that provides indexed access to individual array elements. (Note however that there are sorting algorithms that are much more efficient than insertion sort for large arrays.)
For instance, we can sort an array z of 20 floating-point numbers, z through z in increasing order by calling isort (1, 20,zprec,zswap), where the functions zprec and zswap are defined as
For another example, let M be a matrix of integers with 10 rows and 20 columns, with indices starting at 1. The following code will rearrange the elements in each row so that all the even values come before all odd values:
Note that the effects of eoprec and eoswap depend on the row number i, but the isort procedure does not need to know that.
The following example uses isort to define a procedure vecsort that takes an integer n and an integer vector v with elements v through v[n−1] and sorts them in either increasing or decreasing order, depending on whether a third parameter incr is true or false, respectively:
Note the use of nested function definitions to get a function vprec whose effect depends on the parameter incr passed to vecsort. In languages that do not allow nested function definitions, like standard C, obtaining this effect would require rather complicated and/or thread-unsafe code.

The following example illustrates the use of procedural parameters to process abstract data structures independently of their concrete implementation.  The problem is to merge two ordered sequences of records into a single sorted sequence, where the nature of the records and the ordering criterion can be chosen by the client. The following implementation assumes only that each record can be referenced by a memory address, and there is a ""null address"" Λ that is not the address of any valid record.  The client must provide the addresses A, B of the first records in each sequence, and functions prec, next, and append, to be described later.
The function prec should take the addresses r, s of two records, one from each sequence, and  return true if the first record should come before the other in the output sequence.  The function nextA should take the address of a record from the first sequence, and return the address of the next record in the same sequence, or Λ if there is none. The function appendA should append the first record from sequence A to the output sequence; its arguments are the address A of the record to be appended, and the address fin of the last record of the output list (or Λ if that list is still empty). The procedure appendA should return the updated address of the final element of the output list. The procedures nextB and appendB are analogous for the other input sequence.
To illustrate the use of the generic merge procedure, here is the code for merging two simple linked lists, starting with nodes at addresses R, S.  Here we assume that each record x contains an integer field x.INFO and an address field x.NEXT that points to the next node;  where the info fields are in increasing order in each list. The input lists are dismantled by the merge, and their nodes are used to build the output list.
The following code illustrates the independence of the generic merge procedure from the actual representation of the sequences. It merges the elements of two ordinary arrays U through U[m−1] and V through V[n−1] of floating-point numbers, in decreasing order. The input arrays are not modified, and the merged sequence of values is stored into a third vector W through W[m+n−1].  As in the C programming language, we assume that the expression ""&V"" yields the address of variable V, ""*p"" yields the variable whose address is the value of p, and that ""&(X[i])"" is equivalent to ""&amp(X) + i"" for any array X and any integer i.
The following procedure computes the approximate integral ∫ab{\displaystyle \textstyle \int _{a}^{b}} f (x) dx of a given real-valued function f over a given interval [a,b] of the real line.  The numerical method used is  the trapezium rule with a given number n of steps; the real numbers are approximated by floating-point numbers.
Consider now the problem of integrating a given function g, with two arguments, over a disk D with given center (xc,yc) and given radius R.  This problem can be reduced to two nested single-variable integrals by  the change of variables
The following code implements the right-hand-side formula:
This code uses the integration procedure Intg in two levels. The outer level (last line) uses Intg to compute the integral of gring(z) for z varying from 0 to R. The inner level (next-to-last line) defines gring(z) as being the line integral of g(x,y) over the circle with center (xc,yc) and radius z.
Procedural parameters were invented before the age of electronic computers,  by mathematician Alonzo Church, as part of his lambda calculus model of computation. 
Procedural parameters as a programming language feature were introduced by ALGOL 60.  In fact, ALGOL 60 had a powerful ""call by name"" parameter-passing mechanism that could simplify some uses of procedural parameters; see Jensen's Device.
Procedural parameters were an essential feature of the LISP programming language, which also introduced the concept of function closure or funarg. The C programming language allows function pointers to be passed as parameters, which accomplish the same end, and are often used as callbacks in event-driven programming and as error handlers.  However, only a few modern C compilers allow nested function definitions, so that its other uses are relatively uncommon. Procedural parameters were provided also in Pascal, together with nested procedure definitions; however, since standard Pascal did not allow separate compilation, the feature was little used in that language, too."
"102","This is an overview of Fortran 95 language features. Included are the additional features of TR-15581:Enhanced Data Type Facilities, that have been universally implemented. Old features that have been superseded by new ones are not described — few of those historic features are used in modern programs although most have been retained in the language to maintain backward compatibility. Although the current standard is Fortran 2008, even many of those features first introduced into Fortran 2003 are still being implemented. The additional features of Fortran 2003 and Fortran 2008 are described by Metcalf, Reid and Cohen.
Fortran is case-insensitive. The convention of writing Fortran keywords in upper case and all other names in lower case is adopted in this article; except, by way of contrast, in the input/output descriptions (Data transfer and Operations on external files).
The basic component of the Fortran language is its character set. Its members are
Tokens that have a syntactic meaning to the compiler are built from those components. There are six classes of tokens: 
From the tokens, statements are built. These can be coded using the new free source form which does not require positioning in a rigid column structure: 
Note the trailing comments and the trailing continuation mark. There may be 39 continuation lines, and 132 characters per line. Blanks are significant. Where a token or character constant is split across two lines: 
a leading & on the continued line is also required.
Automatic conversion of source form for existing programs can be carried out by convert.f90.
Its options are
Fortran has five intrinsic data types: INTEGER, REAL, COMPLEX, LOGICAL and CHARACTER. Each of those types can be additionally characterized by a kind. Kind, basically, defines internal representation of the type: for the three numeric types, it defines the precision and range, and for the other two, the specifics of storage representation. Thus, it is an abstract concept which models the limits of data types' representation; it is expressed as a member of a set of whole numbers (e.g. it may be {1, 2, 4, 8} for integers, denoting bytes of storage), but those values are not specified by the Standard and not portable. For every type, there is a default kind, which is used if no kind is explicitly specified. For each intrinsic type, there is a corresponding form of literal constant. The numeric types INTEGER and REAL can only be signed (there is no concept of sign for type COMPLEX).
Integer literal constants of the default kind take the form
Kind can be defined as a named constant. If the desired range is ±10kind, the portable syntax for defining the appropriate kind, two_bytes is
that allows subsequent definition of constants of the form
Here, two_bytes is the kind type parameter; it can also be an explicit default integer literal constant, like 
but such use is non-portable.
The KIND function supplies the value of a kind type parameter: 
and the RANGE function supplies the actual decimal range (so the user must make the actual mapping to bytes):
Also, in DATA (initialization) statements, binary (B), octal (O) and hexadecimal (Z) constants may be used (often informally referred to as ""BOZ constants""):
There are at least two real kinds—the default and one with greater precision (this replaces DOUBLE PRECISION). SELECTED_REAL_KIND functions returns the kind number for desired range and precision; for at least 9 decimal digits of precision and a range of 10−99 to 1099, it can be specified as:
and literals subsequently specified as
Also, there are the intrinsic functions 
that give in turn the kind type value, the actual precision (here at least 9), and the actual range (here at least 99).
COMPLEX data type is built of two integer or real components: 
There are only two basic values of logical constants: .TRUE. and .FALSE.. Here, there may also be different kinds. Logicals don't have their own kind inquiry functions, but use the kinds specified for INTEGERs; default kind of LOGICAL is the same as of INTEGER.
and the KIND function operates as expected: 
The forms of literal constants for CHARACTER data type are
(the last being an empty string). Different kinds are allowed (for example, to distinguish ASCII and UNICODE strings), but not widely supported by compilers. Again, the kind value is given by the KIND function: 
The numeric types are based on number models with associated inquiry functions (whose values are independent of the values of their arguments; arguments are used only to provide kind). These functions are important for portable numerical software:
Scalar variables corresponding to the five intrinsic types are specified as follows:
where the optional KIND parameter specifies a non-default kind, and the :: notation delimits the type and attributes from variable name(s) and their optional initial values, allowing full variable specification and initialization to be typed in one statement (in previous standards, attributes and initializers had to be declared in several statements). While it is not required in above examples (as there are no additional attributes and initialization), most Fortran-90 programmers acquire the habit to use it everywhere.
LEN= specifier is applicable only to CHARACTERs and specifies the string length (replacing the older *len form). 
The explicit KIND= and LEN= specifiers are optional:
works just as well.
There are some other interesting character features. Just as a substring as in 
was previously possible, so now is the substring
Also, zero-length strings are allowed: 
Finally, there is a set of intrinsic character functions, examples being
For derived data types, the form of the type must be defined first: 
and then, variables of that type can be defined: 
To select components of a derived type, % qualifier is used:  
Literal constants of derived types have the form TypeName(1stComponentLiteral, 2ndComponentLiteral, ...):
which is known as a structure constructor. Definitions may refer to a previously defined type: 
and for a variable of type triangle, as in 
each component of type point is accessed as
which, in turn, have ultimate components of type real: 
(Note that the % qualifier was chosen rather than dot (.) because of potential ambiguity with operator notation, like .OR.).
Unless specified otherwise, all variables starting with letters I, J, K, L, M and N are default INTEGERs, and all others are default REAL; other data types must be explicitly declared. This is known as implicit typing and is a heritage of early FORTRAN days. Those defaults can be overridden by IMPLICIT TypeName (CharacterRange) statements, like:
However, it is a good practice to explicitly type all variables, and this can be forced by inserting the statement IMPLICIT NONE
at the beginning of each program unit.
Arrays are considered to be variables in their own right. Every array is characterized by its type, rank, and shape (which defines the extents of each dimension). Bounds of each dimension are by default 1 and size, but arbitrary bounds can be explicitly specified. DIMENSION keyword is optional and considered an attribute; if omitted, the array shape must be specified after array-variable name. For example,
declares two arrays, rank-1 and rank-2, whose elements are in column-major order. Elements are, for example,
and are scalars. The subscripts may be any scalar integer expression.
Sections are parts of the array variables, and are arrays themselves:
Whole arrays and array sections are array-valued objects. Array-valued constants (constructors) are available, enclosed in (/ ... /): 
making use of an implied-DO loop notation. Fortran 2003 allows the use of brackets: 
[1, 2, 3, 4] and [([1,2,3], i=1,4)]
instead of the first two examples above, and many compilers support this now.
A derived data type may, of course, contain array components: 
so that 
Variables can be given initial values as specified in a specification statement:
and a default initial value can be given to the component of a derived data type:
When local variables are initialized within a procedure they implicitly acquire the SAVE attribute:
This declaration is equivalent to
for local variables within a subroutine or function.  The SAVE attribute causes local variables to retain their value after a procedure call and then to initialize the variable to the saved value upon returning to the procedure.
A named constant can be specified directly by adding the PARAMETER attribute and the constant values to a type statement:
The DATA statement can be used for scalars and also for arrays and variables of derived type. It is also the only way to initialise just parts of such objects, as well as to initialise to binary, octal or hexadecimal values: 
The values used in DATA and PARAMETER statements, or with these attributes, are constant expressions that may include references to: array and structure constructors, elemental intrinsic functions with integer or character arguments and results, and the six transformational functions REPEAT, SELECTED_INT_KIND, TRIM, SELECTED_REAL_KIND, RESHAPE and TRANSFER (see Intrinsic procedures): 
It is possible to specify details of variables 
using any non-constant, scalar, integer expression that may also include inquiry 
function references:
The usual arithmetic operators are available — +, -, *, /, ** (given here in increasing order of precedence).
Parentheses are used to indicate the order of evaluation where necessary:
The rules for scalar numeric expressions and assignments accommodate the non-default kinds. Thus, the mixed-mode numeric expression and assignment rules incorporate different kind type parameters in an expected way: 
converts integer0 to a real value of the same kind as real1; the result is of same kind, and is converted to the kind of real2 for assignment.
These functions are available for controlled rounding of real numbers to integers:
For scalar relational operations of numeric types, there is a set of built-in operators: 
(the forms above are new to Fortran-90, and older equivalent forms are given below them). Example expressions:
In the case of scalar characters and given CHARACTER(8) result
it is legal to write 
Concatenation is performed by the operator '//'.
No built-in operations (except assignment, defined on component-by component basis) exist between derived data types mutually or with intrinsic types. The meaning of existing or user-specified operators can be (re)defined though:
we can write 
Notice the ""overloaded"" use of the intrinsic symbol // and the named operator, .concat. . A difference between the two cases is that, for an intrinsic operator token, the usual precedence rules apply, whereas for named operators, precedence is the highest as a unary operator or the lowest as a binary one. In 
the two expressions are equivalent only if appropriate parentheses are 
added as shown. In each case there must be defined, in a module, procedures defining the operator and assignment, and corresponding operator-procedure association, as follows:
The string concatenation function is a more elaborated version of that shown already in Basics.  Note that in order to handle the error condition that arises when the two strings together exceed the preset 80-character limit, it would be safer to use a subroutine to perform the concatenation (in this case operator-overloading would not be applicable.)
Defined operators such as these are required for the expressions that are 
allowed also in structure constructors (see Derived-data types): 
In the case of arrays then, as long as they are of the same shape (conformable), operations and assignments are extended in an obvious way, on an element-by-element basis. For example, given declarations of
it can be written:
The order of expression evaluation is not specified in order to allow for optimization on parallel and vector machines. Of course, any operators for arrays of derived type must be defined.
Some real intrinsic functions that are useful for numeric 
computations are
These are array valued for array arguments (elemental), like all FORTRAN 77 functions (except LEN):
(the last seven are for characters).
The simple GO TO label exists, but is usually avoided — in most cases, a more specific branching construct will accomplish the same logic with more clarity.
The simple conditional test is the IF statement: IF (a > b) x = y
A full-blown IF construct is illustrated by
The CASE construct is a replacement for the computed GOTO, but is better 
structured and does not require the use of statement labels: 
Each CASE selector list may contain a list and/or range of integers, 
character or logical constants, whose values may not overlap within or between 
selectors: 
A default is available: 
There is only one evaluation, and only one match.
A simplified but sufficient form of the DO construct is illustrated by 
where we note that loops may be optionally named so that any EXIT or CYCLE 
statement may specify which loop is meant.
Many, but not all, simple loops can be replaced by array expressions and 
assignments, or by new intrinsic functions. For instance 
becomes simply tot = SUM( a(m:n) )
In order to discuss this topic we need some definitions. In logical terms, an 
executable program consists of one main program and zero or more 
subprograms (or procedures) - these do something. 
Subprograms are either functions or subroutines, which are 
either external, internal or module subroutines. (External 
subroutines are what we knew from FORTRAN 77.)
From an organizational point of view, however, a complete program consists of 
program units. These are either main programs, external 
subprograms or modules and can be separately compiled.
An example of a main (and complete) program is
An example of a main program and an external subprogram, forming an executable program, is
The form of a function is
The form of reference of a function is x = name(a, b)
An internal subprogram is one contained in another (at a maximum 
of one level of nesting) and provides a replacement for the statement function: 
We say that outer is the host of inner, and that inner obtains 
access to entities in outer by host association (e.g. to x), whereas 
y is a local variable to inner.
The scope of a named entity is a scoping unit, here 
outer less inner, and inner.
The names of program units and external procedures are global, and 
the names of implied-DO variables have a scope of the statement that contains 
them.
Modules are used to package
An example of a module 
containing a type definition, interface block and function subprogram is
and the simple statement 
provides use association to all the module's entities. Module 
subprograms may, in turn, contain internal subprograms.
The PUBLIC and PRIVATE attributes are used in specifications in 
modules to limit the scope of entities. The attribute form is 
and the statement form is 
The statement form has to be used to limit access to operators, and can 
also be used to change the overall default: 
For derived types there are three possibilities: the type and its 
components are all PUBLIC, the type is PUBLIC and its components PRIVATE (the 
type only is visible and one can change its details easily), or all of it is 
PRIVATE (for internal use in the module only): 
The USE statement's purpose is to gain access to entities in a module. 
It has options to resolve name clashes if an imported name is the 
same as a local one: 
or to restrict the used entities to a specified set: 
These may be combined: 
We may specify the intent of dummy arguments: 
Also, INOUT is possible: here the actual argument must be a variable 
(unlike the default case where it may be a constant).
Arguments may be optional: 
allows us to call mincon by 
Arguments may be keyword rather than positional (which come first): 
Optional and keyword arguments are handled by explicit interfaces, that is 
with internal or module procedures or with interface blocks.
Any reference to an internal or module subprogram is 
through an interface that is 'explicit' (that is, the compiler can see all the 
details). A reference to an external (or dummy) procedure is usually 'implicit' 
(the compiler assumes the details). However, we can provide an explicit 
interface in this case too. It is a copy of the header, specifications and END 
statement of the procedure concerned, either placed in a module or inserted 
directly: 
An explicit interface is obligatory for
It allows 
full checks at compile time between actual and dummy arguments.
In general, the best way to ensure that a procedure interface is explicit is either to place the procedure concerned in a module or to use it as an internal procedure.
Interface blocks provide the 
mechanism by which we are able to define generic names for specific procedures: 
where a given set of specific names corresponding to a generic name must 
all be of functions or all of subroutines. If this interface is within a module, 
then it is simply 
We can use existing names, e.g. SIN, and the compiler sorts out the 
correct association.
We have already seen the use of interface blocks for defined operators and 
assignment (see Modules).
Indirect recursion is useful for multi-dimensional 
integration. For 
We might have 
and to integrate f(x, y) over a rectangle: 
Direct recursion is when a procedure calls itself, as in 
Here, we note the RESULT clause and termination test.
This is a feature for parallel computing.
In the FORALL Statement and Construct, any
side effects in a function can
impede optimization on a parallel processor—the order of execution of the assignments could affect the results.
To control this situation, we
add the PURE keyword to the
SUBROUTINE or  FUNCTION
statement—an assertion that the procedure (expressed simply):
A compiler can
check that this is the case, as in
All the intrinsic functions are pure.
Array handling is included in Fortran for two main reasons:
At the same time, major extensions of the functionality in this area have been 
added. We have already met whole arrays above #Arrays 1 and here  #Arrays 2 - now 
we develop the theme.
A zero-sized array is handled by Fortran as a 
legitimate object, without special coding by the programmer. Thus, in 
no special code is required for the final iteration where i = n. We note 
that a zero-sized array is regarded as being defined; however, an array of shape 
(0,2) is not conformable with one of shape (0,3), whereas x(1:0) = 3 is a valid 'do nothing' statement.
These are an extension and replacement for 
assumed-size arrays. Given an actual argument like: 
the corresponding dummy argument specification defines only the type and 
rank of the array, not its shape. This information has to be made available by an 
explicit interface, often using an interface block (see Interface blocks). Thus we write just 
and this is as if da were dimensioned (11,21). However, we can specify any 
lower bound and the array maps accordingly.
The shape, not bounds, is passed, where the default lower bound is 1 and the default upper bound is the corresponding extent.
A partial replacement for the uses to which EQUIVALENCE 
was put is provided by this facility, useful for local, temporary arrays, as in 
The actual storage is typically maintained on a stack.
Fortran provides dynamic allocation of 
storage; it relies on a heap storage mechanism (and replaces another use of 
EQUIVALENCE). An example for establishing a work array for a whole program is 
The work array can be propagated through the whole program via a USE 
statement in each program unit. We may specify an explicit lower bound and 
allocate several entities in one statement. To free dead storage we write, for 
instance, 
Deallocation of arrays is automatic when they go out of scope.
We have already met whole array 
assignments and operations: 
In the second assignment, an intrinsic function returns an array-valued 
result for an array-valued argument. We can write array-valued functions 
ourselves (they require an explicit interface): 
Elemental procedures are specified with scalar dummy arguments that may be called with
array actual arguments. In the case of a function, the shape of the result is the shape of the array
arguments.
Most intrinsic functions are elemental and
Fortran 95 extends this feature to non-intrinsic procedures, thus providing the effect
of writing, in Fortran 90, 22 different versions, for ranks 0-0, 0-1, 1-0, 1-1, 0-2,
2-0, 2-2, ... 7-7, and is further an aid to optimization on parallel processors.
An elemental procedure must be pure.
The dummy arguments cannot be used in specification expressions 
(see above) except as
arguments to certain intrinsic functions (BIT_SIZE, KIND, 
LEN, and the numeric inquiry ones, (see below).
Often, we need to mask an assignment. This we can do using the 
WHERE, either as a statement: 
(note: the test is element-by-element, not on whole array), or as a construct: 
or 
Further:
When a DO construct
is executed, each successive
iteration is performed in order and one after the other—an impediment to optimization
on a parallel processor.
where
the individual assignments may be carried out in any order, and
even simultaneously.
The FORALL may be considered to be an array assignment
expressed with the help of indices.
with masking condition.
The FORALL construct
allows several
assignment statements to be executed in order.    
is equivalent to the array assignments
The FORALL version is more readable.
Assignment in a FORALL
is like an array assignment: 
as if all the expressions were evaluated in any order, held
in temporary storage, then all the assignments performed in any order.
The first statement must fully complete before the second can begin.
A FORALL
may be nested, and
may include a WHERE.
Procedures referenced within a FORALL
must be pure.
For a simple case, given 
we can reference a single element as, for instance, a(1, 1). For a 
derived-data type like 
we can declare an array of that type: 
and a reference like tar(n, 2) is an element (a scalar!) of type fun_del, but tar(n, 2)%du is an array of type real, and tar(n, 2)%du(2) is an element of it. The basic rule to remember is that an array element 
always has a subscript or subscripts qualifying at least the last name.
The general form of subscript for an array 
section is 
(where [ ] indicates an optional item) as in 
Note that a vector subscript with duplicate values cannot appear on the 
left-hand side of an assignment as it would be ambiguous. Thus, 
is illegal. Also, a section with a vector subscript must not be supplied 
as an actual argument to an OUT or INOUT dummy argument. Arrays of arrays are not allowed:
We note that a given value in an array can be referenced both as an 
element and as a section: 
depending on the circumstances or requirements. By qualifying objects of 
derived type, we obtain elements or sections depending on the rule stated 
earlier: 
Vector and matrix multiply
Array reduction
Array inquiry
Array construction
Array reshape
Array manipulation
Array location
Pointers are variables with the POINTER attribute; they are not a 
distinct data type (and so no 'pointer arithmetic' is possible). 
They are conceptually a descriptor listing the attributes of the objects 
(targets) that the pointer may point to, and the address, if any, of a target. 
They have no associated storage until it is allocated or otherwise associated 
(by pointer assignment, see below): 
and they are dereferenced automatically, so no special symbol required. In 
the value of the target of var is used and modified. Pointers cannot be 
transferred via I/O. The statement
writes the value of the target of var and not the pointer descriptor 
itself.
A pointer can point to another pointer, and hence to its target, or to a 
static object that has the TARGET attribute: 
but they are strongly typed: 
and, similarly, for arrays the ranks as well as the type must agree.
A pointer can be a component of a derived type: 
and we can define the beginning of a linked chain of such entries: 
After suitable allocations and definitions, the first two entries could be 
addressed as 
but we would normally define additional pointers to point at, for 
instance, the first and current entries in the list.
A pointer's association status is one of 
Some care has to be taken not to leave a pointer 'dangling' by 
use of DEALLOCATE on its target without nullifying any other pointer referring 
to it.
The intrinsic function ASSOCIATED can test the association status of a 
defined pointer: 
or between a defined pointer and a defined target (which may, itself, be a 
pointer): 
An alternative way to initialize a pointer, also in a specification statement,
is to use the NULL function:
For intrinsic types we can 
'sweep' pointers over different sets of target data using the same code without 
any data movement. Given the matrix manipulation y = B C z, we can write the 
following code (although, in this case, the same result could be achieved more 
simply by other means): 
For objects of derived type we have to distinguish between pointer and 
normal assignment. In 
the assignment causes first to point at current, whereas 
causes current to overwrite first and is equivalent to 
If an actual argument is a pointer then, if the dummy 
argument is also a pointer,
If the dummy argument is not a 
pointer, it becomes associated with the target of the actual argument: 
Function results may also have the POINTER attribute; 
this is useful if the result size depends on calculations performed in the 
function, as in 
where the module data_handler contains 
The result can be used in an expression (but must be associated with a 
defined target).
These do not exist as such: given 
then 
would be such an object, but with an irregular storage pattern. For this 
reason they are not allowed. However, we can achieve the same effect by defining 
a derived data type with a pointer as its sole component: 
and then defining arrays of this data type
where the storage for the rows can be allocated by, for instance, 
The array assignment s = tis then equivalent to the pointer assignments s(i)%r => t(i)%r for all components.
Given an array 
that is frequently referenced with the fixed subscripts 
these references may be replaced by 
The subscripts of window are 1:n-m+1, 1:q-p+1. Similarly, for tar%u
(as defined in already), we can use, say, taru => tar%u to point at all the u components of tar, and subscript it as taru(1, 2)
The subscripts are as those of tar itself. (This replaces yet more of EQUIVALENCE.)
In the pointer association
the lower bounds for pointer are determined as if lbound was applied to array_expression. Thus, when a pointer is assigned to a whole array variable, it inherits the lower bounds of the variable, otherwise, the lower bounds default to 1.
Fortran 2003 allows specifying arbitrary lower bounds on pointer association, like
so that the bounds of window become r:r+n-m,s:s+q-p.
Fortran 95 does not have this feature; however, it can be simulated using the
following trick (based on the pointer association rules for assumed shape array dummy arguments):
The source code of an extended example of the use of pointers to support a 
data structure is in pointer.f90.
Most of the intrinsic functions have already been mentioned. Here, we deal 
only with their general classification and with those that have so far been 
omitted. All intrinsic procedures can be used with keyword arguments: 
and many have optional arguments.
The intrinsic procedures are grouped into four categories:
The procedures not already 
introduced are
Bit inquiry
Bit manipulation
Transfer function, as in
(replaces part of EQUIVALENCE)
Subroutines
(This is a subset only of the actual features and, exceptionally, lower case is used
in the code examples.)
These examples illustrate various forms of I/O lists with some simple formats 
(see below):
Variables, but not expressions, are equally valid in input
statements using the read statement:
If an array appears as an item, it is treated as if the elements were
specified in array element order.
Any pointers in an I/O list
must be associated with a target, and transfer takes place
between the file and the targets.
An item of derived type is treated as if the components were specified
in the same order as in the type declaration, so
has the same effect as the statement
An object in an I/O list is not permitted to be of a derived type
that has a pointer component at any level of component selection.
Note that a zero-sized array
may occur as an item in an I/O list.
Such an item corresponds to no actual data transfer.
The format specification may also
be given in the form of a character expression:
or as an asterisk—this is a type of I/O known as
list-directed
I/O (see below), in which the format is defined by the computer system:
Input/output operations are used to transfer data between the
storage of an executing program and an external medium, specified by a unit number.
However, two I/O statements,  print and a variant of
read, do not
reference any unit number: this is referred to as terminal I/O. 
Otherwise the form is:
where unit= is optional.
The value may be any nonnegative integer allowed by the system
for this purpose (but 0, 5 and 6 often denote the error, keyboard and terminal, respectively).
An asterisk is a variant—again from the keyboard:
A read with a unit specifier allows exception handling:
There a second type of formatted output statement, the
write statement:
These allow format conversion between various representations to be carried out by the program in a storage area defined within the program itself.
If an internal file is a scalar, it has a single record whose length is that of the scalar.
If it is an array, its elements, in array element order, are treated as successive records of the file and each has length that of an array element.
An example using a write statement is
that might write
An example of a read without a specified format for input is
If this reads the input record
(in which blanks are used as separators),
then i, a, 
field, flag, and title will acquire the values 10, 6.4, 
(1.0,0.0) and (2.0,0.0), .true.
and test respectively,
while word remains unchanged.
Quotation marks or apostrophes are required as delimiters for a string that
contains a blank.
This is a form of reading and writing
without always advancing the file position to ahead of the next record.
Whereas an advancing I/O statement always repositions the file after the last
record accessed, a non-advancing I/O statement performs no
such repositioning and may therefore leave the file positioned within a
record.
A non-advancing read might read the first
few characters of a record and a normal read the remainder.
In order to write a prompt to a
terminal screen and to read from the next character position on the
screen without an intervening line-feed, we can write
Non-advancing I/O is for external files, and is  
not available for list-directed I/O.
It is possible to specify that an edit descriptor be repeated a specified number of times, 
using a repeat count: 10f12.3
The slash edit descriptor (see below)
may have a repeat count, and a repeat count 
can also apply to a group of edit
descriptors, enclosed in parentheses, with nesting:
Entire format specifications can be repeated:
writes 10 integers, each occupying 8 character positions, on each of 20 lines (repeating the format specification advances to the next line).
Control edit descriptors setting conditions:
Control edit descriptors for immediate processing:
Note that
terminates format control if there are no further items in
an I/O list.
This type of I/O should be used only in cases where the records are
generated by a program on one computer, to be read back on the same
computer or another computer using the
same internal number representations:
This form of I/O is also known as random access or indexed I/O.
Here, all the records have the same
length, and each
record is identified by an index number. It is possible to write,
read, or re-write any specified record without regard to position.
The file must be an external file and 
list-directed formatting and non-advancing I/O are
unavailable.
Once again, this is an overview only.
The statement is used to connect an external file to a unit,
create a file that is preconnected, or create a file and connect it to a
unit.
The syntax is
where olist is a list of optional specifiers.
The specifiers may appear in any order.
Other specifiers are form and position.
This is used to disconnect a file from a unit.
as in
At any time during the execution of a program it is possible to inquire about the status and attributes of a file using this statement.
Using a variant of this statement, it is similarly possible to determine the status of a unit, for instance whether the unit number exists for that system.
Another variant permits an inquiry about the length of an output list when used to write an unformatted record.
For inquire by unit
or for inquire by file
or for inquire by I/O list
As an example
yields
(assuming no intervening read or write operations).
Other specifiers are iostat, opened, number,
named, formatted, position, action, read, write, readwrite."
"103","In computer programming, a subroutine is a sequence of program instructions that perform a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed.
Subprograms may be defined within programs, or separately in libraries that can be used by multiple programs.  In different programming languages, a subroutine may be called a procedure, a function, a routine, a method, or a subprogram.  The generic term  callable unit is sometimes used.
The name subprogram suggests a subroutine behaves in much the same way as a computer program that is used as one step in a larger program or another subprogram. A subroutine is often coded so that it can be started (called) several times and from several places during one execution of the program, including from other subroutines, and then branch back (return) to the next instruction after the call, once the subroutine's task is done.  Maurice Wilkes, David Wheeler, and Stanley Gill are credited with the invention of this concept, which they termed a closed subroutine, contrasted with an open subroutine or macro.
Subroutines are a powerful programming tool, and the syntax of many programming languages includes support for writing and using them. Judicious use of subroutines (for example, through the structured programming approach) will often substantially reduce the cost of developing and maintaining a large program, while increasing its quality and reliability. Subroutines, often collected into libraries, are an important mechanism for sharing and trading software. The discipline of object-oriented programming is based on objects and methods (which are subroutines attached to these objects or object classes).
In the compiling method called threaded code, the executable program is basically a sequence of subroutine calls.
The content of a subroutine is its body, which is the piece of program code that is executed when the subroutine is called or invoked.
A subroutine may be written so that it expects to obtain one or more data values from the calling program (to replace its parameters or formal parameters).  The calling program provides actual values for these parameters, called arguments. Different programming languages may use different conventions for passing arguments:
The subroutine may return a computed value to its caller (its return value), or provide various result values or output parameters. Indeed, a common use of subroutines is to implement mathematical functions, in which the purpose of the subroutine is purely to compute one or more results whose values are entirely determined by the arguments passed to the subroutine. (Examples might include computing the logarithm of a number or the determinant of a matrix.)
A subroutine call may also have side effects such as modifying data structures in a computer memory, reading from or writing to a peripheral device, creating a file, halting the program or the machine, or even delaying the program's execution for a specified time. A subprogram with side effects may return different results each time it is called, even if it is called with the same arguments. An example is a random number function, available in many languages, that returns a different pseudo-random number each time it is called. The widespread use of subroutines with side effects is a characteristic of imperative programming languages.
A subroutine can be coded so that it may call itself recursively, at one or more places, to perform its task. This method allows direct implementation of functions defined by mathematical induction and recursive divide and conquer algorithms.
A subroutine whose purpose is to compute one boolean-valued function (that is, to answer a yes/no question) is sometimes called a predicate. In logic programming languages, often[vague] all subroutines are called predicates, since they primarily[vague] determine success or failure.[citation needed]
High-level programming languages usually include specific constructs to:
Some programming languages, such as Pascal, Fortran, Ada and many dialects of BASIC, distinguish between functions or function subprograms, which provide an explicit return value to the calling program, and subroutines or procedures, which do not. In those languages, function calls are normally embedded in expressions (e.g., a sqrt function may be called as y = z + sqrt(x)). Procedure calls either behave syntactically as statements (e.g., a print procedure may be called as if x > 0 then print(x) or are explicitly invoked by a statement such as CALL or GOSUB (e.g. call print(x)). Other languages, such as C and Lisp, do not distinguish between functions and subroutines.
In strictly functional programming languages such as Haskell, subprograms can have no side effects, which means that various internal states of the program will not change. Functions will always return the same result if repeatedly called with the same arguments. Such languages typically only support functions, since subroutines that do not return a value have no use unless they can cause a side effect.
In programming languages such as C, C++, and C#, subroutines may also simply be called functions, not to be confused with mathematical functions or functional programming, which are different concepts.
A language's compiler will usually translate procedure calls and returns into machine instructions according to a well-defined calling convention, so that subroutines can be compiled separately from the programs that call them. The instruction sequences corresponding to call and return statements are called the procedure's prologue and epilogue.
The advantages of breaking a program into subroutines include:
Invoking a subroutine (versus using in-line code) imposes some computational overhead in the call mechanism.
A subroutine typically requires standard housekeeping code – both at entry to, and exit from, the function (function prologue and epilogue – usually saving general purpose registers and return address as a minimum).
The idea of a subroutine was worked out after computing machines had already existed for some time.
The arithmetic and conditional jump instructions were planned ahead of time and have changed relatively little; but the special instructions used for procedure calls have changed greatly over the years.
The earliest computers and microprocessors, such as the Small-Scale Experimental Machine and the RCA 1802, did not have a single subroutine call instruction.
Subroutines could be implemented, but they required programmers to use the call sequence—a series of instructions—at each call site.
Some very early computers and microprocessors, such as the IBM 1620, the Intel 8008, and the PIC microcontrollers, have a single-instruction subroutine call that uses dedicated hardware stack to store return addresses—such hardware supports only a few levels of subroutine nesting, but can support recursive subroutines.
Machines before the mid 1960s—such as the UNIVAC I, the PDP-1, and the IBM 1130—typically use a calling convention which saved the instruction counter in the first memory location of the called subroutine. This allows arbitrarily deep levels of subroutine nesting, but does not support recursive subroutines.
The PDP-11 (1970) is one of the first computers with a stack-pushing subroutine call instruction; this feature supports both arbitrarily deep subroutine nesting and also supports recursive subroutines.
In the very early assemblers, subroutine support was limited. Subroutines were not explicitly separated from each other or from the main program, and indeed the source code of a subroutine could be interspersed with that of other subprograms. Some assemblers would offer predefined macros to generate the call and return sequences. By the 1960s, assemblers usually had much more sophisticated support for both inline and separately assembled subroutines that could be linked together.
Even with this cumbersome approach, subroutines proved very useful. For one thing they allowed use of the same code in many different programs. Moreover, memory was a very scarce resource on early computers, and subroutines allowed significant savings in the size of programs.
Many early computers loaded the program instructions into memory from a punched paper tape. Each subroutine could then be provided by a separate piece of tape, loaded or spliced before or after the main program (or ""mainline""); and the same subroutine tape could then be used by many different programs. A similar approach applied in computers which used punched cards for their main input. The name subroutine library originally meant a library, in the literal sense, which kept indexed collections of tapes or card-decks for collective use.
To remove the need for self-modifying code, computer designers eventually provided an indirect jump instruction, whose operand, instead of being the return address itself, was the location of a variable or processor register containing the return address.
On those computers, instead of modifying the subroutine's return jump, the calling program would store the return address in a variable so that when the subroutine completed, it would execute an indirect jump that would direct execution to the location given by the predefined variable.
Another advance was the jump to subroutine instruction, which combined the saving of the return address with the calling jump, thereby minimizing overhead significantly.
In the IBM System/360, for example, the branch instructions BAL or BALR, designed for procedure calling, would save the return address in a processor register specified in the instruction. To return, the subroutine had only to execute an indirect branch instruction (BR) through that register. If the subroutine needed that register for some other purpose (such as calling another subroutine), it would save the register's contents to a private memory location or a register stack.
In systems such as the HP 2100, the JSB instruction would perform a similar task, except that the return address was stored in the memory location that was the target of the branch. Execution of the procedure would actually begin at the next memory location. In the HP 2100 assembly language, one would write, for example
to call a subroutine called MYSUB from the main program.  The subroutine would be coded as
The JSB instruction placed the address of the NEXT instruction (namely, BB) into the location specified as its operand (namely, MYSUB), and then branched to the NEXT location after that (namely, AA = MYSUB + 1). The subroutine could then return to the main program by executing the indirect jump JMP MYSUB,I which branched to the location stored at location MYSUB.
Compilers for Fortran and other languages could easily make use of these instructions when available. This approach supported multiple levels of calls; however, since the return address, parameters, and return values of a subroutine were assigned fixed memory locations, it did not allow for recursive calls.
Incidentally, a similar method was used by Lotus 1-2-3, in the early 1980s, to discover the recalculation dependencies in a spreadsheet. Namely, a location was reserved in each cell to store the return address. Since circular references are not allowed for natural recalculation order, this allows a tree walk without reserving space for a stack in memory, which was very limited on small computers such as the IBM PC.
Most modern implementations use a call stack, a special case of the stack data structure, to implement subroutine calls and returns. Each procedure call creates a new entry, called a stack frame, at the top of the stack; when the procedure returns, its stack frame is deleted from the stack, and its space may be used for other procedure calls. Each stack frame contains the private data of the corresponding call, which typically includes the procedure's parameters and internal variables, and the return address.
The call sequence can be implemented by a sequence of ordinary instructions (an approach still used in reduced instruction set computing (RISC) and very long instruction word (VLIW) architectures), but many traditional machines designed since the late 1960s have included special instructions for that purpose.
The call stack is usually implemented as a contiguous area of memory. It is an arbitrary design choice whether the bottom of the stack is the lowest or highest address within this area, so that the stack may grow forwards or backwards in memory; however, many architectures chose the latter.[citation needed]
Some designs, notably some Forth implementations, used two separate stacks, one mainly for control information (like return addresses and loop counters) and the other for data. The former was, or worked like, a call stack and was only indirectly accessible to the programmer through other language constructs while the latter was more directly accessible.
When stack-based procedure calls were first introduced, an important motivation was to save precious memory.[citation needed] With this scheme, the compiler does not have to reserve separate space in memory for the private data (parameters, return address, and local variables) of each procedure. At any moment, the stack contains only the private data of the calls that are currently active (namely, which have been called but haven't returned yet). Because of the ways in which programs were usually assembled from libraries, it was (and still is) not uncommon to find programs that include thousands of subroutines, of which only a handful are active at any given moment.[citation needed] For such programs, the call stack mechanism could save significant amounts of memory. Indeed, the call stack mechanism can be viewed as the earliest and simplest method for automatic memory management.
However, another advantage of the call stack method is that it allows recursive subroutine calls, since each nested call to the same procedure gets a separate instance of its private data.
One disadvantage of the call stack mechanism is the increased cost of a procedure call and its matching return.[clarification needed] The extra cost includes incrementing and decrementing the stack pointer (and, in some architectures, checking for stack overflow), and accessing the local variables and parameters by frame-relative addresses, instead of absolute addresses. The cost may be realized in increased execution time, or increased processor complexity, or both.
This overhead is most obvious and objectionable in leaf procedures or leaf functions, which return without making any procedure calls themselves.
To reduce that overhead, many modern compilers try to delay the use of a call stack until it is really needed.[citation needed] For example, the call of a procedure P may store the return address and parameters of the called procedure in certain processor registers, and transfer control to the procedure's body by a simple jump. If procedure P returns without making any other call, the call stack is not used at all. If P needs to call another procedure Q, it will then use the call stack to save the contents of any registers (such as the return address) that will be needed after Q returns.
In the C and C++ programming languages, subprograms are termed functions (further classified as member functions when associated with a class, or free functions when not). These languages use the special keyword void to indicate that a function takes no parameters (especially in C) or does not return any value. Note that C/C++ functions can have side-effects, including modifying any variables whose addresses are passed as parameters (i.e., passed by reference). Examples:
The function does not return a value and has to be called as a stand-alone function, e.g., function1();
This function returns a result (the number 5), and the call can be part of an expression, e.g., x + function2()
This function converts a number between 0 and 6 into the initial letter of the corresponding day of the week, namely 0 to 'S', 1 to 'M', ..., 6 to 'S'. The result of calling it might be assigned to a variable, e.g., num_day = function3(number);.
This function does not return a value but modifies the variable whose address is passed as the parameter; it would be called with ""function4(&variable_to_increment);"".
In the example above, Example() calls the subroutine.To define the actual subroutine, the Sub keyword must be used, with the subroutine name following Sub. After content has followed, 
EndSub must be typed.
In the Visual Basic 6 language, subprograms are termed functions or subs (or methods when associated with a class). Visual Basic 6 uses various terms called types to define what is being passed as a parameter. By default, an unspecified variable is registered as a variant type and can be passed as ByRef (default) or ByVal. Also, when a function or sub is declared, it is given a public, private, or friend designation, which determines whether it can be accessed outside the module or project that it was declared in.
The function does not return a value and has to be called as a stand-alone function, e.g., Function1
This function returns a result (the number 5), and the call can be part of an expression, e.g., x + Function2()
This function converts a number between 0 and 6 into the initial letter of the corresponding day of the week, namely 0 to 'M', 1 to 'T', ..., 6 to 'S'. The result of calling it might be assigned to a variable, e.g., num_day = Function3(number).
This function does not return a value but modifies the variable whose address is passed as the parameter; it would be called with ""Function4(variable_to_increment)"".
In PL/I a called procedure may be passed a descriptor providing information about the argument, such as string lengths and array bounds.  This allows the procedure to be more general and eliminates the need for the programmer to pass such information.  By default PL/I passes arguments by reference.  A (trivial) subroutine to change the sign of each element of a two-dimensional array might look like:
This could be called with various arrays as follows:
A subprogram may find it useful to make use of a certain amount of scratch space; that is, memory used during the execution of that subprogram to hold intermediate results. Variables stored in this scratch space are termed local variables, and the scratch space is termed an activation record. An activation record typically has a return address that tells it where to pass control back to when the subprogram finishes.
A subprogram may have any number and nature of call sites. If recursion is supported, a subprogram may even call itself, causing its execution to suspend while another nested execution of the same subprogram occurs. Recursion is a useful means to simplify some complex algorithms and break down complex problems. Recursive languages generally provide a new copy of local variables on each call. If the programmer desires the value of local variables to stay the same between calls, they can be declared static in some languages, or global values or common areas can be used. Here is an example of recursive subroutine in C/C++ to find Fibonacci numbers:
Early languages like Fortran did not initially support recursion because variables were statically allocated, as well as the location for the return address. Most computers before the late 1960s such as the PDP-8 did not have support for hardware stack registers.[citation needed]
Modern languages after ALGOL such as PL/1 and C almost invariably use a stack, usually supported by most modern computer instruction sets to provide a fresh activation record for every execution of a subprogram. That way, the nested execution is free to modify its local variables without concern for the effect on other suspended executions in progress. As nested calls accumulate, a call stack structure is formed, consisting of one activation record for each suspended subprogram. In fact, this stack structure is virtually ubiquitous, and so activation records are commonly termed stack frames.
Some languages such as Pascal and Ada also support nested subroutines, which are subroutines callable only within the scope of an outer (parent) subroutine. Inner subroutines have access to the local variables of the outer subroutine that called them. This is accomplished by storing extra context information within the activation record, also termed a display.
If a subprogram can be executed properly even when another execution of the same subprogram is already in progress, that subprogram is said to be reentrant. A recursive subprogram must be reentrant. Reentrant subprograms are also useful in multi-threaded situations, since multiple threads can call the same subprogram without fear of interfering with each other. In the IBM CICS transaction processing system, quasi-reentrant was a slightly less restrictive, but similar, requirement for application programs that were shared by many threads.
In a multi-threaded environment, there is generally more than one stack. An environment that fully supports coroutines or lazy evaluation may use data structures other than stacks to store their activation records.
In strongly typed languages, it is sometimes desirable to have a number of functions with the same name, but operating on different types of data, or with different parameter profiles. For example, a square root function might be defined to operate on reals, complex values or matrices. The algorithm to be used in each case is different, and the return result may be different. By writing three separate functions with the same name, the programmer has the convenience of not having to remember different names for each type of data. Further if a subtype can be defined for the reals, to separate positive and negative reals, two functions can be written for the reals, one to return a real when the parameter is positive, and another to return a complex value when the parameter is negative.
In object-oriented programming, when a series of functions with the same name can accept different parameter profiles or parameters of different types, each of the functions is said to be overloaded.
Here is an example of subroutine overloading in C++:
In this code there are two functions of same name but they have different parameters.
As another example, a subroutine might construct an object that will accept directions, and trace its path to these points on screen. There are a plethora of parameters that could be passed in to the constructor (colour of the trace, starting x and y co-ordinates, trace speed). If the programmer wanted the constructor to be able to accept only the color parameter, then he could call another constructor that accepts only color, which in turn calls the constructor with all the parameters passing in a set of default values for all the other parameters (X and Y would generally be centered on screen or placed at the origin, and the speed would be set to another value of the coder's choosing).
A closure is a subprogram together with the values of some of its variables captured from the environment in which it was created. Closures were a notable feature of the Lisp programming language, introduced by John McCarthy. Depending on the implementation, closures can serve as a mechanism for side-effects.
A wide number of conventions for the coding of subroutines have been developed. Pertaining to their naming, many developers have adopted the approach that the name of a subroutine should be a verb when it does a certain task, an adjective when it makes some inquiry, and a noun when it is used to substitute variables.
Some programmers suggest that a subroutine should perform only one task, and if a subroutine does perform more than one task, it should be split up into more subroutines. They argue that subroutines are key components in code maintenance, and their roles in the program must remain distinct.
Proponents of modular programming (modularizing code) advocate that each subroutine should have minimal dependency on other pieces of code. For example, the use of global variables is generally deemed unwise by advocates for this perspective, because it adds tight coupling between the subroutine and these global variables. If such coupling is not necessary, their advice is to refactor subroutines to accept passed parameters instead. However, increasing the number of parameters passed to subroutines can affect code readability.
Besides its main or normal effect, a subroutine may need to inform the calling program about exceptional conditions that may have occurred during its execution. In some languages and programming standards, this is often done through a return code, an integer value placed by the subroutine in some standard location, which encodes the normal and exceptional conditions.
In the IBM System/360, where a return code was expected from the subroutine, the return value was often designed to be a multiple of 4—so that it could be used as a direct branch table index into a branch table often located immediately after the call instruction to avoid extra conditional tests, further improving efficiency. In the System/360 assembly language, one would write, for example:
There is a significant runtime overhead in a calling a subroutine, including passing the arguments, branching to the subprogram, and branching back to the caller. The overhead often includes saving and restoring certain processor registers, allocating and reclaiming call frame storage, etc.. In some languages, each subroutine call also implies automatic testing of the subroutine's return code, or the handling of exceptions that it may raise. In object-oriented languages, a significant source of overhead is the intensively used dynamic dispatch for method calls.
There are some seemingly obvious optimizations of procedure calls that cannot be applied if the procedures may have side effects. For example, in the expression (f(x)-1)/(f(x)+1), the function f must be called twice, because the two calls may return different results. Moreover, the value of x must be fetched again before the second call, since the first call may have changed it. Determining whether a subprogram may have a side effect is very difficult (indeed, undecidable).[citation needed] So, while those optimizations are safe in purely functional programming languages, compilers of typical imperative programming usually have to assume the worst.
A method used to eliminate this overhead is inline expansion or inlining of the subprogram's body at each call site (versus branching to the subroutine and back). Not only does this avoid the call overhead, but it also allows the compiler to optimize the procedure's body more effectively by taking into account the context and arguments at that call. The inserted body can be optimized by the compiler. Inlining however, will usually increase the code size, unless the program contains only one call to the subroutine, or the subroutine body is less code than the call overhead."
"104","A class in C++ is a user defined type or data structure declared with keyword class that has data and functions (also called methods) as its members whose access is governed by the three access specifiers private, protected or public (by default access to members of a class is private). The private members are not accessible outside the class; they can be accessed only through methods of the class. The public members form an interface to the class and are accessible outside the class. 
Instances of a class data type are known as objects and can contain  member variables, constants, member functions, and overloaded operators defined by the programmer.
In C++, a class defined with the class keyword has private members and base classes by default. A structure is a class defined with the struct keyword. Its members and base classes are public by default.
An aggregate class is a class with no user-declared constructors, no private or protected
non-static data members, no base classes, and no virtual functions. Such a class can be initialized with a brace-enclosed comma-separated list of initializer-clauses. The following code has the same semantics in both C and C++.
A POD-struct (Plain Old Data Structure) is an aggregate class that has no non-static data members of type non-POD-struct, non-POD-union (or array of such types) or reference, and has no user-defined assignment operator and no user-defined destructor. A POD-struct could be said to be the C++ equivalent of a C struct. In most cases, a POD-struct will have the same memory layout as a corresponding struct declared in C. For this reason, POD-structs are sometimes colloquially referred to as ""C-style structs"".
C++ classes have their own members. These members include variables (including other structures and classes), functions (specific identifiers or overloaded operators) known as methods, constructors and destructors. Members are declared to be either publicly or privately accessible using the public: and private: access specifiers respectively. Any member encountered after a specifier will have the associated access until another specifier is encountered. There is also inheritance between classes which can make use of the protected: specifier.
A class defined outside all methods is a global class because its objects can be created from anywhere in the program. If it is defined within a function body then it's a local class because objects of such a class are local to the function scope.
Classes are declared with the class or struct keyword. Declaration of members are placed within this declaration.
The above definitions are functionally equivalent. Either code will define objects of type person as having two public data members, name and age. The semicolons after the closing braces are mandatory.
After one of these declarations (but not both), person can be used as follows to create newly defined variables of the person datatype:
Executing the above code will output
An important feature of the C++ class and structure are member functions. Each datatype can have its own built-in functions (referred to as methods) that have access to all (public and private) members of the datatype. In the body of these non-static member functions, the keyword this can be used to refer to the object for which the function is called. This is commonly implemented by passing the address of the object as an implicit first argument to the function. Take the above person type as an example again:
In the above example the print() function is declared in the body of the class and defined by qualifying it with the name of the class followed by ::. Both name and age are private (default for class) and print() is declared as public which is necessary if it is to be used from outside the class.
With the member function print(), printing can be simplified into:
where a and b above are called senders, and each of them will refer to their own member variables when the print() function is executed.
It is common practice to separate the class or structure declaration (called its interface) and the definition (called its implementation) into separate units. The interface, needed by the user, is kept in a header and the implementation is kept separately in either source or compiled form.
The layout of non-POD classes in memory is not specified by the C++ standard. For example, many popular C++ compilers implement single inheritance by concatenation of the parent class fields with the child class fields, but this is not required by the standard. This choice of layout makes referring to a derived class via a pointer to the parent class type a trivial operation.
For example, consider
An instance of P with a P* p pointing to it might look like this in memory:
An instance of C with a P* p pointing to it might look like this:
Therefore, any code that manipulates the fields of a P object can manipulate the P fields inside the C object without having to consider anything about the definition of C's fields. A properly written C++ program shouldn't make any assumptions about the layout of inherited fields, in any case. Using the static_cast or dynamic_cast type conversion operators will ensure that pointers are properly converted from one type to another.
Multiple inheritance is not as simple. If a class D inherits P and C, then the fields of both parents need to be stored in some order, but (at most) only one of the parent classes can be located at the front of the derived class. Whenever the compiler needs to convert a pointer from the D type to either P or C, the compiler will provide an automatic conversion from the address of the derived class to the address of the base class fields (typically, this is a simple offset calculation).
For more on multiple inheritance, see virtual inheritance.
In C++, operators, such as + - * /, can be overloaded to suit the needs of programmers. These operators are called overloadable operators.
By convention, overloaded operators should behave nearly the same as they do in built-in datatypes (int, float, etc.), but this is not required. One can declare a structure called integer in which the variable really stores an integer, but by calling integer * integer the sum, instead of the product, of the integers might be returned:
The code above made use of a constructor to ""construct"" the return value. For clearer presentation (although this could decrease efficiency of the program if the compiler cannot optimize the statement into the equivalent one above), the above code can be rewritten as:
Programmers can also put a prototype of the operator in the struct declaration and define the function of the operator in the global scope:
i above represents the sender's own member variable, while k.i represents the member variable from the argument variable k.
The const keyword appears twice in the above code. The first occurrence, the argument const integer &k, indicated that the argument variable will not be changed by the function. The second incidence at the end of the declaration promises the compiler that the sender would not be changed by the function run.
In const integer &k, the ampersand (&) means ""pass by reference"". When the function is called, a pointer to the variable will be passed to the function, rather than the value of the variable.
The same overloading properties above apply also to classes.
Note that arity, associativity and precedence of operators cannot be changed.
Binary operators (operators with two arguments) are overloaded by declaring a function with an ""identifier"" operator (something) which calls one single argument. The variable on the left of the operator is the sender while that on the right is the argument.
'3' would be printed.
The following is a list of binary overloadable operators:
The '=' (assignment) operator between two variables of the same structure type is overloaded by default to copy the entire content of the variables from one to another. It can be overwritten with something else, if necessary.
Operators must be overloaded one by one, in other words, no overloading is associated with one another. For example, < is not necessarily the opposite of >.
While some operators, as specified above, takes two terms, sender on the left and the argument on the right, some operators have only one argument - the sender, and they are said to be ""unary"". Examples are the negative sign (when nothing is put on the left of it) and the ""logical NOT"" (exclamation mark, !).
Sender of unary operators may be on the left or on the right of the operator. The following is a list of unary overloadable operators:
The syntax of an overloading of a unary operator, where the sender is on the right, is as follows:
When the sender is on the left, the declaration is:
@ above stands for the operator to be overloaded. Replace return_type with the datatype of the return value (int, bool, structures etc.)
The int parameter essentially means nothing but a convention to show that the sender is on the left of the operator.
const arguments can be added to the end of the declaration if applicable.
The square bracket [] and the round bracket () can be overloaded in C++ structures. The square bracket must contain exactly one argument, while the round bracket can contain any specific number of arguments, or no arguments.
The following declaration overloads the square bracket.
The content inside the bracket is specified in the argument part.
Round bracket is overloaded a similar way.
Contents of the bracket in the operator call are specified in the second bracket.
In addition to the operators specified above, the arrow operator (->), the starred arrow (->*), the new keyword and the delete keyword can also be overloaded. These memory-or-pointer-related operators must process memory-allocating functions after overloading. Like the assignment (=) operator, they are also overloaded by default if no specific declaration is made.
Sometimes programmers may want their variables to take a default or specific value upon declaration. This can be done by declaring constructors.
Member variables can be initialized in an initializer list, with utilization of a colon, as in the example below.  This differs from the above in that it initializes (using the constructor), rather than using the assignment operator.  This is more efficient for class types, since it just needs to be constructed directly; whereas with assignment, they must be first initialized using the default constructor, and then assigned a different value.  Also some types (like references and const types) cannot be assigned to and therefore must be initialized in the initializer list.
Note that the curly braces cannot be omitted, even if empty.
Default values can be given to the last arguments to help initializing default values.
When no arguments are given to the constructor in the example above, it is equivalent to calling the following constructor with no arguments (a default constructor):
The declaration of a constructor looks like a function with the same name as the datatype. In fact, a call to a constructor can take the form of a function call. In that case a person type variable would be the return value:
The above code creates a temporary person object, and then assigns it to r using the copy constructor.  A better way of creating the object (without unnecessary copying) is:
Specific program actions, which may or may not relate to the variable, can be added as part of the constructor.
With the above constructor, a ""Hello!"" will be printed in case a person variable with no specific value is initialized.
Default constructors are called when constructors are not defined for the classes. 
However, if a user defined constructor was defined for the class, both of the above declarations will call this user defined constructor, whose defined code will be executed, but no default values will be assigned to the variable b.
A destructor is the reverse of a constructor. It is called when an instance of a class is destroyed, e.g. when an object of a class created in a block (set of curly braces ""{}"") is deleted after the closing brace, then the destructor is called automatically. It will be called upon emptying of the memory location storing the variables. Destructors can be used to release resources, such as heap-allocated memory and opened files when an instance of that class is destroyed.
The syntax for declaring a destructor is similar to that of a constructor. There is no return value and the name of the method is the same as the name of the class with a tilde (~) in front.
In C++, class declarations can be generated from class templates. Such class templates represent a family of classes. An actual class declaration is obtained by instantiating the template with one or more template arguments. A template instantiated with a particular set of arguments is called a template specialization.
The syntax of C++ tries to make every aspect of a structure look like that of the basic datatypes. Therefore, overloaded operators allow structures to be manipulated just like integers and floating-point numbers, arrays of structures can be declared with the square-bracket syntax (some_structure variable_name[size]), and pointers to structures can be dereferenced in the same way as pointers to built-in datatypes.
The memory consumption of a structure is at least the sum of the memory sizes of  constituent variables. Take the two nums structure below as an example.
The structure consists of two integers. In many current C++ compilers, integers are  32-bit integers by default, so each of the member variables consume four bytes of memory. The entire structure, therefore, consumes at least (or exactly) eight bytes of memory, as follows.
However, the compiler may add padding between the variables or at the end of the structure to ensure proper data alignment for a given computer architecture, often padding variables to be 32-bit aligned. For example, the structure
could look like 
in memory, where XX are two unused bytes.
As structures may make use of pointers and arrays to declare and initialize its member variables, memory consumption of structures is not necessarily constant. Another example of non-constant memory size is template structures.
Bit fields are used to define the class members that can occupy less storage than an integral type. This field is applicable only for integral types (int, char, short, long, etc.) and excludes float or double.
Bit fields are not allowed in a union. It is applicable only for the classes defined using the keyword struct or class.
Many programmers prefer to use the ampersand (&) to declare the arguments of a function involving structures. This is because by using the dereferencing ampersand only one word (typically 4 bytes on a 32 bit machine, 8 bytes on a 64 bit machine) is required to be passed into the function, namely the memory location to the variable. Otherwise, if pass-by-value is used, the argument needs to be copied every time the function is called, which is costly with large structures.
Since pass-by-reference exposes the original structure to be modified by the function, the const keyword should be used to guarantee that the function does not modify the parameter (see const-correctness), when this is not intended.
To facilitate structures' ability to reference themselves, C++ implements the this keyword for all member functions. The this keyword acts as a pointer to the current object. Its type is that of a pointer to the current object.
The this keyword is especially important for member functions with the structure itself as the return value:
As stated above, this is a pointer, so the use of the asterisk (*) is necessary to convert it into a reference to be returned.
General References:
"
"105","C++14 is a version of the ISO/IEC 14882 standard for the programming language C++. It is intended to be a small extension over C++11, featuring mainly bug fixes and small improvements. Its approval was announced on August 18, 2014. C++14 was released on December 15, 2014.
Because earlier C++ standard revisions were noticeably late, the name ""C++1y"" was sometimes used instead until its approval, similarly to how the C++11 standard used to be termed ""C++0x"" with the expectation of its release before 2010 (although in fact it slipped into 2010 and finally 2011).
These are the features added to the core language of C++14.
C++11 allowed lambda functions to deduce the return type based on the type of the expression given to the return statement. C++14 provides this ability to all functions. It also extends these facilities to lambda functions, allowing return type deduction for functions that are not of the form return expression;.
In order to induce return type deduction, the function must be declared with auto as the return type, but without the trailing return type specifier in C++11:
If multiple return expressions are used in the function's implementation, then they must all deduce the same type.
Functions that deduce their return types can be forward declared, but they cannot be used until they have been defined. Their definitions must be available to the translation unit that uses them.
Recursion can be used with a function of this type, but the recursive call must happen after at least one return statement in the definition of the function:
In C++11, two methods of type deduction were added. auto was a way to create a variable of the appropriate type, based on a given expression. decltype was a way to compute the type of a given expression. However, decltype and auto deduce types in different ways. In particular, auto always deduces a non-reference type, as though by using std::decay, while auto&& always deduces a reference type. However, decltype can be prodded into deducing a reference or non-reference type, based on the value category of the expression and the nature of the expression it is deducing:
C++14 adds the decltype(auto) syntax. This allows auto declarations to use the decltype rules on the given expression.
The decltype(auto) syntax can also be used with return type deduction, by using decltype(auto) syntax instead of auto for the function's return type deduction.
C++11 introduced the concept of a constexpr-declared function; a function which could be executed at compile time. Their return values could be consumed by operations that require constant expressions, such as an integer template argument. However, C++11 constexpr functions could only contain a single expression that is returned (as well as static_asserts and a small number of other declarations).
C++14 relaxes these restrictions. Constexpr-declared functions may now contain the following:
goto statements are forbidden in C++14 relaxed constexpr-declared functions.
Also, C++11 stated that all non-static member functions that were declared constexpr were also implicitly declared const, with respect to this. That has since been removed; non-static member functions may be non-const. However, per the above restrictions, a non-const constexpr member function can only modify a class member if that object's lifetime began within the constant expression evaluation.
In prior versions of C++, only functions, classes or type aliases could be templated. C++14 now allows the creation of variables that are templated. An example given in the proposal is a variable pi that can be read to get the value of pi for various types (e.g., 3 when read as an integral type; the closest value possible with float, double or long double precision when read as float, double or long double, respectively; etc.).
The usual rules of templates apply to such declarations and definitions, including specialization.
C++11 added member initializers, expressions to be applied to members at class scope if a constructor did not initialize the member itself. The definition of aggregates was changed to explicitly exclude any class with member initializers; therefore, they are not allowed to use aggregate initialization.
C++14 relaxes this restriction, allowing aggregate initialization on such types. If the braced init list does not provide a value for that argument, the member initializer takes care of it.
Numeric literals in C++14 can be specified in binary form. The syntax uses the prefixes 0b or 0B. The syntax is also used in other languages e.g. Java, C#, Swift, Go, Scala, Ruby, Python, OCaml, and as an unofficial extension in some C compilers since at least 2007.
In C++14, the single-quote character may be used arbitrarily as a digit separator in numeric literals, both integer literals and floating point literals. This can make it easier for human readers to parse large numbers through subitizing.
auto integer_literal = 1'000'000;auto floating_point_literal = 0.000'015'3;auto binary_literal = 0b0100'1100'0110;auto silly_example = 1'0'0'000'00;
In C++11, lambda function parameters need to be declared with concrete types. C++14 relaxes this requirement, allowing lambda function parameters to be declared with the auto type specifier.
As for auto type deduction, generic lambdas follow the rules of template argument deduction (which are similar, but not identical in all respects). The above code is equivalent to this:
Generic lambdas are essentially templated functor lambdas.
C++11 lambda functions capture variables declared in their outer scope by value-copy or by reference. This means that value members of a lambda cannot be move-only types. C++14 allows captured members to be initialized with arbitrary expressions. This allows both capture by value-move and declaring arbitrary members of the lambda, without having a correspondingly named variable in an outer scope.
This is done via the use of an initializer expression:
The lambda function lambda returns 1, which is what value was initialized with. The declared capture deduces the type from the initializer expression as if by auto.
This can be used to capture by move, via the use of the standard std::move function:
The deprecated attribute allows marking an entity deprecated, which makes it still legal to use but puts users on notice that use is discouraged and may cause a warning message to be printed during compilation. An optional string literal can appear as the argument of deprecated, to explain the rationale for deprecation and/or to suggest a replacement.
C++14 adds a shared timed mutex and a companion shared lock type.
The C++ Standard Library defines four associative container classes. These classes allow the user to look up a value based on a value of that type. The map containers allow the user to specify a key and a value, where lookup is done by key and returns a value. However, the lookup is always done by the specific key type, whether it is the key as in maps or the value itself as in sets.
C++14 allows the lookup to be done via an arbitrary type, so long as the comparison operator can compare that type with the actual key type. For the unordered containers, the hash function also needs to support the given key type such that the usual hashing invariant holds (i.e. objects that compare as equal must also have equal hash values). This would allow a map from std::string to some value to compare against a const char* or any other type for which an operator<  overload is available. It is also useful for indexing composite objects in a std::set by the value of a single member without forcing the user of find to create a dummy object (for example creating an entire struct Person to find a person by name).
To preserve backwards compatibility, heterogeneous lookup is only allowed when the comparator given to the associative container allows it. The standard library classes std::less<> and std::greater<> are augmented to allow heterogeneous lookup.
C++11 defined the syntax for user-defined literal suffixes, but the standard library did not use any of them. C++14 adds the following standard literals:
The two ""s"" literals do not interact, as the string one only operates on string literals, and the one for seconds operates only on numbers.
The std::tuple type introduced in C++11 allows an aggregate of typed values to be indexed by a compile-time constant integer. C++14 extends this to allow fetching from a tuple by type instead of by index. If the tuple has more than one element of the type, a compile-time error results:
std::make_unique can be used like std::make_shared for std::unique_ptr objects.
std::integral_constant gained an operator() overload to return the constant value.
The class template std::integer_sequence and related alias templates were added for representing compile-time integer sequences, such as the indices of elements in a parameter pack.
The global std::begin/std::end functions were augmented with std::cbegin/std::cend functions, which return constant iterators, and std::rbegin/std::rend and std::crbegin/std::crend which return reverse iterators.
The std::exchange function template assigns a new value to a variable and returns the old value.
New overloads of std::equal, std::mismatch, and std::is_permutation take a pair of iterators for the second range, so that the caller does not need to separately check that the two ranges are of the same length.
The std::is_final type trait detects if a class is marked final.
The std::quoted stream I/O manipulator allows inserting and extracting strings with embedded spaces, by placing delimiters (defaulting to double-quotes) on output and stripping them on input, and escaping any embedded delimiters.
Clang finished support for C++14 in 3.4 though under the standard name c++1y.GCC finished support for C++14 in GCC 5, and made C++14 the default C++ standard in GCC 6.Microsoft Visual Studio 2017 also has implemented ""almost all"" C++14 features."
"106","
 (Learn how and when to remove this template message)
Templates are a feature of the C++ programming language that allows functions and classes to operate with generic types.  This allows a function or class to work on many different data types without being rewritten for each one.
Templates are of great utility to programmers in C++, especially when combined with multiple inheritance and operator overloading. The C++ Standard Library provides many useful functions within a framework of connected templates.
Major inspirations for C++ templates were the parameterized modules provided by CLU and the generics provided by Ada.
There are three kinds of templates: function templates, class templates and, since C++14, variable templates. Since C++11, templates may be either variadic or non-variadic; in earlier versions of C++ they are always non-variadic.
A function template behaves like a function except that the template can have arguments of many different types (see example). In other words, a function template represents a family of functions. The format for declaring function templates with type parameters is:
Both expressions have the same meaning and behave in exactly the same way. The latter form was introduced to avoid confusion, since a type parameter need not be a class. (It can also be a basic type such as int or double.)
For example, the C++ Standard Library contains the function template max(x, y) which returns the larger of x and y. That function template could be defined like this:
This single function definition works with many data types. Specifically, it works with all data types for which > (the greater-than operator) is defined. The usage of a function template saves space in the source code file in addition to limiting changes to one function description and making the code easier to read.
A template does not produce smaller object code, though, compared to writing separate functions for all the different data types used in a specific program. For example, if a program uses both an int and a double version of the max() function template shown above, the compiler will create an object code version of max() that operates on int arguments and another object code version that operates on double arguments. The compiler output will be identical to what would have been produced if the source code had contained two separate non-templated versions of max(), one written to handle int and one written to handle double.
Here is how the function template could be used:
In the first two cases, the template argument Type is automatically deduced by the compiler to be int and double, respectively. In the third case automatic deduction of max(3, 7.0) would fail because the type of the parameters must in general match the template arguments exactly. Therefore, we explicitly instantiate the double version with max<double>().
This function template can be instantiated with any copy-constructible type for which the expression y > x is valid. For user-defined types, this implies that the greater-than operator (>) must be overloaded in the type.
A class template provides a specification for generating classes based on parameters. Class templates are generally used to implement containers. A class template is instantiated by passing a given set of types to it as template arguments. The C++ Standard Library contains many class templates, in particular the containers adapted from the Standard Template Library, such as vector.
In C++14, templates can be also used for variables, as in the following example:
When a function or class is instantiated from a template, a specialization of that template is created by the compiler for the set of arguments used, and the specialization is referred to as being a generated specialization.
Sometimes, the programmer may decide to implement a special version of a function (or class) for a given set of template type arguments which is called an explicit specialization. In this way certain template types can have a specialized implementation that is optimized for the type or more meaningful implementation than the generic implementation.
Explicit specialization is used when the behavior of a function or class for particular choices of the template parameters must deviate from the generic behavior: that is, from the code generated by the main template, or templates. For example, the template definition below defines a specific implementation of max() for arguments of type bool:
C++11 introduced variadic templates, which can take a variable number of arguments in a manner somewhat similar to variadic functions such as std::printf. Both function templates and class templates can be variadic.
Some uses of templates, such as the max() function mentioned above, were previously fulfilled by function-like preprocessor macros. For example, the following is a C++ max() macro that evaluates to the maximum of its two arguments as defined by the < operator:
Both macros and templates are expanded at compile time. Macros are always expanded inline, while templates are only expanded inline when the compiler deems it appropriate. When expanded inline, macro functions and function templates have no extraneous runtime overhead. Template functions with many lines of code will incur runtime overhead when they are not expanded inline, but the reduction in code size may help the code to fit into the CPU's instruction cache.
Macro arguments are not evaluated prior to expansion. The expression using the macro defined above
may evaluate to a negative number (because std::rand() will be called twice as specified in the macro, using different random numbers for comparison and output respectively), while the call to template function
will always evaluate to a non-negative number.
As opposed to macros, templates are considered type-safe; that is, they require type-checking at compile time. Hence, the compiler can determine at compile time whether the type associated with a template definition can perform all of the functions required by that template definition.
By design, templates can be utilized in very complex problem spaces, whereas macros are substantially more limited.
There are fundamental drawbacks to the use of templates:
Additionally, the use of the ""less than"" and ""greater than"" signs as delimiters is problematic for tools (such as text editors) which analyze source code syntactically. It is difficult for such tools to determine whether a use of these tokens is as comparison operators or template delimiters. For example, this line of code:
may be a function call with two parameters, each the result of a comparison expression, or possibly a function call with one parameter, utilizing the C++ comma operator (whose end result would depend on possible side effects of a, b, c, and/or d). Alternatively, it could be a declaration of a constructor for class foo taking a parameter d whose type is the parameterized a < b, c >.
Initially, the concept of templates was not included in some languages, such as Java and C# 1.0. Java's adoption of generics mimics the behavior of templates, but is technically different. C# added generics (parameterized types) in .NET 2.0. The generics in Ada predate C++ templates.
Although C++ templates, Java generics, and .NET generics are often considered similar, generics only mimic the basic behavior of C++ templates. Some of the advanced template features utilized by libraries such as Boost and STLSoft, and implementations of the STL itself, for template metaprogramming (explicit or partial specialization, default template arguments, template non-type arguments, template template arguments, ...) are not available with generics.
In C++ templates, compile-time cases were historically performed by pattern matching over the template arguments. For example, the template base class in the Factorial example below is implemented by matching 0 rather than with an inequality test, which was previously unavailable. However, the arrival in C++11 of standard library features such as std::conditional has provided another, more flexible way to handle conditional template instantiation.
With these definitions, one can compute, say 6! at compile time using the expression Factorial<6>::value.
Alternatively, constexpr in C++11 can be used to calculate such values directly using a function at compile-time."
"107","Substitution failure is not an error (SFINAE) refers to a situation in C++ where an invalid substitution of template parameters is not in itself an error.  David Vandevoorde first introduced the acronym SFINAE to describe related programming techniques.
Specifically, when creating a candidate set for overload resolution, some (or all) candidates of that set may be the result of instantiated templates with (potentially deduced) template arguments substituted for the corresponding template parameters. If an error occurs during the substitution of a set of arguments for any given template, the compiler removes the potential overload from the candidate set instead of stopping with a compilation error, provided the substitution error is one the C++ standard grants such treatment. If one or more candidates remain and overload resolution succeeds, the invocation is well-formed.
The following example illustrates a basic instance of SFINAE:
Here, attempting to use a non-class type in a qualified name (T::foo) results in a deduction failure for f<int> because int has no nested type named foo, but the program is well-formed because a valid function remains in the set of candidate functions.
Although SFINAE was initially introduced to avoid creating ill-formed programs when unrelated template declarations were visible (e.g., through the inclusion of a header file), many developers later found the behavior useful for compile-time introspection. Specifically, it allows a template to determine certain properties of its template arguments at instantiation time.
For example, SFINAE can be used to determine if a type contains a certain typedef:
When T has the nested type foobar defined, the instantiation of the first test works and the null pointer constant is successfully passed. (And the resulting type of the expression is yes.) If it does not work, the only available function is the second test, and the resulting type of the expression is no. An ellipsis is used not only because it will accept any argument, but also because its conversion rank is lowest, so a call to the first function will be preferred if it is possible; this removes ambiguity.
In C++11, the above code could be simplified to:
With the standardisation of the detection idiom in the Library fundamental v2 (n4562) proposal, the above code could be re-written as follows:
The developers of Boost used SFINAE in boost::enable_if and in other ways.
"
"108","In computer science and computer programming, a data type or simply type is a classification of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support various types of data, for example: real, integer or Boolean. A data type provides a set of values from which an expression (i.e. variable, function...) may take its values. The type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored. A type of value from which an expression may take its value.
Data types are used within type systems, which offer various ways of defining, implementing and using them. Different type systems ensure varying degrees of type safety.
Almost all programming languages explicitly include the notion of data type, though different languages may use different terminology.  
Common data types include:
For example, in the Java programming language, the type int represents the set of 32-bit integers ranging in value from -2,147,483,648 to 2,147,483,647, as well as the operations that can be performed on integers, such as addition, subtraction, and multiplication. Colors, on the other hand, are represented by three bytes denoting the amounts each of red, green, and blue, and one string representing that color's name; allowable operations include addition and subtraction, but not multiplication.
Most programming languages also allow the programmer to define additional data types, usually by combining multiple elements of other types and defining the valid operations of the new data type.  For example, a programmer might create a new data type named ""complex number"" that would include real and imaginary parts. 
A data type also represents a constraint placed upon the interpretation of data in a type system, describing representation, interpretation and structure of values or objects stored in computer memory. The type system uses data type information to check correctness of computer programs that access or manipulate the data.
Most data types in statistics have comparable types in computer programming, and vice versa, as shown in the following table:
(Parnas, Shore & Weiss 1976) identified five definitions of a ""type"" that were used—sometimes implicitly—in the literature.  Types including behavior align more closely with object-oriented models, whereas a structured programming model would tend to not include code, and are called plain old data structures.
The five types are:
The definition in terms of a representation was often done in imperative languages such as ALGOL and Pascal, while the definition in terms of a value space and behaviour was used in higher-level languages such as Simula and CLU.
Primitive data types are typically types that are built-in or basic to a language implementation.
All data in computers based on digital electronics is represented as bits (alternatives 0 and 1) on the lowest level. The smallest addressable unit of data is usually a group of bits called a byte (usually an octet, which is 8 bits). The unit processed by machine code instructions is called a word (as of 2011, typically 32 or 64 bits). Most instructions interpret the word as a binary number, such that a 32-bit word can represent unsigned integer values from 0 to 232−1{\displaystyle 2^{32}-1} or signed integer values from −231{\displaystyle -2^{31}} to 231−1{\displaystyle 2^{31}-1}. Because of two's complement, the machine language and machine doesn't need to distinguish between these unsigned and signed data types for the most part.
There is a specific set[which?] of arithmetic instructions that use a different[clarification needed] interpretation of the bits in word as a floating-point number.
Machine data types need to be exposed or made available in systems or low-level programming languages, allowing fine-grained control over hardware. The C programming language, for instance, supplies integer types of various widths, such as short and long. If a corresponding native type does not exist on the target platform, the compiler will break them down into code using types that do exist. For instance, if a 32-bit integer is requested on a 16 bit platform, the compiler will tacitly treat it as an array of two 16 bit integers.
Several languages allow binary and hexadecimal literals, for convenient manipulation of machine data.
In higher level programming, machine data types are often hidden or abstracted as an implementation detail that would render code less portable if exposed.  For instance, a generic numeric type might be supplied instead of integers of some specific bit-width.
The Boolean type represents the values true and false. Although only two values are possible, they are rarely implemented as a single binary digit for efficiency reasons. Many programming languages do not have an explicit Boolean type, instead interpreting (for instance) 0 as false and other values as true.
Boolean data simply refers to the logical structure of how the language is interpreted to the machine language. In this case a Boolean 0 refers to the logic False. True is always a non zero, especially a one which is known as Boolean 1.
Such as:
Composite types are derived from more than one primitive type. This can be done in a number of ways. The ways they are combined are called data structures. Composing a primitive type into a compound type generally results in a new type, e.g. array-of-integer is a different type to integer.
Many others are possible, but they tend to be further variations and compounds of the above.
The enumerated type has distinct values, which can be compared and assigned, but which do not necessarily have any particular concrete representation in the computer's memory; compilers and interpreters can represent them arbitrarily. For example, the four suits in a deck of playing cards may be four enumerators named CLUB, DIAMOND, HEART, SPADE, belonging to an enumerated type named suit.  If a variable V is declared having suit as its data type, one can assign any of those four values to it. Some implementations allow programmers to assign integer values to the enumeration values, or even treat them as type-equivalent to integers.
Such as:
Character and string types can store sequences of characters from a character set such as ASCII. Since most character sets include the digits, it is possible to have a numeric string, such as ""1234"". However, many languages treat these as belonging to a different type to the numeric value 1234.
Character and string types can have different subtypes according to the required character ""width"". The original 7-bit wide ASCII was found to be limited, and superseded by 8 and 16-bit sets, which can encode a wide variety of non-Latin alphabets (Hebrew, Chinese) and other symbols.
Strings may be either stretch-to-fit or of fixed size, even in the same programming language. They may also be subtyped by their maximum size.
Note: strings are not primitive in all languages, for instance C: they may be composed from arrays of characters.
Types can be based on, or derived from, the basic types explained above. In some languages, such as C, functions have a type derived from the type of their return value.
The main non-composite, derived type is the pointer, a data type whose value refers directly to (or ""points to"") another value stored elsewhere in the computer memory using its address. It is a primitive kind of reference. (In everyday terms, a page number in a book could be considered a piece of data that refers to another one). Pointers are often stored in a format similar to an integer; however, attempting to dereference or ""look up"" a pointer whose value was never a valid memory address would cause a program to crash. To ameliorate this potential problem, pointers are considered a separate type to the type of data they point to, even if the underlying representation is the same.
Any type that does not specify an implementation is an abstract data type. For instance, a stack (which is an abstract type) can be implemented as an array (a contiguous block of memory containing multiple values), or as a linked list (a set of non-contiguous memory blocks linked by pointers).
Abstract types can be handled by code that does not know or ""care"" what underlying types are contained
in them. Programming that is agnostic about concrete data types is called generic programming. Arrays and records can also contain underlying types, but are considered concrete because they specify how their contents or elements are laid out in memory.
Examples include:
For convenience, high-level languages may supply ready-made ""real world"" data types, for instance times, dates and monetary values and memory, even where the language allows them to be built from primitive types.
A type system associates types with computed values. By examining the flow of these values, a type system attempts to prove that no type errors can occur. The type system in question determines what constitutes a type error, but a type system generally seeks to guarantee that operations expecting a certain kind of value are not used with values for which that operation does not make sense.
A compiler may use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).
The depth of type constraints and the manner of their evaluation affect the typing of the language. A programming language may further associate an operation with varying concrete algorithms on each type in the case of type polymorphism. Type theory is the study of type systems, although the concrete type systems of programming languages originate from practical issues of computer architecture, compiler implementation, and language design.
Type systems may be variously static or dynamic, strong or weak typing, and so forth."
"109","In programming language theory, a reference type is a data type that refers to an object in memory. A pointer type on the other hand refers to a memory address. Reference types can be thought of as pointers that are implicitly dereferenced. The objects being referred to are dynamically allocated on the heap whereas value types are allocated automatically on the stack. In languages supporting garbage collection the objects being referred to are destroyed automatically after they become unreachable. 
When a reference type variable refers to an immutable object it behaves with the same semantics as a primitive value type. The fact that the object being referred to cannot be modified by any of the references to it means the only way to change the value of the reference variable is through assignment. The use of the Number classes in the Java programming language is an example of this behavior.
"
"110","Action Message Format (AMF) is a binary format used to serialize object graphs such as ActionScript objects and XML, or send messages between an Adobe Flash client and a remote service, usually a Flash Media Server or third party alternatives. The Actionscript 3 language provides classes for encoding and decoding from the AMF format.
The format is often used in conjunction with Adobe's RTMP to establish connections and control commands for the delivery of streaming media. In this case, the AMF data is encapsulated in a chunk which has a header which defines things such as the message length and type (whether it is a ""ping"", ""command"" or media data).
AMF was introduced with Flash Player 6, and this version is referred to as AMF0. It was unchanged until the release of Flash Player 9 and ActionScript 3.0, when new data types and language features prompted an update, called AMF3. Flash Player 10 added vector and dictionary data types documented in a revised specification of January 2013.
Adobe Systems published the AMF binary data protocol specification in December 2007 and announced that it will support the developer community to make this protocol available for every major server platform.
The following amf-packet is for transmission of messages outside of defined Adobe/Macromedia containers or transports such as Flash Video or the Real Time Messaging Protocol.
If either the header-length or message-length are unknown then they are set to -1 or 0xFFFFFFFF
uimsbf: unsigned integer, most significant bit first
simsbf: signed integer, most significant bit first
The format specifies the various data types that can be used to encode data. Adobe states that AMF is mainly used to represent object graphs that include named properties in the form of key-value pairs, where the keys are encoded as strings and the values can be of any data type such as strings or numbers as well as arrays and other objects. XML is supported as a native type. Each type is denoted by a single byte preceding the actual data. The values of that byte are as below (for AMF0):
AMF objects begin with a (0x03) followed by a set of key-value pairs and end with a (0x09) as value (preceded by 0x00 0x00 as empty key entry). Keys are encoded as strings with the (0x02) 'type-definition' byte being implied (not included in the message). Values can be of any type including other objects and whole object graphs can be serialized in this way. Both object keys and strings are preceded by two bytes denoting their length in number of bytes. This means that strings are preceded by a total of three bytes which includes the 0x02 type byte. Null types only contain their type-definition (0x05). Numbers are encoded as double-precision floating point and are composed of eight bytes.
As an example, when encoding the object below in actionscript 3 code.
The data held in the ByteArray is:
. . . n a m e . . . M i k e . . a g e . @ > . . . . . . . . a l i a s . . . M i k e . . .
Note: the object properties can be sorted in a different order from the one in which they are placed in actionscript. For coloring/markup, refer to the legend below.
The code above will work only for built-in classes like Object. To serialise and deserialise custom classes, the user needs to declare them using the registerClassAlias command or else an error will be thrown by the player.
Although, strictly speaking, AMF is only a data encoding format, it is usually found encapsulated in a RTMP message or Flex RPC call. An example of the former can be found below (it is the ""_result"" message returned in response to the ""connect"" command sent from the flash client):
legend: object start/end object keys object values ecma_array
The AMF message starts with a 0x03 which denotes an RTMP packet with Header Type of 0, so 12 bytes are expected to follow. It is of Message Type 0x14, which denotes a command in the form of a string of value ""_result"" and two serialized objects as arguments. The message can be decoded as follows:
Here one can see an array (in turquoise) as a value of the 'data' key which has one member. We can see the objectEncoding value to be 3. This means that subsequent messages are going to be sent with the 0x11 message type, which will imply an AMF3 encoding.
The latest version of the protocol specifies significant changes that allow for a more compressed format. The data markers are as follows:
The first 4 types are not followed by any data (Booleans have two types in AMF3).
Additional markers used by Flash Player 10 (the format is still referred to as AMF3) are as follows:
AMF3 aims for more compression and one of the ways it achieves this is by avoiding string duplication by saving them into an array against which all new string are checked. The byte following the string marker is no longer denoting pure length but it is a complex byte where the least significant bit indicated whether the string is 'inline' (1) i.e. not in the array or 'reference' (0) in which case the index of the array is saved. The table includes keys as well as values.
In older versions of Flash player there existed one number type called 'Number' which was a 64-bit double precision encoding. In the latest releases there is an int and a uint which are included in AMF3 as separate types. Number types are identical to AMF0 encoding while Integers have variable length from 1 to 4 bytes where the most significant bit of bytes 1-3 indicates that they are followed by another byte.
The various AMF Protocols are supported by many server-side languages and technologies, in the form of libraries and services that must be installed and integrated by the application developer.
Platforms:
Frameworks:"
"111","Some programming languages provide a complex data type for complex number storage and arithmetic as a built-in (primitive) data type.
In some programming environments the term complex data type (in contrast to primitive data types) is a synonym for the composite data type.
A complex variable or value is usually represented as a pair of floating point numbers.  Languages that support a complex data type usually provide special syntax for building such values, and extend the basic arithmetic operations ('+', '−', '×', '÷') to act on them.  These operations are usually translated by the compiler into a sequence of floating-point machine instructions or into library calls.  Those languages may also provide support for other operations, such as formatting, equality testing, etc.  As in mathematics, those languages often interpret a floating-point value as equivalent to a complex value with a zero imaginary part.
The COMPLEX data type was provided in FORTRAN IV.
"
"112","A character literal is a type of literal in programming for the representation of a single character's value within the source code of a computer program.
Languages that have a dedicated character data type generally include character literals; these include C, C++, Java, and Visual Basic, but not Python or PHP. Languages without character data types will typically use strings of length 1 to serve the same purpose a character data type would fulfil. This simplifies the implementation and basic usage of a language but also introduces new scope for programming errors.
A common convention for expressing a character literal is to use a single quote (') for character literals, as contrasted by the use of a double quote ("") for string literals. For example, 'a' indicates the single character a while ""a"" indicates the string a of length 1.
The representation of a character within the computer memory, in storage, and in data transmission, is dependent on a particular character encoding scheme. For example, an ASCII (or extended ASCII) scheme will use a single byte of computer memory, while a UTF-8 scheme will use one or more bytes, depending on the particular character being encoded.
Alternative ways to encode character values include specifying an integer value for a code point, such as an ASCII code value or a Unicode code point. This may be done directly via converting an integer literal to a character, or via an escape sequence."
"113","In computer science, conditional statements, conditional expressions and conditional constructs are features of a programming language, which perform different computations or actions depending on whether a programmer-specified boolean condition evaluates to true or false. Apart from the case of branch predication, this is always achieved by selectively altering the control flow based on some condition.
In imperative programming languages, the term ""conditional statement"" is usually used, whereas in functional programming, the terms ""conditional expression"" or ""conditional construct"" are preferred, because these terms all have distinct meanings.
A conditional is sometimes colloquially referred to as an ""if-check,"" especially when perceived as a simple one and when its specific form is irrelevant or unknown.
Although dynamic dispatch is not usually classified as a conditional construct, it is another way to select between alternatives at runtime.
The if–then construct (sometimes called if–then–else) is common across many programming languages. Although the syntax varies from language to language, the basic structure (in pseudocode form) looks like this:
In the example code above, the part represented by (boolean condition) constitutes a conditional expression, having intrinsic value (e.g., it may be substituted by either of the values True or False) but having no intrinsic meaning. In contrast, the combination of this expression, the If and Then surrounding it, and the consequent that follows afterward constitute a conditional statement, having intrinsic meaning (e.g., expressing a coherent logical rule) but no intrinsic value.
When an interpreter finds an If, it expects a boolean condition – for example, x > 0, which means ""the variable x contains a number that is greater than zero"" – and evaluates that condition. If the condition is true, the statements following the then are executed. Otherwise, the execution continues in the following branch – either in the else block (which is usually optional), or if there is no else branch, then after the end If.
After either branch has been executed, control returns to the point after the end If.
In early programming languages, especially some dialects of BASIC in the 1980s home computers, an if–then statement could only contain GOTO statements. This led to a hard-to-read style of programming known as spaghetti programming, with programs in this style called spaghetti code.  As a result, structured programming, which allows (virtually) arbitrary statements to be put in statement blocks inside an if statement, gained in popularity, until it became the norm even in most BASIC programming circles. Such mechanisms and principles were based on the older but more advanced ALGOL family of languages, and ALGOL-like languages such as Pascal and Modula-2 influenced modern BASIC variants for many years. While it is possible while using only GOTO statements in if–then statements to write programs that are not spaghetti code and are just as well structured and readable as programs written in a structured programming language, structured programming makes this easier and enforces it.  Structured if–then–else statements like the example above are one of the key elements of structured programming, and they are present in most popular high-level programming languages such as C, Java, JavaScript and Visual Basic .
A subtlety is that the optional else clause found in many languages means that the context-free grammar is ambiguous, since nested conditionals can be parsed in multiple ways. Specifically, 
can be parsed as
or
depending on whether the else is associated with the first if or second if. This is known as the dangling else problem, and is resolved in various ways, depending on the language.
By using else if, it is possible to combine several conditions. Only the statements following the first condition that is found to be true will be executed. All other statements will be skipped. The statements of
elseif, in Ada, is simply syntactic sugar for else followed by if. In Ada, the difference is that only one end if is needed, if one uses elseif instead of else followed by if. This is similar in Perl, which provides the keyword elsif to avoid the large number of braces that would be required by multiple if and else statements and also in Python, which uses the special keyword elif because structure is denoted by indentation rather than braces, so a repeated use of else and if would require increased indentation after every condition. Similarly, the earlier UNIX shells (later gathered up to the POSIX shell syntax) use elif too, but giving the choice of delimiting with spaces, line breaks, or both.
However, in many languages more directly descended from Algol, such as Algol68, Simula, Pascal, BCPL and C, this special syntax for the else if construct is not present, nor is it present in the many syntactical derivatives of C, such as Java, ECMA-script, PHP, and so on. This works because in these languages, any single statement (in this case if cond...) can follow a conditional without being enclosed in a block.
This design choice has a slight ""cost"" in that code else if branch is, effectively, adding an extra nesting level, complicating the job for some compilers (or its implementors), which has to analyse and implement arbitrarily long else if chains recursively.
If all terms in the sequence of conditionals are testing the value of a single expression (e.g., if x=0 ... else if x=1 ... else if x=2...), then an alternative is the switch statement, also called case-statement or select-statement. Conversely, in languages that do not have a switch statement, these can be produced by a sequence of else if statements.
Many languages support if expressions, which are similar to if statements, but return a value as a result. Thus, they are true expressions (which evaluate to a value), not statements (which changes the program state or perform some kind of action).
ALGOL 60 and some other members of the ALGOL family allow if–then–else as an expression:
In dialects of Lisp – Scheme, Racket and Common Lisp – the first of which was inspired to a great extent by ALGOL:
In Haskell 98, there is only an if expression, no if statement, and the else part is compulsory, as every expression must have some value. Logic that would be expressed with conditionals in other languages is usually expressed with pattern matching in recursive functions.
Because Haskell is lazy, it is possible to write control structures, such as if, as ordinary expressions; the lazy evaluation means that an if function can evaluate only the condition and proper branch (where a strict language would evaluate all three). It can be written like this:
C and C-like languages have a special ternary operator (?:) for conditional expressions with a function that may be described by a template like this:
This means that it can be inlined into expressions, unlike if-statements, in C-like languages:
which can be compared to the Algol-family if–then–else expressions (and similar in Ruby and Scala, among others).
To accomplish the same using an if-statement, this would take more than one line of code (under typical layout conventions):
Some argue that the explicit if/then statement is easier to read and that it may compile to more efficient code than the ternary operator, while others argue that concise expressions are easier to read than statements spread over several lines.
First, when the user runs the program, a cursor appears waiting for the reader to type a number. If that number is greater than 10, the text ""My variable is named 'foo'."" is displayed on the screen. If the number is smaller than 10, then the message ""My variable is named 'bar'."" is printed on the screen.
In Visual Basic and some other languages, a function called IIf is provided, which can be used as a conditional expression. However, it does not behave like a true conditional expression, because both the true and false branches are always evaluated; it is just that the result of one of them is thrown away, while the result of the other is returned by the IIf function.
Up to Fortran 77, the language Fortran has an ""arithmetic if"" statement which is halfway between a computed IF and a case statement, based on the trichotomy x < 0, x = 0, x > 0. This was the earliest conditional statement in Fortran:
Where e is any numeric expression (not necessarily an integer); this is equivalent to
Because this arithmetic IF is equivalent to multiple GOTO statements that could jump to anywhere, it is considered to be an unstructured control statement, and should not be used if more structured statements can be used. In practice it has been observed that most arithmetic IF statements referenced the following statement with one or two of the labels.
This was the only conditional control statement in the original implementation of Fortran on the IBM 704 computer. On that computer the test-and-branch op-code had three addresses for those three states. Other computers would have ""flag"" registers such as positive, zero, negative, even, overflow, carry, associated with the last arithmetic operations and would use instructions such as 'Branch if accumulator negative' then 'Branch if accumulator zero' or similar. Note that the expression is evaluated once only, and in cases such as integer arithmetic where overflow may occur, the overflow or carry flags would be considered also.
In contrast to other languages, in Smalltalk the conditional statement is not a language construct but defined in the class Boolean as an abstract method that takes two parameters, both closures. Boolean has two subclasses, True and False, which both define the method, True executing the first closure only, False executing the second closure only.
In Lambda Calculus, the concept of an if-then-else conditional can be expressed using the expressions:
note: if ifThenElse is passed two functions as the left and right conditionals; it is necessary to also pass an empty tuple () to the result of ifThenElse in order to actually call the chosen function, otherwise ifThenElse will just return the function object without getting called.
In a system where numbers can be used without definition(like Lisp, Traditional paper math, so on), the above can be expressed as a single closure below:
Here, true, false, and ifThenElse are bound to their respective definitions which are passed to their scope at the end of their block.
A working JavaScript analogy(using only functions of single variable for rigor) to this is:
The code above with multivariable functions looks like this: 
another version of the earlier example without a system where numbers are assumed is below.
First example shows the  first branch being taken, while second example shows the second branch being taken.
Smalltalk uses a similar idea for its  true and false representations, with True and False being singleton objects that respond to messages ifTrue/ifFalse differently.
Haskell used to use this exact model for its Boolean type, but at the time of writing, most Haskell programs use syntactic sugar ""if a then b else c"" construct which unlike ifThenElse does not compose unless
either wrapped in another function or re-implemented as shown in The Haskell section of this page.
Switch statements (in some languages, case statements or multiway branches) compare a given value with specified constants and take action according to the first constant to match. There is usually a provision for a default action ('else','otherwise') to be taken if no match succeeds. Switch statements can allow compiler optimizations, such as lookup tables. In dynamic languages, the cases may not be limited to constant expressions, and might extend to pattern matching, as in the shell script example on the right, where the '*)' implements the default case as a regular expression matching any string.
Pattern matching may be seen as a more sophisticated alternative to both if–then–else, and case statements. It is available in many programming languages with functional programming features, such as Wolfram Language, ML and many others. Here is a simple example written in the OCaml language:
The power of pattern matching is the ability to concisely match not only actions but also values to patterns of data. Here is an example written in Haskell which illustrates both of these features:
This code defines a function map, which applies the first argument (a function) to each of the elements of the second argument (a list), and returns the resulting list. The two lines are the two definitions of the function for the two kinds of arguments possible in this case – one where the list is empty (just return an empty list) and the other case where the list is not empty.
Pattern matching is not strictly speaking always a choice construct, because it is possible in Haskell to write only one alternative, which is guaranteed to always be matched – in this situation, it is not being used as a choice construct, but simply as a way to bind names to values. However, it is frequently used as a choice construct in the languages in which it is available.
In programming languages that have associative arrays or comparable data structures, such as Python, Perl, PHP or Objective-C, it is idiomatic to use them to implement conditional assignment.
In languages that have anonymous functions or that allow a programmer to assign a named function to a variable reference, conditional flow can be implemented by using a hash as a dispatch table.
An alternative to conditional branch instructions is predication. Predication is an architectural feature that enables instructions to be conditionally executed instead of modifying the control flow.
This table refers to the most recent language specification of each language. For languages that do not have a specification, the latest officially released implementation is referred to."
"114","
In computer science, a relational operator is a programming language construct or operator that tests or defines some kind of relation between two entities. These include numerical equality (e.g., 5 = 5) and inequalities (e.g., 4 ≥ 3).
In programming languages that include a distinct boolean data type in their type system, like Pascal, Ada, or Java, these operators usually evaluate to true or false, depending on if the conditional relationship between the two operands holds or not. In languages such as C, relational operators return the integers 0 or 1, where 0 stands for false and any non-zero value stands for true.
An expression created using a relational operator forms what is termed a relational expression or a condition. Relational operators can be seen as special cases of logical predicates.
Equality is being used in many programming-language constructs and data types. It is used to test if an element already exists in a set, or to access to a value through a key. It is used in switch statements to dispatch the control flow to the correct branch, and during the unification process in logic programming.
One of the possible meaning of equality is that ""if a equals to b, then we can use either a or b interchangeably in any context without noticing any difference"". But this statement does not necessarily hold, particularly when taking into account mutability together with content equality.
Sometimes, particularly in object-oriented programming, the comparison raises questions of data types and inheritance, equality, and identity. It is often necessary to distinguish between:
In many modern programming languages, objects and data structures are accessed through references. In such languages, there becomes a need to test for two different types of equality:
The first type of equality usually implies the second (except for things like not a number (NaN) which are unequal to themselves), but the converse is not necessarily true. For example, two string objects may be distinct objects (unequal in the first sense) but contain the same sequence of characters (equal in the second sense). See identity for more of this issue.
Real numbers, including many simple fractions, cannot be represented exactly in floating-point arithmetic, and it may be necessary to test for equality within a given tolerance. Such tolerance, however, can easily break desired properties such as transitivity, whereas reflexivity breaks too: the IEEE floating point standard requires that Nan ≠ NaN holds.
Other programming elements such as computable functions, may either have no sense of equality, or an equality that is uncomputable. For these reasons, some languages define an explicit notion of ""comparable"", in the form of a base class, an interface, a trait or a protocol, which is used either explicitly, by declaration in source code, or implicitly, via the structure of the type involved.
In JavaScript, PHP, VBScript and a few other dynamically typed languages, the standard equality operator evaluates to true if two values are equal, even if they have different types, making the number 4 compare equal to the text string ""4"", for instance. A typed equality operator is often available also, in such languages, returning true only for values with identical or equivalent types (in PHP 5, 4 === ""4"" is false although 4 == ""4"" is true). For languages where the number 0 may be interpreted as false, this operator may simplify things such as checking for zero (as x == 0 would be true for x being either 0 or ""0"" using the type agnostic equality operator).
Greater than and less than comparison of non-numeric data is performed according to a sort convention (such as, for text strings, lexicographical order) which may be built into the programming language and/or configurable by a programmer.
When it is desired to associate a numeric value with the result of a comparison between two data items, say a and b, the usual convention is to assign −1 if a < b, 0 if a = b and 1 if a > b. For example, the C function strcmp performs a three-way comparison and returns −1, 0, or 1 according to this convention, and qsort expects the comparison function to return values according to this convention. In sorting algorithms, the efficiency of comparison code is critical since it is one of the major factors contributing to sorting performance.
Comparison of programmer-defined data types (data types for which the programming language has no in-built understanding) may be carried out by custom-written or library functions (such as strcmp mentioned above), or, in some languages, by overloading a comparison operator – that is, assigning a programmer-defined meaning that depends on the data types being compared. Another alternative is using some convention such as memberwise comparison.
Though perhaps unobvious at first, like the boolean logical operators XOR, AND, OR, and NOT, relational operators can be designed to have logical equivalence, such that they can all be defined in terms of one another. The following four conditional statements all have the same logical equivalence E (either all true or all false) for any given x and y values:
This relies on the domain being well ordered.
The most common numerical relational operators used in programming languages are shown below.
Other conventions are less common: Common Lisp and Macsyma/Maxima use Basic-like operators except for inequality, which is /= in Common Lisp and # in Macsyma/Maxima. Older Lisps used equal, greaterp, and lessp; and negated them using not for the remaining operators.
Relational operators are also used in technical literature instead of words. Relational operators are usually written in infix notation, if supported by the programming language, which means that they appear between their operands (the two expressions being related). For example, an expression in Python will print the message if the x is less than y:
Other programming languages, such as Lisp, use prefix notation, as follows:
In mathematics, it is common practice to chain relational operators, such as in 3 < x < y < 20 (meaning 3 < x and x < y and y < 20). The syntax is clear since these relational operators in mathematics are transitive.
However, many recent programming languages would see an expression like 3 < x < y as consisting of two left (or right-) associative operators, interpreting it as something like (3 < x) < y. If we say that x=4, we then get (3 < 4) < y, and evaluation will give true < y which generally does not make sense. However, it does compile in C/C++ and some other languages, yielding surprising result (as true would be represented by the number 1 here).
It is possible to give the expression x < y < z its familiar mathematical meaning, and some programming languages such as Python and Perl 6 do that. Others, such as C# and Java, do not, partly because it would differ from the way most other infix operators work in C-like languages. The D programming language do not do that since it maintains some compatibility with C, and ""Allowing C expressions but with subtly different semantics (albeit arguably in the right direction) would add more confusion than convenience"".
Some languages, like Common Lisp, use multiple argument predicates for this. In Lisp (<= 1 x 10) is true when x is between 1 and 10.
Early FORTRAN (1956–57) was bounded by heavily restricted character sets where = was the only relational operator available. There were no < or > (and certainly no ≤ or ≥). This forced the designers to define symbols such as .GT., .LT., .GE., .EQ. etc. and subsequently made it tempting to use the remaining = character for copying, despite the obvious incoherence with mathematical usage (X=X+1 should be impossible).
International Algebraic Language (IAL, ALGOL 58) and ALGOL (1958 and 1960) thus introduced := for assignment, leaving the standard = available for equality, a convention followed by CPL, ALGOL W, ALGOL 68, Basic Combined Programming Language (BCPL), Simula, SET Language (SETL), Pascal, Smalltalk, Modula-2, Ada, Standard ML, OCaml, Eiffel, Object Pascal (Delphi), Oberon, Dylan, VHSIC Hardware Description Language (VHDL), and several other languages.
This uniform de facto standard among most programming languages was eventually changed, indirectly, by a minimalist compiled language named B. Its sole intended application was as a vehicle for a first port of (a then very primitive) Unix, but it also evolved into the very influential C language.
B started off as a syntactically changed variant of the systems programming language BCPL, a simplified (and typeless) version of CPL. In what has been described as a ""strip-down"" process, the and and or operators of BCPL were replaced with & and | (which would later become && and ||, respectively.). In the same process, the ALGOL style := of BCPL was replaced by = in B. The reason for all this being unknown. As variable updates had no special syntax in B (such as let or similar) and were allowed in expressions, this non standard meaning of the equal sign meant that the traditional semantics of the equal sign now had to be associated with another symbol. Ken Thompson used the ad hoc == combination for this.
As a small type system was later introduced, B then became C. The popularity of this language along with its association with Unix, led to Java, C#, and many other languages following suit, syntactically, despite this needless conflict with the mathematical meaning of the equal sign.
Assignments in C have a value and since any non-zero scalar value is interpreted as true in conditional expressions, the code if (x = y) is legal, but has a very different meaning from if (x == y). The former code fragment means ""assign y to x, and if the new value of x is not zero, execute the following statement"". The latter fragment means ""if and only if x is equal to y, execute the following statement"".
Though Java and C# have the same operators as C, this mistake usually causes a compile error in these languages instead, because the if-condition must be of type boolean, and there is no implicit way to convert from other types (e.g., numbers) into booleans. So unless the variable that is assigned to has type boolean (or wrapper type Boolean), there will be a compile error.
In ALGOL-like languages such as Pascal, Delphi, and Ada (in the sense that they allow nested function definitions), and in Python, and many functional languages, among others, assignment operators cannot appear in an expression (including if clauses), thus precluding this class of error. Some compilers, such as GNU Compiler Collection (GCC), provide a warning when compiling code containing an assignment operator inside an if statement, though there are some legitimate uses of an assignment inside an if-condition. In such cases, the assignment must be wrapped in an extra pair of parentheses explicitly, to avoid the warning.
Similarly, some languages, such as BASIC use just the = symbol for both assignment and equality, as they are syntactically separate (as with Pascal, Ada, Python, etc., assignment operators cannot appear in expressions).
Some programmers get in the habit of writing comparisons against a constant in the reverse of the usual order:
If = is used accidentally, the resulting code is invalid because 2 is not a variable. The compiler will generate an error message, on which the proper operator can be substituted. This coding style is termed left-hand comparison, or Yoda conditions.
This table lists the different mechanisms to test for these two types of equality in various languages:
Ruby uses a === b to mean ""b is a member of the set a"", though the details of what it means to be a member vary considerably depending on the data types involved.  === is here known as the ""case equality"" or ""case subsumption"" operator."
"115"," (Learn how and when to remove this template message)
The computer programming languages C and Pascal have similar times of origin, influences, and purposes. Both were used to design (and compile) their own compilers early in their lifetimes. The original Pascal definition appeared in 1969 and a first compiler in 1970. The first version of C appeared in 1972.
Both are descendants of the ALGOL language series. ALGOL introduced programming language support for structured programming, where programs are constructed of single entry and single exit constructs such as if, while, for and case. Pascal stems directly from ALGOL W, while it shared some new ideas with ALGOL 68. The C language is more indirectly related to ALGOL, originally through B, BCPL, and CPL, and later through ALGOL 68 (for example in case of struct and union) and also Pascal (for example in case of enumerations, const, typedef and booleans). Some Pascal dialects also incorporated traits from C.
What is documented here is the Pascal of Niklaus Wirth, as standardized as ISO 7185 in 1982. The C documented is the language of Brian Kernighan and Dennis Ritchie, as standardized in 1989. The reason is that these versions both represent the mature version of the language, and also because they are comparatively close in time. ANSI C and C99 (the later C standards) features, and features of later implementations of Pascal (Turbo Pascal, Free Pascal) are not included in the comparison, despite the improvements in robustness and functionality that they conferred.
Syntactically, Pascal is much more ALGOL-like than C. English keywords are retained where C uses punctuation symbols – Pascal has and, or, and mod where C uses &&, ||, and % for example. However, C is actually more ALGOL-like than Pascal regarding (simple) declarations, retaining the type-name variable-name syntax. For example, C can accept declarations at the start of any block, not just the outer block of a function.
Another, more subtle, difference is the role of the semicolon. In Pascal semicolons separate individual statements within a compound statement whereas they terminate the statement in C. They are also syntactically part of the statement itself in C (transforming an expression into a statement). This difference manifests itself primarily in two situations:
A superfluous semicolon can be put on the last line before end, thereby formally inserting an empty statement.
In traditional C, there are only /* block comments */. This is only supported by certain Pascal dialects like MIDletPascal.
In traditional Pascal, there are { block comments } and (* block comments *).
Modern Pascal, like Object Pascal (Delphi, FPC), as well as modern C implementations allow C++ style comments // line comments
C and Pascal differ in their interpretation of upper and lower case. C is case sensitive while Pascal is not, thus MyLabel and mylabel are distinct names in C but identical in Pascal. In both languages, identifiers consist of letters and digits, with the rule that the first character may not be a digit. In C, the underscore counts as a letter, so even _abc is a valid name. Names with a leading underscore are often used to differentiate special system identifiers in C.
Both C and Pascal use keywords (words reserved for use by the language itself). Examples are if, while, const, for and goto, which are keywords that happen to be common to both languages. In C, the basic built-in type names are also keywords (e.g. int, char) or combinations of keywords (e.g. unsigned char), while in Pascal the built-in type names are predefined normal identifiers.
In Pascal, procedure definitions start with keywords procedure or function and type definitions with type. In C, function definitions are determined by syntactical context while type definitions use the keyword typedef. Both languages use a mix of keywords and punctuation for definitions of complex types; for instance, arrays are defined by the keyword array in Pascal and by punctuation in C, while enumerations are defined by the keyword enum in C but by punctuation in Pascal.
In Pascal functions, begin and end delimit a block of statements (proper), while C functions use ""{"" and ""}"" to delimit a block of statements optionally preceded by declarations. C (before C99) strictly defines that any declarations must occur before the statements within a particular block but allows blocks to appear within blocks, which is a way to go around this. Pascal is strict that declarations must occur before statements, but allows definitions of types and functions - not only variable declarations - to be encapsulated by function definitions to any level of depth.
The grammars of both languages are of a similar size.  From an implementation perspective the main difference between the two languages is that to parse C it is necessary to have access to a symbol table for types, while in Pascal there is only one such construct, assignment.  For instance, the C fragment X * Y; could be a declaration of Y to be an object whose type is pointer to X, or a statement-expression that multiplies X and Y. The corresponding Pascal fragment var Y:^X; is unambiguous without a symbol table.
Pascal requires all variable and function declarations to specify their type explicitly. In traditional C, a type name may be omitted in most contexts and the default type int (which corresponds to integer in Pascal) is then implicitly assumed (however, such defaults are considered bad practice in C and are often flagged by warnings).
C accommodates different sizes and signed and unsigned modes for integers by using modifiers such as long, short, signed, unsigned, etc. The exact meaning of the resulting integer type is machine-dependent, however, what can be guaranteed is that  int  is at least 16 bits, long int is no shorter than int and short int is no longer than int.
In Pascal, a similar end is performed by declaring a subrange of integer (a compiler may then choose to allocate a smaller amount of storage for the declared variable):
This subrange feature is not supported by C.
A major, if subtle, difference between C and Pascal is how they promote integer operations. In Pascal, the result of an operation is defined for all integer/subrange types, even if intermediate results do not fit into an integer. The result is undefined only if it does not fit into the integer/subrange on the left hand side of the assignment. This may imply an artificial restriction on the range of integer types, or may require slow execution to handle the intermediate results: However, the compiler may take advantage of restricted subranges to produce more efficient code.
In C, operands must first be promoted to the size of the required result: intermediate results are undefined if they do not fit into the range of the promoted operands. If range of the required result is greater than the range of operands, this normally produces slow inefficient code, even from a good optimising compiler. However, a C compiler is never required or expected to handle out of range intermediate results: it is the programmers responsibility to ensure that all intermediate results fit into the operand range.
The (only) pre-Standard implementation of C as well as Small-C et al. allowed integer and pointer types to be relatively freely intermixed.
In C the character type is char which is a kind of integer that is no longer than short int, . Expressions such as 'x'+1 are therefore perfectly legal, as are declarations such as int i='i'; and char c=74;.
This integer nature of char (one byte) is clearly illustrated by declarations such as
Whether the char type should be regarded as signed or unsigned by default is up to the implementation.
In Pascal, characters and integers are distinct types. The inbuilt compiler functions ord() and chr() can be used to typecast single characters to the corresponding integer value of the character set in use, and vice versa. e.g. on systems using the ASCII character set ord('1') = 49 and chr(9) is a TAB character.
In Pascal, boolean is an enumerated type. The possible values of boolean are false and true. For conversion to integer, ord is used:
There is no standard function for integer to boolean, however, the conversion is simple in practice:
C has binary valued relational operators (<, >, ==, !=, <=, >=) which may be regarded as boolean in the sense that they always give results which are either zero or one. As all tests (&&, ||, ?:, if, while, etc.) are performed by zero-checks, false is represented by zero, while true is represented by any other value.
C allows using bitwise operators to perform boolean operations. Care must be taken because the semantics are different when operands make use of more than one bit to represent a value.
Pascal has another more abstract, high level method of dealing with bitwise data, sets.  Sets allow the programmer to set, clear, intersect, and unite bitwise data values, rather than using direct bitwise operators (which are available in modern Pascal as well).  Example;
Pascal:  
or
Pascal:  

C:  
Although bit operations on integers and operations on sets can be considered similar if the sets are implemented using bits, there is no direct parallel between their uses unless a non-standard conversion between integers and sets is possible.
During expression evaluation, and in both languages, a boolean value may be internally stored as a single bit, a single byte, a full machine word, a position in the generated code, or as a condition code in a status register, depending on machine, compiler, and situation; these factors are usually more important than the language compiled.
C has a less strict model of floating point types than Pascal. In C, integers may be implicitly converted to floating point numbers, and vice versa (though possible precision loss may be flagged by warnings). In Pascal, integers may be implicitly converted to real, but conversion of real to integer (where information may be lost) must be done explicitly via the functions trunc() and round(), which truncate or round off the fraction, respectively.
Both C and Pascal include enumeration types.  A Pascal example:
A C example:
The behavior of the types in the two languages however is very different.  In C, red becomes just a synonym for 0, green for 1, blue for 2, and nothing prevents a value outside this range to be assigned to the variable a.  Furthermore, operations like a = a + 1; are strictly forbidden in Pascal; instead you would use a := succ(a);. In C, enums can be freely converted to and from ints, but in Pascal, the function ord() must be used to convert from enumerated types to integers, and there is no function to convert from integer to enumerated types.
Both C and Pascal allow arrays of other complex types, including other arrays. However, there the similarity between the languages ends. C arrays are simply defined by a base type and the number of elements:
and are always indexed from 0 up to SIZE-1 (i.e. modulo SIZE).
In Pascal, the range of indices is often specified by a subrange (as introduced under simple types above). The ten elements of
would be indexed by 0..9 (just as in C in this case). Array indices can be any ordinal data type, however, not just ranges:
Strings consisting of n (>1) characters are defined as packed arrays with range 1..n.
In C expressions, an identifier representing an array is treated as a constant pointer to the first element of the array, thus, given the declarations int a and int *p; the assignment p = a is valid and causes p and a to point to the same array. As the identifier a represents a constant address, a = p is not valid however.
While arrays in C are fixed, pointers to them are interchangeable. This flexibility allows C to manipulate any length array using the same code. It also leaves the programmer with the responsibility not to write outside the allocated array, as no checks are built in into the language itself.
In Pascal, arrays are a distinct type from pointers. This makes bounds checking for arrays possible from a compiler perspective. Practically all Pascal compilers support range checking as a compile option. 
The ability to both have arrays that change length at runtime, and be able to check them under language control, is often termed ""dynamic arrays"". In Pascal the number of elements in each array type is determined at compile-time and cannot be changed during the execution of the program. Hence, it is not possible to define an array whose length depends in any way on program data.
C has the ability to initialize arrays of arbitrary length. The sizeof operator can be used to obtain the size of a statically initialized array in C code.  For instance in the following code, the terminating index for the loop automatically adjusts should the list of strings be changed.
Pascal has neither array initialization (outside of the case of strings) nor a means of determining arbitrary array sizes at compile time.
One way of implementing the above example in Pascal, but without the automatic size adjustment, is:
In both languages, a string is a primitive array of characters.
In Pascal a string literal of length n is compatible with the type packed array [1..n] of char. In C a string generally has the type char[n].
Pascal has no support for variable-length arrays, and so any set of routines to perform string operations is dependent on a particular string size. The now standardized Pascal ""conformant array parameter"" extension solves this to a great extent, and many or even most implementations of Pascal have support for strings native to the language.
C string literals are null-terminated; that is to say, a trailing null character as an end-of-string sentinel:
Null-termination must be manually maintained for string variables stored in arrays (this is often partly handled by library routines).
C does not have built-in string or array assignment, so the string is not actually being transferred to p, but rather p is being made to point to the constant string in memory.
In Pascal, unlike C, the string's first character element is at index 1 and not 0 (leading it to be length-prefixed). This is because Pascal stores the length of the string at the 0th element of the character array. If this difference is not well understood it can lead to errors when porting or trying to interface object code generated by both languages.
FreeBSD developer Poul-Henning Kamp, writing in ACM Queue, would later refer to the victory of null-terminated strings over length-prefixed strings as ""the most expensive one-byte mistake"" ever.
Both C and Pascal can declare ""record"" types. In C, they are termed ""structures"".
In Pascal, we can use the sentence ""with <name_of_record> do"" in order to use directly the fields of that record, like local variables, instead of write <name_of_record>.<name_of_field>. Here there is an example:
There is no equivalent feature to with in C.
In C, the exact bit length of a field can be specified:
How much storage is actually used depends on traits (e.g. word-alignment) of the target system.
This feature is available in Pascal by using the subrange construct (3 bits gives a range from 0 to 7) in association with the keyword packed:
Both C and Pascal support records which can include different fields overlapping each other:
Both language processors are free to allocate only as much space for these records as needed to contain the largest type in the union/record.
The biggest difference between C and Pascal is that Pascal supports the explicit use of a ""tagfield"" for the language processor to determine if the valid component of the variant record is being accessed:
In this case, the tagfield q must be set to the right state to access the proper parts of the record.
In C, pointers can be made to point at most program entities, including objects or functions:
In C, since arrays and pointers have a close equivalence, the following are the same:
Thus, pointers are often used in C as just another method to access arrays.
To create dynamic data, the library functions malloc() and free() are used to obtain and release dynamic blocks of data. Thus, dynamic memory allocation is not built into the language processor. This is especially valuable when C is being used in operating system kernels or embedded targets as these things are very platform (not just architecture) specific and would require changing the C compiler for each platform (or operating system) that it would be used on.
Pascal doesn't have the same kind of pointers as C, but it does have an indirection operator that covers the most common use of C pointers. Each pointer is bound to a single dynamic data item, and can only be moved by assignment:
Pointers in Pascal are type safe; i.e. a pointer to one data type can only be assigned to a pointer of the same data type. Also pointers can never be assigned to non-pointer variables. Pointer arithmetic (a common source of programming errors in C, especially when combined with endianness issues and platform-independent type sizes) is not permitted in Pascal.
All of these restrictions reduce the possibility of pointer-related errors in Pascal compared to C, but do not prevent invalid pointer references in Pascal altogether. For example, a runtime error will occur if a pointer is referenced before it has been initialized or after it has been disposed.
The languages differ significantly when it comes to expression evaluation, but all-in-all they are comparable.
Pascal
C
Most operators serve several purposes in Pascal, for instance, the minus sign may be used for negation, subtraction, or set difference (depending on both type and syntactical context), the >= operator may be used to compare numbers, strings, or sets, and so on. C uses dedicated operator symbols to a greater extent.
The two languages use different operators for assignment. Pascal, like ALGOL, uses the mathematical equality operator = for the equality test and the symbol := for assignment, whereas C, like B, uses the mathematical equality operator for assignment. In C (and B) the new == symbol was therefore introduced for the equality test.
It is a common mistake in C, due either to inexperience or to a simple typing error, to accidentally put assignment expressions in conditional statements such as if (a = 10) { ... }. The code in braces will always execute because the assignment expression a = 10 has the value 10 which is non-zero and therefore considered ""true"" in C; this is in part because C (and ALGOL) allow multiple assignment in the form a = b = c = 10; which is not supported by Pascal. Also note that a now has the value 10, which may affect the following code. Recent C compilers try to detect these cases and warn the user, asking for a less ambiguous syntax like if ((a=10) != 0 ) { ... }.
This kind of mistake cannot happen in Pascal, as assignments are not expressions and do not have a value: using the wrong operator will cause an unambiguous compilation error, and it's also less likely that anyone would mistake the := symbol for an equality test.
It is notable that ALGOL's conditional expression in the form a := if a > b then a else b; has an equivalent in C but not in Pascal.
When Niklaus Wirth designed Pascal, the desire was to limit the number of levels of precedence (fewer parse routines, after all). So the OR and exclusive OR operators are treated just like an Addop and processed at the level of a math  expression. Similarly, the AND is treated like a Mulop and processed with Term. The precedence levels are
Notice that there is only ONE set of syntax rules, applying to both kinds of operators. According to this grammar, then, expressions like
are perfectly legal. And, in fact, they are, as far as the parser is concerned. Pascal doesn't allow the mixing of arithmetic and Boolean variables, and things like this are caught at the semantic level, when it comes time to generate code for them, rather than at the syntax level.
The authors of C took a diametrically opposite approach: they treat the operators as different, and in fact, in C there are no fewer than 15 levels. That's because C also has the operators '=', '+=' and its kin, '<<', '>>', '++', '--', etc. Although in C the arithmetic and Boolean operators are treated separately, the variables are not: a Boolean test can be made on any integer value.
In Pascal a boolean expression that relies on a particular evaluation ordering (possibly via side-effects in function calls) is, more or less, regarded as an error. The Pascal compiler has the freedom to use whatever ordering it may prefer and must always evaluate the whole expression even if the result can be determined by partial evaluation.
In C, dependence on boolean evaluation order is perfectly legal, and often systematically employed using the && and || operators together with operators such as ++, +=, the comma operator, etc.  The && and || operators thereby function as combinations of logical operators and conditional statements.
Short circuit expression evaluation has been commonly considered an advantage for C because of the ""evaluation problem"":
This seemingly straightforward search is problematic in Pascal because the array access a[i] would be invalid for i equal to 11.
However, in superscalar processors there is a penalty for all jumps because they cause pipeline stalls, and programs created for them are more efficient if jumps are removed where possible. Pascal's ability to evaluate using a fixed formula without jumps can be an advantage with highly optimizing compilers, whereas C has effectively prevented this by requiring short circuit optimization.
Statements for building control structures are roughly analogous and relatively similar (at least the first three).
Pascal has:
C has:
Pascal, in its original form, did not have an equivalent to default, but an equivalent else clause is a common extension. Pascal programmers otherwise had to guard case-statements with an expression such as:  if expr not in [A..B] then default-case.
C has the so-called early-out statements break and continue, and some Pascals have them as well.
Both C and Pascal have a goto statement. However, since Pascal has nested procedures/functions, jumps can be done from an inner procedure or function to the containing one; this was commonly used to implement error recovery. C has this ability via the ANSI C setjmp and longjmp. This is equivalent, but arguably less safe, since it stores program specific information like jump addresses and stack frames in a programmer accessible structure.
Pascal routines that return a value are called functions; routines that don't return a value are called procedures. All routines in C are called functions; C functions that do not return a value are declared with a return type of void.
Pascal procedures are considered equivalent to C ""void"" functions, and Pascal functions are equivalent to C functions that return a value.
The following two declarations in C:
are equivalent to the following declarations in Pascal:
Pascal has two different types of parameters: pass-by-value, and pass-by-reference (VAR).
In C all parameters are passed by value but pass-by-reference can be simulated using pointers. The following segment is similar to the Pascal segment above:
C allows for functions to accept a variable number of parameters, known as variadic functions.
The function f() uses a special set of functions that allow it to access each of the parameters in turn.
Additionally Pascal has I/O statements built into the language to handle variable amount of parameters, like Writeln.  Pascal allows procedures and functions to be nested. This is convenient to allow variables that are local to a group of procedures, but not global. C does not have this feature and the localization of variables or functions could only be done for a compilation module wherein the variables or functions would have been declared static.
C allows functions to be indirectly invoked through a function pointer.  In the following example, the statement (*cmpar)(s1, s2) is equivalent to strcmp(s1, s2):
Pascal also allows functions and procedures to be passed as parameters to functions or procedures:
Early C had neither constant declarations nor type declarations, and the C language was originally defined as needing a ""preprocessor""; a separate program, and pass, that handled constant, include and macro definitions, to keep memory usage down. Later, with ANSI C, it obtained constant and type definitions features and the preprocessor also became part of the language itself, leading to the syntax we see today.
Pascal constant and type defines are built in, but there were programmers using a preprocessor also with Pascal (sometimes the same one used with C), certainly not as common as with C. Although often pointed out as a ""lack"" in Pascal, technically  C doesn't have program modularity nor macros built in either. It has a simple low level separate compilation facility, however (traditionally using the same generic linker used for assembly language), Pascal does not.
In C, the programmer may inspect the byte-level representation of any object by pointing a char pointer to it:
It may be possible to do something similar in Pascal using an undiscriminated variant record:
Although casting is possible on most Pascal compilers and interpreters, even in the code above a2c.a and a2c.b aren't required by any Pascal standardizations to share the same address space. Niklaus Wirth, the designer of Pascal, has written about the problematic nature of attempting type escapes using this approach:
""Most implementors of Pascal decided that this checking would be too expensive, enlarging code and deteriorating program efficiency. As a consequence, the variant record became a favourite feature to breach the type system by all programmers in love with tricks, which usually turn into pitfalls and calamities"".
Several languages now specifically exclude such type escapes, for example Java, C# and Wirth's own Oberon.
In C files do not exist as a built-in type (they are defined in a system header) and all I/O takes place via library calls.  Pascal has file handling built into the language.
The typical statements used to perform I/O in each language are:
The main difference is that C uses a ""format string"" that is interpreted to find the arguments to the printf function and convert them, whereas Pascal performs that under the control of the language processor.  The Pascal method is arguably faster, because no interpretation takes place, but the C method is highly extensible.
Some popular Pascal implementations have incorporated virtually all C constructs into Pascal. Examples include type casts, being able to obtain the address of any variable, local or global, and different types of integers with special promotion properties.
However, the incorporation of C's lenient attitude towards types and type conversions can result in a Pascal that loses some or all of its type security. For example, Java and C# were created in part to address some of the perceived type security issues of C, and have ""managed"" pointers that cannot be used to create invalid references. In its original form (as described by Niklaus Wirth), Pascal qualifies as a managed pointer language, some 30 years before either Java or C#. However, a Pascal amalgamated with C would lose that protection by definition. In general, the lower dependence on pointers for basic tasks makes it safer than C in practice.
The Extended Pascal standard extends Pascal to support many things C supports, which the original standard Pascal did not, in a type safer manner. For example, schema types support (besides other uses) variable-length arrays while keeping the type-safety of mandatory carrying the array dimension with the array, allowing automatic run-time checks for out-of-range indices also for dynamically sized arrays."
"116","
C (/siː/, as in the letter c) is a general-purpose, imperative computer programming language, supporting structured programming, lexical variable scope and recursion, while a static type system prevents many unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and therefore it has found lasting use in applications that had formerly been coded in assembly language, including operating systems, as well as various application software for computers ranging from supercomputers to embedded systems.
C was originally developed by Dennis Ritchie between 1969 and 1973 at Bell Labs, and used to re-implement the Unix operating system.  It has since become one of the most widely used programming languages of all time, with C compilers from various vendors available for the majority of existing computer architectures and operating systems.  C has been standardized by the American National Standards Institute (ANSI) since 1989 (see ANSI C) and subsequently by the International Organization for Standardization (ISO).
C is an imperative procedural language.  It was designed to be compiled using a relatively straightforward compiler, to provide low-level access to memory, to provide language constructs that map efficiently to machine instructions, and to require minimal run-time support. Despite its low-level capabilities, the language was designed to encourage cross-platform programming.  A standards-compliant and portably written C program can be compiled for a very wide variety of computer platforms and operating systems with few changes to its source code.  The language has become available on a very wide range of platforms, from embedded microcontrollers to supercomputers.
Like most imperative languages in the ALGOL tradition, C has facilities for structured programming and allows lexical variable scope and recursion, while a static type system prevents many unintended operations.  In C, all executable code is contained within subroutines, which are called ""functions"" (although not in the strict sense of functional programming). Function parameters are always passed by value. Pass-by-reference is simulated in C by explicitly passing pointer values. C program source text is free-format, using the semicolon as a statement terminator and curly braces for grouping blocks of statements.
The C language also exhibits the following characteristics:
While C does not include some features found in some other languages, such as object orientation or garbage collection, such features can be implemented or emulated in C, often by way of external libraries (e.g., the Boehm garbage collector or the GLib Object System).
Many later languages have borrowed directly or indirectly from C, including C++, C#, Unix's C shell, D, Go, Java, JavaScript, Limbo, LPC, Objective-C, Perl, PHP, Python, Rust, Swift, and Verilog (hardware description language).  These languages have drawn many of their control structures and other basic features from C.  Most of them (with Python being the most dramatic exception) are also very syntactically similar to C in general, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.
The origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Dennis Ritchie and Ken Thompson, incorporating several ideas from colleagues.  Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was developed in assembly language. The developers were considering rewriting the system using the B language, Thompson's simplified version of BCPL. However B's inability to take advantage of some of the PDP-11's features, notably byte addressability, led to C. The name of C was chosen simply as the next after B.
The development of C started in 1972 on the PDP-11 Unix system and first appeared in Version 2 Unix. The language was not initially designed with portability in mind, but soon ran on different platforms as well: a compiler for the Honeywell 6000 was written within the first year of C's history, while an IBM System/370 port followed soon.
Also in 1972, a large part of Unix was rewritten in C. By 1973, with the addition of struct types, the C language had become powerful enough that most of the Unix kernel was now in C.
Unix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system which was written in PL/I), and Master Control Program (MCP) for the Burroughs B5000 written in ALGOL in 1961. In around  1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system.  Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.
In 1978, Brian Kernighan and Dennis Ritchie published the first edition of The C Programming Language. This book, known to C programmers as ""K&R"", served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as K&R C. The second edition of the book covers the later ANSI C standard, described below.
K&R introduced several language features:
Even after the publication of the 1989 ANSI standard, for many years K&R C was still considered the ""lowest common denominator"" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&R C code can be legal Standard C as well.
In early versions of C, only functions that return types other than int must be declared if used before the function definition; functions used without prior declaration were presumed to return type int.
For example:
The int type specifiers which are commented out could be omitted in K&R C, but are required in later standards.
Since K&R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if multiple calls to an external function used different numbers or types of arguments.  Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.
In the years following the publication of K&R C, several features were added to the language, supported by compilers from AT&T (in particular PCC) and some other vendors. These included:
The large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&R specification, led to the necessity of standardization.
During the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.
In 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 ""Programming Language C"".  This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.
In 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms ""C89"" and ""C90"" refer to the same programming language.
ANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14.  National adoption of an update to the international standard typically occurs within a year of ISO publication.
One of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), void pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.
C89 is supported by current C compilers, and most C code being written today is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits.  Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.
In cases where code must be compilable by either standard-conforming or K&R C-based compilers, the __STDC__ macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.
After the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995, Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.[citation needed]
The C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as ""C99"". It has since been amended three times by Technical Corrigenda.
C99 introduced several new features, including inline functions, several new data types (including long long int and a complex type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with //, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.
C99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has int implicitly assumed. A standard macro __STDC_VERSION__ is defined with value 199901L to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.
In 2007, work began on another revision of the C standard, informally called ""C1X"" until its official publication on 2011-12-08. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.
The C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions.  It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro __STDC_VERSION__ is defined as 201112L to indicate that C11 support is available.
Historically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.
In 2008, the C Standards Committee published a technical report extending the C language to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.
C has a formal grammar specified by the C standard. Line endings are generally not significant in C; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters /* and */, or (since C99)  following // until the end of the line. Comments delimited by /* and */ do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.
C source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as struct, union, and enum, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as char and int specify built-in types. Sections of code are enclosed in braces ({ and }, sometimes called ""curly brackets"") to limit the scope of declarations and to act as a single statement for control structures.
As an imperative language, C uses statements to specify actions. The most common statement is an expression statement, consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by if(-else) conditional execution and by do-while, while, and for iterative execution (looping). The for statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. break and continue can be used to leave the innermost enclosing loop statement or skip to its reinitialization. There is also a non-structured goto statement which branches directly to the designated label within the function. switch selects a case to be executed based on the value of an integer expression.
Expressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next ""sequence point""; sequence points include the end of each expression statement, and the entry to and return from each function call.  Sequence points also occur during evaluation of expressions containing certain operators (&&, ||, ?: and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.
Kernighan and Ritchie say in the Introduction of The C Programming Language: ""C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better."" The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.
The basic C source character set includes the following characters:
Newline indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as one.
Additional multi-byte encoded characters may be used in string literals, but they are not entirely portable.  The latest C standard (C11) allows multi-national Unicode characters to be embedded portably within C source text by using \uXXXX or \UXXXXXXXX encoding (where the X denotes a hexadecimal character), although this feature is not yet widely implemented.
The basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return. Run-time support for extended character sets has increased with each revision of the C standard.
C89 has 32 reserved words, also known as keywords, which are the words that cannot be used for any purposes other than those for which they are predefined:
C99 reserved five more words:
C11 reserved seven more words:
Most of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations.  Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language.  Some standard headers do define more convenient synonyms for underscored identifiers. The language previously included a reserved word called entry, but this was seldom implemented, and has now been removed as a reserved word.
C supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:
C uses the operator = (used in mathematics to express equality) to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. C uses the operator == to test for equality.  The similarity between these two operators (assignment and equality) may result in the accidental use of one in place of the other, and in many cases, the mistake does not produce an error message (although some compilers produce warnings). For example, the conditional expression  if(a==b+1) might mistakenly be written as if(a=b+1), which will be evaluated as true if a is not zero after the assignment.
The C operator precedence is not always intuitive.  For example, the operator == binds more tightly than (is executed prior to) the operators & (bitwise AND) and | (bitwise OR) in expressions such as x & 1 == 0, which must be written as (x & 1) == 0 if that is the coder's intent.
The ""hello, world"" example, which appeared in the first edition of K&R, has become the model for an introductory program in most programming textbooks, regardless of programming language. The program prints ""hello, world"" to the standard output, which is usually a terminal or screen display.
The original version was:
A standard-conforming ""hello, world"" program is:[lower-alpha 1]
The first line of the program contains a preprocessing directive, indicated by #include.  This causes the compiler to replace that line with the entire text of the stdio.h standard header, which contains declarations for standard input and output functions such as printf. The angle brackets surrounding stdio.h indicate that stdio.h is located using a search strategy that prefers headers provided with the compiler to other headers having the same name, as opposed to double quotes which typically include local or project-specific header files.
The next line indicates that a function named main is being defined. The main function serves a special purpose in C programs; the run-time environment calls the main function to begin program execution. The type specifier int indicates that the value that is returned to the invoker (in this case the run-time environment) as a result of evaluating the main function, is an integer. The keyword void as a parameter list indicates that this function takes no arguments.[lower-alpha 2]
The opening curly brace indicates the beginning of the definition of the main function.
The next line calls (diverts execution to) a function named printf, which in this case is supplied from a system library.  In this call, the printf function is passed (provided with) a single argument, the address of the first character in the string literal ""hello, world\n"". The string literal is an unnamed array with elements of type char, set up automatically by the compiler with a final 0-valued character to mark the end of the array (printf needs to know this). The \n is an escape sequence that C translates to a newline character, which on output signifies the end of the current line.  The return value of the printf function is of type int, but it is silently discarded since it is not used. (A more careful program might test the return value to determine whether or not the printf function succeeded.) The semicolon ; terminates the statement.
The closing curly brace indicates the end of the code for the main function. According to the C99 specification and newer, the main function, unlike any other function, will implicitly return a value of 0 upon reaching the } that terminates the function. (Formerly an explicit return 0; statement was required.) This is interpreted by the run-time system as an exit code indicating successful execution.
The type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal.  There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, and enumerated types (enum).  Integer type char is often used for single-byte characters.  C99 added a boolean datatype.  There are also derived types including arrays, pointers, records (struct), and untagged unions (union).
C is often used in low-level systems programming where escapes from the type system may be necessary.  The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a type cast to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.
Some find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: ""declaration reflects use"".)
C's usual arithmetic conversions allow for efficient code to be generated, but can sometimes produce unexpected results.  For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned.  This can generate unexpected results if the signed value is negative.
C supports the use of pointers, a type of reference that records the address or location of an object or function in memory.  Pointers can be dereferenced to access data stored at the address pointed to, or to invoke a pointed-to function.  Pointers can be manipulated using assignment or pointer arithmetic.  The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time.  Pointer arithmetic is automatically scaled by the size of the pointed-to data type. Pointers are used for many purposes in C.  Text strings are commonly manipulated using pointers into arrays of characters.  Dynamic memory allocation is performed using pointers.  Many data types, such as trees, are commonly implemented as dynamically allocated struct objects linked together using pointers.  Pointers to functions are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch) or as callbacks to be invoked by event handlers.
A null pointer value explicitly points to no valid location.  Dereferencing a null pointer value is undefined, often resulting in a segmentation fault.  Null pointer values are useful for indicating special cases such as no ""next"" pointer in the final node of a linked list, or as an error indication from functions returning pointers.  In appropriate contexts in source code, such as for assigning to a pointer variable, a null pointer constant can be written as 0, with or without explicit casting to a pointer type, or as the NULL macro defined by several standard headers.  In conditional contexts, null pointer values evaluate to false, while all other pointer values evaluate to true.
Void pointers (void *) point to objects of unspecified type, and can therefore be used as ""generic"" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.
Careless use of pointers is potentially dangerous.  Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects.  Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer.  In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.
Array types in C are traditionally of a fixed, static size specified at compile time.  (The more recent C99 standard also allows a form of variable-length arrays.)  However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's malloc function, and treat it as an array.  C's unification of arrays and pointers means that declared arrays and these dynamically allocated simulated arrays are virtually interchangeable.
Since arrays are always accessed (in effect) via pointers, array accesses are typically not checked against the underlying array size, although some compilers may provide bounds checking as an option.  Array bounds violations are therefore possible and rather common in carelessly written code, and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions.  If bounds checking is desired, it must be done manually.
C does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing.  The index values of the resulting ""multi-dimensional array"" can be thought of as increasing in row-major order.
Multi-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, since arrays are passed merely as pointers, the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this is to allocate the array with an additional ""row vector"" of pointers to the columns.)
C99 introduced ""variable-length arrays"" which address some, but not all, of the issues with ordinary C arrays.
The subscript notation x[i] (where x designates a pointer) is syntactic sugar for *(x+i). Taking advantage of the compiler's knowledge of the pointer type, the address that x + i points to is not the base address (pointed to by x) incremented by i bytes, but rather is defined to be the base address incremented by i multiplied by the size of an element that x points to.  Thus, x[i] designates the i+1th element of the array.
Furthermore, in most expression contexts (a notable exception is as operand of sizeof), the name of an array is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.
The size of an element can be determined by applying the operator sizeof to any dereferenced element of x, as in n = sizeof *x or n = sizeof x, and the number of elements in a declared array A can be determined as sizeof A / sizeof A. The latter only applies to array names: variables declared with subscripts (int A). Due to the semantics of C, it is not possible to determine the entire size of arrays through pointers to arrays or those created by dynamic allocation (malloc); code such as sizeof arr / sizeof arr (where arr designates a pointer) will not work since the compiler assumes the size of the pointer itself is being requested. Since array name arguments to sizeof are not converted to pointers, they do not exhibit such ambiguity. However, arrays created by dynamic allocation are accessed by pointers rather than true array variables, so they suffer from the same sizeof issues as array pointers.
Thus, despite this apparent equivalence between array and pointer variables, there is still a distinction to be made between them. Even though the name of an array is, in most expression contexts, converted into a pointer (to its first element), this pointer does not itself occupy any storage; the array name is not an l-value, and its address is a constant, unlike a pointer variable. Consequently, what an array ""points to"" cannot be changed, and it is impossible to assign a new address to an array name. Array contents may be copied, however, by using the memcpy function, or by accessing the individual elements.
One of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three distinct ways to allocate memory for objects:
These three approaches are appropriate in different situations and have various trade-offs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.
Where possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary.  Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on malloc for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated.  (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)
Unless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.
Another issue is that heap memory allocation has to be synchronized with its actual usage in any program in order for it to be reused as much as possible.  For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before free() is called, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a memory leak. Conversely, it is possible for memory to be freed but continue to be referenced, leading to unpredictable results. Typically, the symptoms will appear in a portion of the program far removed from the actual error, making it difficult to track down the problem. (Such issues are ameliorated in languages with automatic garbage collection.)
The C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single ""archive"" file.  Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., -lm, shorthand for ""link the math library"").
The most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation (implementations which target limited environments such as embedded systems may provide only a subset of the standard library). This library supports stream input and output, memory allocation, mathematics, character strings, and time values.  Several separate standard headers (for example, stdio.h) specify the interfaces for these and other standard library facilities.
Another common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.
Since many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.
A number of tools have been developed to help C programmers find and fix statements with undefined behavior or possibly erroneous expressions, with greater rigor than that provided by the compiler.  The tool lint was the first such, leading to many others.
Automated source code checking and auditing are beneficial in any language, and for C many such tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.
There are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.
Tools such as Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.
C is widely used for system programming in implementing operating systems and embedded system applications, because C code, when written for portability, can be used for most purposes, yet when needed, system-specific code can be used to access specific hardware addresses and to perform type punning to match externally imposed interface requirements, with a low run-time demand on system resources.
C can also be used for website programming using CGI as a ""gateway"" for information between the Web application, the server, and the browser. C is often chosen over interpreted languages because of its speed, stability, and near-universal availability.
One consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. The reference implementations of Python, Perl and PHP, for example, are all written in C.
Because the layer of abstraction is thin and the overhead is low, C enables programmers to create efficient implementations of algorithms and data structures, useful for computationally intense programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C.
C is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, additional machine-specific code generators are not necessary.  C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, that support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--.
C has also been widely used to implement end-user applications. However, such applications can also be written in newer, higher-level languages.
C has both directly and indirectly influenced many later languages such as C#, D, Go, Java, JavaScript, Limbo, LPC, Perl, PHP, Python, and Unix's C shell. The most pervasive influence has been syntactical, all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models and/or large-scale program structures that differ from those of C, sometimes radically.
Several C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.
When object-oriented languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.
The C++ programming language was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax. C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.
Objective-C was originally a very ""thin"" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.
In addition to C++ and Objective-C, Ch, Cilk and Unified Parallel C are nearly supersets of C."
"117","In computing, a segmentation fault (often shortened to segfault) or access violation is a fault, or failure condition, raised by hardware with memory protection, notifying an operating system (OS) the software has attempted to access a restricted area of memory (a memory access violation). On standard x86 computers, this is a form of general protection fault. The OS kernel will, in response, usually perform some corrective action, generally passing the fault on to the offending process by sending the process a signal. Processes can in some cases install a custom signal handler, allowing them to recover on their own, but otherwise the OS default signal handler is used, generally causing abnormal termination of the process (a program crash), and sometimes a core dump.
Segmentation faults are a common class of error in programs written in languages like C that provide low-level memory access. They arise primarily due to errors in use of pointers for virtual memory addressing, particularly illegal access. Another type of memory access error is a bus error, which also has various causes, but is today much rarer; these occur primarily due to incorrect physical memory addressing, or due to misaligned memory access – these are memory references that the hardware cannot address, rather than references that a process is not allowed to address.
Newer programming languages may employ mechanisms designed to avoid segmentation faults and improve memory safety. For example, the Rust programming language which appeared in 2010 employs an 'Ownership' based model to ensure memory safety.
A segmentation fault occurs when a program attempts to access a memory location that it is not allowed to access, or attempts to access a memory location in a way that is not allowed (for example, attempting to write to a read-only location, or to overwrite part of the operating system).
The term ""segmentation"" has various uses in computing; in the context of ""segmentation fault"", a term used since the 1950s, it refers to the address space of a program.[citation needed] With memory protection, only the program's own address space is readable, and of this, only the stack and the read-write portion of the data segment of a program are writable, while read-only data and the code segment are not writable. Thus attempting to read outside of the program's address space, or writing to a read-only segment of the address space, results in a segmentation fault, hence the name.
On systems using hardware memory segmentation to provide virtual memory, a segmentation fault occurs when the hardware detects an attempt to refer to a non-existent segment, or to refer to a location outside the bounds of a segment, or to refer to a location in a fashion not allowed by the permissions granted for that segment.  On systems using only paging, an invalid page fault generally leads to a segmentation fault, and segmentation faults and page faults are both faults raised by the virtual memory management system. Segmentation faults can also occur independently of page faults: illegal access to a valid page is a segmentation fault, but not an invalid page fault, and segmentation faults can occur in the middle of a page (hence no page fault), for example in a buffer overflow that stays within a page but illegally overwrites memory.
At the hardware level, the fault is initially raised by the memory management unit (MMU) on illegal access (if the referenced memory exists), as part of its memory protection feature, or an invalid page fault (if the referenced memory does not exist). If the problem is not an invalid logical address but instead an invalid physical address, a bus error is raised instead, though these are not always distinguished.
At the operating system level, this fault is caught and a signal is passed on to the offending process, activating the process's handler for that signal. Different operating systems have different signal names to indicate that a segmentation fault has occurred. On Unix-like operating systems, a signal called SIGSEGV (abbreviated from segmentation violation) is sent to the offending process. On Microsoft Windows, the offending process receives a STATUS_ACCESS_VIOLATION exception.
The conditions under which segmentation violations occur and how they manifest themselves are specific to hardware and the operating system: different hardware raises different faults for given conditions, and different operating systems convert these to different signals that are passed on to processes. The proximate cause is a memory access violation, while the underlying cause is generally a software bug of some sort. Determining the root cause – debugging the bug – can be simple in some cases, where the program will consistently cause a segmentation fault (e.g., dereferencing a null pointer), while in other cases the bug can be difficult to reproduce and depend on memory allocation on each run (e.g., dereferencing a dangling pointer).
The following are some typical causes of a segmentation fault:
These in turn are often caused by programming errors that result in invalid memory access:
In C code, segmentation faults most often occur because of errors in pointer use, particularly in C dynamic memory allocation. Dereferencing a null pointer will always result in a segmentation fault, but wild pointers and dangling pointers point to memory that may or may not exist, and may or may not be readable or writable, and thus can result in transient bugs. For example:
Now, dereferencing any of these variables could cause a segmentation fault: dereferencing the null pointer generally will cause a segfault, while reading from the wild pointer may instead result in random data but no segfault, and reading from the dangling pointer may result in valid data for a while, and then random data as it is overwritten.
The default action for a segmentation fault or bus error is abnormal termination of the process that triggered it. A core file may be generated to aid debugging, and other platform-dependent actions may also be performed. For example, Linux systems using the grsecurity patch may log SIGSEGV signals in order to monitor for possible intrusion attempts using buffer overflows.
Writing to read-only memory raises a segmentation fault. At the level of code errors, this occurs when the program writes to part of its own code segment or the read-only portion of the data segment, as these are loaded by the OS into read-only memory.
Here is an example of ANSI C code that will generally cause a segmentation fault on platforms with memory protection. It attempts to modify a string literal, which is undefined behavior according to the ANSI C standard. Most compilers will not catch this at compile time, and instead compile this to executable code that will crash:
When the program containing this code is compiled, the string ""hello world"" is placed in the rodata section of the program executable file: the read-only section of the data segment. When loaded, the operating system places it with other strings and constant data in a read-only segment of memory. When executed, a variable, s, is set to point to the string's location, and an attempt is made to write an H character through the variable into the memory, causing a segmentation fault. Compiling such a program with a compiler that does not check for the assignment of read-only locations at compile time, and running it on a Unix-like operating system produces the following runtime error:
Backtrace of the core file from GDB:
This code can be corrected by using an array instead of a character pointer, as this allocates memory on stack and initializes it to the value of the string literal:
Even though string literals should not be modified (this has undefined behavior in the C standard), in C they are of static char [] type, so there is no implicit conversion in the original code (which points a char * at that array), while in C++ they are of static const char [] type, and thus there is an implicit conversion, so compilers will generally catch this particular error.
Because a very common program error is a null pointer dereference (a read or write through a null pointer, used in C to mean ""pointer to no object"" and as an error indicator), most operating systems map the null pointer's address such that accessing it causes a segmentation fault.
This sample code creates a null pointer, and then tries to access its value (read the value). Doing so causes a segmentation fault at runtime on many operating systems.
Dereferencing a null pointer and then assigning to it (writing a value to a non-existent target) also usually causes a segmentation fault:
The following code includes a null pointer dereference, but when compiled will often not result in a segmentation fault, as the value is unused and thus the dereference will often be optimized away by dead code elimination:
Another example is recursion without a base case:
which causes the stack to overflow which results in a segmentation fault.  Infinite recursion may not necessarily result in a stack overflow depending on the language, optimizations performed by the compiler and the exact structure of a code.  In this case, the behavior of unreachable code (the return statement) is undefined, so the compiler can eliminate it and use a tail call optimization that might result in no stack usage.  Other optimizations could include translating the recursion into iteration, which given the structure of the example function would result in the program running forever, while probably not overflowing its stack."
"118","Donald Knuth, Structured Programming with go to Statements
In computer science, a pointer is a programming language object, whose value refers to (or ""points to"") another value stored elsewhere in the computer memory using its memory address. A pointer references a location in memory, and obtaining the value stored at that location is known as dereferencing the pointer. As an analogy, a page number in a book's index could be considered a pointer to the corresponding page; dereferencing such a pointer would be done by flipping to the page with the given page number and reading the text found on the indexed page.
Pointers to data significantly improve performance for repetitive operations such as traversing strings, lookup tables, control tables and tree structures. In particular, it is often much cheaper in time and space to copy and dereference pointers than it is to copy and access the data to which the pointers point.
Pointers are also used to hold the addresses of entry points for called subroutines in procedural programming and for run-time linking to dynamic link libraries (DLLs). In object-oriented programming, pointers to functions are used for binding methods, often using what are called virtual method tables.
A pointer is a simple, more concrete implementation of the more abstract reference data type. Several languages support some type of pointer, although some have more restrictions on their use than others. While ""pointer"" has been used to refer to references in general, it more properly applies to data structures whose interface explicitly allows the pointer to be manipulated (arithmetically via pointer arithmetic) as a memory address, as opposed to a magic cookie or capability where this is not possible. [citation needed] Because pointers allow both protected and unprotected access to memory addresses, there are risks associated with using them particularly in the latter case. Primitive pointers are often stored in a format similar to an integer; however, attempting to dereference or ""look up"" a pointer whose value was never a valid memory address would cause a program to crash. To alleviate this potential problem, as a matter of type safety, pointers are considered a separate type parameterized by the type of data they point to, even if the underlying representation is an integer. Other measures may also be taken (such as validation & bounds checking), to verify the contents of the pointer variable contain a value that is both a valid memory address and within the numerical range that the processor is capable of addressing.
Harold Lawson is credited with the 1964 invention of the pointer. In 2000, Lawson was presented the Computer Pioneer Award by the IEEE “[f]or inventing the pointer variable and introducing this concept into PL/I, thus providing for the first time, the capability to flexibly treat linked lists in a general-purpose high level language”. According to the Oxford English Dictionary, the word pointer first appeared in print as a stack pointer in a technical memorandum by the System Development Corporation.
In computer science, a pointer is a kind of reference.
A data primitive (or just primitive) is any datum that can be read from or written to computer memory using one memory access (for instance, both a byte and a word are primitives).
A data aggregate (or just aggregate) is a group of primitives that are logically contiguous in memory and that are viewed collectively as one datum (for instance, an aggregate could be 3 logically contiguous bytes, the values of which represent the 3 coordinates of a point in space). When an aggregate is entirely composed of the same type of primitive, the aggregate may be called an array; in a sense, a multi-byte word primitive is an array of bytes, and some programs use words in this way.
In the context of these definitions, a byte is the smallest primitive; each memory address specifies a different byte. The memory address of the initial byte of a datum is considered the memory address (or base memory address) of the entire datum.
A memory pointer (or just pointer) is a primitive, the value of which is intended to be used as a memory address; it is said that a pointer points to a memory address. It is also said that a pointer points to a datum [in memory] when the pointer's value is the datum's memory address.
More generally, a pointer is a kind of reference, and it is said that a pointer references a datum stored somewhere in memory; to obtain that datum is to dereference the pointer. The feature that separates pointers from other kinds of reference is that a pointer's value is meant to be interpreted as a memory address, which is a rather low-level concept.
References serve as a level of indirection: A pointer's value determines which memory address (that is, which datum) is to be used in a calculation. Because indirection is a fundamental aspect of algorithms, pointers are often expressed as a fundamental data type in programming languages; in statically (or strongly) typed programming languages, the type of a pointer determines the type of the datum to which the pointer points.
When setting up data structures like lists, queues and trees, it is necessary to have pointers to help manage how the structure is implemented and controlled. Typical examples of pointers are start pointers, end pointers, and stack pointers. These pointers can either be absolute (the actual physical address or a virtual address in virtual memory) or relative (an offset from an absolute start address (""base"") that typically uses fewer bits than a full address, but will usually require one additional arithmetic operation to resolve).
Relative addresses are a form of manual memory segmentation, and share many of its advantages and disadvantages. A two-byte offset, containing a 16-bit, unsigned integer, can be used to provide relative addressing for up to 64 kilobytes of a data structure. This can easily be extended to 128K, 256K or 512K if the address pointed to is forced to be aligned on a half-word, word or double-word boundary (but, requiring an additional ""shift left"" bitwise operation—by 1, 2 or 3 bits—in order to adjust the offset by a factor of 2, 4 or 8, before its addition to the base address). Generally, though, such schemes are a lot of trouble, and for convenience to the programmer absolute addresses (and underlying that, a flat address space) is preferred.
A one byte offset, such as the hexadecimal ASCII value of a character (e.g. X'29') can be used to point to an alternative integer value (or index) in an array (e.g. X'01'). In this way, characters can be very efficiently translated from 'raw data' to a usable sequential index and then to an absolute address without a lookup table.
Control tables that are used to control program flow usually make extensive use of pointers. The pointers, usually embedded in a table entry, may, for instance, be used to hold the entry points to subroutines to be executed, based on certain conditions defined in the same table entry. The pointers can however be simply indexes to other separate, but associated, tables comprising an array of the actual addresses or the addresses themselves (depending upon the programming language constructs available). They can also be used to point to earlier table entries (as in loop processing) or forward to skip some table entries (as in a switch or ""early"" exit from a loop). For this latter purpose, the ""pointer"" may simply be the table entry number itself and can be transformed into an actual address by simple arithmetic.
Pointers are a very thin abstraction on top of the addressing capabilities provided by most modern architectures. In the simplest scheme, an address, or a numeric index, is assigned to each unit of memory in the system, where the unit is typically either a byte or a word – depending on whether the architecture is byte-addressable or word-addressable – effectively transforming all of memory into a very large array. The system would then also provide an operation to retrieve the value stored in the memory unit at a given address (usually utilizing the machine's general purpose registers).
In the usual case, a pointer is large enough to hold more addresses than there are units of memory in the system. This introduces the possibility that a program may attempt to access an address which corresponds to no unit of memory, either because not enough memory is installed (i.e. beyond the range of available memory) or the architecture does not support such addresses. The first case may, in certain platforms such as the Intel x86 architecture, be called a segmentation fault (segfault). The second case is possible in the current implementation of AMD64, where pointers are 64 bit long and addresses only extend to 48 bits. Pointers must conform to certain rules (canonical addresses), so if a non-canonical pointer is dereferenced, the processor raises a general protection fault.
On the other hand, some systems have more units of memory than there are addresses. In this case, a more complex scheme such as memory segmentation or paging is employed to use different parts of the memory at different times. The last incarnations of the x86 architecture support up to 36 bits of physical memory addresses, which were mapped to the 32-bit linear address space through the PAE paging mechanism. Thus, only 1/16 of the possible total memory may be accessed at a time. Another example in the same computer family was the 16-bit protected mode of the 80286 processor, which, though supporting only 16 MB of physical memory, could access up to 1 GB of virtual memory, but the combination of 16-bit address and segment registers made accessing more than 64 KB in one data structure cumbersome.
In order to provide a consistent interface, some architectures provide memory-mapped I/O, which allows some addresses to refer to units of memory while others refer to device registers of other devices in the computer. There are analogous concepts such as file offsets, array indices, and remote object references that serve some of the same purposes as addresses for other types of objects.
Pointers are directly supported without restrictions in languages such as PL/I, C, C++, Pascal, FreeBASIC, and implicitly in most assembly languages. They are primarily used for constructing references, which in turn are fundamental to constructing nearly all data structures, as well as in passing data between different parts of a program.
In functional programming languages that rely heavily on lists, pointers and references are managed abstractly by the language using internal constructs like cons.
When dealing with arrays, the critical lookup operation typically involves a stage called address calculation which involves constructing a pointer to the desired data element in the array. In other data structures, such as linked lists, pointers are used as references to explicitly tie one piece of the structure to another.
Pointers are used to pass parameters by reference. This is useful if the programmer wants a function's modifications to a parameter to be visible to the function's caller.  This is also useful for returning multiple values from a function.
Pointers can also be used to allocate and deallocate dynamic variables and arrays in memory. Since a variable will often become redundant after it has served its purpose, it is a waste of memory to keep it, and therefore it is good practice to deallocate it (using the original pointer reference) when it is no longer needed. Failure to do so may result in a memory leak (where available free memory gradually, or in severe cases rapidly, diminishes because of an accumulation of numerous redundant memory blocks).
The basic syntax to define a pointer is:
This declares ptr as the identifier of an object of the following type:
This is usually stated more succinctly as ""ptr is a pointer to int.""
Because the C language does not specify an implicit initialization for objects of automatic storage duration, care should often be taken to ensure that the address to which ptr points is valid; this is why it is sometimes suggested that a pointer be explicitly initialized to the null pointer value, which is traditionally specified in C with the standardized macro NULL:
Dereferencing a null pointer in C produces undefined behavior, which could be catastrophic. However, most implementations[citation needed] simply halt execution of the program in question, usually with a segmentation fault.
However, initializing pointers unnecessarily could hinder program analysis, thereby hiding bugs.
In any case, once a pointer has been declared, the next logical step is for it to point at something:
This assigns the value of the address of a to ptr. For example, if a is stored at memory location of 0x8130 then the value of ptr will be 0x8130 after the assignment. To dereference the pointer, an asterisk is used again:
This means take the contents of ptr (which is 0x8130), ""locate"" that address in memory and set its value to 8.
If a is later accessed again, its new value will be 8.
This example may be clearer if memory is examined directly.
Assume that a is located at address 0x8130 in memory and ptr at 0x8134; also assume this is a 32-bit machine such that an int is 32-bits wide. The following is what would be in memory after the following code snippet is executed:
(The NULL pointer shown here is 0x00000000.)
By assigning the address of a to ptr:
yields the following memory values:
Then by dereferencing ptr by coding:
the computer will take the contents of ptr (which is 0x8130), 'locate' that address, and assign 8 to that location yielding the following memory:
Clearly, accessing a will yield the value of 8 because the previous instruction modified the contents of a by way of the pointer ptr.
In C, array indexing is formally defined in terms of pointer arithmetic; that is, the language specification requires that array[i] be equivalent to *(array + i). Thus in C, arrays can be thought of as pointers to consecutive areas of memory (with no gaps), and the syntax for accessing arrays is identical for that which can be used to dereference pointers. For example, an array array can be declared and used in the following manner:
This allocates a block of five integers and names the block array, which acts as a pointer to the block. Another common use of pointers is to point to dynamically allocated memory from malloc which returns a consecutive block of memory of no less than the requested size that can be used as an array.
While most operators on arrays and pointers are equivalent, the result of the sizeof operator differs. In this example, sizeof(array) will evaluate to 5*sizeof(int) (the size of the array), while sizeof(ptr) will evaluate to sizeof(int*), the size of the pointer itself.
Default values of an array can be declared like:
If array is located in memory starting at address 0x1000 on a 32-bit little-endian machine then memory will contain the following (values are in hexadecimal, like the addresses):
Represented here are five integers: 2, 4, 3, 1, and 5. These five integers occupy 32 bits (4 bytes) each with the least-significant byte stored first (this is a little-endian CPU architecture) and are stored consecutively starting at address 0x1000.
The syntax for C with pointers is:
The last example is how to access the contents of array. Breaking it down:
Below is an example definition of a linked list in C.
This pointer-recursive definition is essentially the same as the reference-recursive definition from the Haskell programming language:
Nil is the empty list, and Cons a (Link a) is a cons cell of type a with another link also of type a.
The definition with references, however, is type-checked and does not use potentially confusing signal values.  For this reason, data structures in C are usually dealt with via wrapper functions, which are carefully checked for correctness.
Pointers can be used to pass variables by their address, allowing their value to be changed.  For example, consider the following C code:
In some programs, the required memory depends on what the user may enter. In such cases the programmer needs to allocate memory dynamically. This is done by allocating memory at the heap rather than on the stack, where variables usually are stored. (Variables can also be stored in the CPU registers, but that's another matter) Dynamic memory allocation can only be made through pointers, and names (like with common variables) can't be given.
Pointers are used to store and manage the addresses of dynamically allocated blocks of memory. Such blocks are used to store data objects or arrays of objects. Most structured and object-oriented languages provide an area of memory, called the heap or free store, from which objects are dynamically allocated.
The example C code below illustrates how structure objects are dynamically allocated and referenced. The standard C library provides the function malloc() for allocating memory blocks from the heap. It takes the size of an object to allocate as a parameter and returns a pointer to a newly allocated block of memory suitable for storing the object, or it returns a null pointer if the allocation failed.
The code below illustrates how memory objects are dynamically deallocated, i.e., returned to the heap or free store. The standard C library provides the function free() for deallocating a previously allocated memory block and returning it back to the heap.
On some computing architectures, pointers can be used to directly manipulate memory or memory-mapped devices.
Assigning addresses to pointers is an invaluable tool when programming microcontrollers. Below is a simple example declaring a pointer of type int and initialising it to a hexadecimal address in this example the constant 0x7FFF:
In the mid 80s, using the BIOS to access the video capabilities of PCs was slow. Applications that were display-intensive typically used to access CGA video memory directly by casting the hexadecimal constant 0xB8000 to a pointer to an array of 80 unsigned 16-bit int values. Each value consisted of an ASCII code in the low byte, and a colour in the high byte. Thus, to put the letter 'A' at row 5, column 2 in bright white on blue, one would write code like the following:
In many languages, pointers have the additional restriction that the object they point to has a specific type. For example, a pointer may be declared to point to an integer; the language will then attempt to prevent the programmer from pointing it to objects which are not integers, such as floating-point numbers, eliminating some errors.
For example, in C
money would be an integer pointer and bags would be a char pointer.
The following would yield a compiler warning of ""assignment from incompatible pointer type"" under GCC
because money and bags were declared with different types.
To suppress the compiler warning, it must be made explicit that you do indeed wish to make the assignment by typecasting it
which says to cast the integer pointer of money to a char pointer and assign to bags.
A 2005 draft of the C standard requires that casting a pointer derived from one type to one of another type should maintain the alignment correctness for both types (6.3.2.3 Pointers, par. 7):
In languages that allow pointer arithmetic, arithmetic on pointers takes into account the size of the type. For example, adding an integer number to a pointer produces another pointer that points to an address that is higher by that number times the size of the type. This allows us to easily compute the address of elements of an array of a given type, as was shown in the C arrays example above. When a pointer of one type is cast to another type of a different size, the programmer should expect that pointer arithmetic will be calculated differently. In C, for example, if the money array starts at 0x2000 and sizeof(int) is 4 bytes whereas sizeof(char) is 1 byte, then money + 1 will point to 0x2004, but bags + '1' would point to 0x2001. Other risks of casting include loss of data when ""wide"" data is written to ""narrow"" locations (e.g. bags = 65537;), unexpected results when bit-shifting values, and comparison problems, especially with signed vs unsigned values.
Although it is impossible in general to determine at compile-time which casts are safe, some languages store run-time type information which can be used to confirm that these dangerous casts are valid at runtime. Other languages merely accept a conservative approximation of safe casts, or none at all.
As a pointer allows a program to attempt to access an object that may not be defined, pointers can be the origin of a variety of programming errors. However, the usefulness of pointers is so great that it can be difficult to perform programming tasks without them. Consequently, many languages have created constructs designed to provide some of the useful features of pointers without some of their pitfalls, also sometimes referred to as pointer hazards. In this context, pointers that directly address memory (as used in this article) are referred to as raw pointers, by contrast with smart pointers or other variants.
One major problem with pointers is that as long as they can be directly manipulated as a number, they can be made to point to unused addresses or to data which is being used for other purposes. Many languages, including most functional programming languages and recent imperative languages like Java, replace pointers with a more opaque type of reference, typically referred to as simply a reference, which can only be used to refer to objects and not manipulated as numbers, preventing this type of error. Array indexing is handled as a special case.
A pointer which does not have any address assigned to it is called a wild pointer. Any attempt to use such uninitialized pointers can cause unexpected behavior, either because the initial value is not a valid address, or because using it may damage other parts of the program. The result is often a segmentation fault, storage violation or wild branch (if used as a function pointer or branch address).
In systems with explicit memory allocation, it is possible to create a dangling pointer by deallocating the memory region it points into. This type of pointer is dangerous and subtle because a deallocated memory region may contain the same data as it did before it was deallocated but may be then reallocated and overwritten by unrelated code, unknown to the earlier code. Languages with garbage collection prevent this type of error because deallocation is performed automatically when there are no more references in scope.
Some languages, like C++, support smart pointers, which use a simple form of reference counting to help track allocation of dynamic memory in addition to acting as a reference. In the absence of reference cycles, where an object refers to itself indirectly through a sequence of smart pointers, these eliminate the possibility of dangling pointers and memory leaks. Delphi strings support reference counting natively.
The Rust programming language introduces a borrow checker, pointer lifetimes, and an optimisation based around optional types for null pointers to eliminate pointer bugs, without resorting to a garbage collector.
A null pointer has a value reserved for indicating that the pointer does not refer to a valid object. Null pointers are routinely used to represent conditions such as the end of a list of unknown length or the failure to perform some action; this use of null pointers can be compared to nullable types and to the Nothing value in an option type.
An autorelative pointer is a pointer whose value is interpreted as an offset from the address of the pointer itself; thus, if a data structure has an autorelative pointer member that points to some portion of the data structure itself, then the data structure may be relocated in memory without having to update the value of the auto relative pointer.
The cited patent also uses the term self-relative pointer to mean the same thing. However, the meaning of that term has been used in other ways:
A based pointer is a pointer whose value is an offset from the value of another pointer. This can be used to store and load blocks of data, assigning the address of the beginning of the block to the base pointer.
In some languages, a pointer can reference another pointer, requiring multiple dereference operations to get to the original value. While each level of indirection may add a performance cost, it is sometimes necessary in order to provide correct behavior for complex data structures. For example, in C it is typical to define a linked list in terms of an element that contains a pointer to the next element of the list:
This implementation uses a pointer to the first element in the list as a surrogate for the entire list. If a new value is added to the beginning of the list, head has to be changed to point to the new element. Since C arguments are always passed by value, using double indirection allows the insertion to be implemented correctly, and has the desirable side-effect of eliminating special case code to deal with insertions at the front of the list:
In this case, if the value of item is less than that of head, the caller's head is properly updated to the address of the new item.
A basic example is in the argv argument to the main function in C (and C++), which is given in the prototype as char **argv—this is because the variable argv itself is a pointer to an array of strings (an array of arrays), so *argv is a pointer to the 0th string (by convention the name of the program), and **argv is the 0th character of the 0th string.
In some languages, a pointer can reference executable code, i.e., it can point to a function, method, or procedure. A function pointer will store the address of a function to be invoked. While this facility can be used to call functions dynamically, it is often a favorite technique of virus and other malicious software writers.
A dangling pointer is a pointer that does not point to a valid object and consequently may make a program crash or behave oddly. In the Pascal or C programming languages, pointers that are not specifically initialized may point to unpredictable addresses in memory.
The following example code shows a dangling pointer:
Here, p2 may point to anywhere in memory, so performing the assignment *p2 = 'b'; can corrupt an unknown area of memory or trigger a segmentation fault.
In doubly linked lists or tree structures, a back pointer held on an element 'points back' to the item referring to the current element. These are useful for navigation and manipulation, at the expense of greater memory use.
These pointer declarations cover most variants of pointer declarations. Of course it is possible to have triple pointers, but the main principles behind a triple pointer already exist in a double pointer.
The () and [] have a higher priority than *.  

Where a pointer is used as the address of the entry point to a program or start of a function which doesn't return anything and is also either uninitialized or corrupted, if a call or jump is nevertheless made to this address, a ""wild branch"" is said to have occurred. The consequences are usually unpredictable and the error may present itself in several different ways depending upon whether or not the pointer is a ""valid"" address and whether or not there is (coincidentally) a valid instruction (opcode) at that address. The detection of a wild branch can present one of the most difficult and frustrating debugging exercises since much of the evidence may already have been destroyed beforehand or by execution of one or more inappropriate instructions at the branch location. If available, an instruction set simulator can usually not only detect a wild branch before it takes effect, but also provide a complete or partial trace of its history.
It is possible to simulate pointer behavior using an index to an (normally one-dimensional) array.
Primarily for languages which do not support pointers explicitly but do support arrays, the array can be thought of and processed as if it were the entire memory range (within the scope of the particular array) and any index to it can be thought of as equivalent to a general purpose register in assembly language (that points to the individual bytes but whose actual value is relative to the start of the array, not its absolute address in memory).
Assuming the array is, say, a contiguous 16 megabyte character data structure, individual bytes (or a string of contiguous bytes within the array) can be directly addressed and manipulated using the name of the array with a 31 bit unsigned integer as the simulated pointer (this is quite similar to the C arrays example shown above). Pointer arithmetic can be simulated by adding or subtracting from the index, with minimal additional overhead compared to genuine pointer arithmetic.
It is even theoretically possible, using the above technique, together with a suitable instruction set simulator to simulate any machine code or the intermediate (byte code) of any processor/language in another language that does not support pointers at all (for example Java / JavaScript). To achieve this, the binary code can initially be loaded into contiguous bytes of the array for the simulator to ""read"", interpret and action entirely within the memory contained of the same array.
If necessary, to completely avoid buffer overflow problems, bounds checking can usually be actioned for the compiler (or if not, hand coded in the simulator).
Ada is a strongly typed language where all pointers are typed and only safe type conversions are permitted. All pointers are by default initialized to null, and any attempt to access data through a null pointer causes an exception to be raised. Pointers in Ada are called access types. Ada 83 did not permit arithmetic on access types (although many compiler vendors provided for it as a non-standard feature), but Ada 95 supports “safe” arithmetic on access types via the package System.Storage_Elements.
Several old versions of BASIC for the Windows platform had support for STRPTR() to return the address of a string, and for VARPTR() to return the address of a variable. Visual Basic 5 also had support for OBJPTR() to return the address of an object interface, and for an ADDRESSOF operator to return the address of a function. The types of all of these are integers, but their values are equivalent to those held by pointer types.
Newer dialects of BASIC, such as FreeBASIC or BlitzMax, have exhaustive pointer implementations, however. In FreeBASIC, arithmetic on ANY pointers (equivalent to C's void*) are treated as though the ANY pointer was a byte width. ANY pointers cannot be dereferenced, as in C. Also, casting between ANY and any other type's pointers will not generate any warnings.
In C and C++ pointers are variables that store addresses and can be null. Each pointer has a type it points to, but one can freely cast between pointer types (but not between a function pointer and an object pointer). A special pointer type called the “void pointer” allows pointing to any (non-function) object, but is limited by the fact that it cannot be dereferenced directly (it shall be cast). The address itself can often be directly manipulated by casting a pointer to and from an integral type of sufficient size, though the results are implementation-defined and may indeed cause undefined behavior; while earlier C standards did not have an integral type that was guaranteed to be large enough, C99 specifies the uintptr_t typedef name defined in <stdint.h>, but an implementation need not provide it.
C++ fully supports C pointers and C typecasting. It also supports a new group of typecasting operators to help catch some unintended dangerous casts at compile-time. Since C++11, the C++ standard library also provides smart pointers (unique_ptr, shared_ptr and weak_ptr) which can be used in some situations as a safer alternative to primitive C pointers. C++ also supports another form of reference, quite different from a pointer, called simply a reference or reference type.
Pointer arithmetic, that is, the ability to modify a pointer's target address with arithmetic operations (as well as magnitude comparisons), is restricted by the language standard to remain within the bounds of a single array object (or just after it), and will otherwise invoke undefined behavior. Adding or subtracting from a pointer moves it by a multiple of the size of its datatype. For example, adding 1 to a pointer to 4-byte integer values will increment the pointer's pointed-to byte-address by 4. This has the effect of incrementing the pointer to point at the next element in a contiguous array of integers—which is often the intended result. Pointer arithmetic cannot be performed on void pointers because the void type has no size, and thus the pointed address can not be added to, although gcc and other compilers will perform byte arithmetic on void* as a non-standard extension, treating it as if it were char *.
Pointer arithmetic provides the programmer with a single way of dealing with different types: adding and subtracting the number of elements required instead of the actual offset in bytes. (Pointer arithmetic with char * pointers uses byte offsets, because sizeof(char) is 1 by definition.) In particular, the C definition explicitly declares that the syntax a[n], which is the n-th element of the array a, is equivalent to *(a + n), which is the content of the element pointed by a + n.  This implies that n[a] is equivalent to a[n], and one can write, e.g., a or 3[a] equally well to access the fourth element of an array a.
While powerful, pointer arithmetic can be a source of computer bugs. It tends to confuse novice programmers, forcing them into different contexts: an expression can be an ordinary arithmetic one or a pointer arithmetic one, and sometimes it is easy to mistake one for the other. In response to this, many modern high-level computer languages (for example Java) do not permit direct access to memory using addresses.  Also, the safe C dialect Cyclone addresses many of the issues with pointers. See C programming language for more discussion.
The void pointer, or void*, is supported in ANSI C and C++ as a generic pointer type. A pointer to void can store the address of any object (not function), and, in C, is implicitly converted to any other object pointer type on assignment, but it must be explicitly cast if dereferenced.
K&R C used char* for the “type-agnostic pointer” purpose (before ANSI C).
C++ does not allow the implicit conversion of void* to other pointer types, even in assignments. This was a design decision to avoid careless and even unintended casts, though most compilers only output warnings, not errors, when encountering other casts.
In C++, there is no void& (reference to void) to complement void* (pointer to void), because references behave like aliases to the variables they point to, and there can never be a variable whose type is void.
In the C# programming language, pointers are supported only under certain conditions: any block of code including pointers must be marked with the unsafe keyword. Such blocks usually require higher security permissions to be allowed to run.
The syntax is essentially the same as in C++, and the address pointed can be either managed or unmanaged memory. However, pointers to managed memory (any pointer to a managed object) must be declared using the fixed keyword, which prevents the garbage collector from moving the pointed object as part of memory management while the pointer is in scope, thus keeping the pointer address valid.
An exception to this is from using the IntPtr structure, which is a safe managed equivalent to int*, and does not require unsafe code.  This type is often returned when using methods from the System.Runtime.InteropServices, for example:
The .NET framework includes many classes and methods in the System and System.Runtime.InteropServices namespaces (such as the Marshal class) which convert .NET types (for example, System.String) to and from many unmanaged types and pointers (for example, LPWSTR or void*) to allow communication with unmanaged code. Most such methods have the same security permission requirements as unmanaged code, since they can affect arbitrary places in memory.
The COBOL programming language supports pointers to variables. Primitive or group (record) data objects declared within the LINKAGE SECTION of a program are inherently pointer-based, where the only memory allocated within the program is space for the address of the data item (typically a single memory word). In program source code, these data items are used just like any other WORKING-STORAGE variable, but their contents are implicitly accessed indirectly through their LINKAGE pointers.
Memory space for each pointed-to data object is typically allocated dynamically using external CALL statements or via embedded extended language constructs such as EXEC CICS or EXEC SQL statements.
Extended versions of COBOL also provide pointer variables declared with USAGE IS POINTER clauses. The values of such pointer variables are established and modified using SET and SET ADDRESS statements.
Some extended versions of COBOL also provide PROCEDURE-POINTER variables, which are capable of storing the addresses of executable code.
The PL/I language provides full support for pointers to all data types (including pointers to structures), recursion, multitasking, string handling, and extensive built-in functions. PL/I was quite a leap forward compared to the programming languages of its time.[citation needed]
The D programming language is a derivative of C and C++ which fully supports C pointers and C typecasting.
The Eiffel object-oriented language employs value and reference semantics without pointer arithmetic. Nevertheless, pointer classes are provided. They offer pointer arithmetic, typecasting, explicit memory management,
interfacing with non-Eiffel software, and other features.
Fortran-90 introduced a strongly typed pointer capability.  Fortran pointers contain more than just a simple memory address.  They also encapsulate the lower and upper bounds of array dimensions, strides (for example, to support arbitrary array sections), and other metadata.  An association operator, => is used to associate a POINTER to a variable which has a TARGET attribute.  The Fortran-90 ALLOCATE statement may also be used to associate a pointer to a block of memory.  For example, the following code might be used to define and create a linked list structure:
Fortran-2003 adds support for procedure pointers.  Also, as part of the C Interoperability feature, Fortran-2003 supports intrinsic functions for converting C-style pointers into Fortran pointers and back.
Go has pointers. Its declaration syntax is equivalent to that of C, but written the other way around, ending with the type. Unlike C, Go has garbage collection, and disallows pointer arithmetic. Reference types, like in C++, do not exist. Some built-in types, like maps and channels, are boxed (i.e. internally they are pointers to mutable structures), and are initialized using the make function. In an approach to unified syntax between pointers and non-pointers, the arrow (->) operator has been dropped: the dot operator on a pointer refers to the field or method of the dereferenced object. This, however, only works with 1 level of indirection.
Unlike C, C++, or Pascal, there is no explicit representation of pointers in Java. Instead, more complex data structures like objects and arrays are implemented using references. The language does not provide any explicit pointer manipulation operators. It is still possible for code to attempt to dereference a null reference (null pointer), however, which results in a run-time exception being thrown. The space occupied by unreferenced memory objects is recovered automatically by garbage collection at run-time.
Pointers are implemented very much as in Pascal, as are VAR parameters in procedure calls. Modula-2 is even more strongly typed than Pascal, with fewer ways to escape the type system. Some of the variants of Modula-2 (such as Modula-3) include garbage collection.
Much as with Modula-2, pointers are available. There are still fewer ways to evade the type system and so Oberon and its variants are still safer with respect to pointers than Modula-2 or its variants. As with Modula-3, garbage collection is a part of the language specification.
Unlike many languages that feature pointers, standard ISO Pascal only allows pointers to reference dynamically created variables that are anonymous and does not allow them to reference standard static or local variables. It does not have pointer arithmetic. Pointers also must have an associated type and a pointer to one type is not compatible with a pointer to another type (e.g. a pointer to a char is not compatible with a pointer to an integer). This helps eliminate the type security issues inherent with other pointer implementations, particularly those used for PL/I or C. It also removes some risks caused by dangling pointers, but the ability to dynamically let go of referenced space by using the dispose standard procedure (which has the same effect as the free library function found in C) means that the risk of dangling pointers has not been entirely eliminated.
However, in some commercial and open source Pascal (or derivatives) compiler implementations —like Free Pascal,Turbo Pascal or the Object Pascal in Embarcadero Delphi— a pointer is allowed to reference standard static or local variables and can be cast from one pointer type to another. Moreover, pointer arithmetic is unrestricted: adding or subtracting from a pointer moves it by that number of bytes in either direction, but using the Inc or Dec standard procedures with it moves the pointer by the size of the data type it is declared to point to. An untyped pointer is also provided under the name Pointer, which is compatible with other pointer types.
The Perl programming language supports pointers, although rarely used, in the form of the pack and unpack functions. These are intended only for simple interactions with compiled OS libraries. In all other cases, Perl uses references, which are typed and do not allow any form of pointer arithmetic. They are used to construct complex data structures."
"119","In computer programming, undefined behavior (UB) is the result of executing computer code whose behavior is not prescribed by the language specification to which the code adheres, for the current state of the program. This happens when the translator of the source code makes certain assumptions, but these assumptions are not satisfied during execution.
The behavior of some programming languages—most famously C and C++—is undefined in some cases. In the standards for these languages the semantics of certain operations is described as undefined. These cases typically represent unambiguous bugs in the code, for example indexing an array outside of its bounds. An implementation is allowed to assume that such operations never occur in correct standard-conforming program code. In the case of C/C++, the compiler is allowed to give a compile-time diagnostic in these cases, but is not required to: the implementation will be considered correct whatever it does in such cases, analogous to don't-care terms in digital logic. It is the responsibility of the programmer to write code that never invokes undefined behavior, although compiler implementations are allowed to issue diagnostics when this happens. This assumption can make various program transformations valid or simplify their proof of correctness, giving flexibility to the implementation. As a result, the compiler can often make more optimizations. It also allows more compile-time checks by both compilers and static program analysis. 
In the C community, undefined behavior may be humorously referred to as ""nasal demons"", after a comp.std.c post that explained undefined behavior as allowing the compiler to do anything it chooses, even ""to make demons fly out of your nose"". Under some circumstances there can be specific restrictions on undefined behavior. For example, the instruction set specifications of a CPU might leave the behavior of some forms of an instruction undefined, but if the CPU supports memory protection then the specification will probably include a blanket rule stating that no user-accessible instruction may cause a hole in the operating system's security; so an actual CPU would be permitted to corrupt user registers in response to such an instruction, but would not be allowed to, for example, switch into supervisor mode.
Documenting an operation as undefined behavior allows compilers to assume that this operation will never happen in a conforming program. This gives the compiler more information about the code and this information can lead to more optimization opportunities.
An example for the C language:
The value of x cannot be negative and, given that signed integer overflow is undefined behavior in C, the compiler can assume that at the line of the if check value >= 2147483600. Thus the if and the call to the function bar can be ignored by the compiler since the if has no side effects and its condition will never be satisfied. The code above is therefore semantically equivalent to:
Had the compiler been forced to assume that signed integer overflow has wraparound behavior, then the transformation above would not have been legal.
Such optimizations become hard to spot by humans when the code is more complex and other optimizations, like inlining, take place.
Another benefit from allowing signed integer overflow to be undefined is that it makes it possible to store and manipulate a variable's value in a processor register that is larger than the size of the variable in the source code. For example, if the type of a variable as specified in the source code is narrower than the native register width (such as ""int"" on a 64-bit machine, a common scenario), then the compiler can safely use a signed 64-bit integer for the variable in the machine code it produces, without changing the defined behavior of the code. If the behavior of a 32-bit integer under overflow conditions was depended upon by the program, then a compiler would have to insert additional logic when compiling for a 64-bit machine, because the overflow behavior of most machine code instructions depends on the register width.
A further important benefit of undefined signed integer overflow is that it enables, though does not require, such erroneous overflows to be detected at compile-time or by static program analysis, or by run-time checks such as the  Clang and GCC sanitizers  and valgrind; if such overflow was defined with a valid semantics such as wrap-around then compile-time checks would not be possible.
C and C++ standards have several forms of undefined behavior throughout, which offers increased liberty in compiler implementations and compile-time checks at the expense of undefined run-time behavior if present. In particular, there is an appendix section dedicated to a non-exhaustive listing of common sources of undefined behavior in C. Moreoever, compilers are not required to diagnose code that relies on undefined behavior, due to current static analysis limitations. Hence, it is common for programmers, even experienced ones, to unintentionally rely on undefined behavior either by mistake, or simply because they are not well-versed in the rules of the language that can span over hundreds of pages. This can result in bugs that are exposed when optimizations are enabled on the compiler, or when a compiler of a different vendor or version is used. Testing or fuzzing with dynamic undefined behavior checks enabled, e.g. the Clang sanitizers, can help to catch undefined behavior not diagnosed by the compiler or static analyzers.
In scenarios where security is critical, undefined behavior can lead to security vulnerabilities in software. When GCC's developers changed their compiler in 2008 such that it omitted certain overflow checks that relied on undefined behavior, CERT issued a warning against the newer versions of the compiler.Linux Weekly News pointed out that the same behavior was observed in PathScale C, Microsoft Visual C++ 2005 and several other compilers; the warning was later amended to warn about various compilers.
The major forms of undefined behavior in C can be broadly classified as : spatial memory safety violations, temporal memory safety violations, integer overflow, strict aliasing violations, alignment violations, unsequenced modifications, data races, and loops that neither perform I/O nor terminate.
In C the use of any automatic variable before it has been initialized yields undefined behavior, as does integer division by zero, signed integer overflow, indexing an array outside of its defined bounds (see buffer overflow), or null pointer dereferencing. In general, any instance of undefined behavior leaves the abstract execution machine in an unknown state, and any subsequent behavior is also undefined. 
Attempting to modify a string literal causes undefined behavior:
Integer division by zero results in undefined behavior:
Certain pointer operations may result in undefined behavior:
In C and C++, the comparison of pointers to objects is only strictly defined if the pointers point to members of the same object, or elements of the same array. Example:
Reaching the end of a value-returning function (other than main()) without a return statement results in undefined behavior if the value of the function call is used by the caller:
Modifying an object between two sequence points more than once produces undefined behavior. It is worth mentioning that there are considerable changes in what causes undefined behavior in relation to sequence points as of C++11. The following example will however cause undefined behavior in both C++ and C. 
When modifying an object between two sequence points, reading the value of the object for any other purpose than determining the value to be stored is also undefined behavior."
"120","In computer programming, run-time type information or run-time type identification (RTTI) refers to a C++ mechanism that exposes information about an object's data type at runtime.  Run-time type information can apply to simple data types, such as integers and characters, or to generic types. This is a C++ specialization of a more general concept called type introspection. Similar mechanisms are also known in other programming languages, such as Object Pascal (Delphi).
In the original C++ design, Bjarne Stroustrup did not include run-time type information, because he thought this mechanism was often misused.
C++ RTTI can be used to do safe typecasts, using the dynamic_cast<> operator, and to manipulate type information at run time, using the typeid operator and std::type_info class.
RTTI is available only for classes which are polymorphic, which means they have at least one virtual method. In practice, this is not a limitation because base classes must have a virtual destructor to allow objects of derived classes to perform proper cleanup if they are deleted from a base pointer.
RTTI is optional with some compilers; the programmer can choose at compile time whether to include the functionality.  There may be a resource cost to making RTTI available even if a program does not use it.
The typeid keyword is used to determine the class of an object at run time.  It returns a reference to std::type_info object, which exists until the end of the program.  The use of typeid, in a non-polymorphic context, is often preferred over dynamic_cast<class_type> in situations where just the class information is needed, because typeid is a constant-time procedure, whereas dynamic_cast must traverse the class derivation lattice of its argument at runtime.[citation needed] Some aspects of the returned object are implementation-defined, such as std::type_info::name(), and cannot be relied on across compilers to be consistent.
Objects of class std::bad_typeid are thrown when the expression for typeid is the result of applying the unary * operator on a null pointer. Whether an exception is thrown for other null reference arguments is implementation-dependent. In other words, for the exception to be guaranteed, the expression must take the form typeid(*p) where p is any expression resulting in a null pointer.
Output (exact output varies by system):
The dynamic_cast operator in C++ is used for downcasting a reference or pointer to a more specific type in the class hierarchy. Unlike the static_cast, the target of the dynamic_cast must be a pointer or reference to class. Unlike static_cast and C-style typecast (where type check is made during compilation), a type safety check is performed at runtime. If the types are not compatible, an exception will be thrown (when dealing with references) or a null pointer will be returned (when dealing with pointers).
A Java typecast behaves similarly; if the object being cast is not actually an instance of the target type, and cannot be converted to one by a language-defined method, an instance of java.lang.ClassCastException will be thrown.
Suppose some function takes an object of type A as its argument, and wishes to perform some additional operation if the object passed is an instance of B, a subclass of A.  This can be accomplished using dynamic_cast as follows.
Console output:
A similar version of my_function can be written with pointers instead of references:"
"121","Exception handling  is the process of responding to the occurrence, during computation, of exceptions – anomalous or exceptional conditions requiring special processing – often changing the normal flow of program execution. It is provided by specialized programming language constructs, computer hardware mechanisms like interrupts or operating system IPC facilities like signals.
In general, an exception breaks the normal flow of execution and executes a pre-registered exception handler. The details of how this is done depends on whether it is a hardware or software exception and how the software exception is implemented. Some exceptions, especially hardware ones, may be handled so gracefully that execution can resume where it was interrupted.
Alternative approaches to exception handling in software are error checking, which maintains normal program flow with later explicit checks for contingencies reported using special return values or some auxiliary global variable such as C's errno or floating point status flags; or input validation to preemptively filter exceptional cases.
Hardware exception mechanisms are processed by the CPU. It is intended to support error detection and redirects the program flow to error handling service routines. The state before the exception is saved on the stack.
Exception handling in the IEEE 754 floating point hardware standard refers in general to exceptional conditions and defines an exception as ""an event that occurs when an operation on some particular operands has no outcome suitable for every reasonable application. That operation might signal one or more exceptions by invoking the default or, if explicitly requested, a language-defined alternate handling.""
By default, an IEEE 754 exception is resumable and is handled by substituting a predefined value for different exceptions, e.g. infinity for a divide by zero exception, and providing status flags for later checking of whether the exception occurred (see C99 programming language for a typical example of handling of IEEE 754 exceptions). An exception-handling style enabled by the use of status flags involves: first computing an expression using a fast, direct implementation; checking whether it failed by testing status flags; and then, if necessary, calling a slower, more numerically robust, implementation.
The IEEE 754 standard uses the term ""trapping"" to refer to the calling of a user-supplied exception-handling routine on exceptional conditions, and is an optional feature of the standard. The standard recommends several usage scenarios for this, including the implementation of non-default pre-substitution of a value followed by resumption, to concisely handle removable singularities.
The default IEEE 754 exception handling behaviour of resumption following pre-substitution of a default value avoids the risks inherent in changing flow of program control on numerical exceptions. For example, in 1996 the maiden flight of the Ariane 5 (Flight 501) ended in a catastrophic explosion due in part to the Ada programming language exception handling policy of aborting computation on arithmetic error, which in this case was a 64-bit floating point to 16-bit integer conversion overflow. In the Ariane Flight 501 case, the programmers protected only four out of seven critical variables against overflow due to concerns about the computational constraints of the on-board computer and relied on what turned out to be incorrect assumptions about the possible range of values for the three unprotected variables because they reused code from the Ariane 4, for which their assumptions were correct. According to William Kahan, the loss of Flight 501 would have been avoided if the IEEE 754 exception-handling policy of default substitution had been used because the overflowing 64-bit to 16-bit conversion that caused the software to abort occurred in a piece of code that turned out to be completely unnecessary on the Ariane 5. The official report on the crash (conducted by an inquiry board headed by Jacques-Louis Lions) noted that ""An underlying theme in the development of Ariane 5 is the bias towards the mitigation of random failure. The supplier of the inertial navigation system (SRI) was only following the specification given to it, which stipulated that in the event of any detected exception the processor was to be stopped. The exception which occurred was not due to random failure but a design error. The exception was detected, but inappropriately handled because the view had been taken that software should be considered correct until it is shown to be at fault. [...] Although the failure was due to a systematic software design error, mechanisms can be introduced to mitigate this type of problem. For example the computers within the SRIs could have continued to provide their best estimates of the required attitude information. There is reason for concern that a software exception should be allowed, or even required, to cause a processor to halt while handling mission-critical equipment. Indeed, the loss of a proper software function is hazardous because the same software runs in both SRI units. In the case of Ariane 501, this resulted in the switch-off of two still healthy critical units of equipment.""
From the processing point of view, hardware interrupts are similar to resumable exceptions, though they are typically unrelated to the user program's control flow.
The operating system may provide facilities for handling exceptions in programs via IPC. Typically, interrupts caused by the execution of a process are handled by the interrupt service routines of the operating system, and the operating system may then send a signal to that process, which may have asked the operating system to register a signal handler to be called when the signal is raised, or let the operating system execute a default action (like terminating the program). Typical examples are SIGSEGV, SIGBUS, SIGILL and SIGFPE.
Software exception handling and the support provided by software tools differs somewhat from what is understood by exception handling in hardware, but similar concepts are involved. In programming language mechanisms for exception handling, the term exception is typically used in a specific sense to denote a data structure storing information about an exceptional condition. One mechanism to transfer control, or raise an exception, is known as a throw. The exception is said to be thrown. Execution is transferred to a ""catch"".
From the point of view of the author of a routine, raising an exception is a useful way to signal that a routine could not execute normally - for example, when an input argument is invalid (e.g. value is outside of the domain of a function) or when a resource it relies on is unavailable (like a missing file, a hard disk error, or out-of-memory errors). In systems without exceptions, routines would need to return some special error code. However, this is sometimes complicated by the semipredicate problem, in which users of the routine need to write extra code to distinguish normal return values from erroneous ones.
Programming languages differ substantially in their notion of what is an exception. Contemporary languages can roughly be divided in two groups:
Kiniry also notes that ""Language design only partially influences the use of exceptions, and consequently, the
manner in which one handles partial and total failures during system execution. The other major influence is examples of use, typically in core libraries and code examples in technical books, magazine articles, and online discussion forums, and in an organization’s code standards.""
Contemporary applications face many design challenges when considering exception handling strategies. Particularly in modern enterprise level applications, exceptions must often cross process boundaries and machine boundaries. Part of designing a solid exception handling strategy is recognizing when a process has failed to the point where it cannot be economically handled by the software portion of the process.
Software exception handling developed in Lisp in the 1960s and 1970s. This originated in LISP 1.5 (1962), where exceptions were caught by the ERRSET keyword, which returned NIL in case of an error, instead of terminating the program or entering the debugger. Error raising was introduced in MacLisp in the late 1960s via the ERR keyword. This was rapidly used not only for error raising, but for non-local control flow, and thus was augmented by two new keywords, CATCH and THROW (MacLisp June 1972), reserving ERRSET and ERR for error handling. The cleanup behavior now generally called ""finally"" was introduced in NIL (New Implementation of LISP) in the mid- to late-1970s as UNWIND-PROTECT. This was then adopted by Common Lisp. Contemporary with this was dynamic-wind in Scheme, which handled exceptions in closures. The first papers on structured exception handling were Goodenough (1975a) and Goodenough (1975b). Exception handling was subsequently widely adopted by many programming languages from the 1980s onward.
Originally, software exception handling included both resumable exceptions (resumption semantics), like most hardware exceptions, and non-resumable exceptions (termination semantics). However, resumption semantics were considered ineffective in practice in the 1970s and 1980s (see C++ standardization discussion, quoted below) and are no longer in common use, though provided by programming languages like Common Lisp and Dylan.
Exception handling mechanisms in contemporary languages are typically non-resumable (""termination semantics"") as opposed to hardware exceptions, which are typically resumable. This is based on experience of using both, as there are theoretical and design arguments in favor of either decision; these were extensively debated during C++ standardization discussions 1989–1991, which resulted in a definitive decision for termination semantics. On the rationale for such a design for the C++ mechanism, Stroustrup notes:
He backed this statement with experience from several operating systems. The key example was Cedar/Mesa: It was written by people who liked and used resumption, but after ten years of use, there was only one use of resumption left in the half million line system – and that was a context inquiry. Because resumption wasn't actually necessary for such a context inquiry, they removed it and found a significant speed increase in that part of the system. In each and every case where resumption had been used it had – over the ten years – become a problem and a more appropriate design had replaced it. Basically, every use of resumption had represented a failure to keep separate levels of abstraction disjoint.
A contrasting view on the safety of exception handling was given by C.A.R Hoare in 1980, describing the Ada programming language as having ""...a plethora of features and notational conventions, many of them unnecessary and some of them, like exception handling, even dangerous. [...] Do not allow this language in its present state to be used in applications where reliability is critical[...]. The next rocket to go astray as a result of a programming language error may not be an exploratory space rocket on a harmless trip to Venus: It may be a nuclear warhead exploding over one of our own cities."" 
Exception handling is often not handled correctly in software, especially when there are multiple sources of exceptions; data flow analysis of 5 million lines of Java code found over 1300 exception handling defects.
Citing multiple prior studies by others (1999–2004) and their own results, Weimer and Necula wrote that a significant problem with exceptions is that they ""create hidden control-flow paths that are difficult for programmers to reason about"".:8:27
Go was initially released with exception handling explicitly omitted, with the developers arguing that it obfuscated control flow. Later, the exception-like panic/recover mechanism was added to the language, which the Go authors advise using only for unrecoverable errors that should halt the entire process.
One case of early criticism against exception handling was dealing with resource leaks or state inconsistency, such as escaping a section locked by a mutex, or one temporarily holding a file open. The RAII and the dispose pattern are one approach to handle this.
Many computer languages have built-in support for exceptions and exception handling. This includes ActionScript, Ada, BlitzMax, C++, C#, COBOL, D, ECMAScript, Eiffel, Java, ML, Object Pascal (e.g. Delphi, Free Pascal, and the like), PowerBuilder, Objective-C, OCaml, PHP (as of version 5), PL/1, PL/SQL, Prolog, Python, REALbasic, Ruby, Scala, Seed7, Tcl, Visual Prolog and most .NET languages. Exception handling is commonly not resumable in those languages, and when an exception is thrown, the program searches back through the stack of function calls until an exception handler is found.
Some languages call for unwinding the stack as this search progresses. That is, if function f, containing a handler  H for exception E, calls function g, which in turn calls function h, and an exception E occurs in h, then functions h and g may be terminated, and H in f will handle E.
An exception-handling language without this unwinding is Common Lisp with its Condition System. Common Lisp calls the exception handler and does not unwind the stack. This allows the program to continue the computation at exactly the same place where the error occurred (for example when a previously missing file has become available). The stackless implementation of the Mythryl programming language supports constant-time exception handling without stack unwinding.
Excluding minor syntactic differences, there are only a couple of exception handling styles in use. In the most popular style, an exception is initiated by a special statement (throw or raise) with an exception object (e.g. with Java or Object Pascal) or a value of a special extendable enumerated type (e.g. with Ada or SML). The scope for exception handlers starts with a marker clause (try or the language's block starter such as begin) and ends in the start of the first handler clause (catch, except, rescue). Several handler clauses can follow, and each can specify which exception types it handles and what name it uses for the exception object.
A few languages also permit a clause (else) that is used in case no exception occurred before the end of the handler's scope was reached.
More common is a related clause (finally or ensure) that is executed whether an exception occurred or not, typically to release resources acquired within the body of the exception-handling block. Notably, C++ does not provide this construct, since it encourages the Resource Acquisition Is Initialization (RAII) technique which frees resources using destructors.
In its whole, exception handling code might look like this (in Java-like pseudocode; note that an exception type called EmptyLineException would need to be declared somewhere):
As a minor variation, some languages use a single handler clause, which deals with the class of the exception internally.
According to a 2008 paper by Westley Wiemer and George Necula, the syntax of the try...finally blocks in Java is a contributing factor to software defects. When a method needs to handle the acquisition and release of 3–5 resources, programmers are apparently unwilling to nest enough blocks due to readability concerns, even when this would be a correct solution. It is possible to use a single try...finally block even when dealing with multiple resources, but that requires a correct use of sentinel values, which is another common source of bugs for this type of problem.:8:6–8:7 Regarding the semantics of the try...catch...finally construct in general, Wiemer and Necula write that ""While try-catch-finally is conceptually simple, it has the most complicated execution description in the language specification [Gosling et al. 1996] and requires four levels of nested “if”s in its official English description. In short, it contains a large number of corner cases that programmers often overlook."":8:13–8:14
C supports various means of error checking, but generally is not considered to support ""exception handling,"" although the setjmp and longjmp standard library functions can be used to implement exception semantics.
Perl has optional support for structured exception handling.
Python's support for exception handling is pervasive and consistent. It's difficult to write a robust Python program without using its try and except keywords.[citation needed]
The implementation of exception handling in programming languages typically involves a fair amount of support from both a code generator and the runtime system accompanying a compiler. (It was the addition of exception handling to C++ that ended the useful lifetime of the original C++ compiler, Cfront.)  Two schemes are most common. The first, dynamic registration, generates code that continually updates structures about the program state in terms of exception handling.  Typically, this adds a new element to the stack frame layout that knows what handlers are available for the function or method associated with that frame; if an exception is thrown, a pointer in the layout directs the runtime to the appropriate handler code. This approach is compact in terms of space, but adds execution overhead on frame entry and exit. It was commonly used in many Ada implementations, for example, where complex generation and runtime support was already needed for many other language features. Dynamic registration, being fairly straightforward to define, is amenable to proof of correctness.
The second scheme, and the one implemented in many production-quality C++ compilers, is a table-driven approach. This creates static tables at compile time and link time that relate ranges of the program counter to the program state with respect to exception handling.  Then, if an exception is thrown, the runtime system looks up the current instruction location in the tables and determines what handlers are in play and what needs to be done. This approach minimizes executive overhead for the case where an exception is not thrown. This happens at the cost of some space, but this space can be allocated into read-only, special-purpose data sections that are not loaded or relocated until an exception is actually thrown.  This second approach is also superior in terms of achieving thread safety[citation needed].
Other definitional and implementation schemes have been proposed as well. For languages that support metaprogramming, approaches that involve no overhead at all have been advanced.
A different view of exceptions is based on the principles of design by contract and is supported in particular by the Eiffel language. The idea is to provide a more rigorous basis for exception handling by defining precisely what is ""normal"" and ""abnormal"" behavior. Specifically, the approach is based on two concepts:
The ""Safe Exception Handling principle"" as introduced by Bertrand Meyer in Object-Oriented Software Construction then holds that there are only two meaningful ways a routine can react when an exception occurs:
In particular, simply ignoring an exception is not permitted; a block must either be retried and successfully complete, or propagate the exception to its caller.
Here is an example expressed in Eiffel syntax. It assumes that a routine send_fast is normally the better way to send a message, but it may fail, triggering an exception; if so, the algorithm next uses send_slow, which will fail less often. If send_slow fails, the routine send as a whole should fail, causing the caller to get an exception.
The boolean local variables are initialized to False at the start. If  send_fast fails, the body (do clause) will be executed again, causing execution of send_slow. If this execution of send_slow fails, the rescue clause will execute to the end with no retry (no else clause in the final if), causing the routine execution as a whole to fail.
This approach has the merit of defining clearly what ""normal"" and ""abnormal"" cases are: an abnormal case, causing an exception, is one in which the routine is unable to fulfill its contract. It defines a clear distribution of roles: the do clause (normal body) is in charge of achieving, or attempting to achieve, the routine's contract; the rescue clause is in charge of reestablishing the context and restarting the process, if this has a chance of succeeding, but not of performing any actual computation.
Although exceptions in Eiffel have a fairly clear philosophy, Kiniry (2006) criticizes their implementation because ""Exceptions that are part of the language definition are represented by INTEGER values, developer-defined exceptions by STRING values. [...] Additionally, because they are basic values and not objects, they have no inherent semantics beyond that which is expressed in a helper routine which necessarily cannot be foolproof because of the representation overloading in effect (e.g., one cannot
differentiate two integers of the same value).""
If an exception is thrown and not caught (operationally, an exception is thrown when there is no applicable handler specified), the uncaught exception is handled by the runtime; the routine that does this is called the uncaught exception handler. The most common default behavior is to terminate the program and print an error message to the console, usually including debug information such as a string representation of the exception and the stack trace. This is often avoided by having a top-level (application-level) handler (for example in an event loop) that catches exceptions before they reach the runtime.
Note that even though an uncaught exception may result in the program terminating abnormally (the program may not be correct if an exception is not caught, notably by not rolling back partially completed transactions, or not releasing resources), the process terminates normally (assuming the runtime works correctly), as the runtime (which is controlling execution of the program) can ensure orderly shutdown of the process.
In a multithreaded program, an uncaught exception in a thread may instead result in termination of just that thread, not the entire process (uncaught exceptions in the thread-level handler are caught by the top-level handler). This is particularly important for servers, where for example a servlet (running in its own thread) can be terminated without the server overall being affected.
This default uncaught exception handler may be overridden, either globally or per-thread, for example to provide alternative logging or end-user reporting of uncaught exceptions, or to restart threads that terminate due to an uncaught exception. For example, in Java this is done for a single thread via Thread.setUncaughtExceptionHandler and globally via Thread.setDefaultUncaughtExceptionHandler; in Python this is done by modifying sys.excepthook.
The designers of Java devised checked exceptions, which are a special set of exceptions. The checked exceptions that a method may raise are part of the method's signature. For instance, if a method might throw an IOException
, it must declare this fact explicitly in its method signature. Failure to do so raises a compile-time error.
Kiniry (2006) notes however that Java's libraries (as they were in 2006) were often inconsistent in their approach to error reporting, because ""Not all erroneous situations in Java are represented by exceptions though. Many methods return special values which indicate failure encoded as constant field of related classes.""
Checked exceptions are related to exception checkers that exist for the OCaml programming language. The external tool for OCaml is both invisible (i.e. it does not require any syntactic annotations) and optional (i.e. it is possible to compile and run a program without having checked the exceptions, although this is not recommended for production code).
The CLU programming language had a feature with the interface closer to what Java has introduced later. A function could raise only exceptions listed in its type, but any leaking exceptions from called functions would automatically be turned into the sole runtime exception, failure, instead of resulting in compile-time error. Later, Modula-3 had a similar feature. These features don't include the compile time checking that is central in the concept of checked exceptions, and hasn't (as of 2006) been incorporated into major programming languages other than Java.
Early versions of the C++ programming language included an optional mechanism for checked exceptions, called exception specifications. By default any function could throw any exception, but this was limited by a throw clause added to the function signature, that specified which exceptions the function may throw. Exception specifications were not enforced at compile-time. Violations resulted in the global function std::unexpected being called.  An empty exception specification could be given, which indicated that the function will throw no exception. This was not made the default when exception handling was added to the language because it would have required too much modification of existing code, would have impeded interaction with code written in other languages, and would have tempted programmers into writing too many handlers at the local level. Explicit use of empty exception specifications could, however, allow C++ compilers to perform significant code and stack layout optimizations that generally have to be suppressed when exception handling may take place in a function.  Some analysts viewed the proper use of exception specifications in C++ as difficult to achieve. In the recent[ref] C++ language standard (C++11), this use of exception specifications as specified in the C++03 version of the standard was deprecated and was removed from the language in C++17. A function that will not throw any exceptions can now be denoted by the noexcept keyword.
In contrast to Java, languages like C# do not enforce that exceptions have to be caught. According to Hanspeter Mössenböck, not distinguishing between to-be-called (checked) exceptions and not-to-be-called (unchecked) exceptions makes the written program more convenient, but less robust, as an uncaught exception results in an abort with a stack trace. Kiniry (2006) notes however that Java's JDK (version 1.4.1) throws a large number of unchecked exceptions: one for every 140 lines of code, whereas Eiffel uses them much more sparingly, with one thrown every 4,600 lines of code. Kiniry also writes that ""As any Java programmer knows, the volume of try catch code in a typical Java application is sometimes larger than the comparable code necessary for explicit formal parameter and return value checking in other languages that do not have checked exceptions. In fact, the general consensus among in-the-trenches Java programmers is that dealing with checked exceptions is nearly as unpleasant a task as writing documentation. Thus, many programmers report that they “resent” checked exceptions. This leads to an abundance of checked-but-ignored exceptions"". Kiniry also notes that the developers of C# apparently were influenced by this kind of user experiences, with the following quote being attributed to them (via Eric Gunnerson): 
 According to Anders Hejlsberg there was fairly broad agreement in their design group to not have checked exceptions as a language feature in C#. Hejlsberg explained in an interview that 
Checked exceptions can, at compile time, reduce the incidence of unhandled exceptions surfacing at runtime in a given application. Unchecked exceptions (such as the Java objects RuntimeException
 and Error
) remain unhandled.
However, checked exceptions can either require extensive throws
 declarations, revealing implementation details and reducing encapsulation, or encourage coding poorly considered try/catch
 blocks that can hide legitimate exceptions from their appropriate handlers.[citation needed]  Consider a growing codebase over time. An interface may be declared to throw exceptions X and Y. In a later version of the code, if one wants to throw exception Z, it would make the new code incompatible with the earlier uses. Furthermore, with the adapter pattern, in which one body of code declares an interface that is then implemented by a different body of code so that code can be plugged in and called by the first, the adapter code may have a rich set of exceptions to describe problems, but is forced to use the exception types declared in the interface.
It is possible to reduce the number of declared exceptions either by declaring a superclass of all potentially thrown exceptions, or by defining and declaring exception types that are suitable for the level of abstraction of the called method and mapping lower level exceptions to these types, preferably wrapped using exception chaining in order to preserve the root cause. In addition, it's very possible that in the example above of the changing interface that the calling code would need to be modified as well, since in some sense the exceptions a method may throw are part of the method's implicit interface anyway.
Using a throws Exception declaration or catch (Exception e) is usually sufficient for satisfying the checking in Java. While this may have some use, it essentially circumvents the checked exception mechanism, which Oracle discourages.
Unchecked exception types should generally not be handled, except possibly at the outermost levels of scope. These often represent scenarios that do not allow for recovery: RuntimeExceptions frequently reflect programming defects, and Errors generally represent unrecoverable JVM failures. Even in a language that supports checked exceptions, there are cases where the use of checked exceptions is not appropriate.
The point of exception handling routines is to ensure that the code can handle error conditions. In order to establish that exception handling routines are sufficiently robust, it is necessary to present the code with a wide spectrum of invalid or unexpected inputs, such as can be created via software fault injection and mutation testing (that is also sometimes referred to as fuzz testing). One of the most difficult types of software for which to write exception handling routines is protocol software, since a robust protocol implementation must be prepared to receive input that does not comply with the relevant specification(s).
In order to ensure that meaningful regression analysis can be conducted throughout a software development lifecycle process, any exception handling testing should be highly automated, and the test cases must be generated in a scientific, repeatable fashion. Several commercially available systems exist that perform such testing.
In runtime engine environments such as Java or .NET, there exist tools that attach to the runtime engine and every time that an exception of interest occurs, they record debugging information that existed in memory at the time the exception was thrown (call stack and heap values). These tools are called automated exception handling or error interception tools and provide 'root-cause' information for exceptions.
Somewhat related with the concept of checked exceptions is exception synchronicity. Synchronous exceptions happen at a specific program statement whereas asynchronous exceptions can raise practically anywhere. It follows that asynchronous exception handling can't be required by the compiler. They are also difficult to program with. Examples of naturally asynchronous events include pressing Ctrl-C to interrupt a program, and receiving a signal such as ""stop"" or ""suspend"" from another thread of execution.
Programming languages typically deal with this by limiting asynchronicity, for example Java has deprecated the use of its ThreadDeath exception that was used to allow one thread to stop another one. Instead, there can be semi-asynchronous exceptions that only raise in suitable locations of the program or synchronously.
Common Lisp, Dylan and Smalltalk have a condition system (see Common Lisp Condition System) that encompasses the aforementioned exception handling systems. In those languages or environments the advent of a condition (a ""generalisation of an error"" according to Kent Pitman) implies a function call, and only late in the exception handler the decision to unwind the stack may be taken.
Conditions are a generalization of exceptions. When a condition arises, an appropriate condition handler is searched for  and selected, in stack order, to handle the condition. Conditions that do not represent errors may safely go unhandled entirely; their only purpose may be to propagate hints or warnings toward the user.
This is related to the so-called resumption model of exception handling, in which some exceptions are said to be continuable: it is permitted to return to the expression that signaled an exception, after having taken corrective action in the handler. The condition system is generalized thus: within the handler of a non-serious condition (a.k.a. continuable exception), it is possible to jump to predefined restart points (a.k.a. restarts) that lie between the signaling expression and the condition handler. Restarts are functions closed over some lexical environment, allowing the programmer to repair this environment before exiting the condition handler completely or unwinding the stack even partially.
Condition handling moreover provides a separation of mechanism from policy. Restarts provide various possible mechanisms for recovering from error, but do not select which mechanism is appropriate in a given situation. That is the province of the condition handler, which (since it is located in higher-level code) has access to a broader view.
An example: Suppose there is a library function whose purpose is to parse a single syslog file entry. What should this function do if the entry is malformed? There is no one right answer, because the same library could be deployed in programs for many different purposes. In an interactive log-file browser, the right thing to do might be to return the entry unparsed, so the user can see it—but in an automated log-summarizing program, the right thing to do might be to supply null values for the unreadable fields, but abort with an error, if too many entries have been malformed.
That is to say, the question can only be answered in terms of the broader goals of the program, which are not known to the general-purpose library function. Nonetheless, exiting with an error message is only rarely the right answer. So instead of simply exiting with an error, the function may establish restarts offering various ways to continue—for instance, to skip the log entry, to supply default or null values for the unreadable fields, to ask the user for the missing values, or to unwind the stack and abort processing with an error message. The restarts offered constitute the mechanisms available for recovering from error; the selection of restart by the condition handler supplies the policy."
"122","In computer science, type conversion, type casting, and type coercion are different ways of changing an entity of one data type into another. An example would be the conversion of an integer value into a floating point value or its textual representation as a string, and vice versa. Type conversions can take advantage of certain features of type hierarchies or data representations. Two important aspects of a type conversion is whether it happens implicitly or explicitly, and whether the underlying data representation is converted from one representation into another, or a given representation is merely reinterpreted as the representation of another data type. In general, both primitive and compound data types can be converted.
Each programming language has its own rules on how types can be converted. Languages with strong typing typically do little implicit conversion and discourage the reinterpretation of representations, while languages with weak typing perform many implicit conversions between data types.  Weak typing language often allow forcing the compiler to arbitrarily interpret a data item as having different representations—this can be a non-obvious programming error, or a technical method to directly deal with underlying hardware.
In most languages, the word coercion is used to denote an implicit conversion, either during compilation or during run time. For example, in an expression mixing integer and floating point numbers (like 5 + 0.1), the compiler will automatically convert integer representation into floating point representation so fractions are not lost. Explicit type conversions are either indicated by writing additional code (e.g. adding type identifiers or calling built-in routines) or by coding conversion routines for the compiler to use when it otherwise would halt with a type mismatch.
In most ALGOL-like languages, such as Pascal, Modula-2, Ada (programming language) and Delphi, conversion and casting are distinctly different concepts. In these languages, conversion refers to either implicitly or explicitly changing a value from one data type storage format to another, e.g. a 16-bit integer to a 32-bit integer. The storage needs may change as a result of the conversion, including a possible loss of precision or truncation. The word cast, on the other hand, refers to explicitly changing the interpretation of the bit pattern representing a value from one type to another. For example, 32 contiguous bits may be treated as an array of 32 booleans, a 4-byte string, an unsigned 32-bit integer or an IEEE single precision floating point value. Because the stored bits are never changed, the programmer must know low level details such as representation format, byte order, and alignment needs, to meaningfully cast.
In the C family of languages and ALGOL 68, the word cast typically refers to an explicit type conversion (as opposed to an implicit conversion), causing some ambiguity about whether this is a re-interpretation of a bit-pattern or a real data representation conversion.  More important is the multitude of ways and rules that apply to what data type (or class) is located by a pointer and how a pointer may be adjusted by the compiler in cases like object (class) inheritance.
Implicit type conversion, also known as coercion, is an automatic type conversion by the compiler. Some programming languages allow compilers to provide coercion; others require it.
In a mixed-type expression, data of one or more subtypes can be converted to a supertype as needed at runtime so that the program will run correctly. For example, the following is legal C language code:
Although d, l and i belong to different data types, they will be automatically converted to equal data types each time a comparison or assignment is executed. This behavior should be used with caution, as unintended consequences can arise. Data can be lost when converting representations from floating-point to integer, as the fractional components of the floating-point values will be truncated (rounded toward zero). Conversely, precision can be lost when converting representations from integer to floating-point, since a floating-point type may be unable to exactly represent an integer type. For example, float might be an IEEE 754 single precision type, which cannot represent the integer 16777217 exactly, while a 32-bit integer type can. This can lead to unintuitive behavior, as demonstrated by the following code:
On compilers that implement floats as IEEE single precision, and ints as at least 32 bits, this code will give this peculiar print-out:
Note that 1 represents equality in the last line above. This odd behavior is caused by an implicit conversion of i_value to float when it is compared with f_value. The conversion causes loss of precision, which makes the values equal before the comparison.
Important takeaways:
One special case of implicit type conversion is type promotion, where the compiler automatically expands the binary representation of objects of integer or floating-point types. Promotions are commonly used with types smaller than the native type of the target platform's arithmetic logic unit (ALU), before arithmetic and logical operations, to make such operations possible, or more efficient if the ALU can work with more than one type. C and C++ perform such promotion for objects of boolean, character, wide character, enumeration, and short integer types which are promoted to int, and for objects of type float, which are promoted to double. Unlike some other type conversions, promotions never lose precision or modify the value stored in the object.
In Java:
Explicit type conversion is a type conversion which is explicitly defined within a program (instead of being done by a compiler for implicit type conversion). It is defined by the user in the program.
There are several kinds of explicit conversion.
In object-oriented programming languages, objects can also be downcast : a reference of a base class is cast to one of its derived classes.
In C#, type conversion can be made in a safe or unsafe (i.e., C-like) manner, the former called checked type cast.
In C++ the similar effect can be achieved using C++-style cast syntax.
In Eiffel the notion of type conversion is integrated into the rules of the type system. The Assignment Rule says that an assignment, such as:
is valid if and only if the type of its source expression, y in this case, is compatible with the type of its target entity, x in this case. In this rule, compatible with means that the type of the source expression either conforms to or converts to that of the target. Conformance of types is defined by the familiar rules for polymorphism in object-oriented programming. For example, in the assignment above, the type of y conforms to the type of x if the class upon which y is based is a descendant of that upon which x is based.
The actions of type conversion in Eiffel, specifically converts to and converts from are defined as:
A type based on a class CU converts to a type T based on a class CT (and T converts from U) if either
Eiffel is a fully compliant language for Microsoft .NET Framework. Before development of .NET, Eiffel already had extensive class libraries. Using the .NET type libraries, particularly with commonly used types such as strings, poses a conversion problem. Existing Eiffel software uses the string classes (such as STRING_8) from the Eiffel libraries, but Eiffel software written for .NET must use the .NET string class (System.String) in many cases, for example when calling .NET methods which expect items of the .NET type to be passed as arguments. So, the conversion of these types back and forth needs to be as seamless as possible.
In the code above, two strings are declared, one of each different type (SYSTEM_STRING is the Eiffel compliant alias for System.String). Because System.String does not conform to STRING_8, then the assignment above is valid only if System.String converts to STRING_8.
The Eiffel class STRING_8 has a conversion procedure make_from_cil for objects of type System.String. Conversion procedures are also always designated as creation procedures (similar to constructors). The following is an excerpt from the STRING_8 class:
The presence of the conversion procedure makes the assignment:
semantically equivalent to:
in which my_string is constructed as a new object of type STRING_8 with content equivalent to that of my_system_string.
To handle an assignment with original source and target reversed:
the class STRING_8 also contains a conversion query to_cil which will produce a System.String from an instance of STRING_8.
The assignment:
then, becomes equivalent to:
In Eiffel, the setup for type conversion is included in the class code, but then appears to happen as automatically as explicit type conversion in client code. The includes not just assignments but other types of attachments as well, such as argument (parameter) substitution.
In hacking, typecasting is the misuse of type conversion to temporarily change a variable's data type from how it was originally defined. This provides opportunities for hackers since in type conversion after a variable is ""typecast"" to become a different data type, the compiler will treat that hacked variable as the new data type for that specific operation.
"
"123","A method in object-oriented programming (OOP) is a procedure associated with a message and an object. An object is mostly made up of data and behavior, which form the interface that an object presents to the outside world. Data is represented as properties of the object and behavior as methods. For example, a Window object would have methods such as open and close, while its state (whether it is opened or closed) would be a property.
In class-based programming, methods are defined in a class, and objects are instances of a given class. One of the most important capabilities that a method provides is method overriding. The same name (e.g., area) can be used for multiple different kinds of classes. This allows the sending objects to invoke behaviors and to delegate the implementation of those behaviors to the receiving object. A method in Java programming sets the behavior of a class object. For example, an object can send an area message to another object and the appropriate formula is invoked whether the receiving object is a rectangle, circle, triangle, etc.
Methods also provide the interface that other classes use to access and modify the data properties of an object. This is known as encapsulation. Encapsulation and overriding are the two primary distinguishing features between methods and procedure calls.
Method overriding and overloading are two of the most significant ways that a method differs from a conventional procedure or function call. Overriding refers to a subclass redefining the implementation of a method of its superclass. For example, findArea may be a method defined on a shape class. The various subclasses: rectangle, circle, triangle, etc. would each define the appropriate formula to calculate their area. The idea is to look at objects as ""black boxes"" so that changes to the internals of the object can be made with minimal impact on the other objects that use it. This is known as encapsulation and is meant to make code easier to maintain and re-use.
Method overloading, on the other hand, refers to differentiating the code used to handle a message based on the parameters of the method. If one views the receiving object as the first parameter in any method then overriding is just a special case of overloading where the selection is based only on the first argument. The following simple Java example illustrates the difference:
Accessor methods are used to read data values of an object. Mutator methods are used to modify the data of an object. Manager methods are used to initialize and destroy objects of a class, e.g. constructors and destructors.
These methods provide an abstraction layer that facilitates encapsulation and modularity. For example, if a bank-account class provides a getBalance() accessor method to retrieve the current balance (rather than directly accessing the balance data fields), then later revisions of the same code can implement a more complex mechanism for balance retrieval (e.g., a database fetch), without the dependent code needing to be changed. The concepts of encapsulation and modularity are not unique to object-oriented programming. Indeed, in many ways the object-oriented approach is simply the logical extension of previous paradigms such as abstract data types and structured programming.
A constructor is a method that is called at the beginning of an object's lifetime to create and initialize the object, a process called construction (or instantiation). Initialization may include an acquisition of resources. Constructors may have parameters but usually, do not return values in most languages. See the following example in Java:
A destructor is a method that is called automatically at the end of an object's lifetime, a process called destruction. Destruction in most languages does not allow destructor method arguments nor return values. Destruction can be implemented so as to perform cleanup chores and other tasks at object destruction.
In garbage-collected languages, such as Java, C#, and Python, destructors are known as finalizers. They have a similar purpose and function to destructors, but because of the differences between languages that utilize garbage-collection and languages with manual memory management, the sequence in which they are called is different.
An abstract method is one with only a signature and no implementation body. It is often used to specify that a subclass must provide an implementation of the method. Abstract methods are used to specify interfaces in some computer languages.
The following Java code shows an abstract class that needs to be extended:
The following subclass extends the main class:
Class methods are methods that are called on a class rather than an instance. They are typically used as part of an object meta-model. I.e, for each class, defined an instance of the class object in the meta-model is created.  Meta-model protocols allow classes to be created and deleted. In this sense, they provide the same functionality as constructors and destructors described above. But in some languages such as the Common Lisp Object System (CLOS) the meta-model allows the developer to dynamically alter the object model at run time: e.g., to create new classes, redefine the class hierarchy, modify properties, etc.
Special methods are very language-specific and a language may support none, some, or all of the special methods defined here. A language's compiler may automatically generate default special methods or a programmer may be allowed to optionally define special methods. Most special methods cannot be directly called, but rather the compiler generates code to call them at appropriate times.
Static methods are meant to be relevant to all the instances of a class rather than to any specific instance. They are similar to static variables in that sense. An example would be a static method to sum the values of all the variables of every instance of a class. For example, if there were a Product class it might have a static method to compute the average price of all products.
In Java, a commonly used static method is:
This static method has no owning object and does not run on an instance. It receives all information from its arguments.
A static method can be invoked even if no instances of the class exist yet. Static methods are called ""static"" because they are resolved at compile time based on the class they are called on and not dynamically as in the case with instance methods, which are resolved polymorphically based on the runtime type of the object.
Copy-assignment operators define actions to be performed by the compiler when a class object is assigned to a class object of the same type.
Operator methods define or redefine operator symbols and define the operations to be performed with the symbol and the associated method parameters. C++ Example:
Some procedural languages were extended with object-oriented capabilities to leverage the large skill sets and legacy code for those languages but still provide the benefits of object-oriented development. Perhaps the most well-known example is C++, an object-oriented extension of the C programming language. Due to the design requirements to add the object-oriented paradigm on to an existing procedural language, message passing in C++ has some unique capabilities and terminologies. For example, in C++ a method is known as a member function. C++ also has the concept of virtual functions which are member functions that can be overridden in derived classes and allow for dynamic dispatch.
Virtual functions are the means by which a C++ class can achieve polymorphic behavior. Non-virtual member functions, or regular methods, are those that do not participate in polymorphism.
C++ Example:"
"124","In object-oriented programming (OOP), the object lifetime (or life cycle) of an object is the time between an object's creation and its destruction. Rules for object lifetime vary significantly between languages, in some cases between implementations of a given language, and lifetime of a particular object may vary from one run of the program to another.
In some cases object lifetime coincides with variable lifetime of a variable with that object as value (both for static variables and automatic variables), but in general object lifetime is not tied to the lifetime of any one variable. In many cases – and by default in many object-oriented languages (OOLs), particularly those that use garbage collection (GC) – objects are allocated on the heap, and object lifetime is not determined by the lifetime of a given variable: the value of a variable holding an object actually corresponds to a reference to the object, not the object itself, and destruction of the variable just destroys the reference, not the underlying object.
While the basic idea of object lifetime is simple – an object is created, used, then destroyed – details vary substantially between languages, and within implementations of a given language, and is intimately tied to how memory management is implemented. Further, many fine distinctions are drawn between the steps, and between language-level concepts and implementation-level concepts. Terminology is relatively standard, but which steps correspond to a given term varies significantly between languages.
Terms generally come in antonym pairs, one for a creation concept, one for the corresponding destruction concept, like initialize/finalize or constructor/destructor. The creation/destruction pair is also known as initiation/termination, among other terms. The terms allocation and deallocation or freeing are also used, by analogy with memory management, though object creation and destruction can involve significantly more than simply memory allocation and deallocation, and allocation/deallocation are more properly considered steps in creation and destruction, respectively.
A major distinction is whether an object's lifetime is deterministic or non-deterministic. This varies by language, and within language varies with the memory allocation of an object; object lifetime may be distinct from variable lifetime.
Objects with static memory allocation, notably objects stored in static variables, and classes modules (if classes or modules are themselves objects, and statically allocated), have a subtle non-determinism in many languages: while their lifetime appears to coincide with the run time of the program, the order of creation and destruction - which static object is created first, which second, etc. - is generally nondeterministic.[lower-alpha 1]
For objects with automatic memory allocation or dynamic memory allocation, object creation generally happens deterministically, either explicitly when an object is explicitly created (such as via new in C++ or Java), or implicitly at the start of variable lifetime, particularly when the scope of an automatic variable is entered, such as at declaration.[lower-alpha 2] Object destruction varies, however – in some languages, notably C++, automatic and dynamic objects are destroyed at deterministic times, such as scope exit, explicit destruction (via manual memory management), or reference count reaching zero; while in other languages, such as C#, Java, and Python, these objects are destroyed at non-deterministic times, depending on the garbage collector, and object resurrection may occur during destruction, extending the lifetime.
In garbage-collected languages, objects are generally dynamically allocated (on the heap) even if they are initially bound to an automatic variable, unlike automatic variables with primitive values, which are typically automatically allocated (on the stack or in a register). This allows the object to be returned from a function (""escape"") without being destroyed. However, in some cases a compiler optimization is possible, namely performing escape analysis and proving that escape is not possible, and thus the object can be allocated on the stack; this is significant in Java. In this case object destruction will occur promptly - possibly even during the variable's lifetime (before the end of its scope), if it is unreachable.
A complex case is the use of an object pool, where objects may be created ahead of time or reused, and thus apparent creation and destruction may not correspond to actual creation and destruction of an object, only (re)initialization for creation and finalization for destruction. In this case both creation and destruction may be nondeterministic.
Object creation can be broken down into two operations: memory allocation and initialization, where initialization both includes assigning values to object fields and possibly running arbitrary other code. These are implementation-level concepts, roughly analogous to the distinction between declaration and initialization (or definition) of a variable, though these later are language-level distinctions. For an object that is tied to a variable, declaration may be compiled to memory allocation (reserving space for the object), and definition to initialization (assigning values), but declarations may also be for compiler use only (such as name resolution), not directly corresponding to compiled code.
Analogously, object destruction can be broken down into two operations, in the opposite order: finalization and memory deallocation. These do not have analogous language-level concepts for variables: variable lifetime ends implicitly (for automatic variables, on stack unwind; for static variables, on program termination), and at this time (or later, depending on implementation) memory is deallocated, but no finalization is done in general. However, when an object's lifetime is tied to a variable's lifetime, the end of the variable's lifetime causes finalization of the object; this is a standard paradigm in C++.
Together these yield four implementation-level steps:
These steps may be done automatically by the language runtime, interpreter, or virtual machine, or may be manually specified by the programmer in a subroutine, concretely via methods – the frequency of this varies significantly between steps and languages. Initialization is very commonly programmer-specified in class-based languages, while in strict prototype-based languages initialization is automatically done by copying. Finalization is also very common in languages with deterministic destruction, notably C++, but much less common in garbage-collected languages. Allocation is more rarely specified, and deallocation generally cannot be specified.
An important subtlety is the status of an object during creation or destruction, and handling cases where errors occur or exceptions are raised, such as if creation or destruction fail. Strictly speaking, an object's lifetime begins when allocation completes and ends when deallocation starts. Thus during initialization and finalization an object is alive, but may not be in a consistent state – ensuring class invariants is a key part of initialization – and the period from when initialization completes to when finalization starts is when the object is both alive and expected to be in a consistent state.
If creation or destruction fail, error reporting (often by raising an exception) can be complicated: the object or related objects may be in an inconsistent state, and in the case of destruction – which generally happens implicitly, and thus in an unspecified environment – it may be difficult to handle errors. The opposite issue – incoming exceptions, not outgoing exceptions – is whether creation or destruction should behave differently if they occur during exception handling, when different behavior may be desired.
Another subtlety is when creation and destruction happen for static variables, whose lifespan coincides with the run time of the program – do creation and destruction happen during regular program execution, or in special phases before and after regular execution – and how objects are destroyed at program termination, when the program may not be in a usual or consistent state. This is particularly an issue for garbage-collected languages, as they may have a lot of garbage at program termination.
In class-based programming, object creation is also known as instantiation (creating an instance of a class), and creation and destruction can be controlled via methods known as a constructor and destructor, or an initializer and finalizer. Creation and destruction are thus also known as construction and destruction, and when these methods are called an object is said to be constructed or destructed (not ""destroyed"") – respectively, initialized or finalized when those methods are called.
The relationship between these methods can be complicated, and a language may have both constructors and initializers (like Python), or both destructors and finalizers (like C++/CLI), or the terms ""destructor"" and ""finalizer"" may refer to language-level construct versus implementation (as in C# versus CLI).
A key distinction is that constructors are class methods, as there is no object (class instance) available until the object is created, but the other methods (destructors, initializers, and finalizers) are instance methods, as an object has been created. Further, constructors and initializers may take arguments, while destructors and finalizers generally do not, as they are usually called implicitly.
In common usage, a constructor is a method directly called explicitly by user code to create an object, while ""destructor"" is the subroutine called (usually implicitly, but sometimes explicitly) on object destruction in languages with deterministic object lifetimes – the archetype is C++ – and ""finalizer"" is the subroutine called implicitly by the garbage collector on object destruction in languages with non-deterministic object lifetime – the archetype is Java.
The steps during finalization vary significantly depending on memory management: in manual memory management (as in C++, or manual reference counting), references need to be explicitly destroyed by the programmer (references cleared, reference counts decremented); in automatic reference counting, this also happens during finalization, but is automated (as in Python, when it occurs after programmer-specified finalizers have been called); and in tracing garbage collection this is not necessary. Thus in automatic reference counting, programmer-specified finalizers are often short or absent, but significant work may still be done, while in tracing garbage collectors finalization is often unnecessary.
In languages where objects have deterministic lifetimes, object lifetime can also be used for resource management, notably via the Resource Acquisition Is Initialization (RAII) idiom: resources are acquired during initialization, and released during finalization. In languages where objects have non-deterministic lifetimes, notably due to garbage collection, resources are managed in other ways, notably the dispose pattern: they still may be acquired during initialization, but are released prior to finalization, via an explicit method call – this is needed because finalization may not occur in a timely manner (or even at all), but the resource should be released promptly.
Using object lifetime for resource management ties memory management to resource management, and thus is not generally used in garbage-collected languages, as it would either constrain the garbage collector (requiring immediate finalization) or result in possibly long-lasting resource leaks, due to finalization being deferred.
In  typical case, the process is as follows:
Those tasks can be completed at once but are sometimes left unfinished and the order of the tasks can vary and can cause several strange behaviors. For example, in multi-inheritance, which initializing code should be called first is a difficult question to answer. However, superclass constructors should be called before subclass constructors.
It is a complex problem to create each object as an element of an array.[further explanation needed] Some languages (e.g. C++) leave this to programmers.
Handling exceptions in the midst of creation of an object is particularly problematic because usually the implementation of throwing exceptions relies on valid object states. For instance, there is no way to allocate a new space for an exception object when the allocation of an object failed before that due to a lack of free space on the memory. Due to this, implementations of OO languages should provide mechanisms to allow raising exceptions even when there is short supply of resources, and programmers or the type system should ensure that their code is exception-safe. Propagating an exception is more likely to free resources than to allocate them. But in object oriented programming, object construction may fail, because constructing an object should establish the class invariants, which are often not valid for every combination of constructor arguments. Thus, constructors can raise exceptions.
The abstract factory pattern is a way to decouple a particular implementation of an object from code for the creation of such an object.
The way to create objects varies across languages. In some class-based languages, a special method known as a constructor, is responsible for validating the state of an object. Just like ordinary methods, constructors can be overloaded in order to make it so that an object can be created with different attributes specified. Also, the constructor is the only place to set the state of immutable objects[Wrong clarification needed]. A copy constructor is a constructor which takes a (single) parameter of an existing object of the same type as the constructor's class, and returns a copy of the object sent as a parameter.
Other programming languages, such as Objective-C, have class methods, which can include constructor-type methods, but are not restricted to merely instantiating objects.
C++ and Java have been criticized[by whom?] for not providing named constructors—a constructor must always have the same name as the class. This can be problematic if the programmer wants to provide two constructors with the same argument types, e.g., to create a point object either from the cartesian coordinates or from the polar coordinates, both of which would be represented by two floating point numbers. Objective-C can circumvent this problem, in that the programmer can create a Point class, with initialization methods, for example, +newPointWithX:andY:, and +newPointWithR:andTheta:. In C++, something similar can be done using static member functions.
A constructor can also refer to a function which is used to create a value of a tagged union, particularly in functional languages.
It is generally the case that after an object is used, it is removed from memory to make room for other programs or objects to take that object's place. However, if there is sufficient memory or a program has a short run time, object destruction may not occur, memory simply being deallocated at process termination. In some cases object destruction simply consists of deallocating the memory, particularly in garbage-collected languages, or if the ""object"" is actually a plain old data structure. In other cases some work is performed prior to deallocation, particularly destroying member objects (in manual memory management), or deleting references from the object to other objects to decrement reference counts (in reference counting). This may be automatic, or a special destruction method may be called on the object.
In class-based OOLs with deterministic object lifetime, notably C++, a destructor is a method called when an instance of a class is deleted, before the memory is deallocated. In C++, destructors differs from constructors in various ways: they cannot be overloaded, must have no arguments, need not maintain class invariants, and can cause program termination if they throw exceptions.
In garbage collecting languages, objects may be destroyed when they can no longer be reached by the running code. In class-based GCed languages, the analog of destructors are finalizers, which are called before an object is garbage-collected. These differ in running at an unpredictable time and in an unpredictable order, since garbage collection is unpredictable, and are significantly less-used and less complex than C++ destructors. Example of such languages include Java, Python, and Ruby.
Destroying an object will cause any references to the object to become invalid, and in manual memory management any existing references become dangling references. In garbage collection (both tracing garbage collection and reference counting), objects are only destroyed when there are no references to them, but finalization may create new references to the object, and to prevent dangling references, object resurrection occurs so the references remain valid.
Related Languages: ""Delphi"", ""Free Pascal"", ""Mac Pascal"".
Socket will be closed at the next garbage collection round, as all references to it have been lost."
"125"," (Learn how and when to remove this template message)
In some programming languages, function overloading or method overloading is the ability to create multiple methods of the same name with different implementations. Calls to an overloaded function will run a specific implementation of that function appropriate to the context of the call, allowing one function call to perform different tasks depending on context.
For example, doTask() and doTask(object O) are overloaded methods. To call the latter, an object must be passed as a parameter, whereas the former does not require a parameter, and is called with an empty parameter field. A common error would be to assign a default value to the object in the second method, which would result in an ambiguous call error, as the compiler wouldn't know which of the two methods to use.
Another appropriate example would be a Print(object O) method. In this case one might like the method to be different when printing, for example, text or pictures. The two different methods may be overloaded as Print(text_object T); Print(image_object P). If we write the overloaded print methods for all objects our program will ""print"", we never have to worry about the type of the object, and the correct function call again, the call is always: Print(something).
It is a classification of static polymorphism in which a function call is resolved using the 'best match technique', i.e. the function is resolved depending upon the argument list. Method overloading is usually associated with statically-typed programming languages that enforce type checking in function calls. When overloading a method, you are really just making a number of different methods that happen to have the same name. The determination of which of these methods are used is resolved at compile time.
Function overloading is also known as compile-time polymorphism in Java and is also known as static polymorphism in Java.
Method overloading should not be confused with forms of polymorphism where the correct method is chosen at runtime, e.g. through virtual functions, instead of statically.
Example: function overloading in C++
In the above example, the volume of various components are calculated using the same function call ""volume"", with arguments differing in their data type or their number.
Constructors, used to create instances of an object, may also be overloaded in some object-oriented programming languages. Because in many languages the constructor’s name is predetermined by the name of the class, it would seem that there can be only one constructor. Whenever multiple constructors are needed, they are to be implemented as overloaded functions. In C++, default constructors take no parameters, instantiating the object members with their appropriate default values. For example, a default constructor for a restaurant bill object written in C++ might set the tip to 15%:
The drawback to this is that it takes two steps to change the value of the created Bill object. The following shows creation and changing the values within the main program:
By overloading the constructor, one could pass the tip and total as parameters at creation. This shows the overloaded constructor with two parameters. This overloaded constructor is placed in the class as well as the original constructor we used before. Which one gets used depends on the number of arguments provided when the new Bill object is created (none, or two):
Now a function that creates a new Bill object could pass two values into the constructor and set the data members in one step. The following shows creation and setting the values:
This can be useful in increasing program efficiency and reducing code length.
Another reason for constructor overloading can be to enforce mandatory data members. In this case the default constructor is declared private or protected (or preferably deleted since C++11) to make it inaccessible from outside. For the Bill above total might be the only constructor parameter - since a Bill has no sensible default for total - whereas tip defaults to 0.15.
Two issues interact with and complicate function overloading: name masking (due to scope) and implicit type conversion.
If a function is declared in one scope, and then another function with the same name is declared in an inner scope, there are two natural possible overloading behaviors: the inner declaration masks the outer declaration (regardless of signature), or both the inner declaration and the outer declaration are both included in the overload, with the inner declaration masking the outer declaration only if the signature matches. The first is taken in C++: ""in C++, there is no overloading across scopes."" As a result, to obtain an overload set with functions declared in different scopes, one needs to explicitly import the functions from the outer scope into the inner scope, using the using keyword
Implicit type conversion complicates function overloading because if the types of arguments do not exactly match the signature of one of the overloaded functions, but can match after type conversion, resolution depends on which type conversion is chosen.
These can combine in confusing ways: an inexact match declared in an inner scope can mask an exact match declared in an outer scope, for instance.
For example, to have a derived class with an overloaded function taking a double or an int, using the function taking an int from the base class, in C++, one would write
Failing to include the using results in an int argument passed to f in the derived class being converted to a double and matching the function in the derived class, rather than in the base class; including using results in an overload in the derived class and thus matching the function in the base class.
If a method is designed with an excessive number of overloads, it may be difficult for developers to discern which overload is being called simply by reading the code. This is particularly true if some of the overloaded parameters are of types that are inherited types of other possible parameters (for example ""object""). An IDE can perform the overload resolution and display (or navigate to) the correct overload.
Type based overloading can also hamper code maintenance, where code updates can accidentally change which method overload is chosen by the compiler."
"126","

 (Learn how and when to remove this template message)
This article compares two programming languages: C# with Java. While the focus of this article is mainly the languages and their features, such a comparison will necessarily also consider some features of platforms and libraries. For a more detailed comparison of the platforms, please see Comparison of the Java and .NET platforms.
C# and Java are similar languages that are typed statically, strongly, and manifestly. Both are object-oriented, and designed with semi-interpretation or runtime just-in-time compilation, and both are curly brace languages, like C and C++.
Both languages are statically typed with class-based object orientation. In Java the primitive types are special in that they are not object-oriented and they could not have been defined using the language itself. They also do not share a common ancestor with reference types. The Java reference types all derive from a common root type. C# has a unified type system in which all types (besides unsafe pointers) ultimately derive from a common root type. Consequently, all types implement the methods of this root type, and extension methods defined for the object type apply to all types, even primitive int literals and delegates. Note, that unlike Java, this allows C# to support objects with encapsulation that are not reference types.
In Java, compound types are synonymous with reference types; methods cannot be defined for a type unless it is also a class reference type. In C# the concepts of encapsulation and methods have been decoupled from the reference requirement so that a type can support methods and encapsulation without being a reference type. Only reference types support virtual methods and specialization, however.
Both languages support many built-in types that are copied and passed by value rather than by reference. Java calls these types primitive types, while they are called simple types in C#. The primitive/simple types typically have native support from the underlying processor architecture.
The C# primitive/simple types implement several interfaces and consequently offer many methods directly on instances of the types, even on the literals. The C# type names are also merely aliases for Common Language Runtime (CLR) types. The C# System.Int64 type is exactly the same type as the long type; the only difference is that the former is the canonical .NET name, while the latter is a C# alias for it.
Java does not offer methods directly on primitive types. Instead, methods that operate on primitive values are offered through companion primitive wrapper classes. A fixed set of such wrapper classes exist, each of which wraps one of the fixed set of primitive types. As an example, the Java Long type is a reference type that wraps the primitive long type. They are not the same type, however.
Both Java and C# support signed integers with bit widths of 8, 16, 32 and 64 bits. They use the same name/aliases for the types, except for the 8-bit integer that is called a byte in Java and a sbyte (signed byte) in C#.
C# supports unsigned in addition to the signed integer types. The unsigned types are byte, ushort, uint and ulong for 8, 16, 32 and 64 bit widths, respectively. Unsigned arithmetic operating on the types are supported as well. For example, adding two unsigned integers (uints) still yields a uint as a result; not a long or signed integer.
Java does not feature unsigned integer types. In particular, Java lacks a primitive type for an unsigned byte. Instead, Java's byte type is sign extended, which is a common source of bugs and confusion.
Unsigned integers were left out of Java deliberately because James Gosling believed that programmers would not understand how unsigned arithmetic works.
C# has a type and literal notation for high-precision (28 decimal digits) decimal arithmetic that is appropriate for financial and monetary calculations. Contrary to the float and double data types, decimal fractional numbers such as 0.1 can be represented exactly in the decimal representation. In the float and double representations, such numbers often have non-terminating binary expansions, making those representations more prone to round-off errors.
While Java lacks such a built-in type, the Java library does feature an arbitrary precision decimal type. This is not considered a language type and it does not support the usual arithmetic operators; rather it is a reference type that must be manipulated using the type methods. See more about arbitrary-size/precision numbers below.
Both languages offer library-defined arbitrary-precision arithmetic types for arbitrary-size integers and decimal point calculations.
Only Java has a data type for arbitrary precision decimal point calculations. Only C# has a type for working with complex numbers.
In both languages, the number of operations that can be performed on the advanced numeric types are limited compared to the built-in IEEE 754 floating point types. For instance, none of the arbitrary-size types support square root or logarithms.
C# allows library-defined types to be integrated with existing types and operators by using custom implicit/explicit conversions and operator overloading. See example in section Integration of library-defined types
Both languages feature a native char (character) datatype as a simple type. Although the char type can be used with bit-wise operators, this is performed by promoting the char value to an integer value before the operation. Thus, the result of a bitwise operation is a numeric type, not a character, in both languages.
Both languages treat strings as (immutable) objects of reference type. In both languages, the type contains several methods to manipulate strings, parse, format, etc. In both languages regular expressions are considered an external feature and are implemented in separate classes.
Both languages' libraries define classes for working with dates and calendars in different cultures. The Java java.util.Date is a mutable reference type, where the C# System.DateTime is a struct value type. C# additionally defines a TimeSpan type for working with time periods. Both languages support date and time arithmetic according to different cultures.
C# allows the programmer to create user-defined value types, using the struct keyword. Unlike classes and like the standard primitives, such value types are passed and assigned by value rather than by reference. They can also be part of an object (either as a field or boxed), or stored in an array without the memory indirection that normally exists for class types.
Because value types have no notion of a null value and can be used in arrays without initialization, they always come with an implicit default constructor that essentially fills the struct memory space with zeroes. The programmer can only define additional constructors with one or more arguments. Value types do not have virtual method tables, and because of that (and the fixed memory footprint), they are implicitly sealed. However, value types can (and frequently do) implement interfaces. For example, the built-in integer types implement several interfaces.
Apart from the built-in primitive types, Java does not include the concept of value types.
Both languages define enumerations, but they are implemented in fundamentally different ways. As such, enumerations are one area where tools designed to automatically translate code between the two languages (such as Java to C# converters) fail.
C# has implemented enumerations in a manner similar to C, that is as wrappers around the bit-flags implemented in primitive integral types (int, byte, short, etc.). This has performance benefits and improves interaction with C/C++ compiled code, but provides fewer features and can lead to bugs if low-level value types are directly cast to an enumeration type, as is allowed in the C# language. Therefore, it is seen as syntactic sugar. In contrast, Java implements enumerations as full featured collection of instances, requiring more memory and not aiding interaction with C/C++ code, but providing additional features in reflection and intrinsic behavior. The implementation in each language is described in the table below.
In both C# and Java, programmers can use enumerations in a switch statement without conversion to a string or primitive integer type. However, C# disallows fall-throughs unless the case statement does not contain any code, as they are a main cause for hard-to-find bugs. Fall-throughs must be explicitly declared using goto case
C# implements object-oriented method pointers in the form of delegates. A delegate is a special type that can capture a type-safe reference to a method. This reference can then be stored in a delegate-type variable or passed to a method through a delegate parameter for later invocation. C# delegates support covariance and contravariance, and can hold a reference to any signature-compatible static method, instance method, anonymous method or lambda expression.
Delegates should not be confused with closures and inline functions. The concepts are related because a reference to a closure/inline function must be captured in a delegate reference to be useful at all. But a delegate does not always reference an inline function; it can also reference existing static or instance methods. Delegates form the basis of C# events, but should not be confused with those either.
Delegates were deliberately left out of Java because they were considered unnecessary and detrimental to the language, and because of potential performance issues. Instead, alternative mechanisms are used. The wrapper pattern, which resembles the delegates of C# in that it allows the client to access one or more client-defined methods through a known interface, is one such mechanism.[citation needed] Another is the use of adapter objects using inner classes, which the designers of Java argued are a better solution than bound method references.
See also example #C# delegates and equivalent Java constructs.
C# allows value/primitive/simple types to be ""lifted"" to allow the special null value in addition to the type's native values. A type is lifted by adding a ? suffix to the type name, this is equivalent to using the Nullable<T> generic type, where T is the type to be lifted. Conversions are implicitly defined to convert between values of the base and the lifted type. The lifted type can be compared against null or it can be tested for HasValue. Also, lifted operators are implicitly and automatically defined based on their non-lifted base, where — with the exception of some boolean operators — a null argument will propagate to the result.
Java does not support type lifting as a concept, but all of the built-in primitive types have corresponding wrapper types, which do support the null value by virtue of being reference types (classes).
According to the Java spec, any attempt to dereference the null reference must result in an exception being thrown at run-time, specifically a NullPointerException. (It would not make sense to dereference it otherwise, because, by definition, it points to no object in memory.) This also applies when attempting to unbox a variable of a wrapper type, which evaluates to null: the program will throw an exception, because there is no object to be unboxed - and thus no boxed value to take part in the subsequent computation.
The following example illustrates the different behavior. In C#, the lifted*operator propagates the null value of the operand; in Java, unboxing the null reference throws an exception.
Not all C# lifted operators have been defined to propagate null unconditionally, if one of the operands is null. Specifically, the boolean operators have been lifted to support ternary logic thus keeping impedance with SQL.
The Java boolean operators do not support ternary logic, nor is it implemented in the base class library.
C# features a late bound dynamic type that supports no-reflection dynamic invocation, interoperability with dynamic languages, and ad-hoc binding to (for example) document object models. The dynamic type resolves member access dynamically at runtime as opposed to statically/virtual at compile time. The member lookup mechanism is extensible with traditional reflection as a fall-back mechanism.
There are several use cases for the dynamic type in C#:
Java does not support a late-bound type. The use cases for C# dynamic type have different corresponding constructs in Java:
See also example #Interoperability with dynamic languages.
Java precludes pointers and pointer-arithmetic within the Java runtime environment. The Java language designers reasoned that pointers are one of the main features that enable programmers to put bugs in their code and chose not to support them. Java does not allow for directly passing and receiving objects/structures to/from the underlying operating system and thus does not need to model objects/structures to such a specific memory layout, layouts that frequently would involve pointers. Java's communication with the underlying operating system is instead based upon Java Native Interface (JNI) where communication with/adaptation to an underlying operating system is handled through an external glue layer.
While C# does allow use of pointers and corresponding pointer arithmetic, the C# language designers had the same concerns that pointers could potentially be used to bypass the strict rules for object access. Thus, C# by default also precludes pointers. However, because pointers are needed when calling many native functions, pointers are allowed in an explicit unsafe mode. Code blocks or methods that use the pointers must be marked with the unsafe keyword to be able to use pointers, and the compiler requires the /unsafe switch to allow compiling such code. Assemblies that are compiled using the /unsafe switch are marked as such and may only execute if explicitly trusted. This allows using pointers and pointer arithmetic to directly pass and receive objects to/from the operating system or other native APIs using the native memory layout for those objects while also isolating such potentially unsafe code in specifically trusted assemblies.
In both languages references are a central concept. All instances of classes are by reference.
While not directly evident in the language syntax per se, both languages support the concept of weak references. An instance that is only referenced by weak references is eligible for garbage collection just as if there were no references at all. In both languages this feature is exposed through the associated libraries, even though it is really a core runtime feature.
Along with weak references, Java has soft references. They are much like weak references, but the JVM will not deallocate softly-referenced objects until the memory is needed.
Arrays and collections are concepts featured by both languages.
The syntax used to declare and access arrays is identical, except that C# has added syntax for declaring and manipulating multidimensional arrays.
Multidimensional arrays can in some cases increase performance because of increased locality (as there is one pointer dereference instead of one for every dimension of the array, as it is the case for jagged arrays). However, since all array element access in a multidimensional array requires multiplication/shift between the two or more dimensions, this is an advantage only in very random access scenarios.
Another difference is that the entire multidimensional array can be allocated with a single application of operator new, while jagged arrays require loops and allocations for every dimension. Note, though, that Java provides a syntactic construct for allocating a jagged array with regular lengths; the loops and multiple allocations are then performed by the virtual machine and need not be explicit at the source level.
Both languages feature an extensive set of collection types that includes various ordered and unordered types of lists, maps/dictionaries, sets, etc.
Java also supports the syntax of C/C++:
Both languages allow automatic boxing and unboxing, i.e. they allow for implicit casting between any primitive types and the corresponding reference types. 
In C#, the primitive types are subtypes of the Object type. In Java this is not true; any given primitive type and the corresponding wrapper type have no specific relationship with each other, except for autoboxing and unboxing, which act as syntactic sugar for interchanging between them. This was done intentionally, to maintain backward compatibility with prior versions of Java, in which no automatic casting was allowed, and the programmer worked with two separate sets of types: the primitive types, and the wrapper (reference) type hierarchy.
This difference has the following consequences. First of all, in C#, primitive types can define methods, such as an override of Object's ToString() method. In Java, this task is accomplished by the primitive wrapper classes. 
Secondly, in Java an extra cast is needed whenever one tries to directly dereference a primitive value, as it will not be boxed automatically. The expression ((Integer)42).toString() will convert an integer literal to string in Java while 42.ToString() performs the same operation in C#. This is because the latter one is an instance call on the primitive value 42, while the former one is an instance call on an object of type java.lang.Integer.
Finally, another difference is that Java makes heavy use of boxed types in generics (see below).
Both languages are considered ""curly brace"" languages in the C/C++ family. Overall the syntaxes of the languages are very similar. The syntax at the statement and expression level is almost identical with obvious inspiration from the C/C++ tradition. At type definition level (classes and interfaces) some minor differences exist. Java is explicit about extending classes and implementing interfaces, while C# infers this from the kind of types a new class/interface derives from.
C# supports more features than Java, which to some extent is also evident in the syntax that specifies more keywords and more grammar rules than Java.
As the languages evolved, the language designers for both languages have faced situations where they wanted to extend the languages with new keywords or syntax. New keywords in particular may break existing code at source level, i.e. older code may no longer compile, if presented to a compiler for a later version of the language. Language designers are keen to avoid such regressions. The designers of the two languages have been following different paths when addressing this problem.
Java language designers have avoided new keywords as much as possible, preferring instead to introduce new syntactic constructs that were not legal before or to reuse existing keywords in new contexts. This way they didn't jeopardize backward compatibility. An example of the former can be found in how the for loop was extended to accept iterable types. An example of the latter can be found in how the extends and (especially) the super keywords were reused for specifying type bounds when generics were introduced in Java 1.5. At one time (Java 1.4) a new keyword assert was introduced that was not reserved as a keyword before. This had the potential to render previously valid code invalid, if for instance the code used assert as an identifier. The designers chose to address this problem with a four-step solution: 1) Introducing a compiler switch that indicates if Java 1.4 or later should be used, 2) Only marking assert as a keyword when compiling as Java 1.4 and later, 3) Defaulting to 1.3 to avoid rendering previous (non 1.4 aware code) invalid and 4) Issue warnings, if the keyword is used in Java 1.3 mode, in order to allow the developers to change the code.
C# language designers have introduced several new keywords since the first version. However, instead of defining these keywords as global keywords, they define them as context sensitive keywords. This means that even when they introduced (among others) the partial and yield keywords in C# 2.0, the use of those words as identifiers is still valid as there is no clash possible between the use as keyword and the use as identifier, given the context. Thus, the present C# syntax is fully backward compatible with source code written for any previous version without specifying the language version to be used.
In Java SE 7 a similar construct has been added called try-with-resources:
Both C# and Java are designed from the ground up as object-oriented languages using dynamic dispatch, with syntax similar to C++ (C++ in turn derives from C). Neither language is a superset of C or C++, however.
C# allows a class definition to be split across several source files using a feature called partial classes. Each part must be marked with the keyword partial. All the parts must be presented to the compiler as part of a single compilation. Parts can reference members from other parts. Parts can implement interfaces and one part can define a base class. The feature is useful in code generation scenarios (such as user interface (UI) design), where a code generator can supply one part and the developer another part to be compiled together. The developer can thus edit their part without the risk of a code generator overwriting that code at some later time. Unlike the class extension mechanism, a partial class allows circular dependencies among its parts as they are guaranteed to be resolved at compile time. Java has no corresponding concept.
Both languages allow inner classes, where a class is defined lexically inside another class. However, in each language these inner classes have rather different semantics.
In Java, unless the inner class is declared static, a reference to an instance of an inner class carries a reference to the outer class with it. As a result, code in the inner class has access to both the static and non-static members of the outer class. To create an instance of a non-static inner class, the instance of the embracing outer class must be named. This is done via a new new-operator introduced in JDK 1.3: outerClassInstance.new Outer.InnerClass(). This can be done in any class that has a reference to an instance of the outer class.
In C#, an inner class is conceptually the same as a normal class. In a sense, the outer class only acts as a namespace. Thus, code in the inner class cannot access non-static members of the outer class unless it does so through an explicit reference to an instance of the outer class. Programmers can declare the inner class private to allow only the outer class to have any access to it.
Java provides another feature called local classes or anonymous classes, which can be defined within a method body. These are generally used to implement an interface with only one or two methods, which are typically event handlers. However, they can also be used to override virtual methods of a superclass. The methods in those local classes have access to the outer method's local variables declared final. C# satisfies the use-cases for these by providing anonymous delegates; see event handling for more about this.
C# also provides a feature called anonymous types/classes, but it is rather different from Java's concept with the same name. It allows the programmer to instantiate a class by providing only a set of names for the properties the class should have, and an expression to initialize each. The types of the properties are inferred from the types of those expressions. These implicitly-declared classes are derived directly from object.
C# multicast-delegates are used with events. Events provide support for event-driven programming and is an implementation of the observer pattern. To support this there is a specific syntax to define events in classes, and operators to register, unregister or combine event handlers.
See here for information about how events are implemented in Java.
Operator overloading and user-defined casts are separate features that both aim to allow new types to become first-class citizens in the type system. By using these features in C#, types such as Complex and decimal have been integrated so that the usual operators like addition and multiplication work with the new types. Unlike C++, C# does restrict the use of operator overloading, prohibiting it for the operators new, ( ), ||, &&, =, and any variations of compound statements like +=. But compound operators will call overloaded simple operators, like -= calling - and =.
Java does not include operator overloading, nor custom conversions in order to prevent abuse of the feature and to keep the language simple.
C# also includes indexers that can be considered a special case of operator overloading (like the C++ operator[]), or parameterized get/set properties. An indexer is a property named this[] that uses one or more parameters (indexes); the indices can be objects of any type:
Java does not include indexers. The common Java pattern involves writing explicit getters and setters where a C# programmer would use an indexer.
In both C# and Java, an object's fields can be initialized either by variable initializers (expressions that can be assigned to variables where they are defined) or by constructors (special subroutines that are executed when an object is being created). In addition, Java contains instance initializers, which are anonymous blocks of code with no arguments that are run after the explicit (or implicit) call to a superclass's constructor but before the constructor is executed.
C# initializes object fields in the following order when creating an object:
Some of the above fields may not be applicable (e.g. if an object does not have static fields). Derived fields are those that are defined in the object's direct class, while base field is a term for the fields that are defined in one of the object's superclasses. Note that an object representation in memory contains all fields defined in its class or any of its superclasses, even, if some fields in superclasses are defined as private.
It is guaranteed that any field initializers take effect before any constructors are called, since both the instance constructor of the object's class and its superclasses are called after field initializers are called. There is, however, a potential trap in object initialization when a virtual method is called from a base constructor. The overridden method in a subclass may reference a field that is defined in the subclass, but this field may not have been initialized because the constructor of the subclass that contains field initialization is called after the constructor of its base class.
In Java, the order of initialization is as follows:
Like in C#, a new object is created by calling a specific constructor. Within a constructor, the first statement may be an invocation of another constructor. If this is omitted, the call to the argumentless constructor of the superclass is added implicitly by the compiler. Otherwise, either another overloaded constructor of the object's class can be called explicitly, or a superclass constructor can be called. In the former case, the called constructor will again call another constructor (either of the object's class or its subclass) and the chain sooner or later ends up at the call to one of the constructors of the superclass.
After another constructor is called (that causes direct invocation of the superclass constructor, and so forth, down to the Object class), instance variables defined in the object's class are initialized. Even if there are no variable initializers explicitly defined for some variables, these variables are initialized to default values. Note that instance variables defined in superclasses are already initialized by this point, because they were initialized by a superclass constructor when it was called (either by the constructor's code or by variable initializers performed before the constructor's code or implicitly to default values). In Java, variable initializers are executed according to their textual order in the source file.
Finally, the constructor body is executed. This ensures proper order of initialization, i.e. the fields of a base class finish initialization before initialization of the fields of an object class begins.
There are two main potential traps in Java's object initialization. First, variable initializers are expressions that can contain method calls. Since methods can reference any variable defined in the class, the method called in a variable initializer can reference a variable that is defined below the variable being initialized. Since initialization order corresponds to textual order of variable definitions, such a variable would not be initialized to the value prescribed by its initializer and would contain the default value.
Another potential trap is when a method that is overridden in the derived class is called in the base class constructor, which can lead to behavior the programmer would not expect when an object of the derived class is created. According to the initialization order, the body of the base class constructor is executed before variable initializers are evaluated and before the body of the derived class constructor is executed. The overridden method called from the base class constructor can, however, reference variables defined in the derived class, but these are not yet initialized to the values specified by their initializers or set in the derived class constructor. The latter issue applies to C# as well, but in a less critical form since in C# methods are not overridable by default.
Both languages mainly use garbage collection as a means of reclaiming memory resources, rather than explicit deallocation of memory. In both cases, if an object holds resources of different kinds other than memory, such as file handles, graphical resources, etc., then it must be notified explicitly when the application no longer uses it. Both C# and Java offer interfaces for such deterministic disposal and both C# and Java (since Java 7) feature automatic resource management statements that will automatically invoke the disposal/close methods on those interfaces.
Using a special this designator on the first parameter of a method, C# allows the method to act as if it were a member method of the type of the first parameter. This extension of the foreign class is purely syntactical. The extension method must be declared static and defined within a purely static class. The method must obey any member access restriction like any other method external to the class; thus static methods cannot break object encapsulation. The ""extension"" is only active within scopes where the namespace of the static host class has been imported.
Since Java 8, Java has a similar feature called default methods, which are methods with a body declared on interfaces. As opposed to C# extension methods, Java default methods are instance methods on the interface that declare them. Definition of default methods in classes that implement the interface is optional: If the class does not define the method, the default definition is used instead.
Both the C# extension methods and the Java default methods allow a class to override the default implementation of the extension/default method, respectively. In both languages this override is achieved by defining a method on the class that should use an alternate implementation of the method.
C# scope rules defines that if a matching method is found on a class, it takes precedence over a matching extension method. In Java any class declared to implement an interface with default method is assumed to have the default methods implementions, unless the class implements the method itself.
Related to partial classes C# allows partial methods to be specified within partial classes. A partial method is an intentional declaration of a method with several restrictions on the signature. The restrictions ensure that if a definition is not provided by any class part, then the method and every call to it can be safely erased. This feature allows code to provide a large number of interception points (like the template method GoF design pattern) without paying any runtime overhead if these extension points are not being used by another class part at compile time. Java has no corresponding concept.
Methods in C# are non-virtual by default, and must be declared virtual explicitly, if desired. In Java, all non-static non-private methods are virtual. Virtuality guarantees that the most recent override for the method will always be called, but incurs a certain runtime cost on invocation as these invocations cannot be normally inlined, and require an indirect call via the virtual method table. However, some JVM implementations, including the Oracle reference implementation, implement inlining of the most commonly called virtual methods.
Java methods are virtual by default (although they can be sealed by using the final modifier to disallow overriding). There is no way to let derived classes define a new, unrelated method with the same name.
This means that by default in Java, and only when explicitly enabled in C#, new methods may be defined in a derived class with the same name and signature as those in its base class. When the method is called on a superclass reference of such an object, the ""deepest"" overridden implementation of the base class' method will be called according to the specific subclass of the object being referenced.
In some cases, when a subclass introduces a method with the same name and signature as a method already present in the base class, problems can occur. In Java, this will mean that the method in the derived class will implicitly override the method in the base class, even though that may not be the intent of the designers of either class.
To mitigate this, C# requires that if a method is intended to override an inherited method, the override keyword must be specified. Otherwise, the method will ""hide"" the inherited method. If the keyword is absent, compiler warning to this effect is issued, which can be silenced by specifying the new keyword. This avoids the problem that can arise from a base class being extended with a non-private method (i.e. an inherited part of the namespace) whose signature is already in use by a derived class. Java has a similar compiler check in the form of the @Override method annotation, but it is not compulsory, and in its absence, most compilers will not provide comment (but the method will be overridden).
In Java, it is possible to prevent reassignment of a local variable or method parameter by using the final
 keyword. Applying this keyword to a primitive type variable causes the variable to become immutable. However, applying final
 to a reference type variable only prevents that another object is assigned to it. It will not prevent the data contained by the object from being mutated. There is no C# equivalent.
Both languages do not support essential feature of const-correctness that exists in C/C++, which makes a method constant.
Interestingly, Java defines the word ""constant"" arbitrarily as a static final
 field. Only these variables are capital-only variables, where the names are separated with an underscore[citation needed]. A parameter that is only final
 is not considered as a constant, although it may be so in the case of a primitive data type or an immutable class, like a String
.
Any C# method declared as returning IEnumerable, IEnumerator or the generic versions of these interfaces can be implemented using yield syntax. This is a form of limited, compiler-generated continuations and can drastically reduce the code needed to traverse or generate sequences, although that code is just generated by the compiler instead. The feature can also be used to implement infinite sequences, e.g., the sequence of Fibonacci numbers.
Java does not have an equivalent feature. Instead generators are typically defined by providing a specialized implementation of a well-known collection or iterable interface, which will compute each element on demand. For such a generator to be used in a for each statement, it must implement interface java.lang.Iterable.
See also example Fibonacci sequence below.
C# also has explicit interface implementation that allows a class to specifically implement methods of an interface, separate to its own class methods, or to provide different implementations for two methods with the same name and signature inherited from two base interfaces.
In either language, if a method (or property in C#) is specified with the same name and signature in multiple interfaces, the members will clash when a class is designed that implements those interfaces. An implementation will by default implement a common method for all of the interfaces. If separate implementations are needed (because the methods serve separate purposes, or because return values differ between the interfaces) C#'s explicit interface implementation will solve the problem, though allowing different results for the same method, depending on the current cast of the object. In Java there is no way to solve this problem other than refactoring one or more of the interfaces to avoid name clashes.
The arguments of primitive types (e.g. int, double) to a method are passed by value in Java whereas objects are passed by reference. This means that a method operates on copies of the primitives passed to it instead of on the actual variables. On the contrary, the actual objects in some cases can be changed. In the following example object String is not changed. Object of class 'a' is changed.
In C#, it is possible to enforce a reference with the ref keyword, similar to C++ and in a sense to C. This feature of C# is particularly useful when one wants to create a method that returns more than one object. In Java trying to return multiple values from a method is unsupported, unless a wrapper is used, in this case named ""Ref"".
Java supports checked exceptions (along with unchecked exceptions). C# only supports unchecked exceptions. Checked exceptions force the programmer to either declare the exception thrown in a method, or to catch the thrown exception using a try-catch clause.
Checked exceptions can encourage good programming practice, ensuring that all errors are dealt with. However Anders Hejlsberg, chief C# language architect, argues that they were to some extent an experiment in Java and that they have not been shown to be worthwhile except in small example programs.
One criticism is that checked exceptions encourage programmers to use an empty catch block (catch (Exception e) {}), which silently swallows exceptions, rather than letting the exceptions propagate to a higher-level exception-handling routine. In some cases, however, exception chaining can be applied instead, by re-throwing the exception in a wrapper exception. For example, if an object is changed to access a database instead of a file, an SQLException could be caught and re-thrown as an IOException, since the caller may not need to know the inner workings of the object.
However, not all programmers agree with this stance. James Gosling and others maintain that checked exceptions are useful, and misusing them has caused the problems. Silently catching exceptions is possible, yes, but it must be stated explicitly what to do with the exception, versus unchecked exceptions that allow doing nothing by default. It can be ignored, but code must be written explicitly to ignore it.
There are also differences between the two languages in treating the try-finally statement. The finally block is always executed, even if the try block contains control-passing statements like throw or return. In Java, this may result in unexpected behavior, if the try block is left by a return statement with some value, and then the finally block that is executed afterward is also left by a return statement with a different value. C# resolves this problem by prohibiting any control-passing statements like return or break in the finally block.
A common reason for using try-finally blocks is to guard resource managing code, thus guaranteeing the release of precious resources in the finally block. C# features the using statement as a syntactic shorthand for this common scenario, in which the Dispose() method of the object of the using is always called.
A rather subtle difference is the moment a stack trace is created when an exception is being thrown. In Java, the stack trace is created in the moment the exception is created.
The exception in the statement above will always contain the constructor's stack-trace - no matter how often foo is called.
In C# on the other hand, the stack-trace is created the moment ""throw"" is executed.
In the code above, the exception will contain the stack-trace of the first throw-line. When catching an exception, there are two options in case the exception should be rethrown: throw will just rethrow the original exception with the original stack, while throw e would have created a new stack trace.
Java allows flow of control to leave the finally block of a try statement, regardless of the way it was entered. This can cause another control flow statement (such as return) to be terminated mid-execution. For example:
In the above code, the return statement within try block causes control to leave it, and thus finally block is executed before the actual return happens. However, the finally block itself also performs a return. Thus, the original return that caused it to be entered is not executed, and the above method returns 1 rather than 0. Informally speaking, it tries to return 0 but finally returns 1.
C# does not allow any statements that allow control flow to leave the finally block prematurely, except for throw. In particular, return is not allowed at all, goto is not allowed if the target label is outside the finally block, and continue and break are not allowed if the nearest enclosing loop is outside the finally block.
In the field of generics the two languages show a superficial syntactical similarity, but they have deep underlying differences.
Generics in Java are a language-only construction; they are implemented only in the compiler. The generated classfiles include generic signatures only in form of metadata (allowing the compiler to compile new classes against them). The runtime has no knowledge of the generic type system; generics are not part of the JVM. Instead, generics classes and methods are transformed during compiling via a process termed type erasure. During this, the compiler replaces all generic types with their raw version and inserts casts/checks appropriately in client code where the type and its methods are used. The resulting byte code will contain no references to any generic types or parameters (See also Generics in Java). 
The language specification intentionally prohibits certain uses of generics; this is necessary to allow for implementing generics through type erasure, and to allow for migration compatibility.
C# builds on support for generics from the virtual execution system, i.e., it is not just a language feature. The language is merely a front-end for cross-language generics support in the CLR. During compiling generics are verified for correctness, but code generation to implement the generics are deferred to class-load time. Client code (code invoking generic methods/properties) are fully compiled and can safely assume generics to be type-safe. This is called reification. At runtime, when a unique set of type parameters for a generic class/method/delegate is encountered for the first time, the class loader/verifier will synthesize a concrete class descriptor and generate method implementations. During the generation of method implementations all reference types will be considered one type, as reference types can safely share the same implementations. This is merely for the purpose of implementing code. Different sets of reference types will still have unique type descriptors; their method tables will merely point to the same code.
The following list illustrates some differences between Java and C# when managing generics. It is not exhaustive:
When a generic type parameter is under inheritance constraints the constraint type may be used instead of Object
C# allows generics directly for primitive types. Java, instead, allows the use of boxed types as type parameters (e.g., List<Integer> instead of List<int>). This comes at a cost since all such values need to be boxed/unboxed when used, and they all need to be heap-allocated. However, a generic type can be specialized with an array type of a primitive type in Java, for example List<int[]> is allowed.
Several third-party libraries implemented the basic collections in Java with backing primitive arrays to preserve the runtime and memory optimization that primitive types provide.
Java's type erasure design was motivated by a design requirement to achieve migration compatibility - not to be confused with backward compatibility. In particular, the original requirement was ""… there should be a clean, demonstrable migration path for the Collections APIs that were introduced in the Java 2 platform"". This was designed so that any new generic collections should be passable to methods that expected one of the pre-existing collection classes.
C# generics were introduced into the language while preserving full backward compatibility, but did not preserve full migration compatibility: Old code (pre C# 2.0) runs unchanged on the new generics-aware runtime without recompilation. As for migration compatibility, new generic collection classes and interfaces were developed that supplemented the non-generic .NET 1.x collections rather than replacing them. In addition to generic collection interfaces, the new generic collection classes implement the non-generic collection interfaces where possible. This prevents the use of new generic collections with pre-existing (non-generic aware) methods, if those methods are coded to use the collection classes.
Covariance and contravariance is supported by both languages. Java has use-site variance that allows a single generic class to declare members using both co- and contravariance. C# has define-site variance for generic interfaces and delegates. Variance is unsupported directly on classes but is supported through their implementation of variant interfaces. C# also has use-site covariance support for methods and delegates.
A closure is an inline function that captures variables from its lexical scope.
C# supports closures as anonymous methods or lambda expressions with full-featured closure semantics.
In Java, anonymous inner classes will remain the preferred way to emulate closures until Java 8 has become the new standard. This is a more verbose construction. This approach also has some differences compared to real closures, notably more controlled access to variables from the enclosing scopes: only final members can be referenced. Java 8, however introduces lambdas that fully inherit the current scope and, in fact, do not introduce a new scope.
When a reference to a method can be passed around for later execution, a problem arises about what to do when the method has references to variables/parameters in its lexical scope. C# closures can access any variable/parameter from its lexical scope. In Java's anonymous inner classes, only references to final members of the lexical scope are allowed, thus requiring the developer to mark which variables to make available, and in what state (possibly requiring boxing).
C# and Java feature a special type of in-line closures called lambdas. These are anonymous methods: they have a signature and a body, but no name. They are mainly used to specify local function-valued arguments in calls to other methods, a technique mainly associated with functional programming.
C#, unlike Java, allows the use of lambda functions as a way to define special data structures called expression trees. Whether they are seen as an executable function or as a data structure depends on compiler type inference and what type of variable or parameter they are assigned or cast to. Lambdas and expression trees play key roles in Language Integrated Query (LINQ).
In C#, namespaces are similar to those in C++. Unlike package names in Java, a namespace is not in any way tied to the location of the source file. While it is not strictly necessary for a Java source file location to mirror its package directory structure, it is the conventional organization.
Both languages allow importing of classes (e.g., import java.util.* in Java), allowing a class to be referenced using only its name. Sometimes classes with the same name exist in multiple namespaces or packages. Such classes can be referenced by using fully qualified names, or by importing only selected classes with different names. To do this, Java allows importing a single class (e.g., import java.util.List). C# allows importing classes under a new local name using the following syntax: using Console = System.Console. It also allows importing specializations of classes in the form of using IntList = System.Collections.Generic.List<int>.
Java has a static import syntax that allows using the short name of some or all of the static methods/fields in a class (e.g., allowing foo(bar) where foo() can be statically imported from another class). C# has a static class syntax (not to be confused with static inner classes in Java), which restricts a class to only contain static methods. C# 3.0 introduces extension methods to allow users to statically add a method to a type (e.g., allowing foo.bar() where bar() can be an imported extension method working on the type of foo).
The Sun Microsystems Java compiler requires that a source file name must match the only public class inside it, while C# allows multiple public classes in the same file, and puts no restrictions on the file name. C# 2.0 and later allows splitting a class definition into several files by using the partial keyword in the source code. In Java, a public class will always be in its own source file. In C#, source code files and logical units separation are not tightly related.
Unlike Java, C# implements conditional compilation using preprocessor directives. It also provides a Conditional attribute to define methods that are only called when a given compilation constant is defined. This way, assertions can be provided as a framework feature with the method Debug.Assert(), which is only evaluated when the DEBUG constant is defined. Since version 1.4, Java provides a language feature for assertions, which are turned off at runtime by default but can be enabled using the -enableassertions or -ea switch when invoking the JVM.
Both languages include thread synchronization mechanisms as part of their language syntax.
With .NET Framework 4.0, a new task-based programming model was introduced to replace the existing event-based asynchronous model. The API is based around the Task and Task<T> classes. Tasks can be composed and chained.
By convention, every method that returns a Task should have its name postfixed with Async.
In C# 5 a set of language and compiler extensions was introduced to make it easier to work with the task model. These language extensions included the notion of async methods and the await statement that make the program flow appear synchronous.
From this syntactic sugar the C# compiler generates a state-machine that handles the necessary continuations without developers having to think about it.
Java supports Threads since JDK 1.0. Java offers a high versatility for running threads, often called tasks. This is done by implementing a functional interface (a java.lang.Runnable interface) defining a single void no-args method as demonstrated in the following example:
Also, it's possible extending java.lang.Thread as below:
Similar to C#, Java has since version 5 a higher level replacement for working with threads directly. Executors are capable of running asynchronous tasks and typically manage a pool of threads. All threads of the internal pool will be reused under the hood for revenant tasks, so we can run as many concurrent tasks as we want throughout the life-cycle of our application with a single executor service.
This is how the first thread-example looks like using executors:
In addition to Runnable, Executors supports a Callable interface, another functional interface like Runnable but returns a value.
Calling the method get() blocks the current thread and waits until the callable completes before returning the value (in the example, a web page content):
A better practice could be ask to the users waiting while the task is completed:
To adequately support applications in the field of mathematical and financial computation, several language features exist.
Javas strictfp keyword enables strict floating-point calculations for a region of code. Strict floating-point calculations require that even if a platform offers higher precision during calculations, intermediate results must be converted to single/double. This ensures that strict floating point calculations return exactly the same result on all platforms. Without strict floating point a platform implementation is free to use higher precision for intermediate results during calculation. C# allows an implementation for a given hardware architecture to always use a higher precision for intermediate results if available, i.e. C# does not allow the programmer to optionally force intermediate results to use the potential lower precision of float/double.
Although Java's floating point arithmetic is largely based on IEEE 754 (Standard for Binary Floating-Point Arithmetic), certain features are unsupported even when using the strictfp modifier, such as Exception Flags and Directed Roundings, abilities mandated by IEEE Standard 754 (see Criticism of Java, Floating point arithmetic).
C# provides a built-in decimal type, which has higher precision (but less range) than the Java/C# double. The decimal type is a 128-bit data type suitable for financial and monetary calculations. The decimal type can represent values ranging from 1.0 × 10−28 to approximately 7.9 × 1028 with 28-29 significant digits. The structure uses C# operator overloading so that decimals can be manipulated using operators such as +, -,*and /, like other primitive data types.
The BigDecimal and BigInteger types provided with Java allow arbitrary-precision representation of decimal numbers and integer numbers, respectively. Java standard library does not have classes to deal with complex numbers.
The BigInteger, and Complex types provided with C# allow representation and manipulation of arbitrary-precision integers and complex numbers, respectively. The structures use C# operator overloading so that instances can be manipulated using operators such as +, -,*and /, like other primitive data types. C# standard library does not have classes to deal with arbitrary-precision floating point numbers (see software for arbitrary-precision arithmetic).
C# can help mathematical applications with the checked and unchecked operators that allow the enabling or disabling of run-time checking for arithmetic overflow for a region of code.
C#s Language Integrated Query (LINQ) is a set of features designed to work together to allow in-language querying abilities and is a distinguishing feature between C# and Java.
LINQ consists of the following features:
The Java Native Interface (JNI) feature allows Java programs to call non-Java code. However, JNI does require the code being called to follow several conventions and imposes restrictions on types and names used. This means that an extra adaption layer between legacy code and Java is often needed. This adaption code must be coded in a non-Java language, often C or C++. Java Native Access (JNA) allows easier calling of native code that only requires writing Java code, but comes at a performance cost.
In addition, third party libraries provide Java-Component Object Model (COM) bridging, e.g., JACOB (free), and J-Integra for COM (proprietary).
.NET Platform Invoke (P/Invoke) offers the same ability by allowing calls from C# to what Microsoft terms unmanaged code. Through metadata attributes the programmer can control exactly how the parameters and results are marshalled, thus avoiding the external glue code needed by the equivalent JNI in Java. P/Invoke allows almost complete access to procedural APIs (such as Win32 or POSIX), but limited access to C++ class libraries.
In addition, .NET Framework also provides a .NET-COM bridge, allowing access to COM components as, if they were first-class .NET objects.
C# also allows the programmer to disable the normal type-checking and other safety features of the CLR, which then enables the use of pointer variables. When using this feature, the programmer must mark the code using the unsafe keyword. JNI, P/Invoke, and ""unsafe"" code are equally risky features, exposing possible security holes and application instability. An advantage of unsafe, managed code over P/Invoke or JNI is that it allows the programmer to continue to work in the familiar C# environment to accomplish some tasks that otherwise would require calling out to unmanaged code. An assembly (program or library) using unsafe code must be compiled with a special switch and will be marked as such. This enables runtime environments to take special precautions before executing potentially harmful code.
Java (the programming language) is designed to execute on the Java platform via the Java Runtime Environment (JRE). The Java platform includes the Java virtual machine (JVM) and a common set of libraries. The JRE was originally designed to support interpreted execution with final compiling as an option. Most JRE environments execute fully or at least partially compiled programs, possibly with adaptive optimization. The Java compiler produces Java bytecode. Upon execution the bytecode is loaded by the Java runtime and either interpreted directly or compiled to machine instructions and then executed. 
C# is designed to execute on the Common Language Runtime (CLR). The CLR is designed to execute fully compiled code. The C# compiler produces Common Intermediate Language instructions. Upon execution the runtime loads this code and compiles to machine instructions on the target architecture. 
Example illustrating how to copy text one line at a time from one file to another, using both languages.
C# allows library-defined types to be integrated with existing types and operators by using custom implicit/explicit conversions and operator overloading as illustrated by the following example:
This example illustrates how Java and C# can be used to create and invoke an instance of class that is implemented in another programming language. The ""Deepthought"" class is implemented using the Ruby programming language and represents a simple calculator that will multiply two input values (a and b) when the Calculate method is invoked.
Notes for the Java implementation:
Notes for the C# implementation:
This example illustrates how the Fibonacci sequence can be implemented using the two languages. The C# version takes advantage of C# generator methods. The Java version takes the advantage of Stream interface and method references. Both the Java and the C# examples use K&R style for code formatting of classes, methods and statements.
"
"127","In the Java programming language, a keyword is one of 50(48 are in use, 2 are not in use(Deprecated)) reserved words that have a predefined meaning in the language; because of this, programmers cannot use keywords as names for variables, methods, classes, or as any other identifier. Due to their special functions in the language, most integrated development environments for Java use syntax highlighting to display keywords in a different colour for easy identification.


import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
public class DataStuff {"
"128","The syntax of the C programming language, the rules governing writing of software in the language, is designed to allow for programs that are extremely terse, have a close relationship with the resulting object code, and yet provide relatively high-level data abstraction. C was the first widely successful high-level language for portable operating-system development.
C syntax makes use of the maximal munch principle.
The C language represents numbers in three forms: integral, real and complex. This distinction reflects similar distinctions in the instruction set architecture of most central processing units. Integral data types store numbers in the set of integers, while real and complex numbers represent numbers (or pair of numbers) in the set of real numbers in floating point form.
All C integer types have signed and unsigned variants. If signed or unsigned is not specified explicitly, in most circumstances signed is assumed. However, for historic reasons plain char is a type distinct from both signed char and unsigned char. It may be a signed type or an unsigned type, depending on the compiler and the character set (C guarantees that members of the C basic character set have positive values). Also, bit field types specified as plain int may be signed or unsigned, depending on the compiler.
C's integer types come in different fixed sizes, capable of representing various ranges of numbers. The type char occupies exactly one byte (the smallest addressable storage unit), which is typically 8 bits wide. (Although char can represent any of C's ""basic"" characters, a wider type may be required for international character sets.) Most integer types have both signed and unsigned varieties, designated by the signed and unsigned keywords. Signed integer types may use a two's complement, ones' complement, or sign-and-magnitude representation. In many cases, there are multiple equivalent ways to designate the type; for example, signed short int and short are synonymous.
The representation of some types may include unused ""padding"" bits, which occupy storage but are not included in the width. The following table provides a complete list of the standard integer types and their minimum allowed widths (including any sign bit).
The char type is distinct from both signed char and unsigned char, but is guaranteed to have the same representation as one of them. The _Bool and long long types are standardized since 1999, and may not be supported by older C compilers. Type _Bool is usually accessed via the typedef name bool defined by the standard header stdbool.h.
In general, the widths and representation scheme implemented for any given platform are chosen based on the machine architecture, with some consideration given to the ease of importing source code developed for other platforms. The width of the int type varies especially widely among C implementations; it often corresponds to the most ""natural"" word size for the specific platform. The standard header limits.h defines macros for the minimum and maximum representable values of the standard integer types as implemented on any specific platform.
In addition to the standard integer types, there may be other ""extended"" integer types, which can be used for typedefs in standard headers. For more precise specification of width, programmers can and should use typedefs from the standard header stdint.h.
Integer constants may be specified in source code in several ways. Numeric values can be specified as decimal (example: 1022), octal with zero (0) as a prefix (01776), or hexadecimal with 0x (zero x) as a prefix (0x3FE). A character in single quotes (example: 'R'), called a ""character constant,"" represents the value of that character in the execution character set, with type int. Except for character constants, the type of an integer constant is determined by the width required to represent the specified value, but is always at least as wide as int. This can be overridden by appending an explicit length and/or signedness modifier; for example, 12lu has type unsigned long. There are no negative integer constants, but the same effect can often be obtained by using a unary negation operator ""-"".
The enumerated type in C, specified with the enum keyword, and often just called an ""enum"" (usually pronounced ee'-num /ˌi.nʌm/ or ee'-noom /ˌi.nuːm/), is a type designed to represent values across a series of named constants. Each of the enumerated constants has type int. Each enum type itself is compatible with char or a signed or unsigned integer type, but each implementation defines its own rules for choosing a type.
Some compilers warn if an object with enumerated type is assigned a value that is not one of its constants. However, such an object can be assigned any values in the range of their compatible type, and enum constants can be used anywhere an integer is expected. For this reason, enum values are often used in place of preprocessor #define directives to create named constants. Such constants are generally safer to use than macros, since they reside within a specific identifier namespace.
An enumerated type is declared with the enum specifier and an optional name (or tag) for the enum, followed by a list of one or more constants contained within curly braces and separated by commas, and an optional list of variable names. Subsequent references to a specific enumerated type use the enum keyword and the name of the enum. By default, the first constant in an enumeration is assigned the value zero, and each subsequent value is incremented by one over the previous constant. Specific values may also be assigned to constants in the declaration, and any subsequent constants without specific values will be given incremented values from that point onward.
For example, consider the following declaration:
This declares the enum colors type; the int constants RED (whose value is 0), GREEN (whose value is one greater than RED, 1), BLUE (whose value is the given value, 5), and YELLOW (whose value is one greater than BLUE, 6); and the enum colors variable paint_color. The constants may be used outside of the context of the enum (where any integer value is allowed), and values other than the constants may be assigned to paint_color, or any other variable of type enum colors.
The floating-point form is used to represent numbers with a fractional component. They do not, however, represent most rational numbers exactly; they are instead a close approximation. There are three types of real values, denoted by their specifiers: single precision (float), double precision (double), and double extended precision (long double). Each of these may represent values in a different form, often one of the IEEE floating-point formats.
Floating-point constants may be written in decimal notation, e.g. 1.23. Decimal scientific notation may be used by adding e or E followed by a decimal exponent, also known as E notation, e.g. 1.23e2 (which has the value 1.23 × 102 = 123.0). Either a decimal point or an exponent is required (otherwise, the number is parsed as an integer constant). Hexadecimal floating-point constants follow similar rules, except that they must be prefixed by 0x and use p or P to specify a binary exponent, e.g. 0xAp-2 (which has the value 2.5, since Ah × 2−2 = 10 × 2−2 = 10 ÷ 4). Both, decimal and hexadecimal floating-point constants may be suffixed by f or F to indicate a constant of type float, by l (letter l) or L to indicate type long double, or left unsuffixed for a double constant.
The standard header file float.h defines the minimum and maximum values of the implementation's floating-point types float, double, and long double. It also defines other limits that are relevant to the processing of floating-point numbers.
Every object has a storage class. This specifies most basically the storage duration, which may be static (default for global), automatic (default for local), or dynamic (allocated), together with other features (linkage and register hint).
Variables declared within a block by default have automatic storage, as do those explicitly declared with the auto or register storage class specifiers. The auto and register specifiers may only be used within functions and function argument declarations; as such, the auto specifier is always redundant. Objects declared outside of all blocks and those explicitly declared with the static storage class specifier have static storage duration. Static variables are initialized to zero by default by the compiler.
Objects with automatic storage are local to the block in which they were declared and are discarded when the block is exited. Additionally, objects declared with the register storage class may be given higher priority by the compiler for access to registers; although they may not actually be stored in registers, objects with this storage class may not be used with the address-of (&) unary operator. Objects with static storage persist for the program's entire duration. In this way, the same object can be accessed by a function across multiple calls. Objects with allocated storage duration are created and destroyed explicitly with malloc, free, and related functions.
The extern storage class specifier indicates that the storage for an object has been defined elsewhere. When used inside a block, it indicates that the storage has been defined by a declaration outside of that block. When used outside of all blocks, it indicates that the storage has been defined outside of the compilation unit. The extern storage class specifier is redundant when used on a function declaration. It indicates that the declared function has been defined outside of the compilation unit.
Note that storage specifiers apply only to functions and objects; other things such as type and enum declarations are private to the compilation unit in which they appear. Types, on the other hand, have qualifiers (see below).
Types can be qualified to indicate special properties of their data. The type qualifier const indicates that a value does not change once it has been initialized. Attempting to modify a const qualified value yields undefined behavior, so some C compilers store them in rodata or (for embedded systems) in read-only memory (ROM). The type qualifier volatile indicates to an optimizing compiler that it may not remove apparently redundant reads or writes, as the value may change even if it was not modified by any expression or statement, or multiple writes may be necessary, such as for memory-mapped I/O.
An incomplete type is a structure or union type whose members have not yet been specified, an array type whose dimension has not yet been specified, or the void type (the void type cannot be completed). Such a type may not be instantiated (its size is not known), nor may its members be accessed (they, too, are unknown); however, the derived pointer type may be used (but not dereferenced).
They are often used with pointers, either as forward or external declarations. For instance, code could declare an incomplete type like this:
This declares pt as a pointer to struct thing and the incomplete type struct thing. Pointers to data always have the same byte-width regardless of what they point to, so this statement is valid by itself (as long as pt is not dereferenced). The incomplete type can be completed later in the same scope by redeclaring it:
Incomplete types are used to implement recursive structures; the body of the type declaration may be deferred to later in the translation unit:
Incomplete types are also used for data hiding; the incomplete type is defined in a header file, and the body only within the relevant source file.
In declarations the asterisk modifier (*) specifies a pointer type. For example, where the specifier int would refer to the integer type, the specifier int* refers to the type ""pointer to integer"". Pointer values associate two pieces of information: a memory address and a data type. The following line of code declares a pointer-to-integer variable called ptr:
When a non-static pointer is declared, it has an unspecified value associated with it. The address associated with such a pointer must be changed by assignment prior to using it. In the following example, ptr is set so that it points to the data associated with the variable a:
In order to accomplish this, the ""address-of"" operator (unary &) is used. It produces the memory location of the data object that follows.
The pointed-to data can be accessed through a pointer value. In the following example, the integer variable b is set to the value of integer variable a, which is 10:
In order to accomplish that task, the unary dereference operator, denoted by an asterisk (*), is used. It returns the data to which its operand—which must be of pointer type—points. Thus, the expression *p denotes the same value as a. Dereferencing a null pointer is illegal.
Arrays are used in C to represent structures of consecutive elements of the same type. The definition of a (fixed-size) array has the following syntax:
which defines an array named array to hold 100 values of the primitive type int. If declared within a function, the array dimension may also be a non-constant expression, in which case memory for the specified number of elements will be allocated. In most contexts in later use, a mention of the variable array is converted to a pointer to the first item in the array. The sizeof operator is an exception: sizeof array yields the size of the entire array (that is, 100 times the size of an int, and sizeof(array) / sizeof(int) will return 100). Another exception is the & (address-of) operator, which yields a pointer to the entire array, for example
The primary facility for accessing the values of the elements of an array is the array subscript operator. To access the i-indexed element of array, the syntax would be array[i], which refers to the value stored in that array element.
Array subscript numbering begins at 0 (see Zero-based indexing). The largest allowed array subscript is therefore equal to the number of elements in the array minus 1. To illustrate this, consider an array a declared as having 10 elements; the first element would be a and the last element would be a.
C provides no facility for automatic bounds checking for array usage. Though logically the last subscript in an array of 10 elements would be 9, subscripts 10, 11, and so forth could accidentally be specified, with undefined results.
Due to arrays and pointers being interchangeable, the addresses of each of the array elements can be expressed in equivalent pointer arithmetic. The following table illustrates both methods for the existing array:
Since the expression a[i] is semantically equivalent to *(a+i), which in turn is equivalent to *(i+a), the expression can also be written as i[a], although this form is rarely used.
C99 standardised variable-length arrays (VLAs) within block scope. Such array variables are allocated based on the value of an integer value at runtime upon entry to a block, and are deallocated at the end of the block. As of C11 this feature is no longer required to be implemented by the compiler.
This syntax produces an array whose size is fixed until the end of the block.
Arrays that can be resized dynamically can be produced with the help of the C standard library. The malloc function provides a simple method for allocating memory. It takes one parameter: the amount of memory to allocate in bytes. Upon successful allocation, malloc returns a generic (void) pointer value, pointing to the beginning of the allocated space. The pointer value returned is converted to an appropriate type implicitly by assignment. If the allocation could not be completed, malloc returns a null pointer. The following segment is therefore similar in function to the above desired declaration:
The result is a ""pointer to int"" variable (a) that points to the first of n contiguous int objects; due to array–pointer equivalence this can be used in place of an actual array name, as shown in the last line. The advantage in using this dynamic allocation is that the amount of memory that is allocated to it can be limited to what is actually needed at run time, and this can be changed as needed (using the standard library function realloc).
When the dynamically-allocated memory is no longer needed, it should be released back to the run-time system. This is done with a call to the free function. It takes a single parameter: a pointer to previously allocated memory. This is the value that was returned by a previous call to malloc.
As a security measure, some programmers then set the pointer variable to NULL:
This ensures that further attempts to dereference the pointer will crash the program. If this is not done, the variable becomes a dangling pointer which can lead to a use-after-free bug. However, if the pointer is a local variable, setting it to NULL does not prevent the program from using other copies of the pointer. Local use-after-free bugs are usually easy for static analyzers to recognize. Therefore, this approach is less useful for local pointers and it is more often used with pointers stored in long-living structs.
In addition, C supports arrays of multiple dimensions, which are stored in row-major order. Technically, C multidimensional arrays are just one-dimensional arrays whose elements are arrays. The syntax for declaring multidimensional arrays is as follows:
where ROWS and COLUMNS are constants. This defines a two-dimensional array. Reading the subscripts from left to right, array2d is an array of length ROWS, each element of which is an array of COLUMNS integers.
To access an integer element in this multidimensional array, one would use
Again, reading from left to right, this accesses the 5th row, and the 4th element in that row. The expression array2d is an array, which we are then subscripting with  to access the fourth integer.
Higher-dimensional arrays can be declared in a similar manner.
A multidimensional array should not be confused with an array of references to arrays (also known as an Iliffe vectors or sometimes an array of arrays). The former is always rectangular (all subarrays must be the same size), and occupies a contiguous region of memory. The latter is a one-dimensional array of pointers, each of which may point to the first element of a subarray in a different place in memory, and the sub-arrays do not have to be the same size. The latter can be created by multiple uses of malloc.
In C, string constants (literals) are surrounded by double quotes (""), e.g. ""Hello world!"" and are compiled to an array of the specified char values with an additional null terminating character (0-valued) code to mark the end of the string.
String literals may not contain embedded newlines; this proscription somewhat simplifies parsing of the language. To include a newline in a string, the backslash escape \n may be used, as below.
There are several standard library functions for operating with string data (not necessarily constant) organized as array of char using this null-terminated format; see below.
C's string-literal syntax has been very influential, and has made its way into many other languages, such as C++, Objective-C, Perl, Python, PHP, Java, Javascript, C#, Ruby. Nowadays, almost all new languages adopt or build upon C-style string syntax. Languages that lack this syntax tend to precede C.
If you wish to include a double quote inside the string, that can be done by escaping it with a backslash (\), for example, ""This string contains \""double quotes\""."". To insert a literal backslash, one must double it, e.g. ""A backslash looks like this: \\"".
Backslashes may be used to enter control characters, etc., into a string:
The use of other backslash escapes is not defined by the C standard, although compiler vendors often provide additional escape codes as language extensions.
C has string literal concatenation, meaning that adjacent string literals are concatenated at compile time; this allows long strings to be split over multiple lines, and also allows string literals resulting from C preprocessor defines and macros to be appended to strings at compile time:
will expand to
which is syntactically equivalent to
Individual character constants are single-quoted, e.g. 'A', and have type int (in C++, char). The difference is that ""A"" represents a null-terminated array of two characters, 'A' and '\0', whereas 'A' directly represents the character value (65 if ASCII is used). The same backslash-escapes are supported as for strings, except that (of course) "" can validly be used as a character without being escaped, whereas ' must now be escaped.
A character constant cannot be empty (i.e. '' is invalid syntax), although a string may be (it still has the null terminating character). Multi-character constants (e.g. 'xy') are valid, although rarely useful — they let one store several characters in an integer (e.g. 4 ASCII characters can fit in a 32-bit integer, 8 in a 64-bit one). Since the order in which the characters are packed into an int is not specified, portable use of multi-character constants is difficult.
Since type char is 1 byte wide, a single char value typically can represent at most 255 distinct character codes, not nearly enough for all the characters in use worldwide. To provide better support for international characters, the first C standard (C89) introduced wide characters (encoded in type wchar_t) and wide character strings, which are written as L""Hello world!""
Wide characters are most commonly either 2 bytes (using a 2-byte encoding such as UTF-16) or 4 bytes (usually UTF-32), but Standard C does not specify the width for wchar_t, leaving the choice to the implementor. Microsoft Windows generally uses UTF-16, thus the above string would be 26 bytes long for a Microsoft compiler; the Unix world prefers UTF-32, thus compilers such as GCC would generate a 52-byte string. A 2-byte wide wchar_t suffers the same limitation as char, in that certain characters (those outside the BMP) cannot be represented in a single wchar_t; but must be represented using surrogate pair
The original C standard specified only minimal functions for operating with wide character strings; in 1995 the standard was modified to include much more extensive support, comparable to that for char strings. The relevant functions are mostly named after their char equivalents, with the addition of a ""w"" or the replacement of ""str"" with ""wcs""; they are specified in <wchar.h>, with <wctype.h> containing wide-character classification and mapping functions.
The now generally recommended method of supporting international characters is through UTF-8, which is stored in char arrays, and can be written directly in the source code if using a UTF-8 editor, because UTF-8 is a direct ASCII extension.
A common alternative to wchar_t is to use a variable-width encoding, whereby a logical character may extend over multiple positions of the string. Variable-width strings may be encoded into literals verbatim, at the risk of confusing the compiler, or using numerical backslash escapes (e.g. ""\xc3\xa9"" for ""é"" in UTF-8). The UTF-8 encoding was specifically designed (under Plan 9) for compatibility with the standard library string functions; supporting features of the encoding include a lack of embedded nulls, no valid interpretations for subsequences, and trivial resynchronisation. Encodings lacking these features are likely to prove incompatible with the standard library functions; encoding-aware string functions are often used in such cases.
Strings, both constant and variable, can be manipulated without using the standard library. However, the library contains many useful functions for working with null-terminated strings.
Structures and unions in C are defined as data containers consisting of a sequence of named members of various types. They are similar to records in other programming languages. The members of a structure are stored in consecutive locations in memory, although the compiler is allowed to insert padding between or after members (but not before the first member) for efficiency or as padding required for proper alignment by the target architecture. The size of a structure is equal to the sum of the sizes of its members, plus the size of the padding.
Unions in C are related to structures and are defined as objects that may hold (at different times) objects of different types and sizes. They are analogous to variant records in other programming languages. Unlike structures, the components of a union all refer to the same location in memory. In this way, a union can be used at various times to hold different types of objects, without the need to create a separate object for each new type. The size of a union is equal to the size of its largest component type.
Structures are declared with the struct keyword and unions are declared with the union keyword. The specifier keyword is followed by an optional identifier name, which is used to identify the form of the structure or union. The identifier is followed by the declaration of the structure or union's body: a list of member declarations, contained within curly braces, with each declaration terminated by a semicolon. Finally, the declaration concludes with an optional list of identifier names, which are declared as instances of the structure or union.
For example, the following statement declares a structure named s that contains three members; it will also declare an instance of the structure known as tee:
And the following statement will declare a similar union named u and an instance of it named n:
Members of structures and unions cannot have an incomplete or function type. Thus members cannot be an instance of the structure or union being declared (because it is incomplete at that point) but can be pointers to the type being declared.
Once a structure or union body has been declared and given a name, it can be considered a new data type using the specifier struct or union, as appropriate, and the name. For example, the following statement, given the above structure declaration, declares a new instance of the structure s named r:
It is also common to use the typedef specifier to eliminate the need for the struct or union keyword in later references to the structure. The first identifier after the body of the structure is taken as the new name for the structure type (structure instances may not be declared in this context). For example, the following statement will declare a new type known as s_type that will contain some structure:
Future statements can then use the specifier s_type (instead of the expanded struct … specifier) to refer to the structure.
Members are accessed using the name of the instance of a structure or union, a period (.), and the name of the member. For example, given the declaration of tee from above, the member known as y (of type float) can be accessed using the following syntax:
Structures are commonly accessed through pointers. Consider the following example that defines a pointer to tee, known as ptr_to_tee:
Member y of tee can then be accessed by dereferencing ptr_to_tee and using the result as the left operand:
Which is identical to the simpler tee.y above as long as ptr_to_tee points to tee. Due to operator precedence (""."" being higher than ""*""), the shorter *ptr_to_tee.y is incorrect for this purpose, instead being parsed as *(ptr_to_tee.y) and thus the parentheses are necessary. Because this operation is common, C provides an abbreviated syntax for accessing a member directly from a pointer. With this syntax, the name of the instance is replaced with the name of the pointer and the period is replaced with the character sequence ->. Thus, the following method of accessing y is identical to the previous two:
Members of unions are accessed in the same way.
This can be chained; for example, in a linked list, one may refer to n->next->next for the second following node (assuming that n->next is not null).
Assigning values to individual members of structures and unions is syntactically identical to assigning values to any other object. The only difference is that the lvalue of the assignment is the name of the member, as accessed by the syntax mentioned above.
A structure can also be assigned as a unit to another structure of the same type. Structures (and pointers to structures) may also be used as function parameter and return types.
For example, the following statement assigns the value of 74 (the ASCII code point for the letter 't') to the member named x in the structure tee, from above:
And the same assignment, using ptr_to_tee in place of tee, would look like:
Assignment with members of unions is identical.
According to the C standard, the only legal operations that can be performed on a structure are copying it, assigning to it as a unit (or initializing it), taking its address with the address-of (&) unary operator, and accessing its members. Unions have the same restrictions. One of the operations implicitly forbidden is comparison: structures and unions cannot be compared using C's standard comparison facilities (==, >, <, etc.).
C also provides a special type of structure member known as a bit field, which is an integer with an explicitly specified number of bits. A bit field is declared as a structure member of type int, signed int, unsigned int, or _Bool, following the member name by a colon (:) and the number of bits it should occupy. The total number of bits in a single bit field must not exceed the total number of bits in its declared type.
As a special exception to the usual C syntax rules, it is implementation-defined whether a bit field declared as type int, without specifying signed or unsigned, is signed or unsigned. Thus, it is recommended to explicitly specify signed or unsigned on all structure members for portability.
Unnamed fields consisting of just a colon followed by a number of bits are also allowed; these indicate padding. Specifying a width of zero for an unnamed field is used to force alignment to a new word.
The members of bit fields do not have addresses, and as such cannot be used with the address-of (&) unary operator. The sizeof operator may not be applied to bit fields.
The following declaration declares a new structure type known as f and an instance of it known as g. Comments provide a description of each of the members:
Default initialization depends on the storage class specifier, described above.
Because of the language's grammar, a scalar initializer may be enclosed in any number of curly brace pairs. Most compilers issue a warning if there is more than one such pair, though.
Structures, unions and arrays can be initialized in their declarations using an initializer list. Unless designators are used, the components of an initializer correspond with the elements in the order they are defined and stored, thus all preceding values must be provided before any particular element’s value. Any unspecified elements are set to zero (except for unions). Mentioning too many initialization values yields an error.
The following statement will initialize a new instance of the structure s known as pi:
Designated initializers allow members to be initialized by name, in any order, and without explicitly providing the preceding values. The following initialization is equivalent to the previous one:
Using a designator in an initializer moves the initialization ""cursor"". In the example below, if MAX is greater than 10, there will be some zero-valued elements in the middle of a; if it is less than 10, some of the values provided by the first five initializers will be overridden by the second five (if MAX is less than 5, there will be a compilation error):
In C89, a union was initialized with a single value applied to its first member. That is, the union u defined above could only have its int x member initialized:
Using a designated initializer, the member to be initialized does not have to be the first member:
If an array has unknown size (i.e. the array was an incomplete type), the number of initializers determines the size of the array and its type becomes complete:
Compound designators can be used to provide explicit initialization when unadorned initializer lists
might be misunderstood. In the example below, w is declared as an array of structures, each structure consisting of a member a (an array of 3 int) and a member b (an int). The initializer sets the size of w to 2 and sets the values of the first element of each a:
This is equivalent to:
There is no way to specify repetition of an initializer in standard C.
It is possible to borrow the initialization methodology to generate compound structure and array literals:
Compound literals are often combined with designated initializers to make the declaration more readable:
C is a free-form language.
Bracing style varies from programmer to programmer and can be the subject of debate. See Indent style for more details.
In the items in this section, any <statement> can be replaced with a compound statement. Compound statements have the form:
and are used as the body of a function or anywhere that a single statement is expected. The declaration-list declares variables to be used in that scope, and the statement-list are the actions to be performed. Brackets define their own scope, and variables defined inside those brackets will be automatically
deallocated at the closing bracket. Declarations and statements can be freely intermixed within a compound statement (as in C++).
C has two types of selection statements: the if statement and the switch statement.
The if statement is in the form:
In the if statement, if the <expression> in parentheses is nonzero (true), control passes to <statement1>. If the else clause is present and the <expression> is zero (false), control will pass to <statement2>. The else <statement2> part is optional and, if absent, a false <expression> will simply result in skipping over the <statement1>. An else always matches the nearest previous unmatched if; braces may be used to override this when necessary, or for clarity.
The switch statement causes control to be transferred to one of several statements depending on the value of an expression, which must have integral type. The substatement controlled by a switch is typically compound. Any statement within the substatement may be labeled with one or more case labels, which consist of the keyword case followed by a constant expression and then a colon (:). The syntax is as follows:
No two of the case constants associated with the same switch may have the same value. There may be at most one default label associated with a switch. If none of the case labels are equal to the expression in the parentheses following switch, control passes to the default label or, if there is no default label, execution resumes just beyond the entire construct.
Switches may be nested; a case or default label is associated with the innermost switch that contains it. Switch statements can ""fall through"", that is, when one case section has completed its execution, statements will continue to be executed downward until a break; statement is encountered. Fall-through is useful in some circumstances, but is usually not desired.
In the preceding example, if <label2> is reached, the statements <statements 2> are executed and nothing more inside the braces. However, if <label1> is reached, both <statements 1> and <statements 2> are executed since there is no break to separate the two case statements.
It is possible, although unusual, to insert the switch labels into the sub-blocks of other control structures. Examples of this include Duff's device and Simon Tatham's implementation of coroutines in Putty.
C has three forms of iteration statement:
In the while and do statements, the sub-statement is executed repeatedly so long as the value of the expression remains non-zero (equivalent to true). With while, the test, including all side effects from <expression>, occurs before each iteration (execution of <statement>); with do, the test occurs after each iteration. Thus, a do statement always executes its sub-statement at least once, whereas while may not execute the sub-statement at all.
The statement:
is equivalent to:
except for the behaviour of a continue; statement (which in the for loop jumps to e3 instead of e2). If e2 is blank, it would have to be replaced with a 1.
Any of the three expressions in the for loop may be omitted. A missing second expression makes the while test always non-zero, creating a potentially infinite loop.
Since C99, the first expression may take the form of a declaration, typically including an initializer, such as:
The declaration's scope is limited to the extent of the for loop.
Jump statements transfer control unconditionally. There are four types of jump statements in C: goto, continue, break, and return.
The goto statement looks like this:
The identifier must be a label (followed by a colon) located in the current function. Control transfers to the labeled statement.
A continue statement may appear only within an iteration statement and causes control to pass to the loop-continuation portion of the innermost enclosing iteration statement. That is, within each of the statements
a continue not contained within a nested iteration statement is the same as goto cont.
The break statement is used to end a for loop, while loop, do loop, or switch statement. Control passes to the statement following the terminated statement.
A function returns to its caller by the return statement. When return is followed by an expression, the value is returned to the caller as the value of the function. Encountering the end of the function is equivalent to a return with no expression. In that case, if the function is declared as returning a value and the caller tries to use the returned value, the result is undefined.
GCC extends the C language with a unary && operator that returns the address of a label. This address can be stored in a void* variable type and may be used later in a goto instruction. For example, the following prints ""hi "" in an infinite loop:
This feature can be used to implement a jump table.
A C function definition consists of a return type (void if no value is returned), a unique name, a list of parameters in parentheses, and various statements:
A function with non-void return type should include at least one return statement. The parameters are given by the <parameter-list>, a comma-separated list of parameter declarations, each item in the list being a data type followed by an identifier: <data-type> <variable-identifier>, <data-type> <variable-identifier>, ....
If there are no parameters, the <parameter-list> may be left empty or optionally be specified with the single word void.
It is possible to define a function as taking a variable number of parameters by providing the ... keyword as the last parameter instead of a data type and variable identifier. A commonly used function that does this is the standard library function printf, which has the declaration:
Manipulation of these parameters can be done by using the routines in the standard library header <stdarg.h>.
A pointer to a function can be declared as follows:
The following program shows use of a function pointer for selecting between addition and subtraction:
After preprocessing, at the highest level a C program consists of a sequence of declarations at file scope. These may be partitioned into several separate source files, which may be compiled separately; the resulting object modules are then linked along with implementation-provided run-time support modules to produce an executable image.
The declarations introduce functions, variables and types. C functions are akin to the subroutines of Fortran or the procedures of Pascal.
A definition is a special type of declaration. A variable definition sets aside storage and possibly initializes it, a function definition provides its body.
An implementation of C providing all of the standard library functions is called a hosted implementation. Programs written for hosted implementations are required to define a special function called main, which is the first function called when a program begins executing.
Hosted implementations start program execution by invoking the main function, which must be defined following one of these prototypes:
The first two definitions are equivalent (and both are compatible with C++). It is probably up to individual preference which one is used (the current C standard contains two examples of main() and two of main(void), but the draft C++ standard uses main()). The return value of main (which should be int) serves as termination status returned to the host environment.
The C standard defines return values 0 and EXIT_SUCCESS as indicating success and EXIT_FAILURE as indicating failure. (EXIT_SUCCESS and EXIT_FAILURE are defined in <stdlib.h>). Other return values have implementation-defined meanings; for example, under Linux a program killed by a signal yields a return code of the numerical value of the signal plus 128.
A minimal correct C program consists of an empty main routine, taking no arguments and doing nothing:
Because no return statement is present, main returns 0 on exit. (This is a special-case feature introduced in C99 that applies only to main.)
The main function will usually call other functions to help it perform its job.
Some implementations are not hosted, usually because they are not intended to be used with an operating system. Such implementations are called free-standing in the C standard. A free-standing implementation is free to specify how it handles program startup; in particular it need not require a program to define a main function.
Functions may be written by the programmer or provided by existing libraries. Interfaces for the latter are usually declared by including header files—with the #include preprocessing directive—and the library objects are linked into the final executable image. Certain library functions, such as printf, are defined by the C standard; these are referred to as the standard library functions.
A function may return a value to caller (usually another C function, or the hosting environment for the function main). The printf function mentioned above returns how many characters were printed, but this value is often ignored.
In C, arguments are passed to functions by value while other languages may pass variables by reference.
This means that the receiving function gets copies of the values and has no direct way of altering the original variables.
For a function to alter a variable passed from another function, the caller must pass its address (a pointer to it), which can then be dereferenced in the receiving function. See Pointers for more information.
The function scanf works the same way:
In order to pass an editable pointer to a function (such as for the purpose of returning an allocated array to the calling code) you have to pass a pointer to that pointer: its address.
The parameter int **a_p is a pointer to a pointer to an int, which is the address of the pointer p defined in the main function in this case.
Function parameters of array type may at first glance appear to be an exception to C's pass-by-value rule. The following program will print 2, not 1:
However, there is a different reason for this behavior. In fact, a function parameter declared with an array type is treated like one declared to be a pointer. That is, the preceding declaration of setArray is equivalent to the following:
At the same time, C rules for the use of arrays in expressions cause the value of a in the call to setArray to be converted to a pointer to the first element of array a. Thus, in fact this is still an example of pass-by-value, with the caveat that it is the address of the first element of the array being passed by value, not the contents of the array.
The following words are reserved, and may not be used as identifiers:
Implementations may reserve other keywords, such as asm, although implementations typically provide non-standard keywords that begin with one or two underscores.
C identifiers are case sensitive (e.g., foo, FOO, and Foo are the names of different objects). Some linkers may map external identifiers to a single case, although this is uncommon in most modern linkers.
Text starting with the token /* is treated as a comment and ignored. The comment ends at the next */; it can occur within expressions, and can span multiple lines. Accidental omission of the comment terminator is problematic in that the next comment's properly constructed comment terminator will be used to terminate the initial comment, and all code in between the comments will be considered as a comment. C-style comments do not nest; that is, accidentally placing a comment within a comment has unintended results:
C++ style line comments start with // and extend to the end of the line. This style of comment originated in BCPL and became valid C syntax in C99; it is not available in the original K&R C nor in ANSI C:
The parameters given on a command line are passed to a C program with two predefined variables - the count of the command-line arguments in argc and the individual arguments as character strings in the pointer array argv. So the command:

myFilt p1 p2 p3

results in something like:
While individual strings are arrays of contiguous characters, there is no guarantee that the strings are stored as a contiguous group.
The name of the program, argv, may be useful when printing diagnostic messages or for making one binary serve multiple purposes. The individual values of the parameters may be accessed with argv, argv, and argv, as shown in the following program:
In any reasonably complex expression, there arises a choice as to the order in which to evaluate the parts of the expression: (1+1)+(3+3) may be evaluated in the order (1+1)+(3+3), (2)+(3+3), (2)+(6), (8), or in the order (1+1)+(3+3), (1+1)+(6), (2)+(6), (8). Formally, a conforming C compiler may evaluate expressions in any order between sequence points (this allows the compiler to do some optimization). Sequence points are defined by:
Expressions before a sequence point are always evaluated before those after a sequence point. In the case of short-circuit evaluation, the second expression may not be evaluated depending on the result of the first expression. For example, in the expression (a() || b()), if the first argument evaluates to nonzero (true), the result of the entire expression cannot be anything else than true, so b() is not evaluated. Similarly, in the expression (a() && b()), if the first argument evaluates to zero (false), the result of the entire expression cannot be anything else than false, so b() is not evaluated.
The arguments to a function call may be evaluated in any order, as long as they are all evaluated by the time the function is entered. The following expression, for example, has undefined behavior:
An aspect of the C standard (not unique to C) is that the behavior of certain code is said to be ""undefined"". In practice, this means that the program produced from this code can do anything, from working as the programmer intended, to crashing every time it is run.
For example, the following code produces undefined behavior, because the variable b is modified more than once with no intervening sequence point:
Because there is no sequence point between the modifications of b in ""b++ + b++"", it is possible to perform the evaluation steps in more than one order, resulting in an ambiguous statement. This can be fixed by rewriting the code to insert a sequence point in order to enforce an unambiguous behavior, for example:"
"129","
A struct in the C programming language (and many derivatives) is a composite data type (or record) declaration that defines a physically grouped list of variables to be placed under one name in a block of memory, allowing the different variables to be accessed via a single pointer, or the struct declared name which returns the same address. The struct can contain many other complex and simple data types in an association, so is a natural organizing type for records like the mixed data types in lists of directory entries reading a hard drive (file length, name, extension, physical (cylinder, disk, head indexes) address, etc.), or other mixed record type (patient names, address, telephone... insurance codes, balance, etc.).
The C struct directly references a contiguous block of physical memory, usually delimited (sized) by word-length boundaries. It corresponds to the similarly named feature available in some assemblers for Intel processors. Language implementations that could utilize half-word or byte boundaries (giving denser packing, using less memory) were considered advanced in the mid-eighties. Being a block of contiguous memory, each field within a struct located at a certain fixed offset from the start. As an illustration, many BASIC interpreters once fielded a string data struct organization with one value recording string length, one indexing (cursor value of) the previous line, one pointing to the string data.
Because the contents of a struct are stored in contiguous memory, the sizeof operator must be used to get the number of bytes needed to store a particular type of struct, just as it can be used for primitives. The alignment of particular fields in the struct (with respect to word boundaries) is implementation-specific and may include padding, although modern compilers typically support the #pragma pack directive, which changes the size in bytes used for alignment.
In the C++ language, a struct is identical to a C++ class but a difference in the default visibility exists: class members are by default private, whereas struct members are by default public.
The struct data type in C was derived from the ALGOL 68 struct data type.
Like its C counterpart, the struct data type in C# (Structure in Visual Basic .NET) is similar to a class. The biggest difference between a struct and a class in these languages is that when a struct is passed as an argument to a function, any modifications to the struct in that function will not be reflected in the original variable (unless pass-by-reference is used).
This differs from C++, where classes or structs can be statically allocated or dynamically allocated either on the stack (similar to C#) or on the heap, with an explicit pointer. In C++, the only difference between a struct and a class is that the members and base classes of a struct are public by default. (A class defined with the class keyword has private members and base classes by default.)
The general syntax for a struct declaration in C is:
Here tag_name is optional in some contexts.
Such a struct declaration may also appear in the context of a typedef declaration of a type alias or the declaration or definition of a variable:
Often, such entities are better declared separately, as in:
For example:
defines a type, referred to as struct account. To create a new variable of this type, we can write
which has an integer component, accessed by s.account_number, and a floating-point component, accessed by s.balance, as well as the first_name and last_name components. The structure s contains all four values, and all four fields may be changed independently.
A pointer to an instance of the ""account"" structure will point to the memory address of the first variable, ""account_number"". The total storage required for a struct object is the sum of the storage requirements of all the fields, plus any internal padding.
There are three ways to initialize a structure. For the struct type
C89-style initializers are used when contiguous members may be given.
For non contiguous or out of order members list, designated initializer style may be used
If an initializer is given or if the object is statically allocated, omitted elements are initialized to 0.
A third way of initializing a structure is to copy the value of an existing object of the same type
The following assignment of a struct to another struct will copy as one might expect. Depending on the contents, a compiler might use memcpy() in order to perform this operation.
Pointers can be used to refer to a struct by its address. This is particularly useful for passing structs to a function by reference or to refer to another instance of the struct type as a field. The pointer can be dereferenced just like any other pointer in C, using the * operator. There is also a -> operator in C which dereferences the pointer to struct (left operand) and then accesses the value of a member of the struct (right operand).
C does not allow recursive declaration of struct; a struct can not contain a field that has the type of the struct itself. But pointers can be used to refer to an instance of it:
Here the instance el would contain a point with coordinates 3 and 7. Its next pointer would be a null pointer since the initializer for that field is omitted. The instance le in turn would have its own point and its next pointer would refer to el.
Typedefs can be used as shortcuts, for example:
Different users have differing preferences; proponents usually claim:
As an example, consider a type that defines a pointer to a function that accepts pointers to struct types and returns a pointer to struct:
Without typedef:
With typedef: 
A common naming convention for such a typedef is to append a ""_t"" (here point_t) to the struct tag name, but such names are reserved by POSIX so such a practice should be avoided. A much easier convention is to use just the same identifier for the tag name and the type name:
Without typedef a function that takes function pointer the following code would have to be used. Although valid, it becomes increasingly hard to read.
Here a second typedef for a function pointer type can be useful
Now with the two typedefs being used the complexity of the function signature is drastically reduced.
However, there are a handful of disadvantages in using them:

"
"130","

In the C, C++, D, and JavaScript programming languages, const is a type qualifier:[lower-alpha 1] a keyword applied to a data type that indicates that the data is read only. While this can be used to declare constants, const in the C family of languages differs from similar constructs in other languages in being part of the type, and thus has complicated behavior when combined with pointers, references, composite data types, and type-checking.
When applied in an object declaration,[lower-alpha 2] it indicates that the object is a constant: its value may not be changed, unlike a variable. This basic use – to declare constants – has parallels in many other languages.
However, unlike in other languages, in the C family of languages the const is part of the type, not part of the object. For example, in C, const int x = 1; declares an object x of const int type – the const is part of the type, as if it were parsed “(const int) x” – while in Ada, X : constant INTEGER := 1_ declares a constant (a kind of object) X of INTEGER type: the constant is part of the object, but not part of the type.
This has two subtle results. Firstly, const can be applied to parts of a more complex type – for example, int const * const x; declares a constant pointer to a constant integer, while int const * x; declares a variable pointer to a constant integer, and int * const x; declares a constant pointer to a variable integer. Secondly, because const is part of the type, it must match as part of type-checking. For example, the following code is invalid:
because the argument to f must be a variable integer, but i is a constant integer. This matching is a form of program correctness, and is known as const-correctness. This allows a form of programming by contract, where functions specify as part of their type signature whether they modify their arguments or not, and whether their return value is modifiable or not. This type-checking is primarily of interest in pointers and references – not basic value types like integers – but also for composite data types or templated types such as containers. It is concealed by the fact that the const can often be omitted, due to type coercion (implicit type conversion) and C being call-by-value (C++ and D are either call-by-value or call-by-reference).
The idea of const-ness does not imply that the variable as it is stored in the computer's memory is unwritable. Rather, const-ness is a compile-time construct that indicates what a programmer should do, not necessarily what they can do. Note, however, that in the case of predefined data (such as const char * string literals), C const is often unwritable.
While a constant does not change its value while the program is running, an object declared const may indeed change its value while the program is running. A common example are read only registers within embedded systems. The data registers for digital inputs are often declared as const and volatile. The content of these registers may change without the program doing anything (volatile) but you shall not write to them either (const).
In addition, a (non-static) member-function can be declared as const. In this case, the this pointer inside such a function is of type object_type const * const rather than merely of type object_type * const. This means that non-const functions for this object cannot be called from inside such a function, nor can member variables be modified. In C++, a member variable can be declared as mutable, indicating that this restriction does not apply to it. In some cases, this can be useful, for example with caching, reference counting, and data synchronization. In these cases, the logical meaning (state) of the object is unchanged, but the object is not physically constant since its bitwise representation may change.
In C, C++, and D, all data types, including those defined by the user, can be declared const, and const-correctness dictates that all variables or objects should be declared as such unless they need to be modified. Such proactive use of const makes values ""easier to understand, track, and reason about,"" and it thus increases the readability and comprehensibility of code and makes working in teams and maintaining code simpler because it communicates information about a value's intended use. This can help the compiler as well as the developer when reasoning about code. It can also enable an optimizing compiler to generate more efficient code.
For simple non-pointer data types, applying the const qualifier is straightforward. It can go on either side of the type for historical reasons (that is, const char foo = 'a'; is equivalent to char const foo = 'a';). On some implementations, using const on both sides of the type (for instance, const char const) generates a warning but not an error.
For pointer and reference types, the meaning of const is more complicated – either the pointer itself, or the value being pointed to, or both, can be const. Further, the syntax can be confusing. A pointer can be declared as a const pointer to writable value, or a writable pointer to a const value, or const pointer to const value. A const pointer cannot be reassigned to point to a different object from the one it is initially assigned, but it can be used to modify the value that it points to (called the pointee). Reference variables are thus an alternate syntax for const pointers. A pointer to a const object, on the other hand, can be reassigned to point to another memory location (which should be an object of the same type or of a convertible type), but it cannot be used to modify the memory that it is pointing to. A const pointer to a const object can also be declared and can neither be used to modify the pointee nor be reassigned to point to another object. The following code illustrates these subtleties:
Following usual C convention for declarations, declaration follows use, and the * in a pointer is written on the pointer, indicating dereferencing. For example, in the declaration int *ptr, the dereferenced form *ptr is an int, while the reference form ptr is a pointer to an int. Thus const modifies the name to its right. The C++ convention is instead to associate the * with the type, as in int* ptr, and read the const as modifying the type to the left. int const * ptrToConst can thus be read as ""*ptrToConst is a int const"" (the value is constant), or ""ptrToConst is a int const *"" (the pointer is a pointer to a constant integer). Thus:
Following C++ convention of analyzing the type, not the value, a rule of thumb is to read the declaration from right to left. Thus, everything to the left of the star can be identified as the pointee type and everything to the right of the star are the pointer properties. For instance, in our example above, int const * can be read as a writable pointer that refers to a non-writable integer, and int * const can be read as a non-writable pointer that refers to a writable integer.
A more generic rule that helps you understand complex declarations and definitions works like this:
Here is an example: 
When reading to the left it's important that you read the elements from right to left. So an int const * becomes a pointer to a const int and not a const pointer to an int.
In some cases C/C++ allows the const keyword to be placed to the left of the type. Here are some examples:
Although C/C++ allows such definitions (which closely match the English language when reading the definitions from left to right), the compiler still reads the definitions according to the abovementioned procedure: from right to left. But putting const before what must be constant quickly introduces mismatches between what you intend to write and what the compiler decides you wrote. Consider pointers to pointers:
As a final note regarding pointer definitions: always write the pointer symbol (the *) as much as possible to the right. Attaching the pointer symbol to the type is tricky, as it strongly suggests a pointer type, which isn't the case. Here are some examples:
Bjarne Stroustrup's FAQ recommends only declaring one variable per line if using the C++ convention, to avoid this issue.
The same considerations apply to defining references and (C++) rvalue references:
More complicated declarations are encountered when using multidimensional arrays and references (or pointers) to pointers. Although it is sometimes argued [who?] that such declarations are confusing and error-prone and that they therefore should be avoided or be replaced by higher-level structures, it should be noted that the  procedure described at the top of this section can always be used without introducing ambiguities or confusion.
const can be declared both on function parameters and on variables (static or automatic, including global or local). The interpretation varies between uses. A const static variable (global variable or static local variable) is a constant, and may be used for data like mathematical constants, such as const double PI = 3.14159 – realistically longer, or overall compile-time parameters. A const automatic variable (non-static local variable) means that single assignment is happening, though a different value may be used each time, such as const int x_squared = x*x. A const parameter in pass-by-reference means that the referenced value is not modified – it is part of the contract – while a const parameter in pass-by-value (or the pointer itself, in pass-by-reference) does not add anything to the interface (as the value has been copied), but indicates that internally, the function does not modify the parameter (it is a single assignment). For this reason, some favor using const in parameters only for pass-by-reference, where it changes the contract, but not for pass-by-value, where it exposes the implementation.
In order to take advantage of the design by contract approach for user-defined types (structs and classes), which can have methods as well as member data, the programmer may tag instance methods as const if they don't modify the object's data members.
Applying the const qualifier to instance methods thus is an essential feature for const-correctness, and is not available in many other object-oriented languages such as Java and C# or in Microsoft's C++/CLI or Managed Extensions for C++.
While const methods can be called by const and non-const objects alike, non-const methods can only be invoked by non-const objects.
The const modifier on an instance method applies to the object pointed to by the ""this"" pointer, which is an implicit argument passed to all instance methods.
Thus having const methods is a way to apply const-correctness to the implicit ""this"" pointer argument just like other arguments.
This example illustrates:
In the above code, the implicit ""this"" pointer to Set() has the type ""C *const""; whereas the ""this"" pointer to Get() has type ""const C *const"", indicating that the method cannot modify its object through the ""this"" pointer.
Often the programmer will supply both a const and a non-const method with the same name (but possibly quite different uses) in a class to accommodate both types of callers. Consider:
The const-ness of the calling object determines which version of MyArray::Get() will be invoked and thus whether or not the caller is given a reference with which he can manipulate or only observe the private data in the object.
The two methods technically have different signatures because their ""this"" pointers have different types, allowing the compiler to choose the right one. (Returning a const reference to an int, instead of merely returning the int by value, may be overkill in the second method, but the same technique can be used for arbitrary types, as in the Standard Template Library.)
There are several loopholes to pure const-correctness in C and C++. They exist primarily for compatibility with existing code.
The first, which applies only to C++, is the use of const_cast, which allows the programmer to strip the const qualifier, making any object modifiable.
The necessity of stripping the qualifier arises when using existing code and libraries that cannot be modified but which are not const-correct. For instance, consider this code:
However, any attempt to modify an object that is itself declared const by means of const cast results in undefined behavior according to the ISO C++ Standard.
In the example above, if ptr references a global, local, or member variable declared as const, or an object allocated on the heap via new const int, the code is only correct if LibraryFunc really does not modify the value pointed to by ptr.
The C language has a need of a loophole because a certain situation exists.  Variables with static storage duration are allowed to be defined with an initial value.  However, the initializer can use only constants like string constants and other literals, and is not allowed to use non-constant elements like variable names, whether the initializer elements are declared const or not, or whether the static duration variable is being declared const or not.  There is a non-portable way to initialize a const variable that has static storage duration.  By carefully constructing a typecast on the left hand side of a later assignment, a const variable can be written to, effectively stripping away the const attribute and 'initializing' it with non-constant elements like other const variables and such.  Writing into a const variable this way may work as intended, but it causes undefined behavior and seriously contradicts const-correctness:
Another loophole applies both to C and C++. Specifically, the languages dictate that member pointers and references are ""shallow"" with respect to the const-ness of their owners — that is, a containing object that is const has all const members except that member pointees (and referees) are still mutable. To illustrate, consider this C++ code:
Although the object s passed to Foo() is constant, which makes all of its members constant, the pointee accessible through s.ptr is still modifiable, though this may not be desirable from the standpoint of const-correctness because s might solely own the pointee.
For this reason, Meyers argues that the default for member pointers and references should be ""deep"" const-ness, which could be overridden by a mutable qualifier when the pointee is not owned by the container, but this strategy would create compatibility issues with existing code.
Thus, for historical reasons[citation needed], this loophole remains open in C and C++.
The latter loophole can be closed by using a class to hide the pointer behind a const-correct interface, but such classes either don't support the usual copy semantics from a const object (implying that the containing class cannot be copied by the usual semantics either) or allow other loopholes by permitting the stripping of const-ness through inadvertent or intentional copying.
Finally, several functions in the C standard library violate const-correctness, as they accept a const pointer to a character string and return a non-const pointer to a part of the same string. strtol and strchr are among these functions.
Some implementations of the C++ standard library, such as Microsoft's try to close this loophole by providing two overloaded versions of some functions: a ""const"" version and a ""non-const"" version.
The use of the type system to express constancy leads to various complexities and problems, and has accordingly been criticized and not adopted outside the narrow C family of C, C++, and D. Java and C#, which are heavily influenced by C and C++, both explicitly rejected const-style type qualifiers, instead expressing constancy by keywords that apply to the identifier (final in Java, const and readonly in C#). Even within C and C++, the use of const varies significantly, with some projects and organizations using it consistently, and others avoiding it.
The const type qualifier causes difficulties when the logic of a function is agnostic to whether its input is constant or not, but returns a value which should be of the same qualified type as an input. In other words, for these functions, if the input is constant (const-qualified), the return value should be as well, but if the input is variable (not const-qualified), the return value should be as well. Because the type signature of these functions differs, it requires two functions (or potentially more, in case of multiple inputs) with the same logic – a form of generic programming.
This problem arises even for simple functions in the C standard library, notably strchr; this observation is credited by Ritchie to Tom Plum in the mid 1980s. The strchr function locates a character in a string; formally, it returns a pointer to the first occurrence of the character c in the string s, and in classic C (K&R C) its prototype is:
The strchr function does not modify the input string, but the return value is often used by the caller to modify the string, such as:
Thus on the one hand the input string can be const (since it is not modified by the function), and if the input string is const the return value should be as well – most simply because it might return exactly the input pointer, if the first character is a match – but on the other hand the return value should not be const if the original string was not const, since the caller may wish to use the pointer to modify the original string.
In C++ this is done via function overloading, typically implemented via a template, resulting in two functions, so that the return value has the same const-qualified type as the input:[lower-alpha 3]
These can in turn be defined by a template:
In D this is handled via the inout keyword, which acts as a wildcard for const, immutable, or unqualified (variable), yielding:[lower-alpha 4]
However, in C neither of these is possible, since C does not have function overloading, and instead this is handled by having a single function where the input is constant but the output is writable:
This allows idiomatic C code, but does strip the const qualifier if the input actually was const-qualified, violating type safety. This solution was proposed by Ritchie, and subsequently adopted. This difference is one of the failures of compatibility of C and C++.
In Version 2 of the D programming language, two keywords relating to const exist. The immutable keyword denotes data that cannot be modified through any reference.
The const keyword denotes a non-mutable view of mutable data.
Unlike C++ const, D const and immutable are ""deep"" or transitive, and anything reachable through a const or immutable object is const or immutable respectively.
Example of const vs. immutable in D
Example of transitive or deep const in D
const was introduced by Bjarne Stroustrup in C with Classes, the predecessor to C++, in 1981, and was originally called readonly. As to motivation, Stroustrup writes:
The first use, as a scoped and typed alternative to macros, was analogously fulfilled for function-like macros via the inline keyword. Constant pointers, and the * const notation, were suggested by Dennis Ritchie and so adopted.
const was then adopted in C as part of standardization, and appears in C89 (and subsequent versions) along with the other type qualifier, volatile. A further qualifier, noalias, was suggested at the December 1987 meeting of the X3J11 committee, but was rejected; its goal was ultimately fulfilled by the restrict keyword in C99. Ritchie was not very supportive of these additions, arguing that they did not ""carry their weight"", but ultimately did not argue for their removal from the standard.
D subsequently inherited const from C++, where it is known as a type constructor (not type qualifier) and added two further type constructors, immutable and inout, to handle related use cases.[lower-alpha 5]
Other languages do not follow C/C++ in having constancy part of the type, though they often have superficially similar constructs and may use the const keyword. Typically this is only used for constants (constant objects).
C# has a const keyword, but with radically different and simpler semantics: it means a compile-time constant, and is not part of the type.
Nim has a const keyword similar to that of C#: it also declares a compile-time constant rather than forming part of the type.  However, in Nim, a constant can be declared from any expression that can be evaluated at compile time.  In C#, only C# built-in types can be declared as const; user-defined types, including classes, structs, and arrays, cannot be const.
Java does not have const – it instead has final, which can be applied to local ""variable"" declarations and applies to the identifier, not the type. It has a different object-oriented use for object members, which is the origin of the name.
Interestingly, the Java language specification regards const as a reserved keyword – i.e., one that cannot be used as variable identifier – but assigns no semantics to it: it is a reserved word (it cannot be used in identifiers) but not a keyword (it has no special meaning). It is thought that the reservation of the keyword occurred to allow for an extension of the Java language to include C++-style const methods and pointer to const type.[citation needed] An enhancement request ticket for implementing const correctness exists in the Java Community Process, but was closed in 2005 on the basis that it was impossible to implement in a backwards-compatible fashion.
The contemporary Ada 83 independently had the notion of a constant object and a constant keyword,[lower-alpha 6] with input parameters and loop parameters being implicitly constant. Here the constant is a property of the object, not of the type.
Javascript has a const declaration that defines a block-scoped variable that cannot be reassigned nor redeclared. It defines a read-only reference to a variable that cannot be redefinded, but in some situations the value of the variable itself may potentially change, such as if the variable refers to an object and a property of it is altered."
"131","C++ doesn't have:
ALGOL 68 doesn't have:
if (x == y) { ... }
if (*x == *y) { ... }
    ( real x = (in x|(real x):x|888); code ); 
Assigning values into an A68 union variable is automatic, 
the type is ""tagged"" to the variable, but pulling the value back out is 
syntactically awkward as a conformity-clause is required.
ALGOL 68 example:
C/C++ example:
The net effect of ""type-tagging"" is that Algol68's strong typing
""half"" encroaches into the union.
A new mode (type) may be declared using a mode declaration:
This has the similar effect as the following C++ code:
Note that for ALGOL 68 only the newtype name appears to the left of the equality, and most notably the construction is made - and can be read - from left to right without regard to priorities."
"132","In the Java computer programming language, an annotation is a form of syntactic metadata that can be added to Java source code. Classes, methods, variables, parameters and packages may be annotated. Like Javadoc tags, Java annotations can be read from source files.  Unlike Javadoc tags, Java annotations can also be embedded in and read from class files generated by the compiler.  This allows annotations to be retained by Java VM at run-time and read via reflection. It is possible to create meta-annotations out of the existing ones in Java.
The Java platform has  various ad-hoc annotation mechanisms—for example, the transient modifier, or the @deprecated javadoc tag. JSR-175 introduced the general-purpose annotation (also known as metadata) facility to the Java Community Process in 2002; it gained approval in September 2004.
Annotations became available in the language itself beginning with version 1.5 of the JDK. The  apt tool provided a provisional interface for compile-time annotation processing in JDK version 1.5; JSR-269 formalized this, and it became integrated into the javac compiler in version 1.6.
Java defines a set of annotations that are built into the language. Of the seven standard annotations, three are part of java.lang, and the remaining four are imported from java.lang.annotation.
Annotations applied to Java code:
Annotations applied to other annotations (also known as ""Meta Annotations""):
Since Java 7, three additional annotations have been added to the language.
This example demonstrates the use of the @Override annotation. It instructs the compiler to check parent classes for matching methods. In this case, an error is generated because the gettype() method of class Cat  doesn't in fact override getType() of class Animal like is desired. If the @Override annotation was absent, a new method of name gettype() would be created in class Cat.
Annotation type declarations are similar to normal interface declarations. An at-sign (@) precedes the interface keyword. Each method declaration defines an element of the annotation type. Method declarations must not have any parameters or a throws clause. Return types are restricted to primitives, String, Class, enums, annotations, and arrays of the preceding types. Methods can have default values.
Annotations may include an optional list of key-value pairs:
Annotations themselves may be annotated to indicate where and when they can be used:
The compiler reserves a set of special annotations (including @Deprecated, @Override and @SuppressWarnings) for syntactic purposes.
Annotations are often used by frameworks as a way of conveniently applying behaviours to user-defined classes and methods that must otherwise be declared in an external source (such as an XML configuration file) or programmatically (with API calls). The following, for example, is an annotated JPA data class:
The annotations are not method calls and will not, by themselves, do anything. Rather, the class object is passed to the JPA implementation at run-time, which then extracts the annotations to generate an object-relational mapping.
A complete example is given below:
When Java source code is compiled, annotations can be processed by compiler plug-ins called annotation processors. Processors can produce informational messages or create additional Java source files or resources, which in turn may be compiled and processed, and also modify the annotated code itself. The Java compiler conditionally stores annotation metadata in the class files, if the annotation has a RetentionPolicy of CLASS or RUNTIME. Later, the JVM or other programs can look for the metadata to determine how to interact with the program elements or change their behavior.
In addition to processing an annotation using an annotation processor, a Java programmer can write their own code that uses reflections to process the annotation. Java SE 5 supports a new interface that is defined in the java.lang.reflect package. This package contains the interface called AnnotatedElement that is implemented by the Java reflection classes including Class, Constructor, Field, Method, and Package. The implementations of this interface are used to represent an annotated element of the program currently running in the Java Virtual Machine. This interface allows annotations to be read reflectively.
The AnnotatedElement interface provides access to annotations having RUNTIME retention. This access is provided by the getAnnotation, getAnnotations, and isAnnotationPresent methods. Because annotation types are compiled and stored in byte code files just like classes, the annotations returned by these methods can be queried just like any regular Java object. A complete example of processing an annotation is provided below:"
"133","
Vala is an object-oriented programming language with a self-hosting compiler that generates C code and uses the GObject system.
Vala is syntactically similar to C# and includes several features such as: anonymous functions, signals, properties, generics, assisted memory management, exception handling, type inference, and foreach statements. Its developers Jürg Billeter and Raffaele Sandrini aim to bring these features to the plain C runtime with little overhead and no special runtime support by targeting the GObject object system. Rather than compiling directly to machine code or assembly language, it compiles to a lower level intermediate language. It source-to-source compiles to C, which is then compiled with a C compiler for a given platform, such as GCC.
For memory management, the GObject system provides reference counting. In C, a programmer must manually manage adding and removing references, but in Vala, managing such reference counts is automated if a programmer uses the language's built-in reference types rather than plain pointers.
Using functionality from native code libraries requires writing vapi files, defining the library interfacing. Writing these interface definitions is well-documented for C libraries, especially when based on GObject. However, C++ libraries are not supported. Vapi files are provided for a large portion of the GNOME platform, including GTK+.
Vala was conceived by Jürg Billeter and was implemented by him and Raffaele Sandrini, finishing a self-hosting compiler in May 2006.
A simple ""Hello, World!"" Vala program:
A more complex version, showing some of Vala's object-oriented features:
An example using GTK+ to create a GUI ""Hello, World!"" program (see also GTK+ hello world):
The last example needs an extra parameter to compile on GNOME 3 platforms:
This is the converted C code:
There are various projects in various states of stability in order to provide syntax highlighting and other text editor/IDE support for Vala:"
"134"," (Learn how and when to remove this template message)

getopt is a C library function used to parse command-line options.
It is also the name of a Unix program for parsing command line arguments in shell scripts.
A long-standing issue with command line programs was how to specify options;[citation needed]
early programs used many ways of doing so, including single character options (-a),
multiple options specified together (-abc is equivalent to -a -b -c),
multicharacter options (-inum), 
options with arguments (-a arg, -inum 3, -a=arg), and
different prefix characters (-a, +b, /c).
The getopt function was written to be a standard mechanism
that all programs could use to parse command-line options 
so that there would be a common interface that everyone could depend on.
As such, the original authors picked out of the variations support for single character options,
multiple options specified together,
and options with arguments (-a arg or -aarg), all controllable by an option string.
getopt dates back to at least 1980 and was first published by AT&T at the 1985 UNIFORUM conference in Dallas, Texas, with the
intent for it to be available in the public domain.[citation needed]
Versions of it were subsequently picked up by other flavors of Unix (4.3BSD, Linux, etc.). 
It is specified in the POSIX.2 standard as part of the unistd.h header file.
Derivatives of getopt have been created for many programming languages to parse command-line options.
A GNU extension, getopt_long, allows parsing of more readable, multicharacter options, which are introduced by two dashes instead of one.
The choice of two dashes allows multicharacter options (--inum) to be differentiated from single character options specified together (-abc).
The GNU extension also allows an alternative format for options with arguments: --name=arg.
getopt is a system dependent function. The implementation of getopt in GNU C Library does permute the contents of the argument vector as it scans, so that eventually all the non-option arguments are at the end. On the contrary, the implementation of getopt in BSD C Library does not permute the argument vector and returns -1 if it encounters a non-option argument.
The Unix shell command program called getopt can be used for parsing command line arguments in shell scripts.
There are two major versions of the getopt shell command program. The original version was implemented by Unix System Laboratories.  Most Linux based operating systems come with a version, getopt(1), that supports enhancements (such as long option names).
Command line parsing in shell scripts can also be performed using the getopts command, also designed and implemented by Unix System Laboratories as a replacement for getopt. This version is built into many shells and standardized by POSIX (contrary to getopt). The syntax for using getopts is very different from the syntax of getopt. Although getopts has more features than the original getopt program, its implementation as specified by POSIX does not have further enhancements (though some implementations like ksh93's do).  For example, implementations of getopts other than ksh93's usually do not support long option names.  The getopts command is a built-in Unix shell command, unlike getopt, which is an external command line program. Being a built-in Unix shell command allows it to interact more seamlessly with other built-in shell options and variables.
The D programming language has a getopt module in the standard library.
Haskell comes with System.Console.GetOpt, which is essentially a Haskell port of the GNU getopt library.
There is no implementation of getopt in the Java standard library. Several open source modules exist, including gnu.getopt.Getopt, which is ported from GNU getopt, and Apache Commons CLI.
Lisp has many different dialects with no common standard library. There are some third party implementations of getopt for some dialects of Lisp. Common Lisp has a prominent third party implementation.
Free Pascal has its own implementation as one of its standard units named GetOpts. It is supported on all platforms.
The Perl programming language has two separate derivatives of getopt in its standard library: Getopt::Long and Getopt::Std.
PHP has a getopt() function.
Python contains a module in its standard library based on C's getopt and GNU extensions. Python's standard library also contains other modules to parse options that are more convenient to use.
Ruby has an implementation of getopt_long in its standard library, GetoptLong. Ruby also has modules in its standard library with a more sophisticated and convenient interface. A third party implementation of the original getopt interface is available.
The .NET Framework does not have getopt functionality in its standard library. Third-party implementations are available."
"135","Different Command-line argument parsing methods are used by different programming languages to parse command-line arguments.
C uses argv to process command-line arguments.
An example of C argument parsing would be:
An example of Java argument parsing would be:
Bash uses $1 $2 ... ($0 is the script filename).
or
Perl uses $ARGV.
or
AWK uses ARGV also.
PHP uses argc as a count of arguments and argv as an array containing the values of the arguments. To create an array from command-line arguments in the -foo:bar format, the following might be used:
PHP can also use getopt().
Python uses sys.argv, e.g.:
Python also has a module called argparse in the standard library for parsing command-line arguments.
Racket uses a current-command-line-arguments parameter, and provides a racket/cmdline library for parsing these arguments.  Example:
The library parses long and short flags, handles arguments, allows combining short flags, and handles -h and --help automatically:"
"136","
ActionScript is an object-oriented programming language originally developed by Macromedia Inc. (later acquired by Adobe Systems). It is a derivation of HyperTalk, the scripting language for HyperCard. It is now a dialect of ECMAScript (meaning it is a superset of the syntax and semantics of the language more widely known as JavaScript), though it originally arose as a sibling, both being influenced by HyperTalk.
ActionScript is used primarily for the development of websites and software targeting the Adobe Flash Player platform, used on Web pages in the form of embedded SWF files.
ActionScript 3 is also used with Adobe AIR system for the development of desktop and mobile applications. The language itself is open-source in that its specification is offered free of charge and both an open source compiler (as part of Apache Flex) and open source virtual machine (Mozilla Tamarin) are available.
ActionScript is also used with Scaleform GFx for the development of 3D video game user interfaces and HUDs.
ActionScript was initially designed for controlling simple 2D vector animations made in Adobe Flash (formerly Macromedia Flash). Initially focused on animation, early versions of Flash content offered few interactivity features and thus had very limited scripting capability. Later versions added functionality allowing for the creation of Web-based games and rich Internet applications with streaming media (such as video and audio). Today, ActionScript is suitable for mobile development through Adobe AIR, use in some database applications, and in basic robotics, as with the Make Controller Kit.
Flash MX 2004 introduced ActionScript 2.0, a scripting language more suited to the development of Flash applications. It is often possible to save time by scripting something rather than animating it, which usually also enables a higher level of flexibility when editing.
Since the arrival of the Flash Player 9 alpha (in 2006) a newer version of ActionScript has been released, ActionScript 3.0. This version of the language is intended to be compiled and run on a version of the ActionScript Virtual Machine that has been itself completely re-written from the ground up (dubbed AVM2). Because of this, code written in ActionScript 3.0 is generally targeted for Flash Player 9 and higher and will not work in previous versions. At the same time, ActionScript 3.0 executes up to 10 times faster than legacy ActionScript code due to the Just-In-Time compiler enhancements.
Flash libraries can be used with the XML capabilities of the browser to render rich content in the browser. This technology is known as Asynchronous Flash and XML, much like AJAX. Adobe offers its Flex product line to meet the demand for Rich Internet Applications built on the Flash runtime, with behaviors and programming done in ActionScript. ActionScript 3.0 forms the foundation of the Flex 2 API.
ActionScript started as an object-oriented language for Macromedia's Flash authoring tool, now developed by Adobe Systems as Adobe Flash. The first three versions of the Flash authoring tool provided limited interactivity features. Early Flash developers could attach a simple command, called an ""action"", to a button or a frame. The set of actions was basic navigation controls, with commands such as ""play"", ""stop"", ""getURL"", and ""gotoAndPlay"".
With the release of Flash 4 in 1999, this simple set of actions became a small scripting language. New capabilities introduced for Flash 4 included variables, expressions, operators, if statements, and loops. Although referred to internally as ""ActionScript"", the Flash 4 user manual and marketing documents continued to use the term ""actions"" to describe this set of commands.
2000–2004: ActionScript ""1.0"" 
With the release of Flash 5 in September 2000, the ""actions"" from Flash 4 were enhanced once more and named ""ActionScript"" for the first time. This was the first version of ActionScript with influences from JavaScript and the ECMA-262 (Third Edition) standard, supporting the said standard's object model and many of its core data types. Local variables may be declared with the var statement, and user-defined functions with parameter passing and return values can also be created. Notably, ActionScript could now also be typed with a text editor rather than being assembled by choosing actions from drop-down lists and dialog box controls. With the next release of its authoring tool, Flash MX, and its corresponding player, Flash Player 6, the language remained essentially unchanged; there were only minor changes, such as the addition of the switch statement and the ""strict equality"" (===) operator, which brought it closer to being ECMA-262-compliant. Two important features of ActionScript that distinguish it from later versions are its loose type system and its reliance on prototype-based inheritance. Loose typing refers to the ability of a variable to hold any type of data. This allows for rapid script development and is particularly well-suited for small-scale scripting projects. Prototype-based inheritance is the ActionScript 1.0 mechanism for code reuse and object-oriented programming. Instead of a class keyword that defines common characteristics of a class, ActionScript 1.0 uses a special object that serves as a ""prototype"" for a class of objects. All common characteristics of a class are defined in the class's prototype object and every instance of that class contains a link to that prototype object.
2003–2006: ActionScript 2.0 
The next major revision of the language, ActionScript 2.0, was introduced in September 2003 with the release of Flash MX 2004 and its corresponding player, Flash Player 7. In response to user demand for a language better equipped for larger and more complex applications, ActionScript 2.0 featured compile-time type checking and class-based syntax, such as the keywords class and extends.
(While this allowed for a more structured object-oriented programming approach, the code would still be compiled to ActionScript 1.0 bytecode, allowing it to be used on the preceding Flash Player 6 as well.
In other words, the class-based inheritance syntax was a layer on top of the existing prototype-based system.) With ActionScript 2.0, developers could constrain variables to a specific type by adding a type annotation so that type mismatch errors could be found at compile-time.
ActionScript 2.0 also introduced class-based inheritance syntax so that developers could create classes and interfaces, much as they would in class-based languages such as Java and C++. This version conformed partially to the ECMAScript Fourth Edition draft specification.
2006–today: ActionScript 3.0 
In June 2006, ActionScript 3.0 debuted with Adobe Flex 2.0 and its corresponding player, Flash Player 9. ActionScript 3.0 was a fundamental restructuring of the language, so much so that it uses an entirely different virtual machine. Flash Player 9 contains two virtual machines, AVM1 for code written in ActionScript 1.0 and 2.0, and AVM2 for content written in ActionScript 3.0. ActionScript 3.0 added limited support for hardware acceleration (DirectX, OpenGL).
The update to the language introduced several new features:
Adobe AIR supports ActionScript, in addition to some extended contents, such as the Stage3D engine Adobe has developed. The number of APIs (Application programming interfaces) available to ActionScript 3.0 has also risen dramatically.
ActionScript code is free form and thus may be created with whichever amount or style of whitespace that the author desires. The basic 
syntax is derived from ECMAScript.
The following code, which works in any compliant player, creates a text field at depth 0, at position (0, 0) on the screen (measured in pixels), that is 100 pixels wide and high. Then the text parameter is set to the ""Hello, world"" string, and it is automatically displayed in the player:
When writing external ActionScript 2.0 class files the above example could be written in a file named Greeter.as as following.
ActionScript 3.0 has a similar syntax to ActionScript 2.0 but a different set of APIs for creating objects. Compare the script below to the previous ActionScript 2.0 version:
Minimal ActionScript 3.0 programs may be somewhat larger and more complicated due to the increased separation of the programming language and the Flash IDE.
Presume the following file to be Greeter.as:
(See also: Sprite.)
ActionScript 3 can also be used in MXML files when using Apache's Flex framework:
ActionScript primarily consists of ""fundamental"" or ""simple"" data types which are used to create other data types. These data types are very similar to Java data types. Since ActionScript 3 was a complete rewrite of ActionScript 2, the data types and their inheritances have changed.
ActionScript 2 top level data types
ActionScript 2 complex data types
There are additional ""complex"" data types. These are more processor and memory intensive and consist of many ""simple"" data types. For AS2, some of these data types are:
ActionScript 3 primitive (prime) data types
ActionScript 3 some complex data types
The basic syntax is:
So in order to make an empty Object:
Or, in an informal way:
Some types are automatically put in place:
Unlike some object-oriented languages, ActionScript makes no distinction between primitive types and reference types. In ActionScript, all variables are reference types. However, objects that belong to the primitive data types, which includes Boolean, Number, int, uint, and String, are immutable.
So if a variable of a supposedly primitive type, e.g. an integer is passed to a function, altering that variable inside the function will not alter the original variable, as a new int Object is created when inside the function. If a variable of another (not primitive) datatype, e.g. XML is passed to a function, altering that variable inside the function will alter the original variable as well, as no new XML Object is created.
Some data types can be assigned values with literals:
A reference in ActionScript is a pointer to an instance of a class. A reference stores the memory address of an object - operations against references will follow the value of the reference to the memory address of the object and carry out the operation on that object. All objects in ActionScript are accessed through references instead of being accessed directly.
Only references to an object may be removed by using the ""delete"" keyword.  Removal of actual objects and data is done by the Flash Player garbage collector which checks for any existing references in the Flash memory space. If none are found (no other reference is made to the orphaned object), it is removed from memory.  For this reason, memory management in ActionScript requires careful application development planning.
As with all intermediate language compiled code such as Flash and Microsoft .NET, once an SWF file is saved locally, it can be decompiled into its source code and assets. Some decompilers are capable of nearly full reconstruction of the original source file, down to the actual code that was used during creation (although results vary on a case-by-case basis).
In opposition to the decompilers, ActionScript obfuscators have been introduced, which transform code into a form that breaks decompiler output while preserving the functionality and structure of the program. Higher-quality obfuscators implement lexical transformations such as identifier renaming, control flow transformation, and data abstraction transformation which collectively make it harder for decompilers to generate output likely to be useful to a human. Less robust obfuscators insert traps for decompilers. Such obfuscators either cause the decompiler software to crash unexpectedly or to generate unintelligible source code.
The following is an example of ActionScript 3.0 code generated by a decompiler program, before and after obfuscation.
Code before obfuscation:
Code after obfuscation:"
"137","
Kotlin is a statically-typed programming language that runs on the Java virtual machine and also can be compiled to JavaScript source code or use the LLVM compiler infrastructure. Its primary development is from a team of JetBrains programmers based in Saint Petersburg, Russia. While the syntax is not compatible with Java, Kotlin is designed to interoperate with Java code and is reliant on Java code from the existing Java Class Library, such as the collections framework. Kotlin uses aggressive type inference to determine the type of values and expressions for which type has been left unstated. This reduces language verbosity relative to Java, which demands often entirely redundant type specifications.
As of Android Studio 3.0 Kotlin is a fully supported programming language on Android and lets the user choose between targeting Java 6- or Java 8-compatible bytecode.
In July 2011 JetBrains unveiled Project Kotlin, a new language for the JVM, which had been under development for a year. JetBrains lead Dmitry Jemerov said that most languages did not have the features they were looking for, with the exception of Scala. However, he cited the slow compile time of Scala as an obvious deficiency. One of the stated goals of Kotlin is to compile as quickly as Java. In February 2012, JetBrains open sourced the project under the Apache 2 license.
The name comes from Kotlin Island, near St. Petersburg. Andrey Breslav mentioned that the team decided to name it after an island just like Java was named after the Indonesian island of Java (though the programming language Java was perhaps named after the coffee.)
JetBrains hopes that the new language will drive IntelliJ IDEA sales.
Kotlin v1.0 was released on February 15, 2016. This is considered to be the first officially stable release and JetBrains has committed to long-term backwards compatibility starting with this version.
At Google I/O 2017, Google announced first-class support for Kotlin on Android.
Kotlin v1.2 was released on November 28, 2017. Sharing Code between JVM and Javascript platforms feature was newly added to this release.
Development lead Andrey Breslav has said that Kotlin is designed to be an industrial-strength object-oriented language, and a ""better language"" than Java, but still be fully interoperable with Java code, allowing companies to make a gradual migration from Java to Kotlin.
Kotlin variable declarations and parameter lists have the data type come after the variable name (and with a colon separator), similar to Pascal. As in Scala and Apache Groovy, semicolons are optional as a statement terminator; in most cases a newline is sufficient for the compiler to deduce that the statement has ended.
In addition to the classes and methods (called member functions in Kotlin) of object-oriented programming, Kotlin also supports procedural programming with the use of functions. As in C and C++, the entry point to a Kotlin program is a function named ""main"", which is passed an array containing any command line arguments. Perl and Unix/Linux shell script-style string interpolation is supported. Type inference is also supported.
Hello, world! example
Variables in Kotlin can be immutable, declared with the val keyword or mutable, declared with the var keyword. 
Kotlin makes a distinction between nullable and non-nullable data types. All nullable objects must be declared with a ""?"" postfix after the type name. Operations on nullable objects need special care from developers: null-check must be performed before using the value. Kotlin provides null-safe operators to help developers:
An example of the use of the safe navigation operator:
Kotlin provides support for higher order functions and Anonymous functions or lambdas. 
Lambdas are declared using braces, {  } . If a lambda takes parameters, they are declared within the braces and followed by the -> operator.
One of the obvious applications of Kotlin is Android development. The platform was stuck on Java 7 for a while (with some contemporary language features made accessible through the use of Retrolambda or the Jack toolchain) and Kotlin introduces many improvements for programmers such as null-pointer safety, extension functions and infix notation. Accompanied by full Java compatibility and good IDE support (Android Studio) it is intended to improve code readability, give an easier way to extend Android SDK classes and speed up development.
Kotlin was announced as an official Android development language at Google I/O 2017. It became the third language fully supported for Android, in addition to Java and C++.
According to the Kotlin website, Prezi is using Kotlin in the backend. DripStat has done a writeup of their experience with Kotlin.
According to Jetbrains blog, Kotlin is used by Amazon Web Services, Pinterest, Coursera, Netflix, Uber, Square, Trello, Basecamp, and others. Corda, a distributed ledger developed by a consortium of well-known banks (such as Goldman Sachs, Wells Fargo, J.P. Morgan, Deutsche Bank, UBS, HSBC, BNP Paribas, Société Générale), has over 90% Kotlin in its codebase.

According to Google, Kotlin has already been adopted by several major developers — Expedia, Flipboard, Pinterest, Square, and others — for their Android production apps."
"138","

Safari is a web browser developed by Apple based on the WebKit engine. First released in 2003 with Mac OS X Panther, a mobile version has been included in iOS devices since the introduction of the iPhone in 2007. It is the default browser on Apple devices. A Windows version, now discontinued, was available from 2007 to 2012.
Until 1997, Apple Macintosh computers were shipped with the Netscape Navigator and Cyberdog web browsers only. Internet Explorer for Mac was later included as the default web browser for Mac OS 8.1 and onwards, as part of a five-year agreement between Apple and Microsoft. During that time, Microsoft released three major versions of Internet Explorer for Mac that were bundled with Mac OS 8 and Mac OS 9, though Apple continued to include Netscape Navigator as an alternative. Microsoft ultimately released a Mac OS X edition of Internet Explorer for Mac, which was included as the default browser in all Mac OS X releases from Mac OS X DP4 up to and including Mac OS X v10.2.
On January 7, 2003, at Macworld San Francisco, Steve Jobs announced that Apple had developed their own web browser, called Safari. It was based on Apple's internal fork of the KHTML rendering engine, called WebKit. Apple released the first beta version for OS X that day. A number of official and unofficial beta versions followed, until version 1.0 was released on June 23, 2003. Initially only available as a separate download for Mac OS X v10.2, it was included with the Mac OS X v10.3 release on October 24, 2003 as the default browser, with Internet Explorer for Mac included only as an alternative browser. 1.0.3, released on August 13, 2004 was the last version to support Mac OS X v10.2, while 1.3.2, released on January 12, 2006 was the last version to support Mac OS X v10.3. However, 10.3 received security updates through 2007.
In April 2005, Dave Hyatt, one of the Safari developers at Apple, documented his study by fixing specific bugs in Safari, thereby enabling it to pass the Acid2 test developed by the Web Standards Project. On April 27, 2005, he announced that his development version of Safari now passed the test, making it the first web browser to do so.
Safari 2.0 was released on April 29, 2005, as the only web browser included with Mac OS X v10.4. This version was touted by Apple as possessing a 1.8x speed boost over version 1.2.4, but did not yet include the Acid2 bug fixes. The necessary changes were initially unavailable to end-users unless they downloaded and compiled the WebKit source code themselves or ran one of the nightly automated builds available at OpenDarwin.org. Apple eventually released version 2.0.2 of Safari, which included the modifications required to pass Acid2, on October 31, 2005.
In June 2005, after some criticism from KHTML developers over lack of access to change logs, Apple moved the development source code and bug tracking of WebCore and JavaScriptCore to OpenDarwin.org. WebKit itself was also released as open source. The source code for non-renderer aspects of the browser, such as its GUI elements, remains proprietary.
The final stable version of Safari 2, Safari 2.0.4, was released on January 10, 2006 for Mac OS X. It was only available as part of Mac OS X Update 10.4.4. This version addresses layout and CPU usage issues, among others. Safari 2.0.4 was the last version to be released exclusively on Mac OS X until version 6 in 2012.
On January 9, 2007, at Macworld SF, Jobs announced Apple's iPhone, which would use a mobile version of the Safari browser.
On June 11, 2007, at the Apple Worldwide Developers Conference, Jobs announced Safari 3 for Mac OS X v10.5, Windows XP, and Windows Vista. During the announcement, he ran a benchmark based on the iBench browser test suite comparing the most popular Windows browsers, hence claiming that Safari was the fastest browser. Later third-party tests of HTTP load times would support Apple's claim that Safari 3 was indeed the fastest browser on the Windows platform in terms of initial data loading over the Internet, though it was found to be only negligibly faster than Internet Explorer 7 and Mozilla Firefox when loading static content from local cache.
The initial Safari 3 beta version for Windows, released on the same day as its announcement at WWDC 2007, had several known bugs and a zero day exploit that allowed remote execution. The addressed bugs were then corrected by Apple three days later on June 14, 2007, in version 3.0.1 for Windows. On June 22, 2007, Apple released Safari 3.0.2 to address some bugs, performance issues and other security issues. Safari 3.0.2 for Windows handles some fonts that are missing in the browser but already installed on Windows computers, such as Tahoma, Trebuchet MS, and others.
The iPhone was formally released on June 29, 2007. It includes a version of Safari based on the same WebKit rendering engine as the desktop version, but with a modified feature set better suited for a mobile device. The version number of Safari as reported in its user agent string is 3.0, in line with the contemporary desktop versions of Safari.
The first stable, non-beta release of Safari for Windows, Safari 3.1, was offered as a free download on March 18, 2008. In June 2008, Apple released version 3.1.2, addressing a security vulnerability in the Windows version where visiting a malicious web site could force a download of executable files and execute them on the user's desktop.
Safari 3.2, released on November 13, 2008, introduced anti-phishing features using Google Safe Browsing and Extended Validation Certificate support. The final version of Safari 3 is 3.2.3, released on May 12, 2009.
On June 2, 2008, the WebKit development team announced SquirrelFish, a new JavaScript engine that vastly improves Safari's speed at interpreting scripts. The engine is one of the new features in Safari 4, released to developers on June 11, 2008. The new JavaScript engine quickly evolved into SquirrelFish Extreme, featuring even further improved performance over SquirrelFish, and was eventually marketed as Nitro. A public beta of Safari 4 was released on February 24, 2009, with new features such as the Top Sites tool (similar to Opera's Speed Dial feature), which displays the user's most visited sites on a 3D wall.Cover Flow, a feature of Mac OS X and iTunes, was also implemented in Safari. In the public beta versions, tabs were placed in the title bar of the window, similar to Google Chrome. The tab bar was moved back to its original location, below the URL bar, in the final release. The Windows version adopted a native Windows theme, rather than the previously employed Mac OS X-style interface. Also Apple removed the blue progress bar located in the address bar (later reinstated in Safari 5). Safari 4.0.1 was released for Mac on June 17 and fixed problems with Faces in iPhoto '09. Safari 4 in Mac OS X v10.6 ""Snow Leopard"" has 64-bit support, which can make JavaScript loading up to 50% faster. It also has built-in crash resistance unique to Snow Leopard; crash resistance will keep the browser intact if a plug-in like Flash player crashes, such that the other tabs or windows will be unaffected. Safari 4.0.4, released on November 11, 2009 for both OS X and Windows, further improves JavaScript performance.
Safari was one of the twelve browsers offered to EU users of Microsoft Windows in 2010. It was one of the five browsers displayed on the first page of browser choices along with Chrome, Firefox, Internet Explorer and Opera.
Beginning with Safari 4, the address bar has been completely revamped:
Safari on Mac OS X and Windows was made to look more similar to Safari on iPhone than previous versions.
Safari 4 also includes the following new features:
Apple released Safari 5 on June 7, 2010, featuring the new Safari Reader for reading articles on the web without distraction (based on Arc90's Readability tool), and a 30 percent JavaScript performance increase over Safari 4. Safari 5 includes improved developer tools and supports more than a dozen new HTML5 technologies, focused on interoperability. With Safari 5, developers can now create secure Safari Extensions to customize and enhance the browsing experience. Apple also re-added the progress bar behind the address bar in this release. Safari 5.0.1 enabled the Extensions PrefPane by default; previously, users had to enable it via the Debug menu.
Apple also released Safari 4.1 concurrently with Safari 5, exclusively for Mac OS X Tiger. The update included the majority of the features and security enhancements found in Safari 5. It did not, however, include Safari Reader or Safari Extensions.
Together with Mac OS X 10.7 Lion, Apple released Safari 5.1 for both Windows and Mac on July 20, 2011, with the new function 'Reading List' and a faster browsing experience. Apple simultaneously released Safari 5.0.6 for Mac OS X 10.5 Leopard, excluding Leopard users from the new functions in Safari 5.1.
Safari 5.1.7 has become the last version of Safari developed for Windows.
Safari 5 includes the following new features:
Additionally, the blue inline progress bar has returned to the address bar, in addition to the spinning bezel and loading indicator introduced in Safari 4. Top Sites view now has a button to switch to Full History Search. Other features include Extension builder for developers of Safari Extensions, which are built using web standards such as HTML5, CSS3, and JavaScript.
Safari 6.0 was previously known as Safari 5.2 until Apple announced the change at WWDC 2012. The stable release of Safari 6 coincided with the release of OS X Mountain Lion on July 25, 2012, and is integrated into the OS. As Apple integrated it with Mountain Lion, it is no longer available for download from the Apple website or other sources. Apple released Safari 6 via Software Update for users of OS X Lion. It has not been released for OS X versions prior to Lion or for Windows. Regarding the unavailability of Safari 6 on Windows, Apple has stated ""Safari 6 is available for Mountain Lion and Lion. Safari 5 continues to be available for Windows."" Microsoft removed Safari from its BrowserChoice page.
On June 11, 2012, Apple released a developer preview of Safari 6.0 with a feature called iCloud Tabs, which allows users to 'sync' their open tabs with any iOS or other OS X device running the latest software. Safari 6 also included new privacy features, including an ""Ask websites not to track me"" preference, and the ability for websites to send OS X 10.8 Mountain Lion users notifications, although it removed RSS support. Safari 6 has the Share Sheets capability in OS X Mountain Lion. The Share Sheet options are: Add to Reading List, Add Bookmark, Email this Page, Message, Twitter and Facebook. Users can now see tabs with full page previews available.
Safari 6 introduced the following features, many of which are only available on OS X 10.8 Mountain Lion:
Additionally various features were removed, including, but not limited to, Activity Window, separate Download Window, direct support for RSS feeds in the URL field and bookmarks. The separate search field is also no longer available as a toolbar configuration option.
Announced at Apple's Worldwide Developer Conference (WWDC) on June 10, 2013, the Safari 7/6.1 developer preview brought improvements in JavaScript performance and memory usage, as well as a new look for Top Sites and the Sidebar, and a new Shared Links feature. Additionally, a new Power Saver feature pauses Plugins which are not in use. Safari 7 for OS X Mavericks and Safari 6.1 (for Lion and Mountain Lion) were released along with OS X Mavericks in an Apple special event on October 22, 2013.
Safari 8 was announced at WWDC 2014 and released with OS X Yosemite. It included WebGL support, stronger privacy features, increased speed and efficiency, enhanced iCloud integration, and updated design.
Safari 8 introduced the following features, available on OS X Yosemite:
Safari 9 was announced at WWDC 2015 and released with OS X El Capitan. It included muting tabs and pinned tabs.
Safari 10 was released alongside macOS Sierra 10.12 for OS X Yosemite and OS X El Capitan. It does not include all of the new features available in macOS Sierra, like Apple Pay on the web and picture-in-picture support for videos, but the update includes the following new functions:
Safari 10 also includes a number of security updates, including fixes for six WebKit vulnerabilities and issues related to Reader and Tabs.
Safari 11 was released as a part of macOS High Sierra but was also made available for OS X El Capitan and macOS Sierra on September 19, 2017. Safari 11 includes several new features such a Intelligent Tracking Prevention feature which prevents websites from cross-site tracking.
Safari Technology Preview was first released alongside OS X El Capitan 10.11.4.  Safari Technology Preview releases include the latest version of WebKit, incorporating Web technologies to be incorporated in future stable releases of Safari, so that developers and users can install the Technology Preview release on a Mac, test those features, and provide feedback.
On macOS, Safari is a Cocoa application. It uses Apple's WebKit for rendering web pages and running JavaScript. WebKit consists of WebCore (based on Konqueror's KHTML engine) and JavaScriptCore (originally based on KDE's JavaScript engine, named KJS). Like KHTML and KJS, WebCore and JavaScriptCore are free software and are released under the terms of the GNU Lesser General Public License. Some Apple improvements to the KHTML code are merged back into the Konqueror project. Apple also releases additional code under an open source 2-clause BSD-like license.
Until Safari 6.0, it included a built-in web feed aggregator that supported the RSS and Atom standards. Current features include Private Browsing (a mode in which no record of information about the user's web activity is retained by the browser), an ""Ask websites not to track me"" privacy setting, the ability to archive web content in WebArchive format, the ability to email complete web pages directly from a browser menu, the ability to search bookmarks, and the ability to share tabs between all Mac and iOS devices running appropriate versions of software via an iCloud account.
iOS-specific features for Safari enable:
WebKit2 is a multiprocess API for WebKit, where the web-content is handled by a separate process than the application using WebKit. Apple announced WebKit2 in April 2010. Safari for OS X switched to the new API with version 5.1. Safari for iOS switched to WebKit2 with iOS 8.
Apple maintains a plugin blacklist that it can remotely update to prevent potentially dangerous or vulnerable plug-ins from running on Safari. So far, Apple has blocked versions of Flash and Java.
The license has common terms against reverse engineering, copying and sub-licensing, except parts that are open source, and it disclaims warranties and liability.
Apple tracks use of the browser. Windows users may not opt out of tracking, since their license omits the opening If clause. Other users may opt out, and all users can opt out of location tracking by not using location services. ""If you choose to allow diagnostic and usage collection, you agree that Apple and its subsidiaries and agents may collect... usage and related information... to provide ... services to you (if any) related to the Apple Software... in a form that does not personally identify you... Apple may also provide any such partner or third party developer with a subset of diagnostic information that is relevant to that partner’s or developer’s software... Apple and its partners, licensees, third party developers and website may transmit, collect, maintain, process and use your location data... and location search queries... in a form that does not personally identify you ... You may withdraw this consent at any time...""
Apple thinks ""personal"" does not cover ""unique device identifiers"" such as serial number, cookie number, or IP address, so they use these where allowed by law. ""We may collect, use, transfer, and disclose non-personal information for any purpose. The following are some examples of non-personal information that we collect ... unique device identifier... We treat information collected by cookies and other technologies as non‑personal information. However, to the extent that Internet Protocol (IP) addresses or similar identifiers are considered personal information by local law, we also treat these identifiers as personal information.""
In September 2017 Apple announced that it will use artificial intelligence (AI) to reduce the ability of advertisers to track Safari users as they browse the web. Cookies used for tracking will be allowed for 24 hours, then disabled, unless AI judges the user wants the cookie. Major advertising groups objected, saying it will reduce the free services supported by advertising, while other experts praised the change.
In the PWN2OWN contest at the 2008 CanSecWest security conference in Vancouver, British Columbia, an exploit of Safari caused Mac OS X to be the first OS to fall in a hacking competition. Participants competed to find a way to read the contents of a file located on the user's desktop in one of three operating systems: Mac OS X Leopard, Windows Vista SP1, and Ubuntu 7.10. On the second day of the contest, when users were allowed to physically interact with the computers (the prior day permitted only network attacks), Charlie Miller compromised Mac OS X through an unpatched vulnerability of the PCRE library used by Safari. Miller was aware of the flaw before the conference and worked to exploit it unannounced, as is the common approach in these contests. The exploited vulnerability and other flaws were patched in Safari 3.1.1.
In the 2009 PWN2OWN contest, Charlie Miller performed another exploit of Safari to hack into a Mac. Miller again acknowledged that he knew about the security flaw before the competition and had done considerable research and preparation work on the exploit. Apple released a patch for this exploit and others on May 12, 2009 with Safari 3.2.3.
Safari 6.0 requires a Mac running Mac OS X v10.7.4 or later. Safari 5.1.7 requires a Mac running Mac OS X v10.6.8 or any PC running Windows XP Service Pack 2 or later, Windows Vista, or Windows 7. Safari 5.0.6 requires a Mac running on Mac OS X 10.5.8.
The version of Safari included in Mac OS X v10.6 (and later versions) is compiled for 64-bit architecture. Apple claims that running Safari in 64-bit mode will increase rendering speeds by up to 50%.
On 64-bit devices, iOS and its stock apps are 64-bit builds including Safari.
An earlier version of Apple Software Update (bundled with Safari, QuickTime, and iTunes for Microsoft Windows) selected Safari for installation from a list of Apple programs to download by default, even when a pre-existing installation of Safari was not detected on a user's machine. John Lilly, former CEO of Mozilla, stated that Apple's use of its updating software to promote its other products was ""a bad practice and should stop."" He argued that the practice ""borders on malware distribution practices"" and ""undermines the trust that we're all trying to build with users."" Apple spokesman Bill Evans sidestepped Lilly's statement, saying that Apple was only ""using Software Update to make it easy and convenient for both Mac and Windows users to get the latest Safari update from Apple."" Apple also released a new version of Apple Software Update that puts new software in its own section, though still selected for installation by default. By late 2008, Apple Software Update no longer selected new installation items in the new software section by default.[citation needed]
Software security firm Sophos detailed how Snow Leopard and Windows users were not supported by the Safari 6 release at the time, while there were over 121 vulnerabilities left unpatched on those platforms. Since then, Snow Leopard has had only three minor version releases (the most recent in September 2013), and Windows has had none. While no official word has been released by Apple, the indication is that these are the final versions available for these operating systems, and both retain significant security issues.
While Safari pioneered several now standard HTML5 features (such as the Canvas API) in its early years, it has increasingly come under attack for failing to keep pace with modern web standards. In the past, Apple did not allow third party web browsers under iOS, but now there are plenty of web browsers available for iOS, including Chrome, Firefox, Opera and Edge. However, due to Apple developer's policies, browsers like Firefox for iOS needed to change its internal browser engine from Gecko to WebKit. There are ongoing lawsuits in France related with Apple policies for developers.
The Safari Developer Program was a free program for writers of extensions and HTML5 websites. It allowed members to develop extensions for Apple's Safari web browser. Since WWDC 2015 it is part of the unified Apple Developer Program, which costs $99 a year."
"139","

Mac OS X 10.4 Tiger is the fifth major release of Mac OS X (now named macOS), Apple's desktop and server operating system for Mac computers. Tiger was released to the public on April 29, 2005 for US$129.95 as the successor to Mac OS X 10.3 Panther. Some of the new features included a fast searching system called Spotlight, a new version of the Safari web browser, Dashboard, a new ‘Unified’ theme, and improved support for 64-bit addressing on Power Mac G5s. Mac OS X 10.4 Tiger shocked executives at Microsoft by offering a number of features, such as fast file searching and improved graphics processing, that Microsoft had spent several years struggling to add to Windows with acceptable performance.
Mac OS X 10.4 Tiger was included with all new Macs, and was also available as an upgrade for existing Mac OS X users, or users of supported pre-Mac OS X systems. The server edition, Mac OS X Server 10.4, was also available for some Macintosh product lines. Six weeks after its official release, Apple had delivered 2 million copies of Mac OS X 10.4 Tiger, representing 16% of all Mac OS X users. Apple claimed that Mac OS X 10.4 Tiger was the most successful Apple OS release in the company's history. At the WWDC on June 11, 2007, Apple's CEO, Steve Jobs, announced that out of the 22 million Mac OS X users, more than 67% were using Mac OS X 10.4 Tiger.
Apple announced a transition to Intel x86 processors during Mac OS X 10.4 Tiger's lifetime, making it the first Apple operating system to work on Apple–Intel architecture machines. The original Apple TV, released in March 2007, shipped with a customized version of Mac OS X 10.4 Tiger branded ""Apple TV OS"" that replaced the usual GUI with an updated version of Front Row.
Mac OS X 10.4 Tiger was succeeded by Mac OS X 10.5 Leopard on October 26, 2007, after 30 months, making Mac OS 10.4 Tiger the longest running version of Mac OS X. The last security update released for Mac OS X 10.4 Tiger users was the 2009-005 update. The next security update, 2009-006 only included support for Mac OS X 10.5 Leopard and Mac OS X 10.6 Snow Leopard. The latest supported version of QuickTime is 7.6.4. The latest version of iTunes that can run on Mac OS X 10.4 Tiger is 9.2.1, because 10.0 only supports Mac OS X 10.5 Leopard and later. Safari 4.1.3 is the final version for Mac OS X 10.4 Tiger as of November 18, 2010. Despite not having received security updates since then, Mac OS X 10.4 Tiger remains popular with Power Mac users and retrocomputing enthusiasts due to its wide software and hardware compatibility, as it runs on a wide variety of older machines and is the last Mac OS X version that supports the Classic Environment, a Mac OS 9 compatibility layer and PowerPC G3 processors.
Mac OS X 10.4 Tiger was initially available in a PowerPC edition, with an Intel edition released beginning at Mac OS X 10.4.4 Tiger. There is no universal version of the client operating system, although Mac OS X 10.4 Tiger Server was made available on a universal DVD from version Mac OS X 10.4.7 Tiger. While Apple shipped the PowerPC edition bundled with PowerPC-based Macs and also sold it as a separate retail box, the only way to obtain the Intel version was to buy an Intel-based Mac bundled with it. However, it was possible to buy the ‘restore’ DVDs containing the Intel version through unofficial channels such as eBay, and officially through Apple if you could provide proof of purchase of the appropriate Intel Mac. These grey colored ‘restore’ DVDs supplied with new Macs, are designed to only restore on the model of Mac that they are intended for. However, they can be modified to work on any Intel Mac. The retail PowerPC-only DVD can be used on any PowerPC-based Mac supported by Mac OS X 10.4 Tiger.
The system requirements of the PowerPC edition are:
Mac OS X 10.4 Tiger removed support for older New World ROM Macs such as the original iMacs and iBooks that were supported in Mac OS X 10.3 Panther; however it is possible to install Mac OS X 10.4 Tiger on these Macs using third-party software (such as XPostFacto) that overrides the checks made at the beginning of the installation process. Likewise, machines such as beige Power Mac G3s and ‘Wall Street’ PowerBook G3s that were dropped by Mac OS X 10.3 Panther can also be made to run both Mac OS X 10.3 Panther and Mac OS X 10.4 Tiger in this way. Also, Mac OS X 10.4 Tiger can be installed on unsupported New World ROM Macs by installing it on a supported Mac, then swapping hard drives. Old World ROM Macs require the use of XPostFacto to install Mac OS X 10.4 Tiger.
Mac OS X 10.4 Tiger was the last version of Mac OS X that supported the PowerPC G3 processor.
Apple CEO Steve Jobs first presented Mac OS X 10.4 Tiger in his keynote presentation at the WWDC on June 28, 2004, ten months before its commercial release in April 2005. Four months before that official release, several non-commercial, developer's releases of Mac OS X 10.4 Tiger leaked onto the internet via BitTorrent file sharers. It was first mentioned on Apple's website on May 4, 2004. Apple sued these file sharers. On April 12, 2005, Apple announced Mac OS X 10.4 Tiger's official, worldwide release would be April 29. All Apple Stores around the world held Mac OS X 10.4 Tiger seminars, presentations and demos.
On June 6, 2005 at the WWDC in San Francisco, Jobs reported that nearly two million copies had been sold in Mac OS X 10.4 Tiger's first six weeks of release, making Mac OS X 10.4 Tiger the most successful operating system release in Apple's history. Jobs then disclosed that Mac OS X had been engineered from its inception to work with Intel's  x86 line of processors in addition to the PowerPC, the CPU for which the operating system had always been publicly marketed. Apple concurrently announced its intent to release the first x86-based computers in June 2006, and to move the rest of its computers to x86 microprocessors by June 2007. On January 10, 2006, Apple presented its new iMac and MacBook Pro computers running on Intel Core Duo processors, and announced that the entire Apple product line would run on Intel processors by the end of 2006. Apple then released the Mac Pro and announced the new Xserve on August 8, completing the Intel transition in 210 days, roughly ten months ahead of the original schedule.
Mac OS X 10.4 Tiger is the first version of Mac OS X to be supplied on a DVD, although the DVD could originally be exchanged for CDs for $9.95. It is also the first (and so far only) version of Mac OS X that would eventually have an update version number ending with a value greater than 9, as the last version of Mac OS X 10.4 Tiger was 10.4.11.
Apple advertises that Mac OS X 10.4 Tiger has over 150 new and improved features, including:
In every major new revision of Mac OS X, Apple alters the graphical user interface somewhat. In Tiger, the menu bar displayed at the top of the screen now features a colored Spotlight button in the upper right corner; the menu itself has a smoother 'glassy' texture to replace the faint pinstripes in Panther.
Also of note, Tiger introduces a new window theme, often described as 'Unified'. A variation on the standard, non-brushed metal theme used since the introduction of Mac OS X, this theme integrates the title bar and the toolbar of a window. A prominent example of an application that utilizes this theme is Mail.
Tiger was the first version of Mac OS X to include the ""Zoom"" screen magnifier functionality which allowed the user to zoom on to the area around the mouse by holding CONTROL and scrolling the mouse wheel up or down (to zoom in and out respectively).
Shortly before the release of Mac OS X Tiger, the computer retailer TigerDirect.com, Inc. filed a lawsuit against Apple, alleging that Apple infringed TigerDirect.com's trademark with the Mac OS X Tiger operating system.
The following is a quotation from TigerDirect.com's court memorandum:
In 2005 TigerDirect was denied a preliminary injunction which would have prevented Apple from using the mark while the case was decided. Apple and TigerDirect reached a settlement in 2006, after which TigerDirect withdrew its opposition.
At the 2005 WWDC, Apple CEO Steve Jobs announced that Apple would begin selling Mac computers with Intel processors in 2006. To allow developers to begin producing software for these Intel-based Macs, Apple made Developer Transition Kits available for sale that included a version of Mac OS X v10.4.1 designed to run on x86 processors.
This build includes Apple's Rosetta — a translation process that allows Intel processor versions of the OS to run PPC software with little penalty. This is contrasted with the current Mac OS 9 Classic mode, which uses comparably larger amounts of system resources.
Soon after the Developer Transition Kits began shipping, copies of Tiger x86 were leaked onto file sharing networks. Although Apple had implemented a Trusted Computing DRM scheme in the transition hardware and OS in an attempt to stop people installing Tiger x86 on non-Apple PCs, the OSx86 project soon managed to remove this restriction. As Apple released each update with newer safeguards to prevent its use on non-Apple hardware, unofficially-modified versions were released that circumvented Apple's safeguards. However, with the release of 10.4.5, 10.4.6, and 10.4.7 the unofficially-modified versions continued to use the kernel from the 10.4.4 because later kernels have hardware locks and depend heavily on EFI. By late 2006, the 10.4.8 kernel was cracked.
At MacWorld San Francisco 2006, Jobs announced the immediate availability of Mac OS X v10.4.4, the first publicly available release of Tiger compiled for both PowerPC and Intel x86-based machines."
"140","

Dashboard is an application for Apple Inc.'s macOS operating systems, used as a secondary desktop for hosting mini-applications known as widgets. These are intended to be simple applications that do not take time to launch. Dashboard applications supplied with macOS include a stock ticker, weather report, calculator and notepad; users can create or download their own.
Before Mac OS X 10.7 Lion, when Dashboard is activated, the user's desktop is dimmed and widgets appear in the foreground. Like application windows, they can be moved around, rearranged, deleted, and recreated (so that more than one of the same Widget is open at the same time, possibly with different settings). New widgets can be opened, via an icon bar on the bottom of the layer, loading a list of available apps similar to the iOS homescreen or the macOS Launchpad. After loading, the widget is ready for use. 
Dashboard was first introduced in Mac OS X 10.4 Tiger. It can be activated as an application, from the Dock, Launchpad or Spotlight. It can also be accessed by a dashboard key. Alternatively, the user can choose to make Dashboard open on moving the cursor into a preassigned hot corner or keyboard shortcut. Starting with Mac OS X 10.7 Lion, the Dashboard can be configured as a space, accessed by swiping four fingers to the right from the Desktops either side of it. In OS X 10.10 Yosemite, the Dashboard is disabled by default, as the Notification Center is now the primary method of displaying widgets.
Dashboard widgets, like web pages, are capable of many different things, often to perform tasks that would be tedious or complicated for the user to access manually. One example is the Google Search widget, which simply opens up the user's browser and performs a Google search. Other widgets, like Wikipedia, grab the contents of web pages and display them within Dashboard. Some widgets can also serve as games, using Adobe Flash (or another multimedia authoring program) to create games just as if they were in a browser. It is also possible for Mac users to create their own widgets using built-in software.
Dashboard uses a variety of graphical effects for displaying, opening, and using widgets. For instance, a 3-D flip effect is used to simulate the widget flipping around, by clicking on a small i icon in the right bottom corner, the user can change the preferences on the reverse side; other effects include crossfading and scaling from icon to body (when opening widgets), a ""spin cycle effect"" when a widget is focused and the user presses Command-R or a suck-in effect when they are closed. On sufficiently powered Macs, widgets will produce a ripple effect when they are opened, like a leaf falling onto water. These effects consume considerable processing power and are arguably merely cosmetic, but with the help of macOS’s Quartz Extreme and Core Image graphics architectures, sufficient computing power to render them in real time is available. As with Exposé, Front Row and the minimise effect, holding shift down while calling the Dashboard or opening the Dashboard menu bar will display the effect in slow motion.
Dashboard widgets are created using Hypertext Markup Language (HTML), Cascading Style Sheets (CSS), and JavaScript. Because the same languages are used for creating websites, many web developers can already build them.Widgets themselves are, at the core, simply HTML files that are displayed within the Dashboard layer; they use the WebKit application framework that is also used in Apple's Safari web browser, meaning even users running earlier versions of macOS — where Dashboard is unavailable — can build them.  There is widget API allows the widgets to use Mac-specific interface elements such as the Apple Slider.
When a Dashboard widget is built, it usually consists of six files:
Once all of these files are in the root of a directory, it is given a name and the extension "".wdgt"", and then it can be opened up in Dashboard as a widget. More complex widgets may also include a Cocoa widget plugin (for platform-specific functionality), one or more JavaScript files (for text scrolling, preferences, etc.) or multiple images (for personalized select menus or buttons).
Mac OS X 10.5 Leopard includes an application called Dashcode, which is a more user-friendly way of creating widgets. Another new feature of Leopard is called ""Web Clip"" which lets users easily create widgets from parts of a webpage. For example, during the WWDC 2007 keynote, Steve Jobs made widgets out of the following: the featured news headlines on Yahoo.com, the top ten most searched terms on Google, the Photo of the Day on National Geographic, the Dilbert comic strip, and the box office information from Rotten Tomatoes. The user can also customize the border to further personalize the widgets.
Many people have made comparisons between Konfabulator and Apple's Dashboard, especially after Apple announced the feature while Mac OS X 10.4 Tiger was in development. It was a subject of debate in the online community following the few months before Tiger's official release.
One school of thought came to the conclusion that Dashboard was a ""rip-off"" of Konfabulator. It points out the visual and functional similarities between Dashboard has been widely compared to Konfabulator (now Yahoo! Widget Engine) and sometimes called a copy of it, due to the similarities between their graphical aspects and the fact that they both use the term “widgets” to describe the objects in their environments. Konfabulator may in turn have been based on Apple’s Desk Accessories, first released in 1984 with the original Macintosh. Desk Accessories, similar to widgets, were small mini-applications that operated on a user’s desktop. After the introduction of System 7 and cooperative multitasking, the necessity of creating Desk Accessories was removed and developers were encouraged to create applications instead. The OS continued to support them, for backward compatibility, until the switch to Mac OS X (In fact, the Calculator desk accessory remained in the Mac OS through version 9, 17 years without a significant update).
The code bases for Konfabulator and Dashboard are also different: Konfabulator uses XML and JavaScript to generate Widgets, whereas Dashboard uses HTML, CSS, JavaScript, and Objective-C.
In the first version of Dashboard released with Mac OS X 10.4 Tiger update 10.4.3. Apple included 14 widgets. They consisted of:
After the Macworld 2006 keynote, Steve Jobs also announced four new widgets (Ski Report, People Finder, Google Search, and ESPN), as well as significant updates to the Phone Book and Calendar widgets. All of these are available through the Mac OS X 10.4.4 Tiger update.
In addition, Mac OS X 10.5 Leopard released in late 2007, includes new widgets.  One of these is Web Clip, which allows any user to turn a rectangular section of any webpage into a widget (This, however, only works with  the Safari Web Browser).  The widget updates as the website does, and all links and other interactive material in the widget's selection of the webpage works as if the website is being accessed from the Safari Web Browser. Another new widget is Movies, which allows users to find currently playing movies at local theaters, view trailers, and purchase tickets directly from Dashboard.
Apple has not, as of early 2018, announced support for the installation of Dashboard widgets on iOS. Even though, in June 2008, an unannounced update of Dashcode that was packaged with the iPhone SDK allowed for the creation of iPhone-oriented web widgets, it is unknown if this most recent version of Dashcode would support the creation of AJAX-driven mobile widgets that could be installed natively on iOS.
It has been demonstrated that installing Dashboard widgets on a jailbroken iOS device is possible in theory, but most desktop-oriented widgets are not oriented to usage or interaction on iOS's multi-touch screen-oriented interface or rely on DashboardClient's widget JavaScript object, which is not part of iOS.
On June 2, 2014, as part of their announcement of iOS 8, Apple announced that in the 'Today' view (which is accessible by swiping down the status bar) will be able to have downloadable widgets from the App Store. While not the same desktop-oriented widgets that are found in the Dashboard, this is the first time that widgets became available officially (i.e., without jailbreaking) on iOS."
"141","
Adobe AIR (formerly Adobe Integrated Runtime) is a cross-platform runtime system developed by Adobe Systems for building desktop applications and mobile applications, programmed using Adobe Flash, ActionScript and optionally Apache Flex. The runtime supports installable applications on Windows, OS X and mobile operating systems including Android, iOS and BlackBerry Tablet OS. It also originally ran on Linux, but support was discontinued as of version 2.6 in 2011.
Adobe AIR is a runtime environment that allows Adobe Flash content and ActionScript 3.0 coders to construct applications and video games that run outside a web browser, and behave as a native application on supported platforms. An application developed for Flash Player or HTML5 and deployed in a browser does not require installation, while AIR applications require installation from an installer file (Windows and OS X) or the appropriate App Store (iOS and Android). AIR applications have unrestricted access to local storage and file systems, while browser-based applications only have access to individual files selected by users.
Adobe AIR internally uses the Flash Player rendering engine and ActionScript 3.0 as the primary programming language. Flash applications must specifically be built for Adobe AIR to use additional features provided, such as multi-touch, file system integration, native client extensions, integration with Taskbar or Dock, and access to accelerometer and GPS devices. HTML5 applications may run on the WebKit engine included in AIR.
Notable applications built with Adobe AIR include eBay Desktop, Pandora One desktop,TweetDeck, the former Adobe Media Player,Angry Birds, and Machinarium, among other multimedia and task management applications. According to Adobe, over 100,000 unique applications were built on AIR, and over 1 billion installations of the same were logged from users across the world, as of May 2014. Adobe AIR was voted as the Best Mobile Application Development product at the Consumer Electronics Show for two consecutive years (CES 2014 and CES 2015).
Using AIR, developers can access the full Adobe Flash functionality, including text, vector graphics, raster graphics, video, audio, camera, and microphone capability. Adobe AIR also includes additional features such as file system integration, native client extensions, desktop integration and access to connected devices. AIR enables applications to work with data in different ways, including using local files, local SQLite databases (for which AIR has built-in support), a database server, or the encrypted local store included with AIR.
Developers can access additional functionality by building AIR Native Extensions, which can access full device functionality being programmed in the native language.
On desktop platforms, AIR supports:
On mobile platforms, AIR supports many mobile hardware features:
In 2011, the addition of Stage3D to the Flash Player allowed Flash and AIR apps access to GPUs for hardware acceleration. Several third-party frameworks have been developed to build upon the functionality of Stage3D, including the Starling Framework and Away3D. These frameworks are also compatible with AIR, and provide vital performance improvements to AIR apps published for mobile devices.
AIR apps can be augmented in functionality with the usage of AIR Native Extensions (ANEs). Native extensions are plug-in code libraries that contain native code wrapped with an ActionScript API, allowing developers to access native features not otherwise usable in AIR, such as Apple Game Center or Google Cloud Messaging.
Native extensions may be developed by anyone using publicly available tools; some are distributed for free or even as open source, while others are sold commercially.
Native extensions may be programmed in the native language on each platform, allowing access to the full set of platform APIs provided by the developer. (C++ for Windows, Java for Android, Objective-C for iOS).
AIR is a cross-platform technology and AIR applications can be repackaged with few or no changes for many popular desktop and mobile platforms. Different installation options exist for each platform.
AIR applications may be published with or without the AIR runtime. Applications packaged with the AIR runtime are larger in file size, and are known as ""captive runtime"" applications. If the runtime is not embedded in the app, it must be installed separately.
In January 2009, Adobe claimed that there were over 100 million installations of Adobe AIR worldwide, and that ""the majority of AIR runtime installations occur at the time the first AIR application is installed by a user"". In May 2014, Adobe claimed that over 100,000 unique applications were built on AIR, and over 1 billion installations of the same were logged from users across the world.
The latest version of Adobe AIR, version 28, contains Adobe Flash Player 28, and is available for Windows 7 and later, as well as OS X 10.9 and later. Official support for desktop Linux distributions ceased in June 2011 with version 2.6.
Adobe AIR applications can be published as native phone applications on certain mobile operating systems, such as Android (ARM Cortex-A8 and above) and Apple iOS.
Adobe AIR runs Flash applications within a contained Flash Player instance. It runs web applications via WebKit rendering engine. Multiple instances of the browser can be started within a single AIR application, but JavaScript content executes with some security limitations.
AIR does not provide direct access to native GUI elements such as navigation bars or controls. Native extensions can be used to access additional native resources.
The AIR SDK is available as a free standalone download for software developers to make AIR applications. SDK users do not need to install any commercial software to use the SDK, although several options are available. AIR apps can be compiled from the command line using the AIR compiler included in the SDK; the compiler can also be called from an IDE to eliminate the need for the command line.
AIR can also be used with Adobe Flex. Flex is an integrated collection of stylable graphical user interface, data manipulation and networking components, and applications built upon it are known as ""Flex"" applications. Flex GUIs are defined in MXML, similar to how Android and Microsoft Visual Studio define GUIs; however, Flex does not give access to native GUI components.
AIR applications built without the Flex framework allow greater flexibility and performance, and are known as ""pure ActionScript"" applications. Video games built on the AIR platform are typically pure-Actionscript projects. Various open-source component frameworks are available for pure ActionScript projects, such as MadComponents, that provide UI Components at significantly smaller SWF file sizes.
Adobe distributes three commercial software products for developing of AIR applications in ActionScript:
Third-party development environments that target the AIR runtime are also available, including:
Adobe Flash Builder is the premium tool for Flex application development, since it includes an integrated drag-and-drop user interface builder, not found in competing tools like FlashDevelop.
Adobe provides for AIR HTML5 and JavaScript development with Adobe Dreamweaver CS5, although any other HTML editor or text editor can be used.
Adobe AIR can run a subset of JavaScript, with no ability to dynamically execute code when running in the application sandbox. According to Adobe, this restriction is designed to prevent malicious remote content from attacking a user's system. Because of this restriction, JavaScript frameworks that make use of dynamic JavaScript functions like eval() were not initially compatible with Adobe AIR. However, several frameworks including Dojo Toolkit,[citation needed]jQuery,[citation needed] and ExtJS[citation needed] were updated to run in Adobe AIR's application sandbox. Some frameworks like MooTools were already compatible.[citation needed]
Dreamweaver CS4/CS3 requires an additional extension to compile AIR applications, as does Flash CS3 in the form of an update.
Adobe made a public preview release of AIR (then called Apollo) along with a software development kit (SDK) and extension for developing Apollo applications with the Flex framework, on March 19, 2007.
On June 10, 2007, Apollo was renamed to AIR and a public beta release of the runtime was launched. Public beta 2 of AIR SDK was released on October 1, 2007. Public beta 3, was released on December 12, 2007.
Version 1.0 of the Adobe AIR runtime and SDK was released on February 25, 2008.[citation needed]
Version 1.1 of Adobe AIR was released on June 16, 2008. This release included a number of new features including:
In addition, version 1.1 works on Microsoft Windows XP Tablet PC Edition and 64-bit editions of Windows Vista Home Premium, Business, Ultimate, and Enterprise.
Adobe AIR 1.5 was released on November 17, 2008. New capabilities included:
Released on February 24, 2009, AIR 1.5.1 was primarily a compatibility update that includes bug fixes and security updates.
Released on July 30, 2009, AIR 1.5.2 introduced a number of minor new features and compatibility issues. Some of the important fixes included:
Adobe AIR 1.5.3 was released on December 8, 2009. It included fixes for a number of compatibility and security related issues. The BBC iPlayer Desktop manager v1.5.15695.18135 is the first version to use AIR 1.5.3.
The Adobe AIR 2 public beta was released on November 16, 2009 followed by the beta 2 on February 2, 2010 and the release candidate on May 11, 2010. In addition, Adobe AIR for Android was announced on February 12, 2010. AIR 2 was officially released for Windows, Mac OS and Linux on June 10, 2010 and Android on October 8, 2010. It dropped the ability to run on PowerPC Macs.
Adobe AIR 2.5 was released on October 24, 2010 at the Adobe MAX 2010 conference.
Adobe AIR 2.6 was released on February 24, 2011 for Android devices. Another update was released on March 22, 2011 for updated iOS interoperability.
Adobe AIR 2.7 was released on June 14, 2011. Ability to run on Linux was dropped.
Adobe released Adobe AIR 3.0 on October 3, 2011. AIR 3.0 added the ability to run on native 64-bit CPU architecture and use hardware accelerated graphics rendering, captive runtime, native extensions, JPEG-XR image format, LZMA compression for SWF files, and H.264 encoding.
Adobe released Adobe AIR 3.1 on November 11, 2011.
Adobe released Adobe AIR 3.2 on March 28, 2012.
Adobe released Adobe AIR 3.3 on June 8, 2012.
Adobe released Adobe AIR 3.4 on August 21, 2012.
Adobe released Adobe AIR 3.5 on November 6, 2012.
Adobe released Adobe AIR 3.6 on February 12, 2013.
Adobe released Adobe AIR 3.7 on April 9, 2013.
Adobe released Adobe AIR 3.8 on July 24, 2013.
Adobe released Adobe AIR 3.9 on October 8, 2013.
Adobe released Adobe AIR 4.0 on January 14, 2014. It was released to beta on October 30, 2013, code named Jones.
Adobe applied a new numbering scheme for the Flash products versions to synchronize them with the version numbering of the Flash Player, starting from Flash Player 13.
Adobe released Adobe AIR 13.0 on April 8, 2014. It was numbered 13 to synchronize itself with the version numbering of Flash Player.
Adobe released Adobe AIR 14.0 on June 10, 2014.
Adobe released Adobe AIR 15.0 on September 9, 2014. It includes improvements to Stage3D technology, AIR Gamepad enhancements, and a new packaging engine for iOS apps that reduces compile times from minutes to seconds.
Adobe released Adobe AIR 16.0 on January 13, 2015.
Adobe released Adobe AIR 17.0 on March 12, 2015.
Adobe released Adobe AIR 18.0 on June 9, 2015.
Adobe released Adobe AIR 19.0 on September 21, 2015.
Adobe released Adobe AIR 20.0 on December 08, 2015. Android SDK (API Level 21) has been upgraded in the AIR Runtime, applications built using this AIR SDK and later will only support Android OS 4.0 or greater.
Adobe released Adobe AIR 21.0 on March 10, 2016.
Adobe released Adobe AIR 22.0 on June 16, 2016.
Adobe released Adobe AIR 23.0 on September 13, 2016.
Adobe released Adobe AIR 24.0 on December 13, 2016.
Adobe released Adobe AIR 25.0 on March 14, 2017.
Adobe released Adobe AIR 26.0 on June 13, 2017.
Adobe released the Adobe AIR 27.0 on September 12, 2017.
Adobe released the Adobe AIR 28.0 on December 12, 2017."
"142","



iOS (formerly iPhone OS) is a mobile operating system created and developed by Apple Inc. exclusively for its hardware. It is the operating system that presently powers many of the company's mobile devices, including the iPhone, iPad, and iPod Touch. It is the second most popular mobile operating system globally after Android.
Originally unveiled in 2007 for the iPhone, iOS has been extended to support other Apple devices such as the iPod Touch (September 2007) and the iPad (January 2010). As of  January 2017[update], Apple's App Store contains more than 2.2 million iOS applications, 1 million of which are native for iPads. These mobile apps have collectively been downloaded more than 130 billion times.
The iOS user interface is based upon direct manipulation, using multi-touch gestures. Interface control elements consist of sliders, switches, and buttons. Interaction with the OS includes gestures such as swipe, tap, pinch, and reverse pinch, all of which have specific definitions within the context of the iOS operating system and its multi-touch interface. Internal accelerometers are used by some applications to respond to shaking the device (one common result is the undo command) or rotating it in three dimensions (one common result is switching between portrait and landscape mode). Apple has been significantly praised for incorporating thorough accessibility functions into iOS, enabling users with vision and hearing disabilities to properly use its products.
Major versions of iOS are released annually. The current version, iOS 11, was released on September 19, 2017. It is available for all iOS devices with 64-bit processors; the iPhone 5S and later iPhone models, the iPad (2017), the iPad Air and later iPad Air models, all iPad Pro models, the iPad Mini 2 and later iPad Mini models, and the sixth-generation iPod Touch.
In 2005, when Steve Jobs began planning the iPhone, he had a choice to either ""shrink the Mac, which would be an epic feat of engineering, or enlarge the iPod"". Jobs favored the former approach but pitted the Macintosh and iPod teams, led by Scott Forstall and Tony Fadell, respectively, against each other in an internal competition, with Forstall winning by creating the iPhone OS. The decision enabled the success of the iPhone as a platform for third-party developers: using a well-known desktop operating system as its basis allowed the many third-party Mac developers to write software for the iPhone with minimal retraining. Forstall was also responsible for creating a software development kit for programmers to build iPhone apps, as well as an App Store within iTunes.
The operating system was unveiled with the iPhone at the Macworld Conference & Expo on January 9, 2007, and released in June of that year. At the time of its unveiling in January, Steve Jobs claimed: ""iPhone runs OS X"" and runs ""desktop applications"", but at the time of the iPhone's release, the operating system was renamed ""iPhone OS"". Initially, third-party native applications were not supported. Jobs' reasoning was that developers could build web applications through the Safari web browser that ""would behave like native apps on the iPhone"". In October 2007, Apple announced that a native Software Development Kit (SDK) was under development and that they planned to put it ""in developers' hands in February"". On March 6, 2008, Apple held a press event, announcing the iPhone SDK.
The iOS App Store was opened on July 10, 2008 with an initial 500 applications available. This quickly grew to 3,000 in September 2008, 15,000 in January 2009, 50,000 in June 2009, 100,000 in November 2009, 250,000 in August 2010, 650,000 in July 2012, 1 million in October 2013, 2 million in June 2016, and 2.2 million in January 2017.As of  March 2016[update], 1 million apps are natively compatible with the iPad tablet computer. These apps have collectively been downloaded more than 130 billion times. App intelligence firm Sensor Tower has estimated that the App Store will reach 5 million apps by the year 2020.
In September 2007, Apple announced the iPod Touch, a redesigned iPod based on the iPhone form factor. In January 2010, Apple announced the iPad, featuring a larger screen than the iPhone and iPod Touch, and designed for web browsing, media consumption, and reading.
In June 2010, Apple rebranded iPhone OS as ""iOS"". The trademark ""IOS"" had been used by Cisco for over a decade for its operating system, IOS, used on its routers. To avoid any potential lawsuit, Apple licensed the ""IOS"" trademark from Cisco.
In October 2016, Apple opened its first iOS Developer Academy in Naples inside University of Naples Federico II's new campus.
Platform usage as measured by the App Store on January 18, 2018.
Apple provides major updates to the iOS operating system annually via iTunes and also, for iOS 5 and later, over-the-air. The latest version is iOS 11, released on September 19, 2017. It is available for iPhone 5S and later, iPad Air and later, iPad Pro, iPad Mini 2 and later, and sixth-generation iPod Touch.
Originally, iPod Touch users had to pay for system software updates. This was due to accounting rules making the device not a ""subscription device"" like iPhone or Apple TV, and significant enhancements to the device required payments. The requirement to pay to upgrade caused iPod Touch owners to stay away from updates. However, in September 2009, a change in accounting rules won tentative approval, significantly affecting both Apple's earnings and stock price, and allowing iPod Touch updates to be delivered for free.
The home screen, rendered by SpringBoard, displays application icons and a dock at the bottom where users can pin their most frequently used apps. The home screen appears whenever the user unlocks the device or presses the physical ""Home"" button whilst in another app. Before iOS 4 on the iPhone 3GS (or later), the screen's background could be customized only through jailbreaking, but can now be changed out-of-the-box. The screen has a status bar across the top to display data, such as time, battery level, and signal strength. The rest of the screen is devoted to the current application. When a passcode is set and a user switches on the device, the passcode must be entered at the Lock Screen before access to the Home screen is granted.
In iPhone OS 3, Spotlight was introduced, allowing users to search media, apps, emails, contacts, messages, reminders, calendar events, and similar content. In iOS 7 and later, Spotlight is accessed by pulling down anywhere on the home screen (except for the top and bottom edges that open Notification Center and Control Center). In iOS 9, there are two ways to access Spotlight. As with iOS 7 and 8, pulling down on any homescreen will show Spotlight. However, it can also be accessed as it was in iOS 3 – 6. This gives a Spotlight endowed with Siri suggestions, which include app suggestions, contact suggestions and news. In iOS 10, Spotlight is at the top of the now-dedicated ""Today"" panel.
Since iOS 3.2, users are able to set a background image for the Home screen. This feature is only available on third-generation devices—iPhone 3GS, third-generation iPod touch (iOS 4.0 or newer), all iPad models (since iOS 3.2)—or newer.
Researchers found that users organize icons on their homescreens based on usage-frequency and relatedness of the applications, as well as for reasons of usability and aesthetics.
iOS originally used Helvetica as the system font. Apple switched to Helvetica Neue exclusively for the iPhone 4 and its Retina Display, and retained Helvetica as the system font for older iPhone devices on iOS 4. With iOS 7, Apple announced that they would change the system font to Helvetica Neue Light, a decision that sparked criticism for inappropriate usage of a light, thin typeface for low-resolution mobile screens. Apple eventually chose Helvetica Neue instead. The release of iOS 7 also introduced the ability to scale text or apply other forms of text accessibility changes through Settings. With iOS 9, Apple changed the font to San Francisco, an Apple-designed font aimed at maximum legibility and font consistency across its product lineup.
iOS 4 introduced folders, which can be created by dragging an application on top of another, and from then on, more items can be added to the folder using the same procedure. A title for the folder is automatically selected by the category of applications inside, but the name can also be edited by the user. When apps inside folders receive notification badges, the individual numbers of notifications are added up and the total number is displayed as a notification badge on the folder itself. Originally, folders on an iPhone could include up to 12 apps, while folders on iPad could include 20. With increasing display sizes on newer iPhone hardware, iOS 7 updated the folders with pages similar to the home screen layout, allowing for a significant expansion of folder functionality. Each page of a folder can contain up to nine apps, and there can be 15 pages in total, allowing for a total of 135 apps in a single folder. In iOS 9, Apple updated folder sizes for iPad hardware, allowing for 16 apps per page, still at 15 pages maximum, increasing the total to 240 apps.
Before iOS 5, notifications were delivered in a modal window and couldn't be viewed after being dismissed. In iOS 5, Apple introduced Notification Center, which allows users to view a history of notifications. The user can tap a notification to open its corresponding app, or clear it. Notifications are now delivered in banners that appear briefly at the top of the screen. If a user taps a received notification, the application that sent the notification will be opened. Users can also choose to view notifications in modal alert windows by adjusting the application's notification settings. Introduced with iOS 8, widgets are now accessible through the Notification Center, defined by 3rd parties.
When an app sends a notification while closed, a red badge appears on its icon. This badge tells the user, at a glance, how many notifications that app has sent. Opening the app clears the badge.
iOS offers various accessibility features to help users with vision and hearing disabilities. One major feature, VoiceOver, provides a voice reading information on the screen, including contextual buttons, icons, links and other user interface elements, and allows the user to navigate the operating system through gestures. Any apps with default controls and developed with a UIKit framework gets VoiceOver functionality built in. One example includes holding up the iPhone to take a photo, with VoiceOver describing the photo scenery. As part of a ""Made for iPhone"" program, introduced with the release of iOS 7 in 2013, Apple has developed technology to use Bluetooth and a special technology protocol to let compatible third-party equipment connect with iPhones and iPads for streaming audio directly to a user's ears. Additional customization available for Made for iPhone products include battery tracking and adjustable sound settings for different environments. Apple made further efforts for accessibility for the release of iOS 10 in 2016, adding a new pronunciation editor to VoiceOver, adding a Magnifier setting to enlarge objects through the device's camera, software TTY support for deaf people to make phone calls from the iPhone, and giving tutorials and guidelines for third-party developers to incorporate proper accessibility functions into their apps.
In 2012, Liat Kornowski from The Atlantic wrote that ""the iPhone has turned out to be one of the most revolutionary developments since the invention of Braille"", and in 2016, Steven Aquino of TechCrunch described Apple as ""leading the way in assistive technology"", with Sarah Herrlinger, Senior Manager for Global Accessibility Policy and Initiatives at Apple, stating that ""We see accessibility as a basic human right. Building into the core of our products supports a vision of an inclusive world where opportunity and access to information are barrier-free, empowering individuals with disabilities to achieve their goals"".
Multitasking for iOS was first released in June 2010 along with the release of iOS 4. Only certain devices—iPhone 4, iPhone 3GS, and iPod Touch 3rd generation—were able to multitask. The iPad did not get multitasking until iOS 4.2.1 in November. Currently, multitasking is supported on iPhone 3GS+, iPod Touch 3rd generation+, and all iPad models.
Implementation of multitasking in iOS has been criticized for its approach, which limits the work that applications in the background can perform to a limited function set and requires application developers to add explicit support for it.
Before iOS 4, multitasking was limited to a selection of the applications Apple included on the device. Users could, however ""jailbreak"" their device in order to unofficially multitask. Starting with iOS 4, on third-generation and newer iOS devices, multitasking is supported through seven background APIs:
In iOS 5, three new background APIs were introduced:
In iOS 7, Apple introduced a new multitasking feature, providing all apps with the ability to perform background updates.  This feature prefers to update the user's most frequently used apps and prefers to use WiFi networks over a cellular network, without markedly reducing the device's battery life.
In iOS 4.0 to iOS 6.x, double-clicking the home button activates the application switcher. A scrollable dock-like interface appears from the bottom, moving the contents of the screen up. Choosing an icon switches to an application. To the far left are icons which function as music controls, a rotation lock, and on iOS 4.2 and above, a volume controller.
With the introduction of iOS 7, double clicking the home button also activates the application switcher. However, unlike previous versions it displays screenshots of open applications on top of the icon and horizontal scrolling allows for browsing through previous apps, and it is possible to close applications by dragging them up, similar to how WebOS handled multiple cards.
With the introduction of iOS 9, the application switcher received a significant visual change; whilst still retaining the card metaphor introduced in iOS 7, the application icon is smaller, and appears above the screenshot (which is now larger, due to the removal of ""Recent and Favorite Contacts""), and each application ""card"" overlaps the other, forming a rolodex effect as the user scrolls. Now, instead of the home screen appearing at the leftmost of the application switcher, it appears rightmost. In iOS 11, the application switcher receives a major redesign. In the iPad, the Control Center and app switcher are combined. The app switcher in the iPad can also be accessed by swiping up from the bottom. In the iPhone, the app switcher cannot be accessed if there are no apps in the RAM.
In iOS 4.0 to iOS 6.x, briefly holding the icons in the application switcher makes them ""jiggle"" (similarly to the homescreen) and allows the user to force quit the applications by tapping the red minus circle that appears at the corner of the app's icon. Clearing applications from multitasking stayed the same from iOS 4.0 through 6.1.6, the last version of iOS 6.
As of iOS 7, the process has become faster and easier. In iOS 7, instead of holding the icons to close them, they are closed by simply swiping them upwards off the screen. Up to three apps can be cleared at a time compared to one in versions up to iOS 6.1.6.
Task completion allows apps to continue a certain task after the app has been suspended. As of iOS 4.0, apps can request up to ten minutes to complete a task in the background. This doesn't extend to background up- and downloads though (e.g. if you start a download in one application, it won't finish if you switch away from the application).
Siri (pronounced /ˈsɪəri/) is an intelligent personal assistant integrated into iOS. The assistant uses voice queries and a natural language user interface to answer questions, make recommendations, and perform actions by delegating requests to a set of Internet services. The software adapts to users' individual language usages, searches, and preferences, with continuing use. Returned results are individualized.
Originally released as an app for iOS in February 2010, it was acquired by Apple two months later, and then integrated into iPhone 4S at its release in October 2011. At that time, the separate app was also removed from the iOS App Store.
Siri supports a wide range of user commands, including performing phone actions, checking basic information, scheduling events and reminders, handling device settings, searching the Internet, navigating areas, finding information on entertainment, and is able to engage with iOS-integrated apps. With the release of iOS 10 in 2016, Apple opened up limited third-party access to Siri, including third-party messaging apps, as well as payments, ride-sharing, and Internet calling apps. With the release of iOS 11, Apple updated Siri's voices for more clear, human voices, it now supports follow-up questions and language translation, and additional third-party actions.
Game Center is an online multiplayer ""social gaming network"" released by Apple. It allows users to ""invite friends to play a game, start a multiplayer game through matchmaking, track their achievements, and compare their high scores on a leaderboard."" iOS 5 and above adds support for profile photos.
Game Center was announced during an iOS 4 preview event hosted by Apple on April 8, 2010. A preview was released to registered Apple developers in August. It was released on September 8, 2010 with iOS 4.1 on iPhone 4, iPhone 3GS, and iPod Touch 2nd generation through 4th generation. Game Center made its public debut on the iPad with iOS 4.2.1.  There is no support for the iPhone 3G, original iPhone and the first-generation iPod Touch (the latter two devices did not have Game Center because they did not get iOS 4). However, Game Center is unofficially available on the iPhone 3G via a hack.
The main hardware platform for iOS is the ARM architecture. iOS releases before iOS 6 can only be run on iOS devices with 32-bit ARM processors (ARMv6 and ARMv7-A architectures). In 2013, iOS 7 was released with full 64-bit support (which includes native 64-bit kernel, libraries, drivers as well as all built-in applications), after Apple announced that they were switching to 64-bit ARMv8-A processors with the introduction of the Apple A7 chip. 64-bit support was also enforced for all apps in the App Store; All new apps submitted to the App Store with a deadline of February 2015, and all app updates submitted to the App Store with a deadline of June 1, 2015.iOS 11 drops support for all iOS devices with 32-bit ARM processors as well as 32-bit applications, making iOS 64-bit only.
The iOS SDK (Software Development Kit) allows for the development of mobile apps on iOS.
While originally developing iPhone prior to its unveiling in 2007, Apple's then-CEO Steve Jobs did not intend to let third-party developers build native apps for iOS, instead directing them to make web applications for the Safari web browser. However, backlash from developers prompted the company to reconsider, with Jobs announcing in October 2007 that Apple would have a software development kit available for developers by February 2008. The SDK was released on March 6, 2008.
The SDK is a free download for users of Mac personal computers. It is not available for Microsoft Windows PCs. The SDK contains sets giving developers access to various functions and services of iOS devices, such as hardware and software attributes. It also contains an iPhone simulator to mimic the look and feel of the device on the computer while developing. New versions of the SDK accompany new versions of iOS. In order to test applications, get technical support, and distribute apps through App Store, developers are required to subscribe to the Apple Developer Program.
Combined with Xcode, the iOS SDK helps developers write iOS apps using officially-supported programming languages, including Swift and Objective-C. Other companies have also created tools that allow for the development of native iOS apps using their respective programming languages.
iOS is the second most popular mobile operating system in the world, after Android. Sales of iPads in recent years are also behind Android, while, by web use (a proxy for all use), iPads (using iOS) are still most popular.
By the middle of 2012, there were 410 million devices activated. At WWDC 2014, Tim Cook said 800 million devices had been sold by June 2014.
During Apple's quarterly earnings call in January 2015, the company announced that they had sold over one billion iOS devices since 2007.
By late 2011, iOS accounted for 60% of the market share for smartphones and tablets. By the end of 2014, iOS accounted for 14.8% of the smartphone market and 27.6% of the tablet and two-in-one market. In February 2015, StatCounter reported iOS was used on 23.18% of smartphones and 66.25% of tablets worldwide, measured by internet usage instead of sales.
In the third quarter of 2015, research from Strategy Analytics showed that iOS adoption of the worldwide smartphone market was at a record-low 12.1%, attributed to lackluster performance in China and Africa. Android accounted for 87.5% of the market, with Windows Phone and BlackBerry accounting for the rest.
Since its initial release, iOS has been subject to a variety of different hacks centered around adding functionality not allowed by Apple. Prior to the 2008 debut of Apple's native iOS App Store, the primary motive for jailbreaking was to bypass Apple's purchase mechanism for installing the App Store's native applications. Apple claimed that it will not release iOS software updates designed specifically to break these tools (other than applications that perform SIM unlocking); however, with each subsequent iOS update, previously un-patched jailbreak exploits are usually patched.
Since the arrival of Apple's native iOS App Store, and—along with it—third-party applications, the general motives for jailbreaking have changed. People jailbreak for many different reasons, including gaining filesystem access, installing custom device themes, and modifying SpringBoard. An additional motivation is that it may enable the installation of pirated apps. On some devices, jailbreaking also makes it possible to install alternative operating systems, such as Android and the Linux kernel. Primarily, users jailbreak their devices because of the limitations of iOS. Depending on the method used, the effects of jailbreaking may be permanent or temporary.
In 2010, the Electronic Frontier Foundation (EFF) successfully convinced the U.S. Copyright Office to allow an exemption to the general prohibition on circumvention of copyright protection systems under the Digital Millennium Copyright Act (DMCA).  The exemption allows jailbreaking of iPhones for the sole purpose of allowing legally obtained applications to be added to the iPhone.  The exemption does not affect the contractual relations between Apple and an iPhone owner, for example, jailbreaking voiding the iPhone warranty; however, it is solely based on Apple's discretion on whether they will fix jailbroken devices in the event that they need to be repaired. At the same time, the Copyright Office exempted unlocking an iPhone from DMCA's anticircumvention prohibitions.  Unlocking an iPhone allows the iPhone to be used with any wireless carrier using the same GSM or CDMA technology for which the particular phone model was designed to operate.
Initially most wireless carriers in the US did not allow iPhone owners to unlock it for use with other carriers. However AT&T allowed iPhone owners who have satisfied contract requirements to unlock their iPhone. Instructions to unlock the device are available from Apple, but it is ultimately the sole discretion of the carrier to authorize the device to be unlocked. This allows the use of a carrier-sourced iPhone on other networks. Modern versions of iOS and the iPhone fully support LTE across multiple carriers despite where the phone was originally purchased from. There are programs to remove SIM lock restrictions, but are not supported by Apple and most often not a permanent unlock – a soft-unlock.
The closed and proprietary nature of iOS has garnered criticism, particularly by digital rights advocates such as the Electronic Frontier Foundation, computer engineer and activist Brewster Kahle, Internet-law specialist Jonathan Zittrain, and the Free Software Foundation who protested the iPad's introductory event and have targeted the iPad with their ""Defective by Design"" campaign. Competitor Microsoft, via a PR spokesman, criticized Apple's control over its platform.
At issue are restrictions imposed by the design of iOS, namely digital rights management (DRM) intended to lock purchased media to Apple's platform, the development model (requiring a yearly subscription to distribute apps developed for the iOS), the centralized approval process for apps, as well as Apple's general control and lockdown of the platform itself. Particularly at issue is the ability for Apple to remotely disable or delete apps at will.
Some in the tech community have expressed concern that the locked-down iOS represents a growing trend in Apple's approach to computing, particularly Apple's shift away from machines that hobbyists can ""tinker with"" and note the potential for such restrictions to stifle software innovation.
Former Facebook developer Joe Hewitt protested against Apple's control over its hardware as a ""horrible precedent"" but praised iOS's sandboxing of apps.
The iOS kernel is the XNU kernel of Darwin. The original iPhone OS (1.0) up to iPhone OS 3.1.3 used Darwin 9.0.0d1. iOS 4 was based on Darwin 10. iOS 5 was based on Darwin 11. iOS 6 was based on Darwin 13. iOS 7 and iOS 8 are based on Darwin 14. iOS 9 is based on Darwin 15. iOS 10 is based on Darwin 16. iOS 11 is based on Darwin 17.
iOS utilizes many security features in both hardware and software. Below are summaries of the most prominent features.
Before fully booting into iOS, there is low-level code that runs from the Boot ROM. Its task is to verify that the Low-Level Bootloader is signed by the Apple Root CA public key before running it. This process is to ensure that no malicious or otherwise unauthorized software can be run on an iOS device. After the Low-Level Bootloader finishes its tasks, it runs the higher level bootloader, known as iBoot. If all goes well, iBoot will then proceed to load the iOS kernel as well as the rest of the operating system.
The Secure Enclave is a coprocessor found in iOS devices that contain Touch ID or Face ID. It has its own secure boot process to ensure that it is completely secure. A hardware random number generator is also included as a part of this coprocessor. Each device's Secure Enclave has a unique ID that is given to it when it is made and cannot be changed. This identifier is used to create a temporary key that encrypts the memory in this portion of the system. The Secure Enclave also contains an anti-replay counter to prevent brute force attacks.
iOS devices can have a passcode that is used to unlock the device, make changes to system settings, and encrypt the device's contents. Until recently, these were typically four numerical digits long. However, since unlocking the devices with a fingerprint by using Touch ID has become more widespread, six-digit passcodes are now the default on iOS with the option to switch back to four or use an alphanumeric passcode.
Touch ID is a fingerprint scanner that is embedded in the home button and can be used to unlock the device, make purchases, and log into applications among other functions. When used, Touch ID only temporarily stores the fingerprint data in encrypted memory in the Secure Enclave, as described above. There is no way for the device's main processor or any other part of the system to access the raw fingerprint data that is obtained from the Touch ID sensor.
Address Space Layout Randomization (ASLR) is a low-level technique of preventing memory corruption attacks such as buffer overflows. It involves placing data in randomly selected locations in memory in order to make it harder to predict ways to corrupt the system and create exploits. ASLR makes app bugs more likely to crash the app than to silently overwrite memory, regardless of whether the behavior is accidental or malicious.
iOS utilizes the ARM architecture's Execute Never (XN) feature. This allows some portions of the memory to be marked as non-executable, working alongside ASLR to prevent buffer overflow attacks including return-to-libc attacks.
As mentioned above, one use of encryption in iOS is in the memory of the Secure Enclave. When a passcode is utilized on an iOS device, the contents of the device are encrypted. This is done by using a hardware AES 256 implementation that is very efficient because it is placed directly between the flash storage and RAM.
The iOS keychain is a database of login information that can be shared across apps written by the same person or organization. This service is often used for storing passwords for web applications.
Third-party applications such as those distributed through the App Store must be code signed with an Apple-issued certificate. This continues the chain of trust all the way from the Secure Boot process as mentioned above to the actions of the applications installed on the device by users. Applications are also sandboxed, meaning that they can only modify the data within their individual home directory unless explicitly given permission to do otherwise. For example, they cannot access data that is owned by other user-installed applications on the device. There is a very extensive set of privacy controls contained within iOS with options to control apps' ability to access a wide variety of permissions such as the camera, contacts, background app refresh, cellular data, and access to other data and services. Most of the code in iOS, including third-party applications, run as the ""mobile"" user which does not have root privileges. This ensures that system files and other iOS system resources remain hidden and inaccessible to user-installed applications.
iOS supports TLS with both low- and high-level APIs for developers. By default, the App Transport Security framework requires that servers use at least TLS 1.2. However, developers are free to override this framework and utilize their own methods of communicating over networks. When Wi-Fi is enabled, iOS uses a randomized MAC address so that devices cannot be tracked by anyone sniffing wireless traffic.
Two-factor authentication is an option in iOS to ensure that even if an unauthorized person knows an Apple ID and password combination, they cannot gain access to the account. It works by requiring not only the Apple ID and password, but also a verification code that is sent to a device that is already known to be trusted. If an unauthorized user attempts to sign in using another user's Apple ID, the owner of the Apple ID receives a notification that allows them to deny access to the unrecognized device."
"143","Apache Cordova (formerly PhoneGap) is a mobile application development framework originally created by Nitobi. Adobe Systems purchased Nitobi in 2011, rebranded it as PhoneGap, and later released an open source version of the software called Apache Cordova. Apache Cordova enables software programmers to build applications for mobile devices using CSS3, HTML5, and JavaScript instead of relying on platform-specific APIs like those in Android, iOS, or Windows Phone. It enables wrapping up of CSS, HTML, and JavaScript code depending upon the platform of the device. It extends the features of HTML and JavaScript to work with the device. The resulting applications are hybrid, meaning that they are neither truly native mobile application (because all layout rendering is done via Web views instead of the platform's native UI framework) nor purely Web-based (because they are not just Web apps, but are packaged as apps for distribution and have access to native device APIs). Mixing native and hybrid code snippets has been possible since version 1.9.
The software was previously called just ""PhoneGap"", then ""Apache Callback"". As open-source software, Apache Cordova allows wrappers around it, such as Appery.io or Intel XDK.
PhoneGap is Adobe’s commercial version of Cordova along with its associated ecosystem. Many other tools and frameworks are also built on top of Cordova, including Ionic,Monaca, TACO, Onsen UI, Visual Studio, GapDebug, App Builder, Cocoon, Framework7, Quasar Framework, Evothings Studio, NSB/AppStudio, Mobiscroll, the Intel XDK, and the Telerik Platform. These tools use Cordova, and not PhoneGap for their core tools.
Contributors to the Apache Cordova project include Adobe, BlackBerry, Google, IBM, Intel, Microsoft, Mozilla, and others.
First developed at an iPhoneDevCamp event in San Francisco, PhoneGap went on to win the People's Choice Award at O'Reilly Media's 2009 Web 2.0 Conference, and the framework has been used to develop many apps.Apple Inc. has confirmed that the framework has its approval, even with the new 4.0 developer license agreement changes. The PhoneGap framework is used by several mobile application platforms such as  Monaca,appMobi,Convertigo,ViziApps, and Worklight as the backbone of their mobile client development engine.
Adobe officially announced the acquisition of Nitobi Software (the original developer) on October 4, 2011. Coinciding with that, the PhoneGap code was contributed to the Apache Software Foundation to start a new project called Apache Cordova. The project's original name, Apache Callback, was viewed as too generic. Then, it also appears in Adobe Systems as Adobe PhoneGap and also as Adobe Phonegap Build.
Early versions of PhoneGap required an Apple computer to create iOS apps and a Windows computer to create Windows Mobile apps. After September 2012, Adobe's PhoneGap Build service allows programmers to upload CSS, HTML, and JavaScript source code to a ""cloud compiler"" that generates apps for every supported platform.
The core of Apache Cordova applications use CSS3 and HTML5 for their rendering and JavaScript for their logic. HTML5 provides access to underlying hardware such as the accelerometer, camera, and GPS. However, browsers' support for HTML5-based device access is not consistent across mobile browsers, particularly older versions of Android. To overcome these limitations, Apache Cordova embeds the HTML5 code inside a native WebView on the device, using a foreign function interface to access the native resources of it.
Apache Cordova can be extended with native plug-ins, allowing developers to add more functionalities that can be called from JavaScript, making it communicate directly between the native layer and the HTML5 page. These plugins allow access to the device's accelerometer, camera, compass, file system, microphone, and more.
However, the use of Web-based technologies leads some Apache Cordova applications to run slower than native applications with similar functionality.Adobe Systems warns that applications built with Apache Cordova may be rejected by Apple for being too slow or not feeling ""native"" enough (having appearance and functionality consistent with what users have come to expect on the platform).
Apache Cordova currently supports development for the operating systems Apple iOS, Bada, BlackBerry, Firefox OS, Google Android, LG webOS, Microsoft Windows Phone (7 and 8), Nokia Symbian OS, Tizen (SDK 2.x), and Ubuntu Touch. The table below is a list of supported features for each operating system."
"144","Thoughts on Flash is an open letter published by Steve Jobs, co-founder and then-chief executive officer of Apple Inc., on April 29, 2010. The letter criticized Adobe Systems' Flash platform and outlined reasons why the technology would not be allowed on the company's iOS hardware products, specifically iPhone, iPod Touch, and iPad. The letter drew immediate attention, with Adobe's CEO Shantanu Narayen firing back at Apple, stating that one of the issues in the criticism was due to Apple's own operating system rather than Flash technology, and that other criticism was false. At the time, various media publications had different opinions on the topic, with some citing business motivations rather than technological, hypocrisy in the letter despite agreeing with Jobs, and outright accusations of lying. However, retroactively, more publications have agreed with Jobs, with notable mentions of poor performance of Flash on Android devices.
That month, Apple had revised its iPhone Developer Agreement, adding new developer restrictions, particularly that only ""approved"" programming languages would be allowed on App Store. The change impacted a number of companies that had developed tools for porting applications from their respective programming languages into native iPhone apps, with one prominent example being an Adobe-developed ""Packager for iPhone"" tool. The policy was criticized as anti-competitive, and in May 2010, there were talks about which US government agency would launch an antitrust investigation into the matter. The following September, Apple again revised its policy, removing restrictions on third-party development tools. In November 2011, Adobe announced that it was ceasing development of its Flash Player plug-in for mobile web browsers, and instead shifting its focus towards building tools for developing applications for mobile app stores. The company announced in July 2017 that it would discontinue Flash altogether by the year 2020.
On April 29, 2010, Steve Jobs, the co-founder and then-chief executive officer of Apple Inc., published an open letter called ""Thoughts on Flash"" explaining why Apple would not allow Flash on the iPhone, iPod Touch and iPad. He cited the rapid energy consumption, computer crashes, poor performance on mobile devices, abysmal security, lack of touch support, and desire to avoid ""a third party layer of software coming between the platform and the developer"". He touched on the idea of Flash being ""open"", claiming that ""By almost any definition, Flash is a closed system"". He tried to dismiss the idea that Apple customers are missing out by being sold devices without Flash compatibility by quoting a number of statistics, concluding with ""Flash is no longer necessary to watch video or consume any kind of web content.""
The letter drew immediate attention. In response to Jobs' accusations, Adobe's CEO Shantanu Narayen described the open letter as an ""extraordinary attack"", and, during an interview with The Wall Street Journal, called the problems mentioned by Jobs' ""really a smokescreen"". He further fired back at Apple, stating that computer crashes were due to Apple's operating system, and that allegations of battery drain were ""patently false"". Various publications had different opinions on the topic. Wired's Brian Chen had in a 2009 article claimed that Apple would not allow Flash on the iPhone for business reasons, due to the technology being able to divert users away from the App Store. John Sullivan of Ars Technica agreed with Jobs, but highlighted the hypocrisy in his reasoning, writing that ""Every criticism he makes of Adobe's proprietary approach applies equally to Apple"". Dan Rayburn of Business Insider accused Steve Jobs of lying, particularly the sentiment that most content on the Internet is available in a different format.
Retroactively, more publications have agreed with Jobs. Ryan Lawler of TechCrunch wrote in 2012 that ""Jobs was right"", adding that Android users had poor experiences with watching Flash content and interactive Flash experiences were ""often wonky or didn’t perform well, even on high-powered phones"". Mike Isaac of Wired wrote in 2011 that ""In [our] testing of multiple Flash-compatible devices, choppiness and browser crashes were common"", and a former Adobe employee stated that ""Flash is a resource hog [...] It’s a battery drain, and it’s unreliable on mobile web browsers"". Kyle Wagner of Gizmodo wrote in 2011 that ""Adobe was never really able to smooth over performance, battery, and security issues"".
In April 2010, Apple announced changes to its iPhone Developer Agreement, with details on new developer restrictions, particularly that only apps built using ""approved"" programming languages would be allowed on App Store. The change impacted a number of companies that had developed tools for porting applications from their respective languages into native iPhone apps, with the most prominent example being Adobe's ""Packager for iPhone"", an iOS development tool in beta at the time.The New York Times quoted an Adobe supporter alleging the policy to be anti-competitive.
On May 3, 2010, New York Post reported that the US Federal Trade Commission (FTC) and the United States Department of Justice (DOJ) were deciding which agency would launch an antitrust investigation into the matter.
In September 2010, after having ""listened to our developers and taken much of their feedback to heart"", Apple removed the restrictions on third-party tools, languages and frameworks, and again allowing the deployment of Flash applications on iOS using Adobe's iOS Packager.
On November 8, 2011, Adobe announced that it was ceasing development of the Flash Player plug-in for web browsers on mobile devices, and shifting its focus towards building tools to develop applications for mobile app stores. In July 2017, the company announced its intention to discontinue Flash altogether by the year 2020."
"145","
A compiler is computer software that transforms computer code written in one programming language (the source language) into another programming language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.
However, there are many different types of compilers. If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is a cross-compiler. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis.
A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.
Compilers are not the only translators used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages.
Theoretical computing concepts developed by scientists, mathematicians, and engineers formed the basis of digital modern computing development during World War II. Primitive binary languages evolved because digital devices only understand ones and zeros and the circuit patterns in the underlying machine architecture. In the late forties, assembly languages were created to offer a more workable abstraction of the computer architectures. Limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed. Therefore, the compilation process needed to be divided into several small programs. The front end programs produce the analysis products used by the back end programs to generate target code. As computer technology provided more resources compiler designs could align better with the compilation process.
The human mind can design better solutions as the language moves from the machine to a higher level. So the development of high-level languages follows naturally from the capabilities offered by the digital computers. High-level languages are formal languages that are strictly defined by their syntax and semantics which form the high-level language architecture. Elements of these formal languages include:
The sentences in a language may be defined by a set rules called a grammar.
Backus-Naur form (BNF) describes the syntax of ""sentences"" of a language and was used for the syntax of Algol 60 by John Backus. The ideas derive from the context-free grammar concepts by Noam Chomsky, a linguist. ""BNF and its extensions have become standard tools for describing the syntax of programming notations, and in many cases parts of compilers are generated automatically from a BNF description.""
In the 1940s, Konrad Zuse designed an algorithmic programming language called Plankalkül (""Plan Calculus""). While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s. APL is a language for mathematical computations.
High-level language design during the formative years of digital computing provided useful programming tools for a variety of applications:
Compiler technology evolved from the need for a strictly defined transformation of the high-level source program into target low-level target program for the digital computer. The compiler could be viewed as a front end to deal with analysis of the source code and a back end to synthesize the analysis into the target code. Optimization between the front end and back end could produce more efficient target code.
Some early milestones in the development of compiler technology:
Early operating system and systems software were written in assembly language. In the 60s and early 70s, high-level languages for system programming was still controversial due to resource limitations. Still several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C.
BCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool. Several compilers have been implemented, Richards' book provides insights to the language and its compiler. BCPL was not only an influential systems programming language that is still used in research but also provided a basis for the design of B and C languages.
BLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W.A. Wulf's Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970.
Multics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbató from MIT. Multics was written in the PL/I language developed by IBM and IBM User Group. IBM's goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented. For the first few years of the Mulitics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs. EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.
Bell Labs left the Multics project in 1969: ""Over time, hope was replaced by frustration as the group effort initially failed to produce an economically useful system."" Continued participation would drive up project support costs. So researchers turned to other development efforts. A system programming language B based on BCPL concepts was written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix.
Bell Labs started development and expansion of C based on B and BCPL. The BCPL compiler had been transported to Multics by Bell Labs and BCPL was a preferred language at Bell Labs. Initially, a front-end program to Bell Labs' B compiler was used while a C compiler was developed. In 1971, a new PDP-11 provided the resource to define extensions to B and rewrite the compiler. By 1973 the design of C language was essentially complete and the Unix kernel for a PDP-11 was rewritten in C. Steve Johnson started development of Portable C Compiler (PCC) to support retargeting of C compilers to new machines.
Object-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science. At Bell Labs, the development of C++ became interested in OOP. C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983. The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew.
In many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex.
DARPA (Defense Advanced Research Projects Agency) sponsored a compiler project with Wulf's CMU research team in 1970. The Production Quality Compiler-Compiler PQCC design would produce a Production Quality Compiler (PQC) from formal definitions of source language and the target. PQCC tried to extend the term compiler-compiler beyond the traditional meaning as a parser generator (e.g., Yacc) without much success. PQCC might more properly be referred to as a compiler generator.
PQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure. The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation. Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the object-oriented programming language Ada.
The Ada Stoneman Document formalized the program support environment (APSE) along with the kernel (KAPSE) and minimal (MAPSE). An Ada interpreter NYU/ED supported development and standardization efforts with American National Standards Institute (ANSI) and the International Standards Organization (ISO). Initial Ada compiler development by the U.S. Military Services included the compilers in a complete integrated design environment along the lines of the Stoneman Document. Army and Navy worked on the Ada Language System (ALS) project targeted to DEC/VAX architecture while the Air Force started on the Ada Integrated Environment (AIE) targeted to IBM 370 series. While the projects did not provide the desired results, they did contribute to the overal effort on Ada development.
Other Ada compiler efforts got under way in Britain at University of York and in Germany at University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation. There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment.
High-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.
""When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security."" The ""Compiler Research: The Next 50 Years"" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets.
A compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end to end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets.
In the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once.
A compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process.
Classifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing lots of work and early computers did not have enough memory to contain one program that did all of this work. So compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations.
The ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal).
In some cases the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass.
The disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once.
Splitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program.
Regardless of the exact number of phases in the compiler design, the phases can be assigned to one of three stages. The stages include a front end, a middle end, and a back end.
This front/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs while sharing the optimizations of the middle end. Practical examples of this approach are the GNU Compiler Collection, LLVM, and the Amsterdam Compiler Kit, which have multiple front-ends, shared optimizations and multiple back-ends.
The front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.
While the frontend can be a single monolithic function or program, as in a scannerless parser, it is more commonly implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly today, the frontend is broken into three phases: lexical analysis (also known as lexing), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably line reconstruction and preprocessing, but these are rare.
The main phases of the front end include the following:
The middle end performs optimizations on the intermediate representation in order to improve the performance and the quality of the produced machine code. The middle end contains those optimizations that are independent of the CPU architecture being targeted.
The main phases of the middle end include the following:
Compiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation.
The scope of compiler analysis and optimizations vary greatly, from as small as a basic block to the procedure/function level, or even over the whole program (interprocedural optimization). Obviously,[clarification needed] a compiler can potentially do a better job using a broader view. But that broad view is not free: large scope analysis and optimizations are very costly in terms of compilation time and memory space; this is especially true for interprocedural analysis and optimizations.
Interprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The open source GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.
Due to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled.
The back end is responsible for the CPU architecture specific optimizations and for code generation.
The main phases of the back end include the following:
Compiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification.[self-published source?][non-primary source needed] Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.
Higher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that requires it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language — for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.
Interpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a directly executed program is needed somewhere at the bottom of the stack (see machine language).
Further, compilers can contain interpreters for optimization reasons. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further.
Some language specifications spell out that implementations must include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.
One classification of compilers is by the platform on which their generated code executes. This is known as the target platform.
A native or hosted compiler is one which output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.
The output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason such compilers are not usually classified as native or cross compilers.
The lower level language that is the target of a compiler may itself be a high-level programming language. C, often viewed as some sort of portable assembler, can also be the target language of a compiler. E.g.: Cfront, the original compiler for C++ used C as target language. The C created by such a compiler is usually not intended to be read and maintained by humans. So indent style and pretty C intermediate code are irrelevant. Some features of C turn it into a good target language. E.g.: C code with #line directives can be generated to support debugging of the original source.
While a common compiler type outputs machine code, there are many other types:
Compiler construction and compiler optimization are taught at universities and schools as part of a computer science curriculum.[non-primary source needed] Such courses are usually supplemented with the implementation of a compiler for an educational programming language. A well-documented example is Niklaus Wirth's PL/0 compiler, which Wirth used to teach compiler construction in the 1970s.[citation needed] In spite of its simplicity, the PL/0 compiler introduced several influential concepts to the field, including uses of:
High-level programming languages mature over time and lead to a need for Standardization. The American National Standards Institute (ANSI) and the International Organization for Standardization (ISO) manage standards for various programming languages such as FORTRAN, COBOL, C, C++ and so on.
Universities in conjunction with industry and government provide active research and development for programming languages and the associated language tools: compilers, integrated development environments, formal validation suites.
Professional organizations have representation from across the research, education, industry, and government. These include the Institute of Electrical and Electronic Engineers (IEEE) and Association for Computing Machinery (ACM).
A number of conferences in the field of programming languages present advances in compiler construction as one of their main topics.
ACM SIGPLAN supports a number of conferences, including:
The European Joint Conferences on Theory and Practice of Software (ETAPS) sponsors the International Conference on Compiler Construction, with papers from both the academic and industrial sectors.
Asian Symposium on Programming Languages and Systems (APLAS) is organized by the Asian Association for Foundation of Software (AAFS)."
"146","Bytecode, also termed portable code or p-code, is a form of instruction set designed for efficient execution by a software interpreter. Unlike human-readable source code, bytecodes are compact numeric codes, constants, and references (normally numeric addresses) that encode the result of compiler parsing and semantic analysis of things like type, scope, and nesting depths of program objects. 
The name bytecode stems from instruction sets that have one-byte opcodes followed by optional parameters. Intermediate representations such as bytecode may be output by programming language implementations to ease interpretation, or it may be used to reduce hardware and operating system dependence by allowing the same code to run cross-platform, on different devices. Bytecode may often be either directly executed on a virtual machine (a p-code machine i.e., interpreter), or it may be further compiled into machine code for better performance.
Since bytecode instructions are processed by software, they may be arbitrarily complex, but are nonetheless often akin to traditional hardware instructions: virtual stack machines are the most common, but virtual register machines have been built also.  Different parts may often be stored in separate files, similar to object modules, but dynamically loaded during execution.
A bytecode program may be executed by parsing and directly executing the instructions, one at a time. This kind of bytecode interpreter is very portable. Some systems, called dynamic translators, or just-in-time (JIT) compilers, translate bytecode into machine code as necessary at runtime. This makes the virtual machine hardware-specific, but doesn't lose the portability of the bytecode. For example, Java and Smalltalk code is typically stored in bytecoded format, which is typically then JIT compiled to translate the bytecode to machine code before execution. This introduces a delay before a program is run, when bytecode is compiled to native machine code, but improves execution speed considerably compared to interpreting source code directly, normally by several orders of magnitude.
Because of its performance advantage, today many language implementations execute a program in two phases, first compiling the source code into bytecode, and then passing the bytecode to the virtual machine. There are bytecode based virtual machines of this sort for Java, Python, PHP,Tcl, mawk and Forth (however, Forth is seldom compiled via bytecodes in this way, and its virtual machine is more generic instead). The implementation of Perl and Ruby 1.8 instead work by walking an abstract syntax tree representation derived from the source code.
More recently, the authors of V8 and Dart have challenged the notion that intermediate bytecode is needed for fast and efficient VM implementation. Both of these language implementations currently do direct JIT compiling from source code to machine code with no bytecode intermediary."
"147","In compiler design, static single assignment form (often abbreviated as SSA form or simply SSA) is a property of an intermediate representation (IR), which requires that each variable is assigned exactly once, and every variable is defined before it is used. Existing variables in the original IR are split into versions, new variables typically indicated by the original name with a subscript in textbooks, so that every definition gets its own version. In SSA form, use-def chains are explicit and each contains a single element.
SSA was proposed by Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck in 1988.Ron Cytron, Jeanne Ferrante and the previous three researchers at IBM developed an algorithm that can compute the SSA form efficiently.
One can expect to find SSA in a compiler for Fortran or C, whereas in functional language compilers, such as those for Scheme, ML and Haskell, continuation-passing style (CPS) is generally used.  SSA is formally equivalent to a well-behaved subset of CPS excluding non-local control flow, which does not occur when CPS is used as intermediate representation. So optimizations and transformations formulated in terms of one immediately apply to the other.
The primary usefulness of SSA comes from how it simultaneously simplifies and improves the results of a variety of compiler optimizations, by simplifying the properties of variables. For example, consider this piece of code:
Humans can see that the first assignment is not necessary, and that the value of y being used in the third line comes from the second assignment of y. A program would have to perform reaching definition analysis to determine this. But if the program is in SSA form, both of these are immediate:
Compiler optimization algorithms which are either enabled or strongly enhanced by the use of SSA include:
Converting ordinary code into SSA form is primarily a simple matter of replacing the target of each assignment with a new variable, and replacing each use of a variable with the ""version"" of the variable reaching that point. For example, consider the following control flow graph:

Changing the name on the left hand side of ""x ←{\displaystyle \leftarrow } x - 3"" and changing the following uses of x to that new name would leave the program unaltered. This can be exploited in SSA by creating two new variables: x1 and x2, each of which is assigned only once. Likewise, giving distinguishing subscripts to all the other variables yields:

It is clear which definition each use is referring to, except for one case: both uses of y in the bottom block could be referring to either y1 or y2, depending on which path the control flow took.
To resolve this, a special statement is inserted in the last block, called a Φ (Phi) function. This statement will generate a new definition of y called y3 by ""choosing"" either y1 or y2, depending on the control flow in the past.

Now, the last block can simply use y3, and the correct value will be obtained either way. A Φ function for x is not needed: only one version of x, namely x2 is reaching this place, so there is no problem (in other words, Φ(x2,x2)=x2).
Given an arbitrary control flow graph, it can be difficult to tell where to insert Φ functions, and for which variables. This general question has an efficient solution that can be computed using a concept called dominance frontiers (see below).
Φ functions are not implemented as machine operations on most machines. A compiler can implement a Φ function simply by using the same location in memory (or the same register) as the destination for any operation that produces an input to the Φ function. However, this approach does not work when simultaneous operations are speculatively producing inputs to a Φ function, as can happen on wide-issue machines. Typically, a wide-issue machine has a selection instruction used in such situations by the compiler to implement the Φ function.
According to Kenny Zadeck, Φ functions were originally known as phony functions while SSA was being developed at IBM Research in the 1980s. The formal name of a Φ function was only adopted when the work was first published in an academic paper.
First, we need the concept of a dominator: we say that a node A strictly dominates a different node B in the control flow graph if it is impossible to reach B without passing through A first. This is useful, because if we ever reach B we know that any code in A has run. We say that A dominates B (B is dominated by A) if either A strictly dominates B or A = B.
Now we can define the dominance frontier: a node B is in the dominance frontier of a node A if A does not strictly dominate B, but does dominate some immediate predecessor of B. (Possibly node A is an immediate predecessor of B. Then, because any node dominates itself and node A dominates itself, node B is in the dominance frontier of node A.) From A's point of view, these are the nodes at which other control paths, which don't go through A, make their earliest appearance.
Dominance frontiers capture the precise places at which we need Φ functions: if the node A defines a certain variable, then that definition and that definition alone (or redefinitions) will reach every node A dominates. Only when we leave these nodes and enter the dominance frontier must we account for other flows bringing in other definitions of the same variable. Moreover, no other Φ functions are needed in the control flow graph to deal with A's definitions, and we can do with no less.
One algorithm for computing the dominance frontier set is:
Note: in the code above, an immediate predecessor of node n is any node from which control is transferred to node n, and idom(b) is the node that immediately dominates node b (a singleton set).
There is an efficient algorithm for finding dominance frontiers of each node. This algorithm was originally described in Cytron et al. 1991. Also useful is chapter 19 of the book ""Modern compiler implementation in Java"" by Andrew Appel (Cambridge University Press, 2002). See the paper for more details.
Keith D. Cooper, Timothy J. Harvey, and Ken Kennedy of Rice University describe an algorithm in their paper titled A Simple, Fast Dominance Algorithm. The algorithm uses well-engineered data structures to improve performance.
""Minimal"" SSA inserts the minimal number of Φ functions required to ensure that each name is assigned a value exactly once and that each reference (use) of a name in the original program can still refer to a unique name.  (The latter requirement is needed to ensure that the compiler can write down a name for each operand in each operation.)
However, some of these Φ functions could be dead.  For this reason, minimal SSA does not necessarily produce the fewest number of Φ functions that are needed by a specific procedure.  For some types of analysis, these Φ functions are superfluous and can cause the analysis to run less efficiently.
Pruned SSA form is based on a simple observation: Φ functions are only needed for variables that are ""live"" after the Φ function. (Here, ""live"" means that the value is used along some path that begins at the Φ function in question.) If a variable is not live, the result of the Φ function cannot be used and the assignment by the Φ function is dead.
Construction of pruned SSA form uses live variable information in the Φ function insertion phase to decide whether a given Φ function is needed.  If the original variable name isn't live at the Φ function insertion point, the Φ function isn't inserted.
Another possibility is to treat pruning as a dead code elimination problem.  Then, a Φ function is live only if any use in the input program will be rewritten to it, or if it will be used as an argument in another Φ function.   When entering SSA form, each use is rewritten to the nearest definition that dominates it.  A Φ function will then be considered live as long as it is the nearest definition that dominates at least one use, or at least one argument of a live Φ.
Semi-pruned SSA form is an attempt to reduce the number of Φ functions without incurring the relatively high cost of computing live variable information.  It is based on the following observation: if a variable is never live upon entry into a basic block, it never needs a Φ function.  During SSA construction, Φ functions for any ""block-local"" variables are omitted.
Computing the set of block-local variables is a simpler and faster procedure than full live variable analysis, making semi-pruned SSA form more efficient to compute than pruned SSA form.  On the other hand, semi-pruned SSA form will contain more Φ functions.
SSA form is not normally used for direct execution (although it is possible to interpret SSA), and it is frequently used ""on top of"" another IR with which it remains in direct correspondence.  This can be accomplished by ""constructing"" SSA as a set of functions which map between parts of the existing IR (basic blocks, instructions, operands, etc.) and its SSA counterpart.  When the SSA form is no longer needed, these mapping functions may be discarded, leaving only the now-optimized IR.
Performing optimizations on SSA form usually leads to entangled SSA-Webs, meaning there are Φ instructions whose operands do not all have the same root operand. In such cases color-out algorithms are used to come out of SSA. Naive algorithms introduce a copy along each predecessor path which caused a source of different root symbol to be put in Φ than the destination of Φ. There are multiple algorithms for coming out of SSA with fewer copies, most use interference graphs or some approximation of it to do copy coalescing.
Extensions to SSA form can be divided into two categories.
Renaming scheme extensions alter the renaming criterion. Recall that SSA form renames each variable when it is assigned a value. Alternative schemes include static single use form (which renames each variable at each statement when it is used) and static single information form (which renames each variable when it is assigned a value, and at the post-dominance frontier).
Feature-specific extensions retain the single assignment property for variables, but incorporate new semantics to model additional features. Some feature-specific extensions model high-level programming language features like arrays, objects and aliased pointers. Other feature-specific extensions model low-level architectural features like speculation and predication.
SSA form is a relatively recent development in the compiler community.  As such, many older compilers only use SSA form for some part of the compilation or optimization process, but most do not rely on it.  Examples of compilers that rely heavily on SSA form include:"
"148","Standard Portable Intermediate Representation (SPIR) is an intermediate language for parallel compute and graphics by Khronos Group, originally developed for use with OpenCL. The current version, SPIR-V, was announced in March 2015.
OpenCL uses just-in-time compilation (JIT), necessitating one of two software distribution patterns: developers can distribute device-specific pre-compiled binaries, or they can distribute relevant source code, which is limited by the desire to protect intellectual property. SPIR enables the creation and distribution of device-independent binaries within the OpenCL stack.
SPIR was originally introduced in 2011, the current version SPIR-V having been introduced in 2015.
SPIR prior to the 2015 SPIR-V release was based on the LLVM Intermediate Representation. A provisional specification for SPIR 1.0 was announced in 2012. Version 1.2 was announced at SIGGRAPH 2013, with version 2.0 following at the same conference a year later.
SPIR-V is a rewritten version of SPIR announced in March 2015, and released on Nov. 16 2015. The SPIR family now includes a true cross-API standard that is fully defined by Khronos with native support for shader and kernel features.
Support for ingestion of SPIR-V is incorporated in the core specification of OpenCL 2.1, the Vulkan API, and OpenGL version 4.6.
SPIR-V is a high-level intermediate language, exchanged in binary form. Functions are represented by a control flow graph of basic blocks, using static single assignment (SSA) form. Data structures retain high-level hierarchical representation. It is not lossy like previous byte-code or virtual machine-like intermediate representations used for graphical shaders. This allows higher performance lowering to target devices."
"149","In programming languages, closures (also lexical closures or function closures) are techniques for implementing lexically scoped name binding in languages with first-class functions. Operationally, a closure is a record storing a function[lower-alpha 1] together with an environment: a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created.[lower-alpha 2] A closure—unlike a plain function—allows the function to access those captured variables through the closure's copies of their values or references, even when the function is invoked outside their scope.
Example. The following program fragment defines a higher-order function startAt with a parameter x and a nested function incrementBy. The nested function incrementBy has access to x, because incrementBy is in the lexical scope of x, even though x is not local to incrementBy. The function startAt returns a closure containing a copy of the value of x or a copy of the reference to x from this invocation of startAt, and the function incrementBy, which adds the value of y to the value of x:
Note that, as startAt returns a function, the variables closure1 and closure2 are of function type. Invoking closure1(3) (Meaning y=3) will return 4, while invoking closure2(3) (Meaning y=3) will return 8. While closure1 and closure2 refer to the same function incrementBy, the associated environments differ, and invoking the closures will bind the name x to two distinct variables with different values in the two invocations, thus evaluating the function to different results.
The concept of closures was developed in the 1960s for the mechanical evaluation of expressions in the λ-calculus and was first fully implemented in 1970 as a language feature in the PAL programming language to support lexically scoped first-class functions.:Turner's section 2, note 8 contains his claim about M-expressions
Peter J. Landin defined the term closure in 1964 as having an environment part and a control part as used by his SECD machine for evaluating expressions.Joel Moses credits Landin with introducing the term closure to refer to a lambda expression whose open bindings (free variables) have been closed by (or bound in) the lexical environment, resulting in a closed expression, or closure. This usage was subsequently adopted by Sussman and Steele when they defined Scheme in 1975, a lexically scoped variant of LISP, and became widespread.
The term closure is often mistakenly used to mean anonymous function. This is probably because many programmers learn about both concepts at the same time, in the form of small helper functions that are anonymous closures. An anonymous function is a function literal without a name, while a closure is an instance of a function, a value, whose non-local variables have been bound either to values or to storage locations (depending on the language; see the lexical environment section below).
For example, in the following Python code:
the values of a and b are closures, in both cases produced by returning a nested function with a free variable from an enclosing function, so that the free variable binds to the parameter x of the enclosing function. However, in the first case the nested function has a name, g, while in the second case the nested function is anonymous. The closures need not be assigned to a variable, and can be used directly, as in the last lines—the original name (if any) used in defining them is irrelevant. This usage may be deemed an ""anonymous closure"".
Note especially that the nested function definitions are not themselves closures: they have a free variable, which is not yet bound. Only once the enclosing function is evaluated with a value for the parameter is the free variable of the nested function bound, creating a closure, which is then returned from the enclosing function.
Lastly, a closure is only distinct from a function with free variables when outside of the scope of the non-local variables, otherwise the defining environment and the execution environment coincide and there is nothing to distinguish these (static and dynamic binding can't be distinguished because the names resolve to the same values). For example, in the below program, functions with a free variable x (bound to the non-local variable x with global scope) are executed in the same environment where x is defined, so it is immaterial whether these are actually closures:
This is most often achieved by a function return, since the function must be defined within the scope of the non-local variables, in which case typically its own scope will be smaller.
This can also be achieved by variable shadowing (which reduces the scope of the non-local variable), though this is less common in practice, as it is less useful and shadowing is discouraged. In this example f can be seen to be a closure because x in the body of f is bound to the x in the global namespace, not the x local to g:
The use of closures is associated with languages where functions are first-class objects, in which functions can be returned as results from higher-order functions, or passed as arguments to other function calls; if functions with free variables are first-class, then returning one creates a closure. This includes functional programming languages such as Lisp and ML, as well as many modern garbage-collected imperative languages, such as Python. Closures are also frequently used with callbacks, particularly for event handlers, such as in JavaScript, where they are used for interactions with a dynamic web page. Traditional imperative languages such as Algol, C and Pascal either do not support nested functions (C) or do not support calling nested functions after the enclosing function has exited (GNU C, Pascal), thus avoiding the need to use closures.
Closures are used to implement continuation-passing style, and in this manner, hide state. Constructs such as objects and control structures can thus be implemented with closures. In some languages, a closure may occur when a function is defined within another function, and the inner function refers to local variables of the outer function. At run-time, when the outer function executes, a closure is formed, consisting of the inner function’s code and references (the upvalues) to any variables of the outer function required by the closure.
Closures typically appear in languages in which functions are first-class values—in other words, such languages enable functions to be passed as arguments, returned from function calls, bound to variable names, etc., just like simpler types such as strings and integers. For example, consider the following Scheme function:
In this example, the lambda expression (lambda (book) (>= (book-sales book) threshold)) appears within the function best-selling-books.  When the lambda expression is evaluated, Scheme creates a closure consisting of the code for the lambda expression and a reference to the threshold variable, which is a free variable inside the lambda expression.
The closure is then passed to the filter function, which calls it repeatedly to determine which books are to be added to the result list and which are to be discarded. Because the closure itself has a reference to threshold, it can use that variable each time filter calls it. The function filter itself might be defined in a completely separate file.
Here is the same example rewritten in JavaScript, another popular language with support for closures:
The function keyword is used here instead of lambda, and an Array.filter method instead of a global filter function, but otherwise the structure and the effect of the code are the same.
A function may create a closure and return it, as in the following example:
Because the closure in this case outlives the execution of the function that creates it, the variables f and dx live on after the function derivative returns, even though execution has left their scope and they are no longer visible. In languages without closures, the lifetime of an automatic local variable coincides with the execution of the stack frame where that variable is declared. In languages with closures, variables must continue to exist as long as any existing closures have references to them. This is most commonly implemented using some form of garbage collection.
A closure can be used to associate a function with a set of ""private"" variables, which persist over several invocations of the function. The scope of the variable encompasses only the closed-over function, so it cannot be accessed from other program code.
In stateful languages, closures can thus be used to implement paradigms for state representation and information hiding, since the closure's upvalues (its closed-over variables) are of indefinite extent, so a value established in one invocation remains available in the next.  Closures used in this way no longer have referential transparency, and are thus no longer pure functions; nevertheless, they are commonly used in impure functional languages such as Scheme.
Closures have many uses:
Note: Some speakers call any data structure that binds a lexical environment a closure, but the term usually refers specifically to functions.
Closures are typically implemented with a special data structure that contains a pointer to the function code, plus a representation of the function's lexical environment (i.e., the set of available variables) at the time when the closure was created. The referencing environment binds the non-local names to the corresponding variables in the lexical environment at the time the closure is created, additionally extending their lifetime to at least as long as the lifetime of the closure itself. When the closure is entered at a later time, possibly with a different lexical environment, the function is executed with its non-local variables referring to the ones captured by the closure, not the current environment.
A language implementation cannot easily support full closures if its run-time memory model allocates all automatic variables on a linear stack. In such languages, a function's automatic local variables are deallocated when the function returns. However, a closure requires that the free variables it references survive the enclosing function's execution. Therefore, those variables must be allocated so that they persist until no longer needed, typically via heap allocation, rather than on the stack, and their lifetime must be managed so they survive until all closures referencing them are no longer in use.
This explains why, typically, languages that natively support closures also use garbage collection. The alternatives are manual memory management of non-local variables (explicitly allocating on the heap and freeing when done), or, if using stack allocation, for the language to accept that certain use cases will lead to undefined behaviour, due to dangling pointers to freed automatic variables, as in lambda expressions in C++11 or nested functions in GNU C. The funarg problem (or ""functional argument"" problem) describes the difficulty of implementing functions as first class objects in a stack-based programming language such as C or C++. Similarly in D version 1, it is assumed that the programmer knows what to do with delegates and automatic local variables, as their references will be invalid after return from its definition scope (automatic local variables are on the stack) – this still permits many useful functional patterns, but for complex cases needs explicit heap allocation for variables. D version 2 solved this by detecting which variables must be stored on the heap, and performs automatic allocation. Because D uses garbage collection, in both versions, there is no need to track usage of variables as they are passed.
In strict functional languages with immutable data (e.g. Erlang), it is very easy to implement automatic memory management (garbage collection), as there are no possible cycles in variables' references. For example, in Erlang, all arguments and variables are allocated on the heap, but references to them are additionally stored on the stack. After a function returns, references are still valid. Heap cleaning is done by incremental garbage collector.
In ML, local variables are lexically scoped, and hence define a stack-like model, but since they are bound to values and not to objects, an implementation is free to copy these values into the closure's data structure in a way that is invisible to the programmer.
Scheme, which has an ALGOL-like lexical scope system with dynamic variables and garbage collection, lacks a stack programming model and does not suffer from the limitations of stack-based languages. Closures are expressed naturally in Scheme. The lambda form encloses the code, and the free variables of its environment persist within the program as long as they can possibly be accessed, and so they can be used as freely as any other Scheme expression.[citation needed]
Closures are closely related to Actors in the Actor model of concurrent computation where the values in the function's lexical environment are called acquaintances. An important issue for closures in concurrent programming languages is whether the variables in a closure can be updated and, if so, how these updates can be synchronized. Actors provide one solution.
Closures are closely related to function objects; the transformation from the former to the latter is known as defunctionalization or lambda lifting; see also closure conversion.[citation needed]
As different languages do not always have a common definition of the lexical environment, their definitions of closure may vary also. The commonly held minimalist definition of the lexical environment defines it as a set of all bindings of variables in the scope, and that is also what closures in any language have to capture. However the meaning of a variable binding also differs. In imperative languages, variables bind to relative locations in memory that can store values.  Although the relative location of a binding does not change at runtime, the value in the bound location can. In such languages, since closure captures the binding, any operation on the variable, whether done from the closure or not, are performed on the same relative memory location. This is often called capturing the variable ""by reference"". Here is an example illustrating the concept in ECMAScript, which is one such language:
Note how function foo and the closures referred to by variables f and g all use the same relative memory location signified by local variable x.
On the other hand, many functional languages, such as ML, bind variables directly to values. In this case, since there is no way to change the value of the variable once it is bound, there is no need to share the state between closures—they just use the same values. This is often called capturing the variable ""by value"". Java's local and anonymous classes also fall into this category—they require captured local variables to be final, which also means there is no need to share state.
Some languages enable you to choose between capturing the value of a variable or its location. For example, in C++11, captured variables are either declared with [&], which means captured by reference, or with [=], which means captured by value.
Yet another subset, lazy functional languages such as Haskell, bind variables to results of future computations rather than values. Consider this example in Haskell:
The binding of r captured by the closure defined within function foo is to the computation (x / y)—which in this case results in division by zero. However, since it is the computation that is captured, and not the value, the error only manifests itself when the closure is invoked, and actually attempts to use the captured binding.
Yet more differences manifest themselves in the behavior of other lexically scoped constructs, such as return, break and continue statements. Such constructs can, in general, be considered in terms of invoking an escape continuation established by an enclosing control statement (in case of break and continue, such interpretation requires looping constructs to be considered in terms of recursive function calls). In some languages, such as ECMAScript, return refers to the continuation established by the closure lexically innermost with respect to the statement—thus, a return within a closure transfers control to the code that called it. However, in Smalltalk, the superficially similar operator ^ invokes the escape continuation established for the method invocation, ignoring the escape continuations of any intervening nested closures. The escape continuation of a particular closure can only be invoked in Smalltalk implicitly by reaching the end of the closure's code. The following examples in ECMAScript and Smalltalk highlight the difference:
The above code snippets will behave differently because the Smalltalk ^ operator and the JavaScript return operator are not analogous.  In the ECMAScript example, return x will leave the inner closure to begin a new iteration of the forEach loop, whereas in the Smalltalk example, ^x will abort the loop and return from the method foo.
Common Lisp provides a construct that can express either of the above actions: Lisp (return-from foo x) behaves as Smalltalk ^x, while Lisp (return-from nil x) behaves as JavaScript return x. Hence, Smalltalk makes it possible for a captured escape continuation to outlive the extent in which it can be successfully invoked. Consider:
When the closure returned by the method foo is invoked, it attempts to return a value from the invocation of foo that created the closure. Since that call has already returned and the Smalltalk method invocation model does not follow the spaghetti stack discipline to facilitate multiple returns, this operation results in an error.
Some languages, such as Ruby, enable the programmer to choose the way return is captured. An example in Ruby:
Both Proc.new and lambda in this example are ways to create a closure, but semantics of the closures thus created are different with respect to the return statement.
In Scheme, definition and scope of the return control statement is explicit (and only arbitrarily named 'return' for the sake of the example). The following is a direct translation of the Ruby sample.
Features of some languages simulate some features of closures. Language features include some  object-oriented techniques,  for example in Java, C++, Objective-C, C#, D.
Some C libraries support 
callbacks.  This is 
sometimes implemented by providing two values when 
registering the callback with the library: a function 
pointer and a separate void* pointer to 
arbitrary data of the user's choice. When the library 
executes the callback function, it passes along the data 
pointer. This enables the callback to maintain state and 
to refer to information captured at the time it was 
registered with the library. The idiom is similar to 
closures in functionality, but not in syntax. The 
void* pointer is not type safe so this C
idiom differs from type-safe closures in C#, Haskell or ML.
Nested function and function pointer(C)
With a gcc extension, a nested function can be used and a function pointer can emulate closures, providing the containing function does not exit. The example below is invalid:
Java enables classes to be defined inside methods.  These are called local classes.  When such classes are not named, they are known as anonymous classes (or anonymous inner classes).  A local class (either named or anonymous) may refer to names in lexically enclosing classes, or read-only variables (marked as final) in the lexically enclosing method.
The capturing of final variables enables you to capture variables by value. Even if the variable you want to capture is non-final, you can always copy it to a temporary final variable just before the class.
Capturing of variables by reference can be emulated by using a final reference to a mutable container, for example, a single-element array. The local class will not be able to change the value of the container reference itself, but it will be able to change the contents of the container.
With the advent of Java 8's lambda expressions, the closure causes the above code to be executed as:
Local classes are one of the types of inner class that are declared within the body of a method.  Java also supports inner classes that are declared as non-static members of an enclosing class. They are normally referred to just as ""inner classes"". These are defined in the body of the enclosing class and have full access to instance variables of the enclosing class. Due to their binding to these instance variables, an inner class may only be instantiated with an explicit binding to an instance of the enclosing class using a special syntax.
Upon execution, this will print the integers from 0 to 9. Beware to not confuse this type of class with the nested class, which is declared in the same way with an accompanied usage of the ""static"" modifier; those have not the desired effect but are instead just classes with no special binding defined in an enclosing class.
As of Java 8, Java supports functions as first class objects. Lambda expressions of this form are considered of type Function<T,U> with T being the domain and U the image type. The expression can be called with its .apply(T t) method, but not with a standard method call.
Apple introduced Blocks, a form of closure, as a nonstandard extension into C, C++, Objective-C 2.0 and in Mac OS X 10.6 ""Snow Leopard"" and iOS 4.0. Apple made their implementation available for the GCC and clang compilers.
Pointers to block and block literals are marked with ^. Normal local variables are captured by value when the block is created, and are read-only inside the block. Variables to be captured by reference are marked with __block. Blocks that need to persist outside of the scope they are created in may need to be copied.
C# anonymous methods and lambda expressions support closure:
In D, closures are implemented by delegates, a function pointer paired with a context pointer (e.g. a class instance, or a stack frame on the heap in the case of closures).
D version 1, has limited closure support. For example, the above code will not work correctly, because the variable a is on the stack, and after returning from test(), it is no longer valid to use it (most probably calling foo via dg(), will return a 'random' integer). This can be solved by explicitly allocating the variable 'a' on heap, or using structs or class to store all needed closed variables and construct a delegate from a method implementing the same code. Closures can be passed to other functions, as long as they are only used while the referenced values are still valid (for example calling another function with a closure as a callback parameter), and are useful for writing generic data processing code, so this limitation, in practice, is often not an issue.
This limitation was fixed in D version 2 - the variable 'a' will be automatically allocated on the heap because it is used in the inner function, and a delegate of that function can escape the current scope (via assignment to dg or return). Any other local variables (or arguments) that are not referenced by delegates or that are only referenced by delegates that don't escape the current scope, remain on the stack, which is simpler and faster than heap allocation. The same is true for inner's class methods that references a function's variables.
C++ enables defining function objects by overloading operator(). These objects behave somewhat like functions in a functional programming language. They may be created at runtime and may contain state, but they do not implicitly capture local variables as closures do. As of the 2011 revision, the C++ language also supports closures, which are a type of function object constructed automatically from a special language construct called lambda-expression. A C++ closure may capture its context either by storing copies of the accessed variables as members of the closure object or by reference. In the latter case, if the closure object escapes the scope of a referenced object, invoking its operator() causes undefined behavior since C++ closures do not extend the lifetime of their context.
Eiffel includes inline agents defining closures. An inline agent is an object representing a routine, defined by giving the code of the routine in-line. For example, in
the argument to subscribe is an agent, representing a procedure with two arguments; the procedure finds the country at the corresponding coordinates and displays it. The whole agent is ""subscribed"" to the event type click_event for a
certain button, so that whenever an instance of the event type occurs on that button — because a user has clicked the button — the procedure will be executed with the mouse coordinates being passed as arguments for x and y.
The main limitation of Eiffel agents, which distinguishes them from closures in other languages, is that they cannot reference local variables from the enclosing scope. This design decision helps in avoiding ambiguity when talking about a local variable value in a closure - should it be the latest value of the variable or the value captured when the agent is created? Only Current (a reference to current object, analogous to this in Java), its features, and arguments of the agent itself can be accessed from within the agent body. The values of the outer local variables can be passed by providing additional closed operands to the agent.
"
"150","In computer programming, a nested function (or nested procedure or subroutine) is a function which is defined within another function, the enclosing function. Due to simple recursive scope rules, a nested function is itself invisible outside of its immediately enclosing function, but can see (access) all local objects (data, functions, types, etc.) of its immediately enclosing function as well as of any function(s) which, in turn, encloses that function. The nesting is theoretically possible to unlimited depth, although only a few levels are normally used in practical programs.
Nested functions are used in many approaches to structured programming, including early ones, such as ALGOL, Simula 67 and Pascal, and also in many modern dynamic languages and functional languages. However, they are traditionally not supported in the (originally simple) C-family of languages.
Nested functions assumes function scope or block scope. The scope of a nested function is inside the enclosing function, i.e. inside one of the constituent blocks of that function, which means that it is invisible outside that block and also outside the enclosing function. A nested function can access other local functions, variables, constants, types, classes, etc. that are in the same scope, or in any enclosing scope, without explicit parameter passing, which greatly simplifies passing data into and out of the nested function. This is typically allowed for both reading and writing.
Nested functions may in certain situations (and languages) lead to the creation of a closure. If it is possible for the nested function to escape the enclosing function, for example if functions are first class objects and a nested function is passed to another function or returned from the enclosing function, then a closure is created and calls to this function can access the environment of the original function. The frame of the immediately enclosing function must continue to be alive until the last referencing closure dies and non-local automatic variables referenced in closures can therefore not be stack allocated. This is known as the funarg problem and is a key reason why nested functions was not implemented in some simpler languages as it significantly complicates code generation and analysis, especially when functions are nested to various levels, sharing different parts of their environment.
An example using Pascal syntax (with ALGOL, Modula 2, Oberon, Ada, etc. similar):
The function F is nested within E. Note that E's parameter x is visible also in F (as F is a part of E) while both x and y are invisible outside E and F respectively.
Similarly, in Standard ML:
One way to write the same example in Haskell syntax:
The same example in GNU C syntax (C extended with nested functions):
A more realistic example is this implementation of quicksort:
Another example is the following implementation of the Hoare partition based quicksort using C++11 lambda expression syntax:
Lexically nested function definitions are a form of information hiding and are useful for dividing procedural tasks into subtasks which are only meaningful locally. This avoids cluttering other parts of the program with functions and variables that are unrelated to those parts.
They are typically used as helper functions or as recursive functions inside another function (as in the quicksort example above). This has the structural benefit of organizing the code, avoids polluting the scope, and also allows functions to share state easily. As nested function can access local variables of the enclosing function, sharing of state is possible without passing parameters to the nested function or use a global variable, simplifying code.
In languages with nested functions, functions may normally also contain local constants, and types (in addition to local variables, parameters, and functions), encapsulated and hidden in the same nested manner, at any level of depth. This may further enhance the code structuring possibilities.
Nested functions can also be used for unstructured control flow, by using the return statement for general unstructured control flow. This can be used for finer-grained control than is possible with other built-in features of the language – for example, it can allow early termination of a for loop if break is not available, or early termination of a nested for loop if a multi-level break or exceptions are not available.
As in most languages functions are valid return types, it is possible to create a nested function that accesses a set of parameters from the outer function and have that function be the outer function's return value. Thus it is possible to return a function that is set to fulfill a certain task with little or no further parameters given to it, which can increase performance quite significantly.
The main alternative to nested functions in languages that lack support for them is to place all relevant functions and variables in a separate module (file) and expose only the top-level wrapper function publicly. In C this will generally be done by using static functions for encapsulation and static variables for communication. This achieves encapsulation and sharing of state, though not the logical organization given by lexical nesting of functions, and comes at the cost of having a separate file. It is also not possible in more than a single level.
Another alternative is to share state between the functions through function parameters, most often passing references as arguments to avoid the cost of copying. In C this is generally implemented by a pointer to a structure containing the context. This significantly increases the complexity of the function calls.
In PHP and other languages the anonymous function is the only alternative: the nested function is declared not as usual function, but by reference, as a local variable. To use local variables in the anonymous function, use closure.
Well known languages supporting lexically nested functions include:
In most functional programming languages, such as Scheme, nested functions are a common way of implementing algorithms with loops in them. A simple (tail) recursive inner function is created, which behaves as the algorithm's main loop, while the outer function performs startup actions that only need to be done once. In more complex cases, a number of mutually recursive functions may be created as inner functions.
Certain languages do not have straightforward syntactic and semantic support to implement nested functions. Nevertheless, for some of them the idea of nested functions can be simulated with some degree of difficulty through the use of other language constructs. The following languages can approximate nested functions through the respective strategies:
Implementation of nested functions can be more involved than it may appear, as a reference to a nested function that references non-local variables creates a closure. For this reason nested functions are not supported in some languages such as C, C++ or Java as this makes compilers more difficult to implement. However, some compilers do support them, as a compiler specific extension. A well known example of this is the GNU C implementation of C which shares code with compilers for languages such as Pascal, Ada and Modula.
There are several ways to implement nested procedures in a lexically scoped language, but the classic way is as follows:
This original method is faster than it may seem, but it is nevertheless often optimized in practical modern compilers (using displays or similar techniques).
Another way to implement nested functions that is used by some compilers is to convert (""lift"") nested functions into non-nested functions (where extra, hidden, parameters replace the access links) using a process known as lambda lifting during an intermediate stage in the compilation.
In order for local functions with lexically scoped nonlocals to be passed as results, the language runtime code must also implicitly pass the environment (data) that the function sees inside its encapsulating function, so that it is reachable also when the current activation of the encosing function no longer exists. This means that the environment must be stored in another memory area than (the subsequently reclaimed parts of) a chronologically based execution stack, which, in turn, implies some sort of freely dynamic memory allocation. Many older Algol based languages (or dialects thereof) does therefore not allow local functions that access nonlocals to be passed as return values, or do they not allow functions as return values at all, although passing of such functions as arguments may still be possible.
At least one implementation of nested functions cause a loss of No-execute stacks (NX stack). GCC's nested function implementation calls nested functions through a jump instruction put in the machine stack at runtime. This requires the stack to be executable.
No execute stacks and nested functions are mutually exclusive under GCC. If a nested function is used in the development of a program, then the NX Stack is silently lost. GCC offers the -Wtrampoline warning to alert of the condition.
Software engineered using Secure Development Lifecycle often do not allow the use of nested functions in this particular compiler (GCC) due to the loss of NX Stacks."
"151","A cross compiler is a compiler capable of creating executable code for a platform other than the one on which the compiler is running.  For example, a compiler that runs on a Windows 7 PC but generates code that runs on Android smartphone is a cross compiler.
A cross compiler is necessary to compile for multiple platforms from one machine. A platform could be infeasible for a compiler to run on, such as for the microcontroller of an embedded system because those systems contain no operating system. In paravirtualization one machine runs many operating systems, and a cross compiler could generate an executable for each of them from one main source.
Cross compilers are not to be confused with source-to-source compilers. A cross compiler is for cross-platform software development of binary code, while a source-to-source compiler translates from one programming language to another in text code. Both are programming tools.
The fundamental use of a cross compiler is to separate the build environment from target environment.  This is useful in a number of situations:
Use of virtual machines (such as Java's JVM) resolves some of the reasons for which cross compilers were developed. The virtual machine paradigm allows the same compiler output to be used across multiple target systems, although this is not always ideal because virtual machines are often slower and the compiled program can only be run on computers with that virtual machine.
Typically the hardware architecture differs (e.g. compiling a program destined for the MIPS architecture on an x86 computer) but cross-compilation is also applicable when only the operating system environment differs, as when compiling a FreeBSD program under Linux, or even just the system library, as when compiling programs with uClibc on a glibc host.
The Canadian Cross is a technique for building cross compilers for other machines. Given three machines A, B, and C, one uses machine A (e.g. running Windows XP on an IA-32 processor) to build a cross compiler that runs on machine B (e.g. running Mac OS X on an x86-64 processor) to create executables for machine C (e.g. running Android on an ARM processor).  When using the Canadian Cross with GCC, there may be four compilers involved:

The end-result cross compiler (4) will not be able to run on build machine A; instead it would run on machine B to compile an application into executable code that would then be copied to machine C and executed on machine C.
For instance, NetBSD provides a POSIX Unix shell script named build.sh which will first build its own toolchain with the host's compiler; this, in turn, will be used to build the cross-compiler which will be used to build the whole system.
The term Canadian Cross came about because at the time that these issues were under discussion, Canada had three national political parties.
GCC, a free software collection of compilers, can be set up to cross compile. It supports many platforms and languages.
GCC requires that a compiled copy of binutils be available for each targeted platform. Especially important is the GNU Assembler. Therefore, binutils first has to be compiled correctly with the switch --target=some-target sent to the configure script. GCC also has to be configured with the same --target option. GCC can then be run normally provided that the tools, which binutils creates, are available in the path, which can be done using the following (on UNIX-like operating systems with bash):
Cross compiling GCC requires that a portion of the target platform's C standard library be available on the host platform. The programmer may choose to compile the full C library, but this choice could be unreliable. The alternative is to use newlib, which is a small C library containing only the most essential components required to compile C source code. 
The GNU autotools packages (i.e. autoconf, automake, and libtool) use the notion of a build platform, a host platform, and a target platform.  The build platform is where the compiler is actually compiled. In most cases, build should be left undefined (it will default from host). The host platform is where the output artifacts from the compiler will be executed. The target platform is used when cross compiling cross compilers, it represents what type of object code the package itself will produce; otherwise the target platform setting is irrelevant.  For example, consider cross-compiling a video game that will run on a Dreamcast.  The machine where the game is compiled is the build platform while the Dreamcast is the host platform.
Another method popularly used by embedded Linux developers involves the combination of GCC compilers with specialized sandboxes like Scratchbox, scratchbox2, or PRoot.   These tools create a ""chrooted"" sandbox where the programmer can build up necessary tools, libc, and libraries without having to set extra paths. Facilities are also provided to ""deceive"" the runtime so that it ""believes"" it is actually running on the intended target CPU (such as an ARM architecture); this allows configuration scripts and the like to run without error. Scratchbox runs more slowly by comparison to ""non-chrooted"" methods, and most tools that are on the host must be moved into Scratchbox to function.
Manx Software Systems, of Shrewsbury, New Jersey, produced C compilers beginning in the 1980s targeted at professional developers for a variety of platforms up to and including PCs and Macs.
Manx's Aztec C programming language was available for a variety of platforms including MS-DOS, Apple II, DOS 3.3 and ProDOS, Commodore 64, Macintosh 68XXX and Amiga.
From the 1980s and continuing throughout the 1990s until Manx Software Systems disappeared, the MS-DOS version of Aztec C was offered both as a native mode compiler or as a cross compiler for other platforms with different processors including the Commodore 64 and Apple II. Internet distributions still exist for Aztec C including their MS-DOS based cross compilers. They are still in use today.
Manx's Aztec C86, their native mode 8086 MS-DOS compiler, was also a cross compiler. Although it did not compile code for a different processor like their Aztec C65 6502 cross compilers for the Commodore 64 and Apple II, it created binary executables for then-legacy operating systems for the 16 bit 8086 family of processors.
When the IBM PC was first introduced it was available with a choice of operating systems, CP/M-86 and PC DOS being two of them. Aztec C86 was provided with link libraries for generating code for both IBM PC operating systems. Throughout the 1980s later versions of Aztec C86 (3.xx, 4.xx and 5.xx) added support for MS-DOS ""transitory"" versions 1 and 2 and which were less robust than the ""baseline"" MS-DOS version 3 and later which Aztec C86 targeted until its demise.
Finally, Aztec C86 provided C language developers with the ability to produce ROM-able ""HEX"" code which could then be transferred using a ROM burner directly to an 8086 based processor. Paravirtualization may be more common today but the practice of creating low-level ROM code was more common per-capita during those years when device driver development was often done by application programmers for individual applications, and new devices amounted to a cottage industry. It was not uncommon for application programmers to interface directly with hardware without support from the manufacturer. This practice was similar to Embedded Systems Development today.
Thomas Fenwick and James Goodnow II were the two principal developers of Aztec-C. Fenwick later became notable as the author of the Microsoft Windows CE kernel or NK (""New Kernel"") as it was then called.
Microsoft C (MSC) has a long history dating back to the 1980s. The first Microsoft C Compilers were made by the same company who made Lattice C and were rebranded by Microsoft as their own, until MSC 4 was released, which was the first version that Microsoft produced themselves.
In 1987 many developers started switching to Microsoft C, and many more would follow throughout the development of Microsoft Windows to its present state. Products like Clipper and later Clarion emerged that offered easy database application development by using cross language techniques, allowing part of their programs to be compiled with Microsoft C.
C programs had long been linked with modules written in assembly language. Most C compilers (even current compilers) offer an assembly language pass (that can be tweaked for efficiency then linked to the rest of the program after assembling).
Compilers like Aztec-C converted everything to assembly language as a distinct pass and then assembled the code in a distinct pass, and were noted for their very efficient and small code, but by 1987 the optimizer built into Microsoft C was very good, and only ""mission critical"" parts of a program were usually considered for rewriting. In fact, C language programming had taken over as the ""lowest-level"" language, with programming becoming a multi-disciplinary growth industry and projects becoming larger, with programmers writing user interfaces and database interfaces in higher-level languages, and a need had emerged for cross language development that continues to this day.
By 1987, with the release of MSC 5.1, Microsoft offered a cross language development environment for MS-DOS. 16 bit binary object code written in assembly language (MASM) and Microsoft's other languages including QuickBASIC, Pascal, and Fortran could be linked together into one program, in a process they called ""Mixed Language Programming"" and now ""InterLanguage Calling"". If BASIC was used in this mix, the main program needed to be in BASIC to support the internal runtime system that compiled BASIC required for garbage collection and its other managed operations that simulated a BASIC interpreter like QBasic in MS-DOS.
The calling convention for C code in particular was to pass parameters in ""reverse order"" on the stack and return values on the stack rather than in a processor register. There were other programming rules to make all the languages work together, but this particular rule persisted through the cross language development that continued throughout Windows 16 and 32 bit versions and in the development of programs for OS/2, and which persists to this day. It is known as the Pascal calling convention.
Another type of cross compilation that Microsoft C was used for during this time was in retail applications that require handheld devices like the Symbol Technologies PDT3100 (used to take inventory), which provided a link library targeted at an 8088 based barcode reader. The application was built on the host computer then transferred to the handheld device (via a serial cable) where it was run, similar to what is done today for that same market using Windows Mobile by companies like Motorola, who bought Symbol.
Throughout the 1990s and beginning with MSC 6 (their first ANSI C compliant compiler) Microsoft re-focused their C compilers on the emerging Windows market, and also on OS/2 and in the development of GUI programs. Mixed language compatibility remained through MSC 6 on the MS-DOS side, but the API for Microsoft Windows 3.0 and 3.1 was written in MSC 6. MSC 6 was also extended to provide support for 32-bit assemblies and support for the emerging Windows for Workgroups and Windows NT which would form the foundation for Windows XP. A programming practice called a thunk was introduced to allow passing between 16 and 32 bit programs that took advantage of runtime binding (dynamic linking) rather than the static binding that was favoured in monolithic 16 bit MS-DOS applications. Static binding is still favoured by some native code developers but does not generally provide the degree of code reuse required by newer best practices like the Capability Maturity Model (CMM).
MS-DOS support was still provided with the release of Microsoft's first C++ Compiler, MSC 7, which was backwardly compatible with the C programming language and MS-DOS and supported both 16 bit and 32 bit code generation.
MSC took over where Aztec C86 left off. The market share for C compilers had turned to cross compilers which took advantage of the latest and greatest Windows features, offered C and C++ in a single bundle, and still supported MS-DOS systems that were already a decade old, and the smaller companies that produced compilers like Aztec C could no longer compete and either turned to niche markets like embedded systems or disappeared.
MS-DOS and 16 bit code generation support continued until MSC 8.00c which was bundled with Microsoft C++ and Microsoft Application Studio 1.5, the forerunner of Microsoft Visual Studio which is the cross development environment that Microsoft provide today.
MSC 12 was released with Microsoft Visual Studio 6 and no longer provided support for MS-DOS 16 bit binaries, instead providing support for 32 bit console applications, but provided support for Windows 95 and Windows 98 code generation as well as for Windows NT. Link libraries were available for other processors that ran Microsoft Windows; a practice that Microsoft continues to this day.
MSC 13 was released with Visual Studio 2003, and MSC 14 was released with Visual Studio 2005, both of which still produce code for older systems like Windows 95, but which will produce code for several target platforms including the mobile market and the ARM architecture.
In 2001 Microsoft developed the Common Language Runtime (CLR), which formed the core for their .NET Framework compiler in the Visual Studio IDE. This layer on the operating system which is in the API allows the mixing of development languages compiled across platforms that run the Windows operating system.
The .NET Framework runtime and CLR provide a mapping layer to the core routines for the processor and the devices on the target computer. The command-line C compiler in Visual Studio will compile native code for a variety of processors and can be used to build the core routines themselves.
Microsoft .NET applications for target platforms like Windows Mobile on the ARM architecture cross-compile on Windows machines with a variety of processors and Microsoft also offer emulators and remote deployment environments that require very little configuration, unlike the cross compilers in days gone by or on other platforms.
Runtime libraries, such as Mono, provide compatibility for cross-compiled .NET programs to other operating systems, such as Linux.
Libraries like Qt and its predecessors including XVT provide source code level cross development capability with other platforms, while still using Microsoft C to build the Windows versions. Other compilers like MinGW have also become popular in this area since they are more directly compatible with the Unixes that comprise the non-Windows side of software development allowing those developers to target all platforms using a familiar build environment.
Free Pascal was developed from the beginning as a cross compiler. The compiler executable (ppcXXX where XXX is a target architecture) is capable of producing executables (or just object files if no internal linker exists, or even just assembly files if no internal assembler exists) for all OS of the same architecture. For example, ppc386 is capable of producing executables for i386-linux, i386-win32, i386-go32v2 (DOS) and all other OSes (see ). For compiling to another architecture, however, a cross architecture version of the compiler must be built first. The resulting compiler executable would have additional 'ross' before the target architecture in its name. i.e. if the compiler is built to target x64, then the executable would be ppcrossx64.
To compile for a chosen architecture-OS, the compiler switch (for the compiler driver fpc) -P and -T can be used. This is also done when cross compiling the compiler itself, but is set via make option CPU_TARGET and OS_TARGET. GNU assembler and linker for the target platform is required if Free Pascal doesn't yet have internal version of the tools for the target platform."
"152","

DJ's GNU Programming Platform (DJGPP) is a software development suite for Intel 80386-level and above, IBM PC compatibles which supports DOS operating systems. It is guided by DJ Delorie, who began the project in 1989. It is a port of the GNU Compiler Collection (GCC), and mostly GNU utilities such as Bash, find, tar, ls, GAWK, sed, and ld to DOS Protected Mode Interface (DPMI). Supported languages include C, C++, Objective-C/C++, Ada, Fortran, and Pascal. DJGPP was described as an ""aging"" product in 2004.
The compiler generates 32-bit code, which runs natively in 32-bit protected mode while switching back to 16-bit DOS calls for basic OS support. However, unlike the Open Watcom C/C++ compiler, it is not a zero-based flat model due to preferring NULL pointer protection for better stability. It is currently based upon a variant of the COFF format. It can access up to 4 GB of RAM in pure DOS when using a suitable DPMI host (e.g., CWSDPMI r7 or HDPMI32).
DJGPP presents the programmer an interface which is compatible with the ANSI C and C99 standards, DOS APIs, and an older POSIX-like environment. Compiled binaries are long filename (LFN) aware and can handle such names under most 32-bit Windows by default, but they cannot use the Win16 or Win32 APIs that graphical programs on Windows need.Terminate and Stay Resident (TSR) programs to support LFNs under plain DOS or Windows NT 4 are available.
While DJGPP runs in 32-bit protected mode, its stub and library heavily rely on many 16-bit DOS and BIOS calls. Because the x86-64 versions of Windows lack support for 16-bit programs, there is no NTVDM, and DJGPP applications cannot be run. Under x86-64 systems these applications function only through emulation (e.g. DOSBox), x86 virtualization (e.g. VirtualBox), or similar (e.g. Linux's DOSEMU). This problem arises because in long mode x86-64 processors do not support the virtual 8086 mode used to run 16-bit code in IA-32 processors. Newer x86 CPUs with VT-x do support paged real mode and unrestricted guest mode execution."
"153","{""type"":""https://mediawiki.org/wiki/HyperSwitch/errors/bad_request"",""title"":""Invalid parameters"",""method"":""get"",""detail"":""data.params.revision should be an integer"",""uri"":""/en.wikipedia.org/v1/page/html/Watcom_C/C%2B%2B""}"
"154","The Portable C Compiler (also known as pcc or sometimes pccm - portable C compiler machine) is an early compiler for the C programming language written by Stephen C. Johnson of Bell Labs in the mid-1970s, based in part on ideas proposed by Alan Snyder in 1973,
and ""distributed as the C compiler by Bell Labs... with the blessing of Dennis Ritchie.""
One of the first compilers that could easily be adapted to output code for different computer architectures, the compiler had a long life span. It debuted in Seventh Edition Unix and shipped with BSD Unix until the release of 4.4BSD in 1994, when it was replaced by the GNU C Compiler. It was very influential in its day, so much so that at the beginning of the 1980s, the majority of C compilers were based on it.  Anders Magnusson and Peter A Jonsson restarted development of pcc in 2007, rewriting it extensively to support the C99 standard.
The keys to the success of pcc were its portability and improved diagnostic capabilities. The compiler was designed so that only a few of its source files were machine-dependent. It was relatively robust to syntax errors and performed more thorough validity checks than its contemporaries.
The first C compiler, written by Dennis Ritchie, used a recursive descent parser, incorporated specific knowledge about the PDP-11, and relied on an optional machine-specific optimizer to improve the assembly language code it generated. In contrast, Johnson's pccm was based on a yacc-generated parser and used a more general target machine model. Both compilers produced target-specific assembly language code which they then assembled to produce linkable object modules.
Later versions of PCC, known within Bell Labs as ""QCC"" and ""RCC,"" supported other target architecture models.[citation needed]
The language that PCC implemented was an extended version of K&R C that Bjarne Stroustrup has called ""Classic C"", incorporating the void return type (for functions that don't return any value), enumerations and structure assignment.
A new version of pcc, based on the original by Steve Johnson, is now maintained by Anders Magnusson. The compiler is provided under the BSD licence and its development is funded by a non-profit organization called BSD Fund. According to Magnusson:
This new version was added to the NetBSD pkgsrc and OpenBSD source trees in September 2007, and later into the main NetBSD source tree. There had been some speculation that it might eventually be used to supplant the GNU C Compiler on BSD-based operating systems, though FreeBSD and NetBSD are both looking to Clang as a potential replacement, and Theo de Raadt of OpenBSD asserts that pcc is not ready yet to be a gcc replacement, and the disposal of gcc is not top priority. On December 29, 2009 pcc became capable of building a functional x86 OpenBSD kernel image.
pcc version 1.0 was released on 1 April 2011.
As of this release, the compiler supports x86 and x64 processor architectures and runs on NetBSD, OpenBSD, FreeBSD, various Linux distributions, and Microsoft Windows.
Further development, including support for more architectures, and FORTRAN 77 and C++ front ends, is continuing.
PCC was removed from the OpenBSD source tree in 2012. Development on it had stalled and no maintainer had stepped up to develop it into something that could make a practical alternative to GNU Compiler Collection.
The latest version of pcc, namely 1.1.0, was released on 10 December 2014."
"155","
KDevelop is a free and open-source integrated development environment (IDE) for Unix-like computer operating systems and Microsoft Windows. It provides editing, navigation and debugging features for several programming languages, and integration with build automation and version-control systems, using a plugin-based architecture.
KDevelop 5 has parser backends for C, C++, Objective-C, OpenCL and JavaScript/QML, with plugins supporting PHP, Python 3 and Ruby. Basic syntax highlighting and code folding are available for dozens of other source-code and markup formats, but without semantic analysis.
KDevelop is part of the KDE project, and is based on KDE Frameworks and Qt. The C/C++ backend uses Clang to provide accurate information even for very complex codebases.
KDevelop 0.1 was released in 1998, with 1.0 following in late 1999. 1.x and 2.x were developed over a period of four years from the original codebase. Bernd Gehrmann started a complete rewrite and announced KDevelop 3.x in March 2001. Its first release was together with K Desktop Environment 3.2 in February 2004, and development of KDevelop 3.x continued until 2008.
KDevelop 4.x, another complete rewrite with a more object-oriented programming model, was developed from August 2005 and released as KDevelop 4.0.0 in May 2010. The last feature update of this branch was version 4.7.0 in September 2014, with bugfix releases continuing until KDevelop 4.7.4 in December 2016
KDevelop 5 development began in August 2014 as a continuation of the 4.x codebase, ported to Qt5 and KDE Frameworks 5. The custom C++ parser used in earlier versions, which had poor support for C++11 syntax, was replaced by a new Clang-based backend. The integrated CMakeFile interpreter was also removed in favour of JSON metadata produced by the upstream CMake tool.
Semantic language support was added for QML and JavaScript, using the parser from Qt Creator, alongside a new QMake project-manager backend.
The first stable 5.x release was KDevelop 5.0.0 in August 2016. In October 2016, official Microsoft Windows builds were released for the first time.
KDevelop uses an embedded text editor component through the KParts framework. The default editor is KDE Advanced Text Editor, which can optionally be replaced with a Qt Designer-based editor. This list focuses on the features of KDevelop itself. For features specific to the editor component, see the article on Kate.
KDevelop 4 is a completely plugin-based architecture. When a developer makes a change, they only must compile the plugin. There is a possibility to keep several profiles each of which determines which plugins to be loaded. KDevelop does not come with a text editor, but instead uses a plugin for this purpose as well. KDevelop is programming language independent and build system-independent, supporting KDE, GNOME, and many other technologies such as Qt, GTK+, and wxWidgets.
KDevelop has supported a variety of programming languages, including C, C++, Perl, Python, PHP, Java, Fortran, Ruby, Ada, Pascal, SQL, and Bash scripting. Supported build systems include GNU (automake), cmake, qmake, and make for custom projects (KDevelop does not destroy user Makefiles if they are used) and scripting projects which don't need one.
Code completion is available for C and C++. Symbols are kept in a Berkeley DB file for quick lookups without re-parsing. KDevelop also offers a developer framework which helps to write new parsers for other programming languages.
An integrated debugger allows graphically doing all debugging with breakpoints and backtraces. It even works with dynamically loaded plugins unlike command line GDB.
Quick Open allows quick navigation between files.
Currently, around 50 to 100 plugins exist for this IDE.  Major ones include persistent project-wide code bookmarks, Code abbreviations which allow expanding text quickly, a Source formatter which reformats code to a style guide before saving, Regular expressions search, and project-wide search/replace which helps in refactoring code."
"156","


Dev-C++ is a free full-featured integrated development environment (IDE) distributed under the GNU General Public License for programming in C and C++. It is written in Delphi.
It is bundled with, and uses, the MinGW or TDM-GCC 64bit port of the GCC as its compiler. Dev-C++ can also be used in combination with Cygwin or any other GCC-based compiler.
Dev-C++ is generally considered a Windows-only program, but there are attempts to create a Linux version: header files and path delimiters are switchable between platforms.
An additional aspect of Dev-C++ is its use of DevPaks: packaged extensions on the programming environment with additional libraries, templates, and utilities. DevPaks often contain, but are not limited to, GUI utilities, including popular toolkits such as GTK+, wxWidgets, and FLTK. Other DevPaks include libraries for more advanced function use. Users of Dev-C++ can download additional libraries, or packages of code that increase the scope and functionality of Dev-C++, such as graphics, compression, animation, sound support and many more. Users can create Devpaks and host them for free on the site. Also, they are not limited to use with Dev-C++ - the site says ""A typical devpak will work with any MinGW  distribution (with any IDE for MinGW)"".
From February 22, 2005 to June 2011 the project was not noticeably active, with no news posted nor any updated versions released. In a 2006 forum post, lead developer Colin Laplace stated that he was busy with real-life issues and did not have time to continue development of Dev-C++.
There are two forks of Dev-C++ since then: wxDev-C++ and the Orwell version.
wxDev-C++ is a development team that has taken Dev-C++ and added new features such as support for multiple compilers and a RAD designer for wxWidgets applications.
On June 30, 2011 an unofficial version 4.9.9.3 of Dev-C++ was released by Orwell (Johan Mes), an independent programmer, featuring the more recent GCC 4.5.2 compiler, Windows' SDK resources (Win32 and D3D), numerous bugfixes, and improved stability. On August 27, after five years of officially being in a beta stage, version 5.0 was released. This version also has its own separate SourceForge page since version 5.0.0.5, because the old developer isn't responding to combining requests. On July 2014, Orwell Dev-C++ 5.7.1 was released featuring the more recent GCC 4.8.1 which supports C++11.
On May 4, 2015, The Singapore Prime Minister Lee Hsien Loong posted his Sudoku solver program in C++ on Facebook. In his screen shot, he's using Microsoft Windows and Dev-C++ as his IDE."
"157","

CodeLite is a free and open-source IDE for the C, C++, PHP, and JavaScript (Node.js) programming languages.
In August 2006, Eran Ifrah started an autocomplete project named CodeLite. The idea was to create a code completion library based on ctags, SQLite (hence, CodeLite), and a Yacc based parser that could be used by other IDEs. Later Clang became an optional parser for code completion, greatly improving its functionality.
LiteEditor, a demo application, was developed for demonstrating CodeLite's functionalities. Eventually, LiteEditor evolved into CodeLite.
CodeLite is a free, open-source, cross-platform IDE for the C/C++ programming languages using the wxWidgets toolkit. To comply with CodeLite's open-source spirit, the program itself is compiled and debugged using only free tools (MinGW and GDB) for Mac OS X, Windows, Linux and FreeBSD, though CodeLite can execute any third-party compiler or tool that has a command-line interface. CodeLite also supports PHP and JavaScript development (including Node.js support).
CodeLite features project management (workspace / projects), code completion, code refactoring, source browsing, syntax highlighting,[citation needed]Subversion integration, cscope integration, UnitTest++ integration, an interactive debugger built over gdb and a source code editor (based on Scintilla).
CodeLite is distributed under the GNU General Public License v2 or Later. It is being developed and debugged using itself as the development platform with daily updates available through its Git repository."
"158","


Qt (/kjuːt/ ""cute"") is a cross-platform application framework that is used for developing application software that can be run on various software and hardware platforms with little or no change in the underlying codebase, while still being a native application with native capabilities and speed. Qt is currently being developed both by The Qt Company, a publicly listed company, and the Qt Project under open-source governance, involving individual developers and firms working to advance Qt. Qt is available with both proprietary and open sourceGPL 2.0, GPL 3.0, and LGPL 3.0 licenses.
Qt is used for developing multi-platform applications and graphical user interfaces (GUIs); however, programs without a GUI can be developed, such as command-line tools and consoles for servers.  An example of a non-GUI program using Qt is the Cutelyst web framework. GUI programs created with Qt can have a native-looking interface, in which case Qt is classified as a widget toolkit.
It has extensive internationalization support. Non-GUI features include SQL database access, XML parsing, JSON parsing, thread management and network support.
Qt uses standard C++ with extensions including signals and slots that simplify handling of events, and this helps in development of both GUI and server applications which receive their own set of event information and should process them accordingly. Qt supports many compilers, including the GCC C++ compiler and the Visual Studio suite. Qt also provides Qt Quick, that includes a declarative scripting language called QML that allows using JavaScript to provide the logic. With Qt Quick, rapid application development for mobile devices became possible, although logic can be written with native code as well to achieve the best possible performance.
Qt can be used in several other programming languages via language bindings. It runs on the major desktop platforms and some of the mobile platforms.
In 2017, the Qt Company estimates a community of approximately 1 million developers worldwide in over 70 industries.
Qt is used by a wide range of companies and organizations such as European Space Agency,DreamWorks,Lucasfilm,Panasonic,Philips,Samsung,Siemens,Volvo,German Air Traffic Control,HP,Walt Disney Animation Studios,Blizzard Entertainment,Electronic Arts,AMD, and Valve Corporation.
Several GUIs and desktop environments utilize Qt as widget toolkit:
Notable applications using Qt or QML are: 
There are cases where applications have ported their entire code base from another toolkit to Qt to make use of a cross-platform native GUI, such as the Wireshark network packet analyzer, LXQt desktop, VLC media player and the Rosegarden audio editor.
There are four editions of Qt available: Community, Indie Mobile, Professional and Enterprise. The Community version is under the open source licenses, while the Indie Mobile, Professional and Enterprise versions, which contain additional functionality and libraries, e.g. Enterprise Controls are commercially sold by The Qt Company.
Qt is available under the following free software licenses:GPL 3.0, LGPL 3.0 and LGPL 2.1 (with Qt special exception). Note that some modules are only available under a GPL license, which means that applications which statically link to these modules need to comply with that license.
In addition, Qt has always been available under a commercial license, like the Qt Commercial License, that allows developing proprietary applications with no restrictions on licensing.
Qt, when it was first released, relied on a few key concepts:
Qt comes with its own set of tools to ease cross-platform development, which can otherwise be cumbersome due to different set of development tools. Qt Creator is a cross-platform IDE for C++ and QML. Qt Designer's GUI layout/design functionality is integrated into the IDE, although Qt Designer can still be started as a standalone tool.
In addition to Qt Creator, Qt provides qmake, a cross-platform build script generation tool that automates the generation of Makefiles for development projects across different platforms.
There are other tools available in Qt, including the Qt Designer interface builder and the Qt Assistant help browser (which are both embedded in Qt Creator), the Qt Linguist translation tool, uic (user interface compiler), and moc (Meta-Object Compiler).
Qt works on many different platforms; the following are officially supported:
After Nokia opened the Qt source code to the community on Gitorious various ports appeared. There are also some ports of Qt that may be available, but are not supported anymore. These platforms are listed in List of platforms supported by Qt.
Starting with Qt 4.0 the framework was split into individual modules. With Qt 5.0 the architecture was modularized even further. Qt is now split into essential and add-on modules.
The upcoming version of Qt is Qt 5.10, which will be released on 30 November 2017 with several new features including initial Vulkan support for Windows, Linux and Android. The current LTS version of Qt is 5.9 LTS which was released on 31 May 2017 and will be supported for 3 years until 31 May 2020.
In the summer of 1990, Haavard Nord and Eirik Chambe-Eng (the original developers of Qt and the CEO and President, respectively, of Trolltech) were working together on a database application for ultrasound images written in C++ and running on Mac OS, Unix, and Windows. They began development of ""Qt"" in 1991, three years before the company was incorporated as Quasar Technologies, then changed the name to Troll Tech and then to Trolltech.
The toolkit was called Qt because the letter Q looked appealing in Haavard's Emacs typeface, and ""t"" was inspired by Xt, the X toolkit.
The first two versions of Qt had only two flavors: Qt/X11 for Unix and Qt/Windows for Windows.
On 20 May 1995 Troll Tech publicly released Qt 0.90 for X11/Linux with the source code under the Qt Free Edition License. This license was viewed as not compliant with the open source principle by the Open Source Initiative and the free software definition by Free Software Foundation because, while the source was available, it did not allow the redistribution of modified versions. Trolltech used this license until version 1.45. Controversy erupted around 1998 when it became clear that the K Desktop Environment was going to become one of the leading desktop environments for Linux. As it was based on Qt, many people in the free software movement worried that an essential piece of one of their major operating systems would be proprietary.
The Windows platform was only available under a proprietary license, which meant free/open source applications written in Qt for X11 could not be ported to Windows without purchasing the proprietary edition.
With the release of version 2.0 of the toolkit, the license was changed to the Q Public License (QPL), a free software license, but one regarded by the Free Software Foundation as incompatible with the GPL. Compromises were sought between KDE and Trolltech whereby Qt would not be able to fall under a more restrictive license than the QPL, even if Trolltech was bought out or went bankrupt. This led to the creation of the KDE Free Qt foundation, which guarantees that Qt would fall under a BSD-style license should no free/open source version of Qt be released during 12 months.
In 2000, Qt/X11 2.2 was released under the GPL v2, ending all controversy regarding GPL compatibility.
At the end of 2001, Trolltech released Qt 3.0, which added support for Mac OS X. The Mac OS X support was available only in the proprietary license until June 2003, when Trolltech released Qt 3.2 with Mac OS X support available under the GPL.
In 2002, members of the KDE on Cygwin project began porting the GPL licensed Qt/X11 code base to Windows. This was in response to Trolltech's refusal to license Qt/Windows under the GPL on the grounds that Windows was not a free/open source software platform. The project achieved reasonable success although it never reached production quality.
This was resolved when Trolltech released Qt 4.0 also for Windows under the GPL in June 2005. Qt 4 supported the same set of platforms in the free software/open source editions as in the proprietary edition, so it is possible, with Qt 4.0 and later releases, to create GPL-licensed free/open source applications using Qt on all supported platforms. The GPL v3 with special exception was later added as an added licensing option. The GPL exception allows the final application to be licensed under various GPL-incompatible free software/open source licenses such as the Mozilla Public License 1.1.
Nokia acquired Trolltech ASA on 17 June 2008 and changed the name first to Qt Software, then to Qt Development Frameworks.
Since then it focused on Qt development to turn it into the main development platform for its devices, including a port to the Symbian S60 platform. Version 1.0 of the Nokia Qt SDK was released on 23 June 2010. The source code was made available over Gitorious, a community oriented git source code repository, to gather an even broader community that is not only using Qt but also helping to improve it.
On 14 January 2009, Qt version 4.5 added another option, the LGPL, which should make Qt even more attractive for non-GPL open source projects and for closed applications.
In February 2011, Nokia announced its decision to drop Symbian technologies and base their future smartphones on the Windows Phone platform instead. One month later, Nokia announced the sale of Qt's commercial licensing and professional services to Digia, with the immediate goal of taking Qt support to Android, iOS and Windows 8 platforms, and to continue focusing on desktop and embedded development, although Nokia was to remain the main development force behind the framework at that time.
In March 2011, Nokia sold the commercial licensing part of Qt to Digia creating Qt Commercial. In August 2012, Digia announced that it would acquire Qt from Nokia. The Qt team at Digia started their work in September 2012. They released Qt 5.0 within a month and newer versions every 6 months with new features and additional supported platforms.
In September 2014, Digia transferred the Qt business and copyrights to their wholly owned subsidiary, The Qt Company, which owns 25 brands related to Qt. In May 2016, Digia and Qt demerged completely into two independent companies.
Qt 5 was officially released on 19 December 2012. This new version marked a major change in the platform, with hardware-accelerated graphics, QML and JavaScript playing a major role. The traditional C++-only QWidgets continued to be supported, but did not benefit from the performance improvements available through the new architecture. Qt 5 brings significant improvements to the speed and ease of developing user interfaces.
Framework development of Qt 5 moved to open governance, taking place at qt-project.org. There it is now possible for developers outside Digia to submit patches and have them reviewed.
Qt Wiki provides a comprehensive list of English books about Qt. This is a list of notable books:"
"159","A graphical user interface builder (or GUI builder), also known as GUI designer, is a software development tool that simplifies the creation of GUIs by allowing the designer to arrange  graphical control elements (often called widgets) using a drag-and-drop WYSIWYG editor. Without a GUI builder, a GUI must be built by manually specifying each widget's parameters in source-code, with no visual feedback until the program is run.
User interfaces are commonly programmed using an event-driven architecture, so GUI builders also simplify creating event-driven code. This supporting code connects widgets with the outgoing and incoming events that trigger the functions providing the application logic.
Some graphical user interface builders, such as e.g. Glade Interface Designer, automatically generate all the source code for a graphical control element. Others, like Interface Builder, generate serialized object instances that are then loaded by the application."
"160","
MonoDevelop (also known as Xamarin Studio) is an open source integrated development environment for Linux, macOS, and Windows. Its primary focus is development of projects that use Mono and .NET frameworks. MonoDevelop integrates features similar to those of NetBeans and Microsoft Visual Studio, such as automatic code completion, source control, a graphical user interface (GUI) and Web designer. MonoDevelop integrates a Gtk# GUI designer called Stetic. It supports
Boo,
C,
C++,
C#,
CIL, 
D, 
F#,
Java,
Oxygene, 
Vala,
and Visual Basic.NET.
MonoDevelop can be used on Windows, macOS and Linux. Officially supported Linux distributions include CentOS, Debian, Fedora, openSUSE, SUSE Linux Enterprise, Red Hat Enterprise Linux and Ubuntu, with many other distributions providing their own unofficial builds of MonoDevelop in their repositories. macOS and Windows have been officially supported since version 2.2.
MonoDevelop has included a C# compiler (an alternative to MSBuild and CSC) since its earliest versions. It currently includes a compiler that supports C# 1.0, C# 2.0, C# 3.0, C# 4.0, C# 5.0 and C# 6.0.
A customized version of MonoDevelop ships with Unity, the game engine by Unity Technologies. It enables advanced C# scripting, which is used to compile cross-platform video games by the Unity compiler.
In late 2003, a group of developers from the Mono community began migrating SharpDevelop, a successful .NET open source IDE from Windows Forms on Windows to the GNOME toolkit (Gtk#) on Linux. The fork was also to target the Mono framework instead of the Microsoft .NET Framework implementation. Being an early fork of SharpDevelop, MonoDevelop architecturally differs from recent SharpDevelop releases.
Over time, the MonoDevelop project was absorbed into the rest of the Mono project and as of 2016, is actively maintained by Xamarin and the Mono community. Since Mono 1.0 Beta 2, MonoDevelop is bundled with Mono releases.
Starting with version 4.x, Xamarin rebranded MonoDevelop as Xamarin Studio, but only for the Windows version of the IDE. As of 2016, Xamarin Studio also runs on macOS.
MonoDevelop is an IDE for the .NET platform with features comparable to Microsoft Visual Studio. Highlights include:
MonoDevelop has included a GTK# GUI designer called Stetic since version 0.1. to develop GTK+ user interfaces in C#. Stetic is very similar to Glade Interface Designer but is integrated into MonoDevelop with features such as drag and drop. It has been criticized for being more difficult to work with than the likes of Qt Designer and the Microsoft Visual Studio Windows Forms Editor when the programmer does not yet have a concrete layout in mind.
Xamarin offers a rebranded version of MonoDevelop 4.0 as Xamarin Studio which now uses platform-specific code in various places to enhance the look and feel. While Mono provides a package for Solaris 10 running on SPARC, MonoDevelop packages for OpenSolaris are only provided by groups from the OpenSolaris community. MonoDevelop on FreeBSD is likewise supported only by the FreeBSD community.
Another rebranded version of MonoDevelop is Visual Studio for Mac. Visual Studio for Mac employs many of the same tools as its Windows counterpart: for example, the Roslyn Compiler Platform is used for refactoring and IntelliSense. Its project system and build engine use MSBuild; and its source editor supports TextMate bundles. It uses the same debugger engines for Xamarin and .NET Core apps, and the same designers for Xamarin.iOS and Xamarin.Android."
"161","V-Play is a cross-platform development tool, based on the Qt framework. It can be used to create mobile apps or 2D games. V-Play apps and games are supported on iOS, Android, Windows Phone, embedded devices and desktop devices. V-Play developers use QML, JavaScript and C++ to create mobile apps and games.
Apps and games built with V-Play use a single code base and work across multiple platforms and screen resolutions. V-Play was founded in 2012 and is based in Vienna, Austria.
V-Play is based on the Qt (software) cross-platform development framework, that provides abstraction layers for timers, threads, storage, networking and UI rendering on different platforms. Applications built with Qt include VLC Media Player, Skype and Autodesk Maya – all of them make use of the GUI module which allows the creation of native-looking applications with a single source code base.
V-Play uses Qt as its core and offers components and plugins on top of it, which further simplify the development of apps and games.
The Qt Creator IDE supports editing QML and JavaScript code with context-sensitive help, code completion of V-Play components, navigation between components and more.
It includes a QML debugger and profiler for debugging custom components and JavaScript functions. It can inspect and change property values and QML code at runtime and is able to measure the time of element creation and binding evaluations.
V-Play apps and games are written in JavaScript and QML, a declarative language that features property bindings, state machines or fluid animations of any property.
V-Play is built to handle all possible aspect ratios and resolutions of modern mobile devices. Developers create their game for a logical scene using a content scaling approach.
V-Play apps and games provide a native look and feel on all platforms. V-Play has abstracted components for displaying native input dialogs and alert boxes without the need of a single native code line.
The V-Play Game Network is a mobile-backend-as-a-service or MBaas. It allows players to compare high scores and achievements, as well as syncing their data across devices. This is a free service for players and doesn't require registration of any kind. The V-Play Game Network can be implemented in less than 50 lines of code.
V-Play apps and games support a number of 3rd party plugins. These plugins can be used by developers to monetize their applications, to gather analytics on user behavior or to engage users. The following plugins are currently supported by V-Play:
V-Play supports the following platforms."
"162","
Cheney's algorithm, first described in a 1970 ACM paper by C.J. Cheney, is a stop and copy method of tracing garbage collection in computer software systems. In this scheme, the heap is divided into two equal halves, only one of which is in use at any one time. Garbage collection is performed by copying live objects from one semispace (the from-space) to the other (the to-space), which then becomes the new heap. The entire old heap is then discarded in one piece.  It is an improvement on the previous stop and copy technique.[citation needed]
Cheney's algorithm reclaims items as follows:
Once all to-space references have been examined and updated, garbage collection is complete.
The algorithm needs no stack and only two pointers outside of the from-space and to-space: a pointer to the beginning of free space in the to-space, and a pointer to the next word in to-space that needs to be examined. For this reason, it is sometimes called a ""two-finger"" collector—it only needs ""two fingers"" pointing into the to-space to keep track of its state. The data between the two fingers represents work remaining for it to do.
The forwarding pointer (sometimes called a ""broken heart"") is used only during the garbage collection process; when a reference to an object already in to-space (thus having a forwarding pointer in from-space) is found, the reference can be updated quickly simply by updating its pointer to match the forwarding pointer.
Because the strategy is to exhaust all live references, and then all references in referenced objects, this is known as a breadth-first list copying garbage collection scheme.
Cheney based his work on the semispace garbage collector, which was published a year earlier by R.R. Fenichel and J.C. Yochelson.
Cheney's algorithm is an example of a tri-color marking garbage collector. The first member of the gray set is the stack itself. Objects referenced on the stack are copied into the to-space, which contains members of the black and gray sets.
The algorithm moves any white objects (equivalent to objects in the from-space without forwarding pointers) to the gray set by copying them to the to-space. Objects that are between the scanning pointer and the free-space pointer on the to-space area are members of the gray set still to be scanned. Objects below the scanning pointer belong to the black set. Objects are moved to the black set by simply moving the scanning pointer over them.
When the scanning pointer reaches the free-space pointer, the gray set is empty, and the algorithm ends."
"163","Dangling pointers and wild pointers in computer programming are pointers that do not point to a valid object of the appropriate type. These are special cases of memory safety violations. More generally, dangling references and wild references are references that do not resolve to a valid destination, and include such phenomena as link rot on the internet.
Dangling pointers arise during object destruction, when an object that has an incoming reference is deleted or deallocated, without modifying the value of the pointer, so that the pointer still points to the memory location of the deallocated memory. The system may reallocate the previously freed memory, and if the program then dereferences the (now) dangling pointer, unpredictable behavior may result, as the memory may now contain completely different data. If the program writes to memory referenced by a dangling pointer, a silent corruption of unrelated data may result, leading to subtle bugs that can be extremely difficult to find. If the memory has been reallocated to another process, then attempting to dereference the dangling pointer can cause segmentation faults (UNIX, Linux) or general protection faults (Windows). If the program has sufficient privileges to allow it to overwrite the bookkeeping data used by the kernel's memory allocator, the corruption can cause system instabilities. In object-oriented languages with garbage collection, dangling references are prevented by only destroying objects that are unreachable, meaning they do not have any incoming pointers; this is ensured either by tracing or reference counting. However, a finalizer may create new references to an object, requiring object resurrection to prevent a dangling reference.
Wild pointers arise when a pointer is used prior to initialization to some known state, which is possible in some programming languages. They show the same erratic behavior as dangling pointers, though they are less likely to stay undetected because many compilers will raise a warning at compile time if declared variables are accessed before being initialized.
In many languages (e.g., the C programming language) deleting an object from memory explicitly or by destroying the stack frame on return does not alter associated pointers. The pointer still points to the same location in memory even though the reference has since been deleted and may now be used for other purposes.
A straightforward example is shown below:
If the operating system is able to detect run-time references to null pointers, a solution to the above is to assign 0 (null) to dp immediately before the inner block is exited. Another solution would be to somehow guarantee dp is not used again without further initialization.
Another frequent source of dangling pointers is a jumbled combination of malloc() and free() library calls: a pointer becomes dangling when the block of memory it points to is freed. As with the previous example one way to avoid this is to make sure to reset the pointer to null after freeing its reference—as demonstrated below.
An all too common misstep is returning addresses of a stack-allocated local variable: once a called function returns, the space for these variables gets deallocated and technically they have ""garbage values"".
Attempts to read from the pointer may still return the correct value (1234) for a while after calling func, but any functions called thereafter will overwrite the stack storage allocated for num with other values and the pointer would no longer work correctly. If a pointer to num must be returned, num must have scope beyond the function—it might be declared as static.

Antoni Kreczmar (pl) (1945-1996) has created a complete object management system which is free of dangling reference phenomenon, see
Consequently:
Note:  the cost of kill is constant O(1){\displaystyle O(1)}.
A similar approach was proposed by Fisher and LeBlanc  under the name Locks-and-keys.
Wild pointers are created by omitting necessary initialization prior to first use. Thus, strictly speaking, every pointer in programming languages which do not enforce initialization begins as a wild pointer.
This most often occurs due to jumping over the initialization, not by omitting it. Most compilers are able to warn about this.
Like buffer-overflow bugs, dangling/wild pointer bugs frequently become security holes. For example, if the pointer is used to make a virtual function call, a different address (possibly pointing at exploit code) may be called due to the vtable pointer being overwritten. Alternatively, if the pointer is used for writing to memory, some other data structure may be corrupted.  Even if the memory is only read once the pointer becomes dangling, it can lead to information leaks (if interesting data is put in the next structure allocated there) or to privilege escalation (if the now-invalid memory is used in security checks). When a dangling pointer is used after it has been freed without allocating a new chunk of memory to it, this becomes known as a ""use after free"" vulnerability. For example, CVE-2014-1776 is a use-after-free vulnerability in Microsoft Internet Explorer 6 through 11 being used by zero-day attacks by an advanced persistent threat.
In C, the simplest technique is to implement an alternative version of the free() (or alike) function which guarantees the reset of the pointer.  However, this technique will not clear other pointer variables which may contain a copy of the pointer.
The alternative version can be used even to guarantee the validity of an empty pointer before calling malloc():
These uses can be masked through #define directives to construct useful macros, creating something like a metalanguage or can be embedded into a tool library apart. In every case, programmers using this technique should use the safe versions in every instance where free() would be used; failing in doing so leads again to the problem. Also, this solution is limited to the scope of a single program or project, and should be properly documented.
Among more structured solutions, a popular technique to avoid dangling pointers in C++ is to use smart pointers. A smart pointer typically uses reference counting to reclaim objects. Some other techniques include the tombstones method and the locks-and-keys method (see paper by Fisher & LeBlanc).
Another approach is to use the Boehm garbage collector, a conservative garbage collector that replaces standard memory allocation functions in C and C++ with a garbage collector. This approach completely eliminates dangling pointer errors by disabling frees, and reclaiming objects by garbage collection.
In languages like Java, dangling pointers cannot occur because there is no mechanism to explicitly deallocate memory. Rather, the garbage collector may deallocate memory, but only when the object is no longer reachable from any references.
In the language Rust, the type system have been extended to include also the variables lifetimes and resource acquisition is initialization. Unless one disables the features of the language, dangling pointers will be caught at compile time and reported as programming errors.
To expose dangling pointer errors, one common programming technique is to set pointers to the null pointer or to an invalid address once the storage they point to has been released. When the null pointer is dereferenced (in most languages) the program will immediately terminate—there is no potential for data corruption or unpredictable behavior. This makes the underlying programming mistake easier to find and resolve.  This technique does not help when there are multiple copies of the pointer.
Some debuggers will automatically overwrite and destroy data that has been freed, usually with a specific pattern, such as 0xDEADBEEF (Microsoft's Visual C/C++ debugger, for example, uses 0xCC, 0xCD or 0xDD depending on what has been freed). This usually prevents the data from being reused by making it useless and also very prominent (the pattern serves to show the programmer that the memory has already been freed).
Tools such as Polyspace, TotalView, Valgrind, Mudflap,AddressSanitizer, or tools based on LLVM can also be used to detect uses of dangling pointers.
Other tools (SoftBound, Insure++, and CheckPointer) instrument the source code to collect and track legitimate values for pointers (""metadata"") and check each pointer access against the metadata for validity.
Another strategy, when suspecting a small set of classes, is to temporarily make all their member functions virtual: after the class instance has been destructed/freed, its pointer to the Virtual Method Table is set to NULL, and any call to a member function will crash the program and it will show the guilty code in the debugger.
The term dangling pointer may also be used in contexts other than programming, especially by technical people. For example, a phone number for a person who has since changed phones is a real-world example of a dangling pointer. Another example is an entry in an online encyclopedia that refers to another entry whose title has been changed, changing any previously existing references to that entry into dangling pointers."
"164","In computer science, a memory leak is a type of resource leak    that occurs when a computer program incorrectly manages memory allocations in such a way that memory which is no longer needed is not released.  In object-oriented programming, a memory leak may happen when an object is stored in memory but cannot be accessed by the running code. A memory leak has symptoms similar to a number of other problems and generally can only be diagnosed by a programmer with access to the program's source code.
A space leak occurs when a computer program uses more memory than necessary. In contrast to memory leaks, where the leaked memory is never released, the memory consumed by a space leak is released, but later than expected. 
Because they can exhaust available system memory as an application runs, memory leaks are often the cause of or a contributing factor to software aging.
A memory leak reduces the performance of the computer by reducing the amount of available memory. Eventually, in the worst case, too much of the available memory may become allocated and all or part of the system or device stops working correctly, the application fails, or the system slows down vastly due to thrashing.
Memory leaks may not be serious or even detectable by normal means. In modern operating systems, normal memory used by an application is released when the application terminates. This means that a memory leak in a program that only runs for a short time may not be noticed and is rarely serious.
Much more serious leaks include those:
The following example, written in pseudocode, is intended to show how a memory leak can come about, and its effects, without needing any programming knowledge. The program in this case is part of some very simple software designed to control an elevator. This part of the program is run whenever anyone inside the elevator presses the button for a floor.
The memory leak would occur if the floor number requested is the same floor that the elevator is on; the condition for releasing the memory would be skipped. Each time this case occurs, more memory is leaked.
Cases like this wouldn't usually have any immediate effects. People do not often press the button for the floor they are already on, and in any case, the elevator might have enough spare memory that this could happen hundreds or thousands of times. However, the elevator will eventually run out of memory. This could take months or years, so it might not be discovered despite thorough testing.
The consequences would be unpleasant; at the very least, the elevator would stop responding to requests to move to another floor (such as when an attempt is made to call the elevator or when someone is inside and presses the floor buttons). If other parts of the program need memory (a part assigned to open and close the door, for example), then someone may be trapped inside, or if no one is in, then no one would be able to use the elevator since the software cannot open the door.
The memory leak lasts until the system is reset. For example: if the elevator's power were turned off or in a power outage, the program would stop running. When power was turned on again, the program would restart and all the memory would be available again, but the slow process of memory leak would restart together with the program, eventually prejudicing the correct running of the system.
The leak in the above example can be corrected by bringing the 'release' operation outside of the conditional:
Memory leaks are a common error in programming, especially when using languages that have no built in automatic garbage collection, such as C and C++.  Typically, a memory leak occurs because dynamically allocated memory has become unreachable. The prevalence of memory leak bugs has led to the development of a number of debugging tools to detect unreachable memory. BoundsChecker, Deleaker, IBM Rational Purify, Valgrind, Parasoft Insure++, Dr. Memory and memwatch are some of the more popular memory debuggers for C and C++ programs. ""Conservative"" garbage collection capabilities can be added to any programming language that lacks it as a built-in feature, and libraries for doing this are available for C and C++ programs.  A conservative collector finds and reclaims most, but not all, unreachable memory.
Although the memory manager can recover unreachable memory, it cannot free memory that is still reachable and therefore potentially still useful.  Modern memory managers therefore provide techniques for programmers to semantically mark memory with varying levels of usefulness, which correspond to varying levels of reachability.  The memory manager does not free an object that is strongly reachable. An object is strongly reachable if it is reachable either directly by a strong reference or indirectly by a chain of strong references.  (A strong reference is a reference that, unlike a weak reference, prevents an object from being garbage collected.)  To prevent this, the developer is responsible for cleaning up references after use, typically by setting the reference to null once it is no longer needed and, if necessary, by deregistering any event listeners that maintain strong references to the object.
In general, automatic memory management is more robust and convenient for developers, as they don't need to implement freeing routines or worry about the sequence in which cleanup is performed or be concerned about whether or not an object is still referenced.  It is easier for a programmer to know when a reference is no longer needed than to know when an object is no longer referenced.  However, automatic memory management can impose a performance overhead, and it does not eliminate all of the programming errors that cause memory leaks.
RAII, short for Resource Acquisition Is Initialization, is an approach to the problem commonly taken in C++, D, and Ada.  It involves associating scoped objects with the acquired resources, and automatically releasing the resources once the objects are out of scope. Unlike garbage collection, RAII has the advantage of knowing when objects exist and when they do not. Compare the following C and C++ examples:
The C version, as implemented in the example, requires explicit deallocation; the array is dynamically allocated (from the heap in most C implementations), and continues to exist until explicitly freed.
The C++ version requires no explicit deallocation; it will always occur automatically as soon as the object array goes out of scope, including if an exception is thrown. This avoids some of the overhead of garbage collection schemes. And because object destructors can free resources other than memory, RAII helps to prevent the leaking of input and output resources accessed through a handle, which mark-and-sweep garbage collection does not handle gracefully. These include open files, open windows, user notifications, objects in a graphics drawing library, thread synchronisation primitives such as critical sections, network connections, and connections to the Windows Registry or another database.
However, using RAII correctly is not always easy and has its own pitfalls.  For instance, if one is not careful, it is possible to create dangling pointers (or references) by returning data by reference, only to have that data be deleted when its containing object goes out of scope.
D uses a combination of RAII and garbage collection, employing automatic destruction when it is clear that an object cannot be accessed outside its original scope, and garbage collection otherwise.
More modern garbage collection schemes are often based on a notion of reachability – if you don't have a usable reference to the memory in question, it can be collected. Other garbage collection schemes can be based on reference counting, where an object is responsible for keeping track of how many references are pointing to it. If the number goes down to zero, the object is expected to release itself and allow its memory to be reclaimed. The flaw with this model is that it doesn't cope with cyclic references, and this is why nowadays most programmers are prepared to accept the burden of the more costly mark and sweep type of systems.
The following Visual Basic code illustrates the canonical reference-counting memory leak:
In practice, this trivial example would be spotted straight away and fixed. In most real examples, the cycle of references spans more than two objects, and is more difficult to detect.
A well-known example of this kind of leak came to prominence with the rise of AJAX programming techniques in web browsers in the lapsed listener problem. JavaScript code which associated a DOM element with an event handler, and failed to remove the reference before exiting, would leak memory (AJAX web pages keep a given DOM alive for a lot longer than traditional web pages, so this leak was much more apparent).
If a program has a memory leak and its memory usage is steadily increasing, there will not usually be an immediate symptom. Every physical system has a finite amount of memory, and if the memory leak is not contained (for example, by restarting the leaking program) it will eventually cause problems.
Most modern consumer desktop operating systems have both main memory which is physically housed in RAM microchips, and secondary storage such as a hard drive.  Memory allocation is dynamic – each process gets as much memory as it requests.  Active pages are transferred into main memory for fast access; inactive pages are pushed out to secondary storage to make room, as needed.  When a single process starts consuming a large amount of memory, it usually occupies more and more of main memory, pushing other programs out to secondary storage – usually significantly slowing performance of the system. Even if the leaking program is terminated, it may take some time for other programs to swap back into main memory, and for performance to return to normal.
When all the memory on a system is exhausted (whether there is virtual memory or only main memory, such as on an embedded system) any attempt to allocate more memory will fail.  This usually causes the program attempting to allocate the memory to terminate itself, or to generate a segmentation fault. Some programs are designed to recover from this situation (possibly by falling back on pre-reserved memory).  The first program to experience the out-of-memory may or may not be the program that has the memory leak.
Some multi-tasking operating systems have special mechanisms to deal with an out-of-memory condition, such as killing processes at random (which may affect ""innocent"" processes), or killing the largest process in memory (which presumably is the one causing the problem).  Some operating systems have a per-process memory limit, to prevent any one program from hogging all of the memory on the system.  The disadvantage to this arrangement is that the operating system sometimes must be re-configured to allow proper operation of programs that legitimately require large amounts of memory, such as those dealing with graphics, video, or scientific calculations.
If the memory leak is in the kernel, the operating system itself will likely fail.  Computers without sophisticated memory management, such as embedded systems, may also completely fail from a persistent memory leak.
Publicly accessible systems such as web servers or routers are prone to denial-of-service attacks if an attacker discovers a sequence of operations which can trigger a leak. Such a sequence is known as an exploit.
A ""sawtooth"" pattern of memory utilization may be an indicator of a memory leak if the vertical drops coincide with reboots or application restarts. Care should be taken though because garbage collection points could also cause such a pattern and would show a healthy usage of the heap.
Note that constantly increasing memory usage is not necessarily evidence of a memory leak. Some applications will store ever increasing amounts of information in memory (e.g. as a cache). If the cache can grow so large as to cause problems, this may be a programming or design error, but is not a memory leak as the information remains nominally in use. In other cases, programs may require an unreasonably large amount of memory because the programmer has assumed memory is always sufficient for a particular task; for example, a graphics file processor might start by reading the entire contents of an image file and storing it all into memory, something that is not viable where a very large image exceeds available memory.
To put it another way, a memory leak arises from a particular kind of programming error, and without access to the program code, someone seeing symptoms can only guess that there might be a memory leak. It would be better to use terms such as ""constantly increasing memory use"" where no such inside knowledge exists.
The following C function deliberately leaks memory by losing the pointer to the allocated memory. The leak can be said to occur as soon as the pointer 'a' goes out of scope, i.e. when function_which_allocates() returns without freeing 'a'."
"165","C dynamic memory allocation refers to performing manual memory management for dynamic memory allocation in the C programming language via a group of functions in the C standard library, namely malloc, realloc, calloc and free.
The C++ programming language includes these functions for compatibility with C; however, the operators new and delete provide similar functionality and are recommended by that language's authors.
Many different implementations of the actual memory allocation mechanism, used by malloc, are available. Their performance varies in both execution time and required memory.
The C programming language manages memory statically, automatically, or dynamically. Static-duration variables are allocated in main memory, usually along with the executable code of the program, and persist for the lifetime of the program; automatic-duration variables are allocated on the stack and come and go as functions are called and return. For static-duration and automatic-duration variables, the size of the allocation must be compile-time constant (except for the case of variable-length automatic arrays). If the required size is not known until run-time (for example, if data of arbitrary size is being read from the user or from a disk file), then using fixed-size data objects is inadequate.
The lifetime of allocated memory can also cause concern.  Neither static- nor automatic-duration memory is adequate for all situations. Automatic-allocated data cannot persist across multiple function calls, while static data persists for the life of the program whether it is needed or not. In many situations the programmer requires greater flexibility in managing the lifetime of allocated memory.
These limitations are avoided by using dynamic memory allocation in which memory is more explicitly (but more flexibly) managed, typically, by allocating it from the free store (informally called the ""heap""), an area of memory structured for this purpose. In C, the library function malloc is used to allocate a block of memory on the heap. The program accesses this block of memory via a pointer that malloc returns.  When the memory is no longer needed, the pointer is passed to free which deallocates the memory so that it can be used for other purposes.
Some platforms provide library calls which allow run-time dynamic allocation from the C stack rather than the heap (e.g. alloca()). This memory is automatically freed when the calling function ends.
The C dynamic memory allocation functions are defined in stdlib.h header (cstdlib header in C++).
Creating an array of ten integers with automatic scope is straightforward in C:
However, the size of the array is fixed at compile time. If one wishes to allocate a similar array dynamically, the following code can be used:
This computes the number of bytes that ten integers occupy in memory, then requests that many bytes from malloc and assigns the result to a pointer named array (due to C syntax, pointers and arrays can be used interchangeably in some situations).
Because malloc might not be able to service the request, it might return a null pointer and it is good programming practice to check for this:
When the program no longer needs the dynamic array, it should call free to return the memory it occupies to the free store:
The memory set aside by malloc is not initialized and may contain cruft: the remnants of previously used and discarded data.  After allocation with malloc, elements of the array are uninitialized variables.  The command calloc will allocate and clear the memory in one step:
allocates a region of memory large enough to hold 10 integers, and sets to zero all the bytes within that memory space.
With realloc we can resize the amount of memory a pointer points to. For example, if we have a pointer acting as an array of size n{\displaystyle n} and we want to change it to an array of size m{\displaystyle m}, we can use realloc.
malloc returns a void pointer (void *), which indicates that it is a pointer to a region of unknown data type. The use of casting is required in C++ due to the strong type system, whereas this is not the case in C. The lack of a specific pointer type returned from malloc is type-unsafe behaviour according to some programmers: malloc allocates based on byte count but not on type. This is different from the C++ new operator that returns a pointer whose type relies on the operand. (See C Type Safety.)
One may ""cast"" (see type conversion) this pointer to a specific type:
There are advantages and disadvantages to performing such a cast.
The improper use of dynamic memory allocation can frequently be a source of bugs. These can include security bugs or program crashes, most often due to segmentation faults.
Most common errors are as follows:
The implementation of memory management depends greatly upon operating system and architecture.  Some operating systems supply an allocator for malloc, while others supply functions to control certain regions of data. The same dynamic memory allocator is often used to implement both malloc and the operator new in C++.
Implementation of the allocator is commonly done using the heap, or data segment.  The allocator will usually expand and contract the heap to fulfill allocation requests.
The heap method suffers from a few inherent flaws, stemming entirely from fragmentation.  Like any method of memory allocation, the heap will become fragmented; that is, there will be sections of used and unused memory in the allocated space on the heap.  A good allocator will attempt to find an unused area of already allocated memory to use before resorting to expanding the heap. The major problem with this method is that the heap has only two significant attributes:  base, or the beginning of the heap in virtual memory space; and length, or its size.  The heap requires enough system memory to fill its entire length, and its base can never change.  Thus, any large areas of unused memory are wasted.  The heap can get ""stuck"" in this position if a small used segment exists at the end of the heap, which could waste any magnitude of address space, from a few megabytes to a few hundred.  On lazy memory allocation schemes, such as those often found in the Linux operating system, a large heap does not necessarily reserve the equivalent system memory; it will only do so at the first write time (reads of non-mapped memory pages return zero). The granularity of this depends on page size.
Doug Lea has developed dlmalloc (""Doug Lea's Malloc"") as a general-purpose allocator, starting in 1987. The GNU C library (glibc) uses ptmalloc, an allocator based on dlmalloc.
Memory on the heap is allocated as ""chunks"", an 8-byte aligned data structure which contains a header, and usable memory. Allocated memory contains an 8 or 16 byte overhead for the size of the chunk and usage flags. Unallocated chunks also store pointers to other free chunks in the usable space area, making the minimum chunk size 24 bytes.
Unallocated memory is grouped into ""bins"" of similar sizes, implemented by using a double-linked list of chunks (with pointers stored in the unallocated space inside the chunk).
For requests below 256 bytes (a ""smallbin"" request), a simple two power best fit allocator is used. If there are no free blocks in that bin, a block from the next highest bin is split in two.
For requests of 256 bytes or above but below the mmap threshold, recent versions of dlmalloc use an in-place bitwise trie algorithm. If there is no free space left to satisfy the request, dlmalloc tries to increase the size of the heap, usually via the brk system call.
For requests above the mmap threshold (a ""largebin"" request), the memory is always allocated using the mmap system call. The threshold is usually 256 KB. The mmap method averts problems with huge buffers trapping a small allocation at the end after their expiration, but always allocates an entire page of memory, which on many architectures is 4096 bytes in size.
Since FreeBSD 7.0 and NetBSD 5.0, the old malloc implementation (phkmalloc) was replaced by jemalloc, written by Jason Evans.  The main reason for this was a lack of scalability of phkmalloc in terms of multithreading.  In order to avoid lock contention, jemalloc uses separate ""arenas"" for each CPU.  Experiments measuring number of allocations per second in multithreading application have shown that this makes it scale linearly with the number of threads, while for both phkmalloc and dlmalloc performance was inversely proportional to the number of threads.
OpenBSD's implementation of the malloc function makes use of mmap. For requests greater in size than one page, the entire allocation is retrieved using mmap; smaller sizes are assigned from memory pools maintained by malloc within a number of ""bucket pages,"" also allocated with mmap.[better source needed] On a call to free, memory is released and unmapped from the process address space using munmap. This system is designed to improve security by taking advantage of the address space layout randomization and gap page features implemented as part of OpenBSD's mmap system call, and to detect use-after-free bugs—as a large memory allocation is completely unmapped after it is freed, further use causes a segmentation fault and termination of the program.
Hoard is an allocator whose goal is scalable memory allocation performance. Like OpenBSD's allocator, Hoard uses mmap exclusively, but manages memory in chunks of 64 kilobytes called superblocks. Hoard's heap is logically divided into a single global heap and a number of per-processor heaps. In addition, there is a thread-local cache that can hold a limited number of superblocks. By allocating only from superblocks on the local per-thread or per-processor heap, and moving mostly-empty superblocks to the global heap so they can be reused by other processors, Hoard keeps fragmentation low while achieving near linear scalability with the number of threads.
Every thread has local storage for small allocations. For large allocations mmap or sbrk can be used. TCMalloc, a malloc developed by Google, has garbage-collection for local storage of dead threads. The TCMalloc is considered to be more than twice as fast as glibc's ptmalloc for multithreaded programs.
Operating system kernels need to allocate memory just as application programs do. The implementation of malloc within a kernel often differs significantly from the implementations used by C libraries, however. For example, memory buffers might need to conform to special restrictions imposed by DMA, or the memory allocation function might be called from interrupt context. This necessitates a malloc implementation tightly integrated with the virtual memory subsystem of the operating system kernel.
Because  malloc and its relatives can have a strong impact on the performance of a program, it is not uncommon to override the functions for a specific application by custom implementations that are optimized for application's allocation patterns. The C standard provides no way of doing this, but operating systems have found various ways to do this by exploiting dynamic linking. One way is to simply link in a different library to override the symbols. Another, employed by Unix System V.3, is to make malloc and free function pointers that an application can reset to custom functions.
The largest possible memory block malloc can allocate depends on the host system, particularly the size of physical memory and the operating system implementation. Theoretically, the largest number should be the maximum value that can be held in a size_t type, which is an implementation-dependent unsigned integer representing the size of an area of memory. In the C99 standard and later, it is available as the SIZE_MAX constant from <stdint.h>. Although not guaranteed by ISO C, it is usually  2CHAR_BIT × sizeof(size_t) − 1.
The C library implementations shipping with various operating systems and compilers may come with alternatives and extensions to the standard malloc package. Notable among these is:
"
"166","
mtrace is the memory debugger included in the GNU C Library.
The function mtrace installs handlers for malloc, realloc and free; the function muntrace disables these handlers.  Their prototypes, defined in the header file mcheck.h, are
The handlers log all memory allocations and frees to a file defined by the environment variable MALLOC_TRACE (if the variable is unset, describes an invalid filename, or describes a filename the user does not have permissions to, the handlers are not installed).
A perl script called mtrace, not to be confused with the function of the same name, is also distributed with the GNU C Library; the script parses through the output file and reports all allocations that were not freed.
The following is an example of bad source code.  The problem with the program is that it allocates memory, but doesn’t free the memory before exiting.
If the mtrace command reports “No Memory Leaks”, then all memory that was allocated in the last execution of that program was also released, which is the way it should be.  If, on the other hand, mtrace gives output such as that below, it means the programmer still has some work to do.
The following is an example of good source code.  It releases memory after it is allocated, and it uses mtrace to notify the programmer if there are memory leaks."
"167","In object-oriented programming, a finalizer or finalize method is a special method that performs finalization, generally some form of cleanup. A finalizer is executed during object destruction, prior to the object being deallocated, and is complementary to an initializer, which is executed during object creation, following allocation. Finalizers are strongly discouraged by some, due to difficulty in proper use and the complexity they add, and alternatives are suggested instead, primarily the dispose pattern – see problems with finalizers.
The term ""finalizer"" is primarily used in object-oriented languages that use garbage collection, of which the archetype is Smalltalk. This is contrasted with a ""destructor"", which is a method called for finalization in languages with deterministic object lifetimes, archetypically C++. These are generally exclusive – a language will have either finalizers (if garbage collected) or destructors (if deterministic), but in rare cases a language may have both, as in C++/CLI and D, and in case of reference counting (instead of tracing garbage collection), terminology varies. In technical usage, ""finalizer"" may also be used to refer to destructors, as these also perform finalization, and some subtler distinctions are drawn – see terminology. The term ""final"" is also used to indicate a class that cannot be inherited; this is unrelated.
The terminology of ""finalizer"" and ""finalization"" versus ""destructor"" and ""destruction"" varies between authors and is sometimes unclear.
In common usage, a destructor is a method called deterministically on object destruction, and the archetype is C++ destructors; while a finalizer is called non-deterministically by the garbage collector, and the archetype is Java finalize methods.
For languages that implement garbage collection via reference counting, terminology varies, with some languages such as Objective-C and Perl using ""destructor"", and other languages such as Python using ""finalizer"" (per spec, Python is garbage collected, but the reference CPython implementation uses reference counting). This reflects the fact that reference counting results in semi-deterministic object lifetime: for objects that are not part of a cycle, objects are destroyed deterministically when the reference count drops to zero, but objects that are part of a cycle are destroyed non-deterministically, as part of a separate form of garbage collection.
In certain narrow technical usage, ""constructor"" and ""destructor"" are language-level terms, meaning ""methods defined in a class"", while ""initializer"" and ""finalizer"" are implementation-level terms, meaning ""methods called during object creation or destruction"". Thus for example the original specification for the C# language referred to ""destructors"", even though C# is garbage-collected, but the specification for the Common Language Infrastructure (CLI), and the implementation of its runtime environment as the Common Language Runtime (CLR), referred to ""finalizers"". This is reflected in the C# language committee's notes, which read in part: ""The C# compiler compiles destructors to ... [probably] instance finalizer[s]"". This terminology is confusing, and thus more recent versions of the C# spec refer to the language-level method as ""finalizers"".
Another language that does not make this terminology distinction is D. Although D classes are garbage collected, their cleanup functions are called destructors.
Finalization is primarily used for cleanup, to release memory or other resources: to deallocate memory allocated via manual memory management; to clear references if reference counting is used (decrement reference counts); to release resources, particularly in the Resource Acquisition Is Initialization (RAII) idiom; or to unregister an object. The amount of finalization varies significantly between languages, from extensive finalization in C++, which has manual memory management, reference counting, and deterministic object lifetimes; to often no finalization in Java, which has non-deterministic object lifetimes and is often implemented with a tracing garbage collector. It is also possible for there to be little or no explicit (user-specified) finalization, but significant implicit finalization, performed by the compiler, interpreter, or runtime; this is common in case of automatic reference counting, as in the CPython reference implementation of Python, or in Automatic Reference Counting in Apple's implementation of Objective-C, which both automatically break references during finalization. A finalizer can include arbitrary code; a particularly complex use is to automatically return the object to an object pool.
Memory deallocation during finalization is common in languages like C++ where manual memory management is standard, but also occurs in managed languages when memory has been allocated outside of the managed heap (externally to the language); in Java this occurs with Java Native Interface (JNI) and ByteBuffer objects in New I/O (NIO). This latter can cause problems due to the garbage collector not being able to track these external resources, so they will not be collected aggressively enough, and can cause out-of-memory errors due to exhausting unmanaged memory – this can be avoided by treating native memory as a resource and using the dispose pattern, as discussed below.
Finalizers are generally both much less necessary and much less used than destructors. They are much less necessary because garbage collection automates memory management, and much less used because they are not generally executed deterministically – they may not be called in a timely manner, or even at all, and the execution environment cannot be predicted – and thus any cleanup that must be done in a deterministic way must instead be done by some other method, most frequently manually via the dispose pattern. Notably, both Java and Python do not guarantee that finalizers will ever be called, and thus they cannot be relied on for cleanup.
Due to the lack of programmer control over their execution, it is usually recommended to avoid finalizers for any but the most trivial operations. In particular, operations often performed in destructors are not usually appropriate for finalizers. A common anti-pattern is to write finalizers as if they were destructors, which is both unnecessary and ineffectual, due to differences between finalizers and destructors. This is particularly common among C++ programmers, as destructors are heavily used in idiomatic C++, following the Resource Acquisition Is Initialization (RAII) idiom.
Programming languages that use finalizers include C++/CLI, C#, Java, and Python. Syntax varies significantly by language.
In Java a finalizer is a method called finalize, which overrides the Object.finalize method.
In Python, a finalizer is a method called __del__.
In C#, a finalizer (called ""destructor"" in earlier versions of the standard) is a method whose name is the class name with ~ prefixed, as in ~Foo – this is the same syntax as a C++ destructor, and these methods were originally called ""destructors"", by analogy with C++, despite having different behavior, but were renamed to ""finalizers"" due to the confusion this caused.
In C++/CLI, which has both destructors and finalizers, a destructor is a method whose name is the class name with ~ prefixed, as in ~Foo (as in C++), and a finalizer is a method whose name is the class name with ! prefixed, as in !Foo.
A finalizer is called when an object is garbage collected – after an object has become garbage (unreachable), but before its memory is deallocated. Finalization occurs non-deterministically, at the discretion of the garbage collector, and might never occur. This contrasts with destructors, which are called deterministically as soon as an object is no longer in use, and are always called, except in case of uncontrolled program termination. Finalizers are most frequently instance methods, due to needing to do object-specific operations.
The garbage collector must also account for the possibility of object resurrection. Most commonly this is done by first executing finalizers, then checking whether any objects have been resurrected, and if so, aborting their destruction. This additional check is potentially expensive – a simple implementation re-checks all garbage if even a single object has a finalizer – and thus both slows down and complicates garbage collection. For this reason, objects with finalizers may be collected less frequently than objects without finalizers (only on certain cycles), exacerbating problems caused by relying on prompt finalization, such as resource leaks.
If an object is resurrected, there is the further question of whether its finalizer is called again, when it is next destroyed – unlike destructors, finalizers are potentially called multiple times. If finalizers are called for resurrected objects, objects may repeatedly resurrect themselves and be indestructible; this occurs in the CPython implementation of Python prior to Python 3.4, and in CLR languages such as C#. To avoid this, in many languages, including Java, Objective-C (at least in recent Apple implementations), and Python from Python 3.4, objects are finalized at most once, which requires tracking if the object has been finalized yet.
In other cases, notably CLR languages like C#, finalization is tracked separately from the objects themselves, and objects can be repeatedly registered or deregistered for finalization.
Finalizers can cause a significant number of problems, and are thus strongly discouraged by a number of authorities. These problems include:
Further, finalizers may fail to run due to object remaining reachable beyond when they are expected to be garbage, either due to programming errors or due to unexpected reachability. For example, when Python catches an exception (or an exception is not caught in interactive mode), it keeps a reference to the stack frame where the exception was raised, which keeps objects referenced from that stack frame alive.
Finalizers in a superclass can also slow down garbage collection in a subclass, as the finalizer can potentially refer to fields in the subclass, and thus the field cannot be garbage collected until the following cycle, once the finalizer has run. This can be avoided by using composition over inheritance.
A common anti-pattern is to use finalizers to release resources, by analogy with the Resource Acquisition Is Initialization (RAII) idiom of C++: acquire a resource in the initializer (constructor), and release it in the finalizer (destructor). This does not work, for a number of reasons. Most basically, finalizers may never be called, and even if called, may not be called in a timely manner – thus using finalizers to release resources will generally cause resource leaks. Further, finalizers are not called in a prescribed order, while resources often need to be released in a specific order, frequently the opposite order in which they were acquired. Also, as finalizers are called at the discretion of the garbage collector, they will often only be called under managed memory pressure (when there is little managed memory available), regardless of resource pressure – if scarce resources are held by garbage but there is plenty of managed memory available, garbage collection may not occur, thus not reclaiming these resources.
Thus instead of using finalizers for automatic resource management, in garbage-collected languages one instead must manually manage resources, generally by using the dispose pattern. In this case resources may still be acquired in the initializer, which is called explicitly on object instantiation, but are released in the dispose method. The dispose method may be called explicitly, or implicitly by language constructs such as C#'s using, Java's try-with-resources, or Python's with.
However, in certain cases both the dispose pattern and finalizers are used for releasing resources. This is primarily found in CLR languages such as C#, where finalization is used as a backup for disposal: when a resource is acquired, the acquiring object is queued for finalization so that the resource is released on object destruction, even if the resource is not released by manual disposal.
In languages with deterministic object lifetimes, notably C++, resource management is frequently done by tying resource possession lifetime to object lifetime, acquiring resources during initialization and releasing them during finalization; this is known as Resource Acquisition Is Initialization (RAII). This ensures that resource possession is a class invariant, and that resources are released promptly when the object is destroyed.
However, in languages with non-deterministic object lifetimes – which include all major languages with garbage collection, such as C#, Java, and Python – this does not work, because finalization may not be timely or may not happen at all, and thus resources may not be released for a long time or even at all, causing resource leaks. In these languages resources are instead generally managed manually via the dispose pattern: resources may still be acquired during initialization, but are released by calling a dispose method. Nevertheless, using finalization for releasing resources in these languages is a common anti-pattern, and forgetting to call dispose will still cause a resource leak.
In some cases both techniques are combined, using an explicit dispose method, but also releasing any still-held resources during finalization as a backup. This is commonly found in C#, and is implemented by registering an object for finalization whenever a resource is acquired, and suppressing finalization whenever a resource is released.
If user-specified finalizers are allowed, it is possible for finalization to cause object resurrection, as the finalizers can run arbitrary code, which may create references from live objects to objects being destroyed. For languages without garbage collection, this is a severe bug, and causes dangling references and memory safety violations; for languages with garbage collection, this is prevented by the garbage collector, most commonly by adding another step to garbage collection (after running all user-specified finalizers, check for resurrection), which complicates and slows down garbage collection.
Further, object resurrection means that an object may not be destroyed, and in pathological cases an object can always resurrect itself during finalization, making itself indestructible. To prevent this, some languages, like Java and Python (from Python 3.4) only finalize objects once, and do not finalize resurrected objects. Concretely this is done by tracking if an object has been finalized on an object-by-object basis. Objective-C also tracks finalization (at least in recent Apple versions) for similar reasons, treating resurrection as a bug.
A different approach is used in the .NET Framework, notably C# and Visual Basic .NET, where finalization is tracked by a ""queue"", rather than by object. In this case, if a user-specified finalizer is provided, by default the object is only finalized once (it is queued for finalization on creation, and dequeued once it is finalized), but this can be changed via calling the GC module. Finalization can be prevented by calling GC.SuppressFinalize, which dequeues the object, or reactivated by calling GC.ReRegisterForFinalize, which enqueues the object. These are particularly used when using finalization for resource management as a supplement to the dispose pattern, or when implementing an object pool.
Finalization is formally complementary to initialization – initialization occurs at the start of lifetime, finalization at the end – but differs significantly in practice. Both variables and objects are initialized, primarily to assign values, but in general only objects are finalized, and in general there is no need to clear values – the memory can simply be deallocated and reclaimed by the operating system.
Beyond assigning initial values, initialization is primarily used to acquire resources or to register an object with some service (like an event handler). These actions have symmetric release or unregister actions, and these can symmetrically be handled in a finalizer, which is done in RAII. However, in many languages, notably those with garbage collection, object lifetime is asymmetric: object creation happens deterministically at some explicit point in the code, but object destruction happens non-deterministically, in some unspecified environment, at the discretion of the garbage collector. This asymmetry means that finalization cannot be effectively used as the complement of initialization, because it does not happen in a timely manner, in a specified order, or in a specified environment. The symmetry is partially restored by also disposing of the object at an explicit point, but in this case disposal and destruction do not happen at the same point, and an object may be in a ""disposed but still alive"" state, which weakens the class invariants and complicates use.
Variables are generally initialized at the start of their lifetime, but not finalized at the end of their lifetime – though if a variable has an object as its value, the object may be finalized. In some cases variables are also finalized: GCC extensions allow finalization of variables.
As reflected in the naming, ""finalization"" and the finally construct both fulfill similar purposes: performing some final action, generally cleaning up, after something else has finished. They differ in when they occur – a finally clause is executed when program execution leaves the body of the associated try clause – this occurs during stack unwind, and there is thus a stack of pending finally clauses, in order – while finalization occurs when an object is destroyed, which happens depending on the memory management method, and in general there is simply a set of objects awaiting finalization – often on the heap – which need not happen in any specific order.
However, in some cases these coincide. In C++, object destruction is deterministic, and the behavior of a finally clause can be produced by having a local variable with an object as its value, whose scope is a block corresponds to the body of a try clause – the object is finalized (destructed) when execution exits this scope, exactly as if there were a finally clause. For this reason, C++ does not have a finally construct – the difference being that finalization is defined in the class definition as the destructor method, rather than at the call site in a finally clause.
Conversely, in the case of a finally clause in a coroutine, like in a Python generator, the coroutine may never terminate – only ever yielding – and thus in ordinary execution the finally clause is never executed. If one interprets instances of a coroutine as objects, then the finally clause can be considered a finalizer of the object, and thus can be executed when the instance is garbage collected. In Python terminology, the definition of a coroutine is a generator function, while an instance of it is a generator iterator, and thus a finally clause in a generator function becomes a finalizer in generator iterators instantiated from this function.
The notion of finalization as a separate step in object destruction dates to Mongomery (1994), by analogy with the earlier distinction of initialization in object construction in Martin & Odell (1992). Literature prior to this point used ""destruction"" for this process, not distinguishing finalization and deallocation, and programming languages dating to this period, like C++ and Perl, use the term ""destruction"". The terms ""finalize"" and ""finalization"" are also used in the influential book Design Patterns (1994).[lower-alpha 1] The introduction of Java in 1995 contained finalize methods, which popularized the term and associated it with garbage collection, and languages from this point generally make this distinction and use the term ""finalization"", particularly in the context of garbage collection.
"
"168","In the C++ programming language, a copy constructor  is a special constructor for creating a new object as a copy of an existing object. Copy constructors are the standard way of copying objects in C++, as opposed to cloning, and have C++-specific nuances.
The first argument of such a constructor is a reference to an object of the same type as is being constructed (const or non-const), which might be followed by parameters of any type (all having default values).
Normally the compiler automatically creates a copy constructor for each class (known as an implicit copy constructor) but for special cases the programmer creates the copy constructor, known as a user-defined copy constructor. In such cases, the compiler does not create one. Hence, there is always one copy constructor that is either defined by the user or by the system.
A user-defined copy constructor is generally needed when an object owns pointers or non-shareable references, such as to a file, in which case a destructor and an assignment operator should also be written (see Rule of three).

Copying of objects is achieved by the use of a copy constructor and an assignment operator. A copy constructor has as its first parameter a (possibly const or volatile) reference to its own class type. It can have more arguments, but the rest must have default values associated with them. The following would be valid copy constructors for class X:
The first one should be used unless there is a good reason to use one of the others. One of the differences between the first and the second is that temporaries can be copied with the first. For example:
Another difference between them is the obvious:
The X& form of the copy constructor is used when it is necessary to modify the copied object. This is very rare but it can be seen used in the standard library's std::auto_ptr. A reference must be provided:
The following are invalid copy constructors (Reason - copy_from_me is not passed as reference) :
because the call to those constructors would require a copy as well, which would result in an infinitely recursive call.
The following cases may result in a call to a copy constructor:
These cases are collectively called copy-initialization and are equivalent to:T x = a;
It is however, not guaranteed that a copy constructor will be called in these cases, because the C++ Standard allows the compiler to optimize the copy away in certain cases, one example being the return value optimization (sometimes referred to as RVO).
An object can be assigned value using one of the two techniques:
An object can be initialized by any one of the following ways.
a. Through declaration
b. Through function arguments
c. Through function return value
The copy constructor is used only for initializations, and does not apply to assignments where the assignment operator is used instead.
The implicit copy constructor of a class calls base copy constructors and copies its members by means appropriate to their type. If it is a class type, the copy constructor is called. If it is a scalar type, the built-in assignment operator is used. Finally, if it is an array, each element is copied in the manner appropriate to its type.
By using a user-defined copy constructor the programmer can define the behavior to be performed when an object is copied.
These examples illustrate how copy constructors work and why they are required sometimes.
Let us consider the following example:
Output
As expected, timmy has been copied to the new object, timmy_clone. While timmy's age was changed, timmy_clone's age remained the same. This is because they are totally different objects.
The compiler has generated a copy constructor for us, and it could be written like this:
So, when do we really need a user-defined copy constructor? The next section will explore that question.
Now, consider a very simple dynamic array class like the following:
Output
Since we did not specify a copy constructor, the compiler generated one for us. The generated constructor would look something like:
The problem with this constructor is that it performs a shallow copy of the data pointer. It only copies the address of the original data member; this means they both share a pointer to the same chunk of memory, which is not what we want.  When the program reaches line (1), copy's destructor gets called (because objects on the stack are destroyed automatically when their scope ends). 
Array's destructor deletes the data array of the original, therefore when it deleted copy's data, because they share the same pointer, it also deleted first's data. Line (2) now accesses invalid data and writes to it! This produces the infamous segmentation fault. 
If we write our own copy constructor that performs a deep copy then this problem goes away. 
Here, we are creating a new int array and copying the contents to it. Now, other's destructor deletes only its data, and not first's data. Line (2) will not produce a segmentation fault anymore.
Instead of doing a deep copy right away, there are some optimization strategies that can be used. These allow you to safely share the same data between several objects, thus saving space. The copy-on-write strategy makes a copy of the data only when it is written to. Reference counting keeps the count of how many objects are referencing the data, and will delete it only when this count reaches zero (e.g. boost::shared_ptr).
Contrary to expectations, a template copy constructor is not a user-defined copy constructor. Thus it is not enough to just have:
(Note that the type of A can be Array.) A user-defined, non-template copy constructor must also be provided for construction of Array from Array.
There is no such thing as ""Bitwise Copy Constructor"" in C++. However, the default generated copy constructor copies by invoking copy constructors on members, and for a raw pointer member this will copy the raw pointer (i.e. not a deep copy).
A logical copy constructor makes a true copy of the structure as well as its dynamic structures. Logical copy constructors come into the picture mainly when there are pointers or complex objects within the object being copied.
An explicit copy constructor is one that is declared explicit by using the explicit keyword. For example:
It is used to prevent copying of objects at function calls or with the copy-initialization syntax.
"
"169","Resource acquisition is initialization (RAII) is a programming idiom used in several object-oriented languages.  In RAII, holding a resource is a class invariant, and is tied to object lifetime: resource allocation (or acquisition) is done during object creation (specifically initialization), by the constructor, while resource deallocation (release) is done during object destruction (specifically finalization), by the destructor. Thus the resource is guaranteed to be held between when initialization finishes and finalization starts (holding the resources is a class invariant), and to be held only when the object is alive. Thus if there are no object leaks, there are no resource leaks.
RAII is associated most prominently with C++ where it originated, but also D, Ada, Vala, and Rust. The technique was developed for exception-safe resource management in C++ during 1984–89, primarily by Bjarne Stroustrup and Andrew Koenig, and the term itself was coined by Stroustrup. RAII is generally pronounced as an initialism, sometimes pronounced as ""R, A, double I"".
Other names for this idiom include Constructor Acquires, Destructor Releases (CADRe)  and one particular style of use is called Scope-based Resource Management (SBRM). This latter term is for the special case of automatic variables. RAII ties resources to object lifetime, which may not coincide with entry and exit of a scope. (Notably variables allocated on the free store have lifetimes unrelated to any given scope.) However, using RAII for automatic variables (SBRM) is the most common use case.
The following C++11 example demonstrates usage of RAII for file access and mutex locking:
This code is exception-safe because C++ guarantees that all stack objects are destroyed at the end of the enclosing scope, known as stack unwinding. The destructors of both the lock and file objects are therefore guaranteed to be called when returning from the function, whether an exception has been thrown or not.
Local variables allow easy management of multiple resources within a single function: they are destroyed in the reverse order of their construction, and an object is destroyed only if fully constructed—that is, if no exception propagates from its constructor.
Using RAII greatly simplifies resource management, reduces overall code size and helps ensure program correctness. RAII is therefore highly recommended in C++, and most of the C++ standard library follows the idiom.
The advantages of RAII as a resource management technique are that it provides encapsulation, exception safety (for stack resources), and locality (it allows acquisition and release logic to be written next to each other).
Encapsulation is provided because resource management logic is defined once in the class, not at each call site. Exception safety is provided for stack resources (resources that are released in the same scope as they are acquired) by tying the resource to the lifetime of a stack variable (a local variable declared in a given scope): if an exception is thrown, and proper exception handling is in place, the only code that will be executed when exiting the current scope are the destructors of objects declared in that scope. Finally, locality of definition is provided by writing the constructor and destructor definitions next to each other in the class definition.
Resource management therefore needs to be tied to the lifespan of suitable objects in order to gain automatic allocation and reclamation. Resources are acquired during initialization, when there is no chance of them being used before they are available, and released with the destruction of the same objects, which is guaranteed to take place even in case of errors.
Comparing RAII with the finally construct used in Java, Stroustrup wrote that “In realistic systems, there are far more resource acquisitions than kinds of resources, so the ""resource acquisition is initialization"" technique leads to less code than use of a ""finally"" construct.”
The RAII design is often used for controlling mutex locks in multi-threaded applications. In that use, the object releases the lock when destroyed. Without RAII in this scenario the potential for deadlock would be high and the logic to lock the mutex would be far from the logic to unlock it. With RAII, the code that locks the mutex essentially includes the logic that the lock will be released when execution leaves the scope of the RAII object.
Another typical example is interacting with files: We could have an object that represents a file that is open for writing, wherein the file is opened in the constructor and closed when execution leaves the object's scope. In both cases, RAII ensures only that the resource in question is released appropriately; care must still be taken to maintain exception safety. If the code modifying the data structure or file is not exception-safe, the mutex could be unlocked or the file closed with the data structure or file corrupted.
Ownership of dynamically allocated objects (memory allocated with new in C++) can also be controlled with RAII, such that the object is released when the RAII (stack-based) object is destroyed. For this purpose, the C++11 standard library defines the smart pointer classes std::unique_ptr for single-owned objects and std::shared_ptr for objects with shared ownership. Similar classes are also available through std::auto_ptr in C++98, and boost::shared_ptr in the Boost libraries.
Both Clang and GNU Compiler Collection implement a non-standard extension to the C language to support RAII: the ""cleanup"" variable attribute. The following macro annotates a variable with a given destructor function that it will call when the variable goes out of scope:
This macro can then be used as follows:
In this example, the compiler arranges for the fclosep function to be called before example_usage returns.
RAII only works for resources acquired and released (directly or indirectly) by stack-allocated objects,
where there is a well-defined static object lifetime.
Heap-allocated objects which themselves acquire and release resources are common in many languages, including C++. RAII depends on heap-based objects to be implicitly or explicitly deleted along all possible execution paths, in order to trigger its resource-releasing destructor (or equivalent).:8:27 This can be achieved by using smart pointers to manage all heap objects, with weak-pointers for cyclically referenced objects.
In C++, stack unwinding is only guaranteed to occur if the exception is caught somewhere. This is because ""If no matching handler is found in a program, the function terminate() is called; whether or not the stack is unwound before this call to terminate() is implementation-defined (15.5.1)."" (C++03 standard, §15.3/9). This behavior is usually acceptable, since the operating system releases remaining resources like memory, files, sockets, etc. at program termination.
Perl, Python (in the CPython implementation),  and PHP manage object lifetime by reference counting, which makes it possible to use RAII. Objects that are no longer referenced are immediately destroyed or finalized and released, so a destructor or finalizer can release the resource at that time. However, it is not always idiomatic in such languages, and is specifically discouraged in Python (in favor of context managers and finalizers from the weakref package).
However, object lifetimes are not necessarily bound to any scope, and objects may be destroyed non-deterministically or not at all. This makes it possible to accidentally leak resources that should have been released at the end of some scope. Objects stored in a static variable (notably a global variable) may not be finalized when the program terminates, so their resources are not released; CPython makes no guarantee of finalizing such objects, for instance. Further, objects with circular references will not be collected by a simple reference counter, and will live indeterminately long; even if collected (by more sophisticated garbage collection), destruction time and destruction order will be non-deterministic. In CPython there is a cycle detector which detects cycles and finalizes the objects in the cycle, though prior to CPython 3.4, cycles are not collected if any object in the cycle has a finalizer."
"170","auto_ptr was a class template available in the C++ Standard Library (declared in the <memory> header file) that provided some basic RAII features for C++ raw pointers. It has been replaced by the unique_ptr class. 
The auto_ptr template class describes an object that stores a pointer to a single allocated object that ensures that the object to which it points gets destroyed automatically when control leaves a scope.
The C++11 standard made auto_ptr deprecated, replacing it with the unique_ptr class template. auto_ptr was fully removed in C++17.
For shared ownership, the shared_ptr template class can be used. shared_ptr was defined in C++11 and is available in the Boost library.
The auto_ptr class is declared in ISO/IEC 14882, section 20.4.5 as:
The auto_ptr has semantics of strict ownership, meaning that the auto_ptr instance is the sole entity responsible for the object's lifetime. If an auto_ptr is copied, the source loses the reference. For example:
This code will print a NULL address for the first auto_ptr object and some non-NULL address for the second, showing that the source object lost the reference during the assignment (=). The raw pointer i in the example should not be deleted, as it will be deleted by the auto_ptr that owns the reference.  In fact, new int could be passed directly into x, eliminating the need for i.
Notice that the object pointed by an auto_ptr is destroyed using operator delete; this means that you should only use auto_ptr for pointers obtained with operator new. This excludes pointers returned by malloc/calloc/realloc, and pointers to arrays (because arrays are allocated by operator new[] and must be deallocated by operator delete[]).
Because of its copy semantics, auto_ptr may not be used in STL containers that may perform element copies in their operations."
"171","In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated.
When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class.
In some languages, classes are only a compile-time feature (new classes cannot be declared at runtime), while in other languages classes are first-class citizens, and are generally themselves objects (typically of type Class or similar). In these languages, a class that creates classes is called a metaclass.
In casual use, people often refer to the ""class"" of an object, but narrowly speaking objects have type: the interface, namely the types of member variables, the signatures of member functions (methods), and properties these satisfy. At the same time, a class has an implementation (specifically the implementation of the methods), and can create objects of a given type, with a given implementation. In the terms of type theory, a class is an implementation‍—‌a concrete data structure and collection of subroutines‍—‌while a type is an interface. Different (concrete) classes can produce objects of the same (abstract) type (depending on type system); for example, the type Stack might be implemented with two classes –  SmallStack (fast for small stacks, but scales poorly) and ScalableStack (scales well but high overhead for small stacks). Similarly, a given class may have several different constructors.
Types generally represent nouns, such as a person, place or thing, or something nominalized, and a class represents an implementation of these. For example, a Banana type might represent the properties and functionality of bananas in general, while the ABCBanana and XYZBanana classes would represent ways of producing bananas (say, banana suppliers or data structures and functions to represent and draw bananas in a video game). The ABCBanana class could then produce particular bananas: instances of the ABCBanana class would be objects of type Banana. Often only a single implementation of a type is given, in which case the class name is often identical with the type name.
Classes are composed from structural and behavioral constituents. Programming languages that include classes as a programming construct offer support, for various class-related features, and the syntax required to use these features varies greatly from one programming language to another.
A class contains data field descriptions (or properties, fields, data members, or attributes). These are usually field types and names that will be associated with state variables at program run time; these state variables either belong to the class or specific instances of the class. In most languages, the structure defined by the class determines the layout of the memory used by its instances. Other implementations are possible: for example, objects in Python use associative key-value containers.
Some programming languages support specification of invariants as part of the definition of the class, and enforce them through the type system. Encapsulation of state is necessary for being able to enforce the invariants of the class.
The behavior of class or its instances is defined using methods. Methods are subroutines with the ability to operate on objects or classes. These operations may alter the state of an object or simply provide ways of accessing it. Many kinds of methods exist, but support for them varies across languages. Some types of methods are created and called by programmer code, while other special methods—such as constructors, destructors, and conversion operators—are created and called by compiler-generated code. A language may also allow the programmer to define and call these special methods.
Every class implements (or realizes) an interface by providing structure and behavior. Structure consists of data and state, and behavior consists of code that specifies how methods are implemented. There is a distinction between the definition of an interface and the implementation of that interface; however, this line is blurred in many programming languages because class declarations both define and implement an interface. Some languages, however, provide features that separate interface and implementation. For example, an abstract class can define an interface without providing implementation.
Languages that support class inheritance also allow classes to inherit interfaces from the classes that they are derived from.[example  needed] In languages that support access specifiers, the interface of a class is considered to be the set of public members of the class, including both methods and attributes (via implicit getter and setter methods); any private members or internal data structures are not intended to be depended on by external  code and thus are not part of the interface.
Object-oriented programming methodology  dictates that the operations of any interface of a class are to be independent of each other. It results in a layered design where clients of an interface use the methods declared in the interface. An interface places no requirements for clients to invoke the operations of one interface in any particular order. This approach has the benefit that client code can assume that the operations of an interface are available for use whenever the client  has access to the object. [citation needed]
The buttons on the front of your television set are the interface between you and the electrical wiring on the other side of its plastic casing. You press the ""power"" button to toggle the television on and off. In this example, your particular television is the instance, each method is represented by a button, and all the buttons together comprise the interface. (Other television sets that are the same model as yours would have the same interface.) In its most common form, an interface is a specification of a group of related methods without any associated implementation of the methods.
A television set also has a myriad of attributes, such as size and whether it supports color, which together comprise its structure. A class represents the full description of a television, including its attributes (structure) and buttons (interface).
Getting the total number of televisions manufactured could be a static method of the television class. This method is clearly associated with the class, yet is outside the domain of each individual instance of the class. Another example would be a static method that finds a particular instance out of the set of all television objects.
The following is a common set of access specifiers:
Although many object-oriented languages support the above access specifiers, their semantics may differ.
Object-oriented design uses the access specifiers in conjunction with careful design of public method implementations to enforce class invariants—constraints on the state of the objects. A common usage of access specifiers is to separate the internal data of a class from its interface: the internal structure is made private, while public accessor methods can be used to inspect or alter such private data.
Access specifiers do not necessarily control visibility, in that even private members may be visible to client external code. In some languages, an inaccessible but visible member may be referred to at run-time (for example, by a pointer returned from a member function), but an attempt to use it by referring to the name of the member from client code will be prevented by the type checker.
The various object-oriented programming languages enforce member accessibility and visibility to various degrees, and depending on the language's type system and compilation policies, enforced at either compile-time or run-time. For example, the Java language does not allow client code that accesses the private data of a class to compile.
 In the C++ language, private methods are visible, but not accessible in the interface; however, they may be made invisible by explicitly declaring fully abstract classes that represent the interfaces of the class.
Some languages feature other accessibility schemes:
In addition to the design of standalone classes, programming languages may support more advanced class design based upon relationships between classes. The inter-class relationship design capabilities commonly provided are compositional and hierarchical.
Classes can be composed of other classes, thereby establishing a compositional relationship between the enclosing class and its embedded classes. Compositional relationship between classes is also commonly known as a has-a relationship. For example, a class ""Car"" could be composed of and contain a class ""Engine"". Therefore, a Car has an Engine. One aspect of composition is containment, which is the enclosure of component instances by the instance that has them. If an enclosing object contains component instances by value, the components and their enclosing object have a similar lifetime. If the components are contained by reference, they may not have a similar lifetime. For example, in Objective-C 2.0:
This Car class has an instance of NSString (a string object), Engine, and NSArray (an array object).
Classes can be derived from one or more existing classes, thereby establishing a hierarchical relationship between the derived-from classes (base classes, parent classes or superclasses) and the derived class (child class or subclass) . The relationship of the derived class to the derived-from classes is commonly known as an is-a relationship. For example, a class 'Button' could be derived from a class 'Control'. Therefore, a Button is a Control. Structural and behavioral members of the parent classes are inherited by the child class. Derived classes can define additional structural members (data fields) and behavioral members (methods) in addition to those that they inherit and are therefore specializations of their superclasses. Also, derived classes can override inherited methods if the language allows.
Not all languages support multiple inheritance. For example, Java allows a class to implement multiple interfaces, but only inherit from one class. If multiple inheritance is allowed, the hierarchy is a directed acyclic graph (or DAG for short), otherwise it is a tree. The hierarchy has classes as nodes and inheritance relationships as links. Classes in the same level are more likely to be associated than classes in different levels. The levels of this hierarchy are called layers or levels of abstraction.
Example (Simplified Objective-C 2.0 code, from iPhone SDK):
In this example, a UITableView is a UIScrollView is a UIView is a UIResponder is an NSObject.
Conceptually, a superclass is a superset of its subclasses. For example, a common class hierarchy would involve GraphicObject as a superclass of Rectangle and Elipse, while Square would be a subclass of Rectangle. These are all subset relations in set theory as well, i.e., all squares are rectangles but not all rectangles are squares.
A common conceptual error is to mistake a part of relation with a subclass. For example, a car and truck are both kinds of vehicles and it would be appropriate to model them as subclasses of a vehicle class. However, it would be an error to model the component parts of the car as subclass relations. For example, a car is composed of an engine and body, but it would not be appropriate to model engine or body as a subclass of car.
In object-oriented modeling these kinds of relations are typically modeled as object properties. In this example the Car class would have a property called parts. parts would be typed to hold a collection of objects such as instances of Body, Engine, Tires,....
Object modeling languages such as UML include capabilities to model various aspects of part of and other kinds of relations. Data such as the cardinality of the objects, constraints on input and output values, etc. This information can be utilized by developer tools to generate additional code beside the basic data definitions for the objects. Things such as error checking on get and set methods.
One important question when modeling and implementing a system of object classes is whether a class can have one or more superclasses. In the real world with actual sets it would be rare to find sets that didn't intersect with more than one other set. However, while some systems such as Flavors and CLOS provide a capability for more than one parent to do so at run time introduces complexity that many in the object-oriented community consider antithetical to the goals of using object classes in the first place. Understanding which class will be responsible for handling a message can get complex when dealing with more than one superclass. If used carelessly this feature can introduce some of the same system complexity and ambiguity classes were designed to avoid.
Most modern object-oriented languages such as Smalltalk and Java require single inheritance at run time. For these languages, multiple inheritance may be useful for modeling but not for an implementation.
However, semantic web application objects do have multiple superclasses. The volatility of the Internet requires this level of flexibility and the technology standards such as the Web Ontology Language (OWL) are designed to support it.
A similar issue is whether or not the class hierarchy can be modified at run time. Languages such as Flavors, CLOS, and Smalltalk all support this feature as part of their meta-object protocols. Since classes are themselves first-class objects, it is possible to have them dynamically alter their structure by sending them the appropriate messages. Other languages that focus more on strong typing such as Java and C++ do not allow the class hierarchy to be modified at run time. Semantic web objects have the capability for run time changes to classes. The rational is similar to the justification for allowing multiple superclasses, that the Internet is so dynamic and flexible that dynamic changes to the hierarchy are required to manage this volatility.
Although class-based languages are commonly assumed to support inheritance, inheritance is not an intrinsic aspect of the concept of classes. Some languages, often referred to as ""object-based languages"", support classes yet do not support inheritance. Examples of object-based languages include earlier versions of Visual Basic.
In object-oriented analysis and in UML, an association between two classes represents a collaboration between the classes or their corresponding instances. Associations have direction; for example, a bi-directional association between two classes indicates that both of the classes are aware of their relationship. Associations may be labeled according to their name or purpose.
An association role is given end of an association and describes the role of the corresponding class. For example, a ""subscriber"" role describes the way instances of the class ""Person"" participate in a ""subscribes-to"" association with the class ""Magazine"". Also, a ""Magazine"" has the ""subscribed magazine"" role in the same association. Association role multiplicity describes how many instances correspond to each instance of the other class of the association. Common multiplicities are ""0..1"", ""1..1"", ""1..*"" and ""0..*"", where the ""*"" specifies any number of instances.
There are many categories of classes, some of which overlap.
In a language that supports inheritance, an abstract class, or abstract base class (ABC), is a class that cannot be instantiated because it is either labeled as abstract or it simply specifies abstract methods (or virtual methods). An abstract class may provide implementations of some methods, and may also specify virtual methods via signatures that are to be implemented by direct or indirect descendants of the abstract class. Before a class derived from an abstract class can be instantiated, all abstract methods of its parent classes must be implemented by some class in the derivation chain.
Most object-oriented programming languages allow the programmer to specify which classes are considered abstract and will not allow these to be instantiated. For example, in Java, C# and PHP, the keyword abstract is used. In C++, an abstract class is a class having at least one abstract method given by the appropriate syntax in that language (a pure virtual function in C++ parlance).
A class consisting of only virtual methods is called a Pure Abstract Base Class (or Pure ABC) in C++ and is also known as an interface by users of the language. Other languages, notably Java and C#, support a variant of abstract classes called an interface via a keyword in the language. In these languages, multiple inheritance is not allowed, but a class can implement multiple interfaces. Such a class can only contain abstract publicly accessible methods.
A concrete class is a class that can be instantiated, as opposed to abstract classes, which cannot. 
In some languages, classes can be declared in scopes other than the global scope. There are various types of such classes.
An inner class is a class defined within another class. The relationship between an inner class and its containing class can also be treated as another type of class association. An inner class is typically neither associated with instances of the enclosing class nor instantiated along with its enclosing class. Depending on language, it may or may not be possible to refer to the class from outside the enclosing class. A related concept is inner types, also known as inner data type or nested type, which is a generalization of the concept of inner classes. C++ is an example of a language that supports both inner classes and inner types (via typedef declarations).
Another type is a local class, which is a class defined within a procedure or function. This limits references to the class name to within the scope where the class is declared. Depending on the semantic rules of the language, there may be additional restrictions on local classes compared to non-local ones. One common restriction is to disallow local class methods to access local variables of the enclosing function. For example, in C++, a local class may refer to static variables declared within its enclosing function, but may not access the function's automatic variables.
Metaclasses are classes whose instances are classes. A metaclass describes a common structure of a collection of classes and can implement a design pattern or describe particular kinds of classes. Metaclasses are often used to describe frameworks.
In some languages, such as Python, Ruby or Smalltalk, a class is also an object; thus each class is an instance of a unique metaclass that is built into the language.

The Common Lisp Object System (CLOS) provides metaobject protocols (MOPs) to implement those classes and metaclasses.

Non-subclassable classes allow programmers to design classes and hierarchies of classes where at some level in the hierarchy, further derivation is prohibited. (A stand-alone class may be also designated as non-subclassable, preventing the formation of any hierarchy). Contrast this to abstract classes, which imply, encourage, and require derivation in order to be used at all. A non-subclassable class is implicitly concrete.
A non-subclassable class is created by declaring the class as sealed in C# or as final in Java or PHP.
For example, Java's String
 class is designated as final.
Non-subclassable classes may allow a compiler (in compiled languages) to perform optimizations that are not available for subclassable classes.[citation needed]
Some languages have special support for mixins, though in any language with multiple inheritance a mixin is simply a class that does not represent an is-a-type-of relationship. Mixins are typically used to add the same methods to multiple classes; for example, a class UnicodeConversionMixin might provide a method called unicode_to_ascii when included in classes FileReader and WebPageScraper that do not share a common parent.
In languages supporting the feature, a partial class is a class whose definition may be split into multiple pieces, within a single source-code file or across multiple files. The pieces are merged at compile-time, making compiler output the same as for a non-partial class.
The primary motivation for introduction of partial classes is to facilitate the implementation of code generators, such as visual designers. It is otherwise a challenge or compromise to develop code generators that can manage the generated code when it is interleaved within developer-written code. Using partial classes, a code generator can process a separate file or coarse-grained partial class within a file, and is thus alleviated from intricately interjecting generated code via extensive parsing, increasing compiler efficiency and eliminating the potential risk of corrupting developer code. In a simple implementation of partial classes, the compiler can perform a phase of precompilation where it ""unifies"" all the parts of a partial class. Then, compilation can proceed as usual.
Other benefits and effects of the partial class feature include:
Partial classes have existed in Smalltalk under the name of Class Extensions for considerable time. With the arrival of the .NET framework 2, Microsoft introduced partial classes, supported in both C# 2.0 and Visual Basic 2005. WinRT also supports partial classes.
This simple example, written in Visual Basic .NET, shows how parts of the same class are defined in two different files.
When compiled, the result is the same as if the two files were written as one, like this:
In Objective-C, partial classes, also known as categories, may even spread over multiple libraries and executables, like this example:
In Foundation, header file NSData.h:
In user-supplied library, a separate binary from Foundation framework, header file NSData+base64.h:
And in an app, yet another separate binary file, source code file main.m:
The dispatcher will find both methods called over the NSData instance and invoke both of them correctly.
Uninstantiable classes allow programmers to group together per-class fields and methods that are accessible at runtime without an instance of the class. Indeed, instantiation is prohibited for this kind of class.
For example, in C#, a class marked ""static"" can not be instantiated, can only have static members (fields, methods, other), may not have instance constructors, and is sealed.

An unnamed class or anonymous class is a class that is not bound to a name or identifier upon definition. This is analogous to named versus unnamed functions.
The benefits of organizing software into object classes fall into three categories:
Object classes facilitate rapid development because they lessen the semantic gap between the code and the users. System analysts can talk to both developers and users using essentially the same vocabulary, talking about accounts, customers, bills, etc. Object classes often facilitate rapid development because most object-oriented environments come with powerful debugging and testing tools. Instances of classes can be inspected at run time to verify that the system is performing as expected. Also, rather than get dumps of core memory, most object-oriented environments have interpreted debugging capabilities so that the developer can analyze exactly where in the program the error occurred and can see which methods were called to which arguments and with what arguments.
Object classes facilitate ease of maintenance via encapsulation. When developers need to change the behavior of an object they can localize the change to just that object and its component parts. This reduces the potential for unwanted side effects from maintenance enhancements.
Software re-use is also a major benefit of using Object classes. Classes facilitate re-use via inheritance and interfaces. When a new behavior is required it can often be achieved by creating a new class and having that class inherit the default behaviors and data of its superclass and then tailor some aspect of the behavior or data accordingly. Re-use via interfaces (also known as methods) occurs when another object wants to invoke (rather than create a new kind of) some object class. This method for re-use removes many of the common errors that can make their way into software when one program re-uses code from another.
These benefits come with a cost of course. One of the most serious obstacles to using object classes has been performance. Interpreted environments that support languages such as Smalltalk and CLOS provided rapid development but the resulting code was not nearly as fast as what could be achieved in some procedural languages such as C. This has been partly addressed by the development of object-oriented languages that are not interpreted such as C++ and Java. Also, due to Moore's law the processing power of computers has increased to the point where efficient code is not as critical for most systems as it was in the past.[citation needed]Still, no matter how well designed the language, there will always be an inevitable bit of required extra overhead to create a class rather than use procedural code and in some circumstances, especially where performance or memory are required to be optimal, that using object classes may not be the best approach.
Also, getting the benefits of object classes requires that they be used appropriately and that requires training. Without the proper training developers may simply code procedural programs in an object-oriented environment and end up with the worst of both worlds.
As a data type, a class is usually considered as a compile-time construct. A language may also support prototype or factory metaobjects that represent run-time information about classes, or even represent metadata that provides access to reflection facilities and ability to manipulate data structure formats at run-time. Many languages distinguish this kind of run-time type information about classes from a class on the basis that the information is not needed at run-time. Some dynamic languages do not make strict distinctions between run-time and compile-time constructs, and therefore may not distinguish between metaobjects and classes.
For example, if Human is a metaobject representing the class Person, then instances of class Person can be created by using the facilities of the Human metaobject."
"172","Smalltalk is an object-oriented, dynamically typed, reflective programming language. Smalltalk was created as the language to underpin the ""new world"" of computing exemplified by ""human–computer symbiosis."" It was designed and created in part for educational use, more so for constructionist learning, at the Learning Research Group (LRG)  of  Xerox PARC by Alan Kay, Dan Ingalls, Adele Goldberg, Ted Kaehler, Scott Wallace, and others during the 1970s.
The language was first generally released as Smalltalk-80. Smalltalk-like languages are in continuing active development and have gathered loyal communities of users around them. ANSI Smalltalk was ratified in 1998 and represents the standard version of Smalltalk.
Smalltalk took second place for ""most loved programming language"" in the Stack Overflow Developer Survey in 2017.
There are a large number of Smalltalk variants. The unqualified word Smalltalk is often used to indicate the Smalltalk-80 language, the first version to be made publicly available and created in 1980.
Smalltalk was the product of research led by Alan Kay at Xerox Palo Alto Research Center (PARC); Alan Kay designed most of the early Smalltalk versions, Adele Goldberg wrote most of the documentation, and Dan Ingalls implemented most of the early versions. The first version, known as Smalltalk-71, was created by Kay in a few mornings on a bet that a programming language based on the idea of message passing inspired by Simula could be implemented in ""a page of code."" A later variant actually used for research work is now known as Smalltalk-72 and influenced the development of the Actor model. Its syntax and execution model were very different from modern Smalltalk variants.
After significant revisions which froze some aspects of execution semantics to gain performance (by adopting a Simula-like class inheritance model of execution), Smalltalk-76 was created.  This system had a development environment featuring most of the now familiar tools, including a class library code browser/editor. Smalltalk-80 added metaclasses, to help maintain the ""everything is an object"" (except private instance variables) paradigm by associating properties and behavior with individual classes, and even primitives such as integer and boolean values (for example, to support different ways of creating instances).
Smalltalk-80 was the first language variant made available outside of PARC, first as Smalltalk-80 Version 1, given to a small number of firms (Hewlett-Packard, Apple Computer, Tektronix, and DEC) and universities (UC Berkeley) for ""peer review"" and implementation on their platforms. Later (in 1983) a general availability implementation, known as Smalltalk-80 Version 2, was released as an image (platform-independent file with object definitions) and a virtual machine specification. ANSI Smalltalk has been the standard language reference since 1998.
Two of the currently popular Smalltalk implementation variants are descendants of those original Smalltalk-80 images. Squeak is an open source implementation derived from Smalltalk-80 Version 1 by way of Apple Smalltalk. VisualWorks is derived from Smalltalk-80 version 2 by way of Smalltalk-80 2.5 and ObjectWorks (both products of ParcPlace Systems, a Xerox PARC spin-off company formed to bring Smalltalk to the market). As an interesting link between generations, in 2001 Vassili Bykov implemented Hobbes, a virtual machine running Smalltalk-80 inside VisualWorks. (Dan Ingalls later ported Hobbes to Squeak.)
During the late 1980s to mid-1990s, Smalltalk environments—including support, training and add-ons—were sold by two competing organizations: ParcPlace Systems and Digitalk, both California based. ParcPlace Systems tended to focus on the Unix/Sun microsystems market, while Digitalk focused on Intel-based PCs running Microsoft Windows or IBM's OS/2. Both firms struggled to take Smalltalk mainstream due to Smalltalk's substantial memory needs, limited run-time performance, and initial lack of supported connectivity to SQL-based relational database servers.  While the high price of ParcPlace Smalltalk limited its market penetration to mid-sized and large commercial organizations, the Digitalk products initially tried to reach a wider audience with a lower price. IBM initially supported the Digitalk product, but then entered the market with a Smalltalk product in 1995 called VisualAge/Smalltalk.  Easel introduced Enfin at this time on Windows and OS/2.  Enfin became far more popular in Europe, as IBM introduced it into IT shops before their development of IBM Smalltalk (later VisualAge).  Enfin was later acquired by Cincom Systems, and is now sold under the name ObjectStudio, and is part of the Cincom Smalltalk product suite.
In 1995, ParcPlace and Digitalk merged into ParcPlace-Digitalk and then rebranded in 1997 as ObjectShare, located in Irvine, CA. ObjectShare (NASDAQ: OBJS) was traded publicly until 1999, when it was delisted and dissolved. The merged firm never managed to find an effective response to Java as to market positioning, and by 1997 its owners were looking to sell the business. In 1999, Seagull Software acquired the ObjectShare Java development lab (including the original Smalltalk/V and Visual Smalltalk development team), and still owns VisualSmalltalk, although worldwide distribution rights for the Smalltalk product remained with ObjectShare who then sold them to Cincom. VisualWorks was sold to Cincom and is now part of Cincom Smalltalk. Cincom has backed Smalltalk strongly, releasing multiple new versions of VisualWorks and ObjectStudio each year since 1999.
Cincom, Gemstone and Object Arts, plus other vendors continue to sell Smalltalk environments. IBM has 'end of life'd VisualAge Smalltalk having in the late 1990s decided to back Java and it is, as of  2006[update], supported by Instantiations, Inc. which has renamed the product VA Smalltalk and released several new versions. The open Squeak implementation has an active community of developers, including many of the original Smalltalk community, and has recently been used to provide the Etoys environment on the OLPC project, a toolkit for developing collaborative applications Croquet Project, and the Open Cobalt virtual world application. GNU Smalltalk is a free software implementation of a derivative of Smalltalk-80 from the GNU project. Pharo Smalltalk is a fork of Squeak oriented towards research and use in commercial environments.
A significant development, that has spread across all current Smalltalk environments, is the increasing usage of two web frameworks, Seaside and AIDA/Web, to simplify the building of complex web applications. Seaside has seen considerable market interest with Cincom, Gemstone and Instantiations incorporating and extending it.
Smalltalk was one of many object-oriented programming languages based on Simula. Smalltalk is also one of the most influential programming languages. Virtually all of the object-oriented languages that came after—Flavors,CLOS, Objective-C, Java, Python, Ruby, and many others—were influenced by Smalltalk. Smalltalk was also one of the most popular languages with the Agile Methods, Rapid Prototyping, and Software Patterns communities. The highly productive environment provided by Smalltalk platforms made them ideal for rapid, iterative development.
Smalltalk emerged from a larger program of ARPA funded research that in many ways defined the modern world of computing. In addition to Smalltalk, working prototypes of things such as hypertext, GUIs, multimedia, the mouse, telepresence, and the Internet were developed by ARPA researchers in the 1960s. Alan Kay (one of the inventors of Smalltalk) also described a tablet computer he called the Dynabook which resembles modern tablet computers like the iPad.
Smalltalk environments were often the first to develop what are now common object-oriented software design patterns. One of the most popular is the Model–view–controller pattern for User Interface design. The MVC pattern enables developers to have multiple consistent views of the same underlying data. It's ideal for software development environments, where there are various views (e.g., entity-relation, dataflow, object model, etc.) of the same underlying specification.  Also, for simulations or games where the underlying model may be viewed from various angles and levels of abstraction.
In addition to the MVC pattern the Smalltalk language and environment were tremendously influential in the history of the Graphical User Interface (GUI) and the What You See Is What You Get (WYSIWYG) user interface, font editors, and desktop metaphors for UI design.  The powerful built-in debugging and object inspection tools that came with Smalltalk environments set the standard for all the Integrated Development Environments, starting with Lisp Machine environments, that came after.
As in other object-oriented languages, the central concept in Smalltalk-80 (but not in Smalltalk-72) is that of an object. An object is always an instance of a class. Classes are ""blueprints"" that describe the properties and behavior of their instances. For example, a GUI's window class might declare that windows have properties such as the label, the position and whether the window is visible or not. The class might also declare that instances support operations such as opening, closing, moving and hiding. Each particular window object would have its own values of those properties, and each of them would be able to perform operations defined by its class.
A Smalltalk object can do exactly three things:
The state an object holds is always private to that object. Other objects can query or change that state only by sending requests (messages) to the object to do so. Any message can be sent to any object: when a message is received, the receiver determines whether that message is appropriate. Alan Kay has commented that despite the attention given to objects, messaging is the most important concept in Smalltalk: ""The big idea is 'messaging'—that is what the kernel of Smalltalk/Squeak is all about (and it's something that was never quite completed in our Xerox PARC phase).""
Smalltalk is a ""pure"" object-oriented programming language, meaning that, unlike Java and C++, there is no difference between values which are objects and values which are primitive types. In Smalltalk, primitive values such as integers, booleans and characters are also objects, in the sense that they are instances of corresponding classes, and operations on them are invoked by sending messages. A programmer can change or extend (through subclassing) the classes that implement primitive values, so that new behavior can be defined for their instances—for example, to implement new control structures—or even so that their existing behavior will be changed. This fact is summarized in the commonly heard phrase ""In Smalltalk everything is an object"", which may be more accurately expressed as ""all values are objects"", as variables are not.
Since all values are objects, classes themselves are also objects. Each class is an instance of the metaclass of that class. Metaclasses in turn are also objects, and are all instances of a class called Metaclass. Code blocks—Smalltalk's way of expressing anonymous functions—are also objects.
Reflection is a term that computer scientists apply to software programs that have the capability to inspect their own structure, for example their parse tree or datatypes of input and output parameters. Reflection was first primarily a feature of interpreted languages such as Smalltalk and Lisp. The fact that statements are interpreted means that the programs have access to information created as they were parsed and can often even modify their own structure.
Reflection is also a feature of having a meta-model as Smalltalk does. The meta-model is the model that describes the language itself and developers can use the meta-model to do things like walk through, examine, and modify the parse tree of an object. Or find all the instances of a certain kind of structure (e.g., all the instances of the Method class in the meta-model).
Smalltalk-80 is a totally reflective system, implemented in Smalltalk-80 itself.  Smalltalk-80 provides both structural and computational reflection.  Smalltalk is a structurally reflective system whose structure is defined by Smalltalk-80 objects.  The classes and methods that define the system are themselves objects and fully part of the system that they help define.  The Smalltalk compiler compiles textual source code into method objects, typically instances of CompiledMethod.  These get added to classes by storing them in a class's method dictionary.  The part of the class hierarchy that defines classes can add new classes to the system.  The system is extended by running Smalltalk-80 code that creates or defines classes and methods.  In this way a Smalltalk-80 system is a ""living"" system, carrying around the ability to extend itself at run time.
Since the classes are themselves objects, they can be asked questions such as ""what methods do you implement?"" or ""what fields/slots/instance variables do you define?"".  So objects can easily be inspected, copied, (de)serialized and so on with generic code that applies to any object in the system.
Smalltalk-80 also provides computational reflection, the ability to observe the computational state of the system.  In languages derived from the original Smalltalk-80 the current activation of a method is accessible as an object named via a pseudo-variable (one of the six reserved words), thisContext.  By sending messages to thisContext a method activation can ask questions like ""who sent this message to me"".  These facilities make it possible to implement co-routines or Prolog-like back-tracking without modifying the virtual machine.  The exception system is implemented using this facility.  One of the more interesting uses of this is in the Seaside web framework which relieves the programmer of dealing with the complexity of a Web Browser's back button by storing continuations for each edited page and switching between them as the user navigates a web site.  Programming the web server using Seaside can then be done using a more conventional programming style.
An example of how Smalltalk can use reflection is the mechanism for handling errors. When an object is sent a message that it does not implement, the virtual machine sends the object the doesNotUnderstand: message with a reification of the message as an argument.  The message (another object, an instance of Message) contains the selector of the message and an Array of its arguments.  In an interactive Smalltalk system the default implementation of doesNotUnderstand: is one that opens an error window (a Notifier) reporting the error to the user.  Through this and the reflective facilities the user can examine the context in which the error occurred, redefine the offending code, and continue, all within the system, using Smalltalk-80's reflective facilities.
By creating a class that understands (implements) only doesNotUnderstand:, one can create an instance that can intercept any message sent to it via its doesNotUnderstand: method.  Such instances are called transparent proxies.  Such proxies can then be used to implement a number of facilities such as distributed Smalltalk where messages are exchanged between multiple Smalltalk systems, database interfaces where objects are transparently faulted out of a database, promises, etc.  The design of distributed Smalltalk influenced such systems as CORBA.
Smalltalk-80 syntax is rather minimalist, based on only a handful of declarations and reserved words. In fact, only six ""keywords"" are reserved in Smalltalk: true, false, nil, self, super, and thisContext.  These are actually called  pseudo-variables, identifiers that follow the rules for variable identifiers but denote bindings that the programmer cannot change. The true, false, and nil pseudo-variables are singleton instances. self and super refer to the receiver of a message within a method activated in response to that message, but sends to super are looked up in the superclass of the method's defining class rather than the class of the receiver, which allows methods in subclasses to invoke methods of the same name in superclasses. thisContext refers to the current activation record.  The only built-in language constructs are message sends, assignment, method return and literal syntax for some objects.  From its origins as a language for children of all ages, standard Smalltalk syntax uses punctuation in a manner more like English than mainstream coding languages. The remainder of the language, including control structures for conditional evaluation and iteration, is implemented on top of the built-in constructs by the standard Smalltalk class library. (For performance reasons, implementations may recognize and treat as special some of those messages; however, this is only an optimization and is not hardwired into the language syntax.)
The adage that ""Smalltalk syntax fits on a postcard"" refers to a code snippet by Ralph Johnson, demonstrating all the basic standard syntactic elements of methods:
The following examples illustrate the most common objects which can be written as literal values in Smalltalk-80 methods.
Numbers. The following list illustrates some of the possibilities.
The last two entries are a binary and a hexadecimal number, respectively. The number before the 'r' is the radix or base. The base does not have to be a power of two; for example 36rSMALLTALK is a valid number equal to 80738163270632 decimal.
Characters are written by preceding them with a dollar sign:
Strings are sequences of characters enclosed in single quotes:
To include a quote in a string, escape it using a second quote:
Double quotes do not need escaping, since single quotes delimit a string:
Two equal strings (strings are equal if they contain all the same characters) can be different objects residing in different places in memory. In addition to strings, Smalltalk has a class of character sequence objects called Symbol. Symbols are guaranteed to be unique—there can be no two equal symbols which are different objects. Because of that, symbols are very cheap to compare and are often used for language artifacts such as message selectors (see below).
Symbols are written as # followed by a string literal. For example:
If the sequence does not include whitespace or punctuation characters,
this can also be written as:
Arrays:
defines an array of four integers.
Many implementations support the following literal syntax for ByteArrays:
defines a ByteArray of four integers.
And last but not least, blocks (anonymous function literals)
Blocks are explained in detail further in the text.
Many Smalltalk dialects implement additional syntaxes for other objects, but the ones above are the essentials supported by all.
The two kinds of variables commonly used in Smalltalk are instance variables and temporary variables. Other variables and related terminology depend on the particular implementation. For example, VisualWorks has class shared variables and namespace shared variables, while Squeak and many other implementations have class variables, pool variables and global variables.
Temporary variable declarations in Smalltalk are variables declared inside a method (see below). They are declared at the top of the method as names separated by spaces and enclosed by vertical bars. For example:
declares a temporary variable named index which contains initially the value nil. 
Multiple variables may be declared within one set of bars: 
declares two variables: index and vowels.  All variables are initialized.  Variables are initialized to nil except the indexed variables of Strings, which are initialized to the null character or ByteArrays which are initialized to 0.
A variable is assigned a value via the ':=' syntax. So:
Assigns the string 'aeiou' to the previously declared vowels variable. The string is an object (a sequence of characters between single quotes is the syntax for literal strings), created by the compiler at compile time.
In the original Parc Place image, the glyph of the underscore character (_) appeared as a left-facing arrow (like in the 1963 version of the ASCII code).  Smalltalk originally accepted this left-arrow as the only assignment operator.  Some modern code still contains what appear to be underscores acting as assignments, hearkening back to this original usage.  Most modern Smalltalk implementations accept either the underscore or the colon-equals syntax.
The message is the most fundamental language construct in Smalltalk. Even control structures are implemented as message sends. Smalltalk adopts by default a synchronous, single dynamic message dispatch strategy (as contrasted to the asynchronous, multiple dispatch strategy adopted by some other object-oriented languages).
The following example sends the message 'factorial' to number 42:
In this situation 42 is called the message receiver, while 'factorial' is the message selector. The receiver responds to the message by returning a value (presumably in this case the factorial of 42). Among other things, the result of the message can be assigned to a variable:
""factorial"" above is what is called a unary message because only one object, the receiver, is involved. Messages can carry additional objects as arguments, as follows:
In this expression two objects are involved: 2 as the receiver and 4 as the message argument. The message result, or in Smalltalk parlance, the answer is supposed to be 16. Such messages are called keyword messages. A message can have more arguments, using the following syntax:
which answers the index of character 'o' in the receiver string, starting the search from index 6. The selector of this message is ""indexOf:startingAt:"", consisting of two pieces, or keywords.
Such interleaving of keywords and arguments is meant to improve readability of code, since arguments are explained by their preceding keywords. For example, an expression to create a rectangle using a C++ or Java-like syntax might be written as:
It's unclear which argument is which. By contrast, in Smalltalk, this code would be written as:
The receiver in this case is ""Rectangle"", a class, and the answer will be a new instance of the class with the specified width and height.
Finally, most of the special (non-alphabetic) characters can be used as what are called binary messages. These allow mathematical and logical operators to be written in their traditional form:
which sends the message ""+"" to the receiver 3 with 4 passed as the argument (the answer of which will be 7). Similarly,
is the message "">"" sent to 3 with argument 4 (the answer of which will be false).
Notice, that the Smalltalk-80 language itself does not imply the meaning of those operators. The outcome of the above is only defined by how the receiver of the message (in this case a Number instance) responds to messages ""+"" and "">"".
A side effect of this mechanism is operator overloading. A message "">"" can also be understood by other objects, allowing the use of expressions of the form ""a > b"" to compare them.
An expression can include multiple message sends. In this case expressions are parsed according to a simple order of precedence. Unary messages have the highest precedence, followed by binary messages, followed by keyword messages. For example:
is evaluated as follows:
The answer of the last message sent is the result of the entire expression.
Parentheses can alter the order of evaluation when needed. For example,
will change the meaning so that the expression first computes ""3 factorial + 4"" yielding 10. That 10 then receives the second ""factorial"" message, yielding 3628800. 3628800 then receives ""between:and:"", answering false.
Note that because the meaning of binary messages is not hardwired into Smalltalk-80 syntax, all of them are considered to have equal precedence and are evaluated simply from left to right. Because of this, the meaning of Smalltalk expressions using binary messages can be different from their ""traditional"" interpretation:
is evaluated as ""(3 + 4) * 5"", producing 35.  To obtain the expected answer of 23, parentheses must be used to explicitly define the order of operations:
Unary messages can be chained by writing them one after another:
which sends ""factorial"" to 3, then ""factorial"" to the result (6), then ""log"" to the result (720), producing the result 2.85733.
A series of expressions can be written as in the following (hypothetical) example, each separated by a period. This example first creates a new instance of class Window, stores it in a variable, and then sends two messages to it.
If a series of messages are sent to the same receiver as in the example above, they can also be written as a cascade with individual messages separated by semicolons:
This rewrite of the earlier example as a single expression avoids the need to store the new window in a temporary variable. According to the usual precedence rules, the unary message ""new"" is sent first, and then ""label:"" and ""open"" are sent to the answer of ""new"".
A block of code (an anonymous function) can be expressed as a literal value (which is an object, since all values are objects.) This is achieved with square brackets:
Where :params is the list of parameters the code can take. This means that the Smalltalk code:
can be understood as:
or expressed in lambda terms as:
and
can be evaluated as
Or in lambda terms as:
The resulting block object can form a closure: it can access the variables of its enclosing lexical scopes at any time. Blocks are first-class objects.
Blocks can be executed by sending them the value message (compound variations exist in order to provide parameters to the block e.g. 'value:value:' and 'valueWithArguments:').
The literal representation of blocks was an innovation which on the one hand allowed certain code to be significantly more readable; it allowed algorithms involving iteration to be coded in a clear and concise way. Code that would typically be written with loops in some languages can be written concisely in Smalltalk using blocks, sometimes in a single line.  But more importantly blocks allow control structure to be expressed using messages and polymorphism, since blocks defer computation and polymorphism can be used to select alternatives.  So if-then-else in Smalltalk is written and implemented as
Note that this is related to functional programming, wherein patterns of computation (here selection) are abstracted into higher-order functions. For example, the message select: on a Collection is equivalent to the higher-order function filter on an appropriate functor.
Control structures do not have special syntax in Smalltalk. They are instead implemented as messages sent to objects. For example, conditional execution is implemented by sending the message ifTrue: to a Boolean object, passing as an argument the block of code to be executed if and only if the Boolean receiver is true.
The following code demonstrates this:
Blocks are also used to implement user-defined control structures, enumerators, visitors, pluggable behavior and many other patterns.
For example:
In the last line, the string is sent the message select: with an argument that is a code block literal. The code block literal will be used as a predicate function that should answer true if and only if an element of the String should be included in the Collection of characters that satisfy the test represented by the code block that is the argument to the ""select:"" message.
A String object responds to the ""select:"" message by iterating through its members (by sending itself the message ""do:""), evaluating the selection block (""aBlock"") once with each character it contains as the argument. When evaluated (by being sent the message ""value: each""), the selection block (referenced by the parameter ""aBlock"", and defined by the block literal ""[:aCharacter | aCharacter isVowel]""), answers a boolean, which is then sent ""ifTrue:"". If the boolean is the object true, the character is added to a string to be returned.
Because the ""select:"" method is defined in the abstract class Collection, it can also be used like this:
This is a stock class definition:
Often, most of this definition will be filled in by the environment. Notice that this is actually a message to the ""Object""-class to create a subclass called ""MessagePublisher"". In other words: classes are first-class objects in Smalltalk which can receive messages just like any other object and can be created dynamically at execution time.
When an object receives a message, a method matching the message name is invoked. The following code defines a method publish, and so defines what will happen when this object receives the 'publish' message.
The following method demonstrates receiving multiple arguments and returning a value:
The method's name is #quadMultiply:and:. The return value is specified with the ^ operator.
Note that objects are responsible for determining dynamically at runtime which method to execute in response to a message—while in many languages this may be (sometimes, or even always) determined statically at compile time.
The following code:
creates (and returns) a new instance of the MessagePublisher class. This is typically assigned to a variable:
However, it is also possible to send a message to a temporary, anonymous object:
The Hello world program is used by virtually all texts to new programming languages as the first program learned to show the most basic syntax and environment of the language. For Smalltalk, the program is extremely simple to write.  The following code, the message ""show:"" is sent to the object ""Transcript"" with the String literal 'Hello, world!' as its argument. Invocation of the ""show:"" method causes the characters of its argument (the String literal 'Hello, world!') to be displayed in the transcript (""terminal"") window.
Note that a Transcript window would need to be open in order to see the results of this example.
Most popular programming systems separate static program code (in the form of class definitions, functions or procedures) from dynamic, or run time, program state (such as objects or other forms of program data).  They load program code when a program starts, and any prior program state must be recreated explicitly from configuration files or other data sources. Any settings the program (and programmer) does not explicitly save must be set up again for each restart. A traditional program also loses much useful document information each time a program saves a file, quits, and reloads. This loses details such as undo history or cursor position. Image based systems don't force losing all that just because a computer is turned off, or an OS updates.
Many Smalltalk systems, however, do not differentiate between program data (objects) and code (classes). In fact, classes are objects themselves. Therefore, most Smalltalk systems store the entire program state (including both Class and non-Class objects) in an image file.  The image can then be loaded by the Smalltalk virtual machine to restore a Smalltalk-like system to a prior state.  This was inspired by FLEX, a language created by Alan Kay and described in his M.Sc. thesis.
Smalltalk images are similar to (restartable) core dumps and can provide the same functionality as core dumps, such as delayed or remote debugging with full access to the program state at the time of error. Other languages that model application code as a form of data, such as Lisp, often use image-based persistence as well. This method of persistence is powerful for rapid development because all the development information (e.g. parse trees of the program) is saved which facilitates debugging. However, it also has serious drawbacks as a true persistence mechanism. For one thing, developers may often want to hide implementation details and not make them available in a run time environment. For legal reasons as well as for maintenance reasons, allowing anyone to modify the program at run time inevitably introduces complexity and potential errors that would not be possible with a compiled system that does not expose source code in the run time environment. Also, while the persistence mechanism is easy to use it lacks the true persistence capabilities needed for most multi-user systems.  The most obvious is the ability to do transactions with multiple users accessing the same database in parallel.
Everything in Smalltalk-80 is available for modification from within a running program. This means that, for example, the IDE can be changed in a running system without restarting it.  In some implementations, the syntax of the language or the garbage collection implementation can also be changed on the fly. Even the statement true become: false is valid in Smalltalk, although executing it is not recommended.  When used judiciously, this level of flexibility allows for one of the shortest required times for new code to enter a production system. [citation needed]
Smalltalk programs are usually compiled to bytecode, which is then interpreted by a virtual machine or dynamically translated into machine-native code."
"173","In computer science, reflection is the ability of a computer program to examine, introspect, and modify its own structure and behavior at runtime.
The earliest computers were programmed in their native assembly language, which were inherently reflective, as these original architectures could be programmed by defining instructions as data and using self-modifying code. As programming moved to compiled higher-level languages such as Algol, Cobol, and Fortran (but also Pascal and C and many other languages), this reflective ability largely disappeared until programming languages with reflection built into their type systems appeared.[citation needed]
Brian Cantwell Smith's 1982 doctoral dissertation introduced the notion of computational reflection in procedural programming languages and the notion of the meta-circular interpreter as a component of 3-Lisp.
Reflection can be used for observing and modifying program execution at runtime. A reflection-oriented program component can monitor the execution of an enclosure of code and can modify itself according to a desired goal related to that enclosure. This is typically accomplished by dynamically assigning program code at runtime.
In object-oriented programming languages such as Java, reflection allows inspection of classes, interfaces, fields and methods at runtime without knowing the names of the interfaces, fields, methods at compile time. It also allows instantiation of new objects and invocation of methods.
Reflection can be used to adapt a given program to different situations dynamically. Reflection-oriented programming almost always requires additional knowledge, framework, relational mapping, and object relevance in order to take advantage of more generic code execution.
Reflection is often used as part of software testing, such as for the runtime creation/instantiation of mock objects.
Reflection is also a key strategy for metaprogramming.
In some object-oriented programming languages, such as C# and Java, reflection can be used to override member accessibility rules. For example, reflection makes it possible to change the value of a field marked ""private"" in a third-party library's class.
A language supporting reflection provides a number of features available at runtime that would otherwise be difficult to accomplish in a lower-level language. Some of these features are the abilities to:
These features can be implemented in different ways. In MOO, reflection forms a natural part of everyday programming idiom. When verbs (methods) are called, various variables such as verb (the name of the verb being called) and this (the object on which the verb is called) are populated to give the context of the call. Security is typically managed by accessing the caller stack programmatically: Since callers() is a list of the methods by which the current verb was eventually called, performing tests on callers() (the command invoked by the original user) allows the verb to protect itself against unauthorised use.
Compiled languages rely on their runtime system to provide information about the source code. A compiled Objective-C executable, for example, records the names of all methods in a block of the executable, providing a table to correspond these with the underlying methods (or selectors for these methods) compiled into the program. In a compiled language that supports runtime creation of functions, such as Common Lisp, the runtime environment must include a compiler or an interpreter.
Reflection can be implemented for languages not having built-in reflection facilities by using a program transformation system to define automated source-code changes.
The following code snippets create an instance foo of class Foo and invoke its method PrintHello. For each programming language, normal and reflection-based call sequences are shown.
The following is an example in C#:
This Delphi example assumes that a TFoo class has been declared in a unit called Unit1:
This is a notable example, since Delphi is an unmanaged, fully natively compiled language, unlike most other languages that support reflection. Its language architecture inherits from strongly typed Pascal, but with significant influence from Smalltalk. Compare with the other examples here, many of which are dynamic or script languages like Perl, Python or PHP, or languages with a runtime like Java or C#.
The following is an example in eC:
The following is an example in ECMAScript, and therefore also applies to JavaScript and ActionScript:
The following is an example in Go:
The following is an example in Java:
The following is an example in Objective-C, implying either the OpenStep or Foundation Kit framework is used:
The following is an example in Perl:
The following is an example in PHP:
The following is an example in Python:
The following is an example in R:
The following is an example in Ruby:
Notes
Documents"
"174","ObjVlisp is a 1984 object-oriented extension of Vlisp–Vincennes LISP, a LISP dialect developed since 1971 at the University of Paris VIII – Vincennes. It is noteworthy as one of the earliest implementations of the concept of metaclasses, and in particular explicit (as opposed to implicit) metaclasses. In the ObjVlisp model, ""each entity is an instance of a single class. Classes are instances of other classes, called metaclasses. This model allows for extension of the static part of OOL, i.e. the structural aspects of objects considered as implementation of abstract data types""
ObjVlisp provided a far more flexible metaclass model than that provided by earlier object-oriented languages, especially Smalltalk. In Smalltalk-80, whenever a new class is created, a corresponding metaclass is created automatically; it does not have a name independent of that of the metaclass for which it was created–metaclasses are implicit rather than explicit. By contrast, in ObjVlisp, it is possible to define named metaclasses, and when defining a class one must specify which named metaclass it will instantiate.
The explicit metaclass support in ObjVlisp influenced the provision of the same capability in the Common Lisp Object System.
The ObjVlisp object model was later implemented in Prolog to produce ObjVProlog. Both Python and Converge implement a meta-class system that is equivalent of that of ObjVLisp.
This article is based on material taken from  the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later.
"
"175","

libffi is a foreign function interface library.  It provides a C programming language interface for calling natively compiled functions given information about the target function at run time instead of compile time. It also implements the opposite functionality: libffi can produce a pointer to a function that can accept and decode any combination of arguments defined at run time.
libffi is most often used as a bridging technology between compiled and interpreted language implementations. libffi may also be used to implement plug-ins, where the plug-in's function signatures are not known at the time of creating the host application.
Notable users include Python, Haskell, Dalvik, F-Script, PyPy, PyObjC, RubyCocoa, JRuby, Rubinius, MacRuby, gcj, GNU Smalltalk, IcedTea, Cycript, Pawn, Squeak, Java Native Access, Common Lisp (via CFFI), Racket,Embeddable Common Lisp and Mozilla.
On Mac OS X, libffi is commonly used with BridgeSupport, which provides programming language neutral descriptions of framework interfaces, and Nu which binds direct Objective-C access from Lisp.
libffi has been widely ported and is released under a MIT license.
libffi, originally developed by Anthony Green, was inspired by the Gencall library from Silicon Graphics. Gencall was developed by Gianni Mariani, then employed by SGI, for the purpose of allowing calls to functions by address and creating a call frame for the particular calling convention. Anthony Green refined the idea and extended it to other architectures and calling conventions and open sourcing libffi.
The libffi library is useful in building a bridge between interpreted and natively compiled code. Some notable users include:"
"176","Nu is an interpreted object-oriented programming language, with a Lisp-like syntax, created by Tim Burks as an alternative scripting language to program OS X through its Cocoa application programming interface (API). Implementations also exist for iPhone and Linux.
The language was first announced at C4, a conference for indie Mac developers held in August 2007.
Considered a niche tool, possibly because of its Lisp-like syntax, it is notable as part of a rise in use of functional programming languages as of 2014.
This Nu code defines a simple complex numbers class.
The example is a basic definition of a complex number: it defines the instance variables, and a method to initialize the object.  It shows the similarity between the code in Nu and the equivalent in Objective-C; it also shows the similarity with Ruby.
This sample, from the nuke tool bundled with Nu, also shows the influence of Objective-C, Lisp, and Ruby in the design of the language."
"177","
MacRuby is a discontinued an implementation of the Ruby language that ran on the Objective-C runtime and CoreFoundation framework under development by Apple Inc. which ""was supposed to replace RubyCocoa"". It targeted Ruby 1.9 and used the high performance LLVM compiler infrastructure starting with version 0.5. It supports both ahead-of-time and just-in-time compilation.
MacRuby supported Interface Builder and shipped with a core library called HotCocoa to simplify Cocoa programming. MacRuby was also used as an embedded scripting language for Objective-C applications.
In May 2012, Laurent Sansonetti announced RubyMotion, a port of MacRuby for iOS.
Development on MacRuby effectively ended in late 2011, coinciding with the principal author's departure from Apple Inc.. As of Jan 5 2015, The MacRuby project is no longer under active development; MacRuby does not work on Mavericks, the team having shifted their focus to a commercial RubyMotion product for iOS and OS X.
MacRuby was originally called ""ruby+objc"" and was developed by Laurent Sansonetti, who began work on it in late 2007.  In March 2008, the first publicly available version, MacRuby 0.1, was announced on the official RubyTalk forum.   Version 0.2 was released in June 2008, and implemented Ruby strings, arrays and hashes as native Cocoa types. In September 2008, MacRuby 0.3 was released and included the HotCocoa library as well as several HotCocoa example programs. In October 2008, Apple created its first MacRuby page on its Developer Connection website.  MacRuby 0.4 was released in March 2009, MacRuby 0.5, 0.6, 0.7 in January, May and October 2010 respectively. MacRuby 0.8, was released on December 13, 2010, 0.9 on February 25, 2011  0.10 on March 23, 2011, 0.11 on October 17, 2011, 0.12 on June 11, 2012."
