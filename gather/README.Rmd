---
title: "Gather text resources"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***

<center><h2><a href="http://htmlpreview.github.io/?https://github.com/MichaelTzach/final-project/blob/master/gather/README.html">HTML PREVIEW</a> </h2></center>

***

Import the needed libraries
```{r}
if(!require(jsonlite)) {
  install.packages('jsonlite')
}
library(jsonlite)
if(!require(XML)) {
  install.packages('XML')
}
library(XML)
if(!require(httr)) {
  install.packages('httr')
}
library(httr)
```

# Gathering Data
We want to gather text data from different resources.
We prefer gathering the data using APIs instead of using web scrapping to have consistency in the data produced.
Some of the APIs have random results. Some have constant results.

## Wikipedia
We use wikipedia as a resource. To gather data from Wikipedia we input a title for an article and then get the related articles for that article recursively.  
Documentation for the API can be found here: [MediaPedia REST Documentation](https://www.mediawiki.org/api/rest_v1/)  
For getting data from Wikipedia, we have the method called **getWikipediaSummaries** which takes two parameters:

1. **minimumCount** - The minimum number of wikipedia entry summeries we need
2. **startingTitle** - The initial title we want the related articles to be from. You can choose any title and the results will be related to that title.

```{r}
getWikipediaSummaries = function(minimumCount, startingTitle) {
  githubTitlesVector = c(startingTitle)
  isUniqueTitle = function(title) {
    return(title %in% githubTitlesVector == FALSE)
  }
  
  relatedSitesAlreadyQueried = c()
  didNotGetRelatedTitlesForTitle = function(title) {
    return(title %in% relatedSitesAlreadyQueried == FALSE)
  }
  
  getRelatedTitlesToTitlesList = function(title) {
    relatedSitesAPIURL = paste0("https://en.wikipedia.org/api/rest_v1/page/related/", title)
    print(paste0("Getting related Wikipedia titles from: ", relatedSitesAPIURL))
    relatedTitles = fromJSON(relatedSitesAPIURL)$pages$title
    return(relatedTitles)
  }
  getWikipediaSummary = function(title) {
    summaryAPIURL = paste0("https://en.wikipedia.org/api/rest_v1/page/html/", title)
    print(paste0("Getting Wikipedia title html from: ", summaryAPIURL))
    html = GET(summaryAPIURL)
    document = htmlParse(html, asText=TRUE)
    plainText = xpathSApply(document, "//p", xmlValue)
    plainText = gsub("\\[\\d+\\]", '', plainText)
    plainText = paste(plainText, collapse = "\n")
    return(c(plainText))
  }
  
  safeGetWikipediaSummary = function(title) {
    result = tryCatch({
      getWikipediaSummary(title)
    }, error = function(e) {
      c()
    })
    return(result)
  }
  
  safeGetRelatedWikipedia = function(title) {
    result = tryCatch({
      getRelatedTitlesToTitlesList(title)
    }, error = function(e) {
      c()
    })
    return(result)
  }
  
  while (length(githubTitlesVector) < minimumCount) {
    titlesDidntSearchRelatedFor = Filter(didNotGetRelatedTitlesForTitle, githubTitlesVector)
    relatedSitesAlreadyQueried = append(relatedSitesAlreadyQueried, titlesDidntSearchRelatedFor)
    relatedTitles = unique(unlist(lapply(titlesDidntSearchRelatedFor, safeGetRelatedWikipedia)))
    uniqueRelatedTitles = Filter(isUniqueTitle, relatedTitles)
    githubTitlesVector = append(githubTitlesVector, uniqueRelatedTitles)
  }
  
  wikipediaSummaries = unlist(lapply(githubTitlesVector, safeGetWikipediaSummary))
  
  return(wikipediaSummaries)
}
```

### getWikipediaSummaries in depth
We start with the starting title. *githubTitlesVector* is where we store it and all the related titles.  
*isUniqueTitle* is a helper method to make sure that we only store unique values.
```{r, eval=FALSE}
githubTitlesVector = c(startingTitle)
isUniqueTitle = function(title) {
  return(title %in% githubTitlesVector == FALSE)
}
```

When we are collecting the related titles recursively, we almost always return to previously visited titles. We store them in *relatedSitesAlreadyQueried* so we don't request for related for them again.
*didNotGetRelatedTitlesForTitle* is a helper method to get only the titles that were not previously checked.
```{r, eval=FALSE}
relatedSitesAlreadyQueried = c()
didNotGetRelatedTitlesForTitle = function(title) {
  return(title %in% relatedSitesAlreadyQueried == FALSE)
}
```

*getRelatedTitlesToTitlesList* is a helper method that gets a title and returns a vector of related titles to that title.
```{r, eval=FALSE}
getRelatedTitlesToTitlesList = function(title) {
  relatedSitesAPIURL = paste0("https://en.wikipedia.org/api/rest_v1/page/related/", title)
  print(paste0("Getting related Wikipedia titles from: ", relatedSitesAPIURL))
  relatedTitles = fromJSON(relatedSitesAPIURL)$pages$title
  return(relatedTitles)
}
```

*getWikipediaSummary* is a helper method that gets a title and returns the summary for that title.  
It downloads the html and then cleans it up by grabbing text in <p> tags and then removing annotations for citations (for example: "[12]")
We wrap this method with *safeGetWikipediaSummary* because it could raise errors.
```{r, eval=FALSE}
  getWikipediaSummary = function(title) {
    summaryAPIURL = paste0("https://en.wikipedia.org/api/rest_v1/page/html/", title)
    print(paste0("Getting Wikipedia title html from: ", summaryAPIURL))
    html = GET(summaryAPIURL)
    document = htmlParse(html, asText=TRUE)
    plainText = xpathSApply(document, "//p", xmlValue)
    plainText = gsub("\\[\\d+\\]", '', plainText)
    plainText = paste(plainText, collapse = "\n")
    return(c(plainText))
  }

safeGetWikipediaSummary = function(title) {
  result = tryCatch({
    getWikipediaSummary(title)
  }, error = function(e) {
    c()
  })
  return(result)
}
```

The main while loop collects titles using a recursive related search.  
For each title that we have in *githubTitlesVector* that hasn't yet been searched, we request the related titles and then append the unique ones back to *githubTitlesVector*.  
This while loop finished when we have at least *minimumCount* titles in *githubTitlesVector*.
```{r, eval=FALSE}
while (length(githubTitlesVector) < minimumCount) {
  titlesDidntSearchRelatedFor = Filter(didNotGetRelatedTitlesForTitle, githubTitlesVector)
  relatedSitesAlreadyQueried = append(relatedSitesAlreadyQueried, titlesDidntSearchRelatedFor)
  relatedTitles = unique(unlist(lapply(titlesDidntSearchRelatedFor, getRelatedTitlesToTitlesList)))
  uniqueRelatedTitles = Filter(isUniqueTitle, relatedTitles)
  githubTitlesVector = append(githubTitlesVector, uniqueRelatedTitles)
}
```

We map each title to its summary using *safeGetWikipediaSummary* and return it.
```{r, eval=FALSE}
  wikipediaSummaries = unlist(lapply(githubTitlesVector, safeGetWikipediaSummary))
  
  return(wikipediaSummaries)
```

### Example use:
```{r results='hide', message=FALSE, warning=FALSE}
wikipediaSummaries = getWikipediaSummaries(1, "Swift_(programming_language)")  
```
```{r}
wikipediaSummaries
```

## Pokemon Abilities
We use pokemontcg.io as a resource for pokemon ability descriptions.  
The api is paginated so we go through the pages until we have enough descriptions.
Documentation for the API can be found here: [Pokemon TCG Developers](https://docs.pokemontcg.io)  
For getting data, we have the method called **getPokemonAbilities** which takes one parameter:

1. **minimumCount** - The minimum number of ability summeries we need. The method will attempt to get at least this number but the maximum number is about 170 which is the number of pokemon in the DB.

```{r}
getPokemonAbilities = function(minimumCount) {
  pokemonAbilities = c()
  morePagesAvailable = TRUE
  while(length(pokemonAbilities) < minimumCount && morePagesAvailable) {
    pageSize = min(50, minimumCount)
    page = 1 + length(pokemonAbilities) / pageSize
    pageSizeQueryParam = paste0("&pageSize=", toString(pageSize))
    pageQueryParam = paste0("&page=", toString(page))
    apiURL = "https://api.pokemontcg.io/v1/cards?supertype=Pok%C3%A9mon&abilityType=Pok%C3%A9mon%20Power"
    apiURL = paste0(apiURL, pageQueryParam)
    apiURL = paste0(apiURL, pageSizeQueryParam)
    
    print(paste0("Getting pokemon abilities from: ", apiURL))
    pokemonData = fromJSON(apiURL)$cards
    if (length(pokemonData$ability$text) < pageSize) {
      morePagesAvailable = FALSE
    }
    pokemonAbilities = append(pokemonAbilities, pokemonData$ability$text)
  }
  return(pokemonAbilities)
}
```

### getPokemonAbilities in depth

*pokemonAbilities* holds the abilities we got from the server.
*morePagesAvailable* is a flag to know if there are more pages in the api.
```{r, eval=FALSE}
  pokemonAbilities = c()
  morePagesAvailable = TRUE
```

The main while loop runs until we get *minimumCount* abilities or until there are no more pages available.  
It first calculates the apiURL. We only want pokemon that have abilities hence the *abilityType* param query.  
If we requested a certain number of abilities and got less than that number, we know that there are no more pages available.
```{r, eval=FALSE}
  while(length(pokemonAbilities) < minimumCount && morePagesAvailable) {
    pageSize = min(50, minimumCount)
    page = 1 + length(pokemonAbilities) / pageSize
    pageSizeQueryParam = paste0("&pageSize=", toString(pageSize))
    pageQueryParam = paste0("&page=", toString(page))
    apiURL = "https://api.pokemontcg.io/v1/cards?supertype=Pok%C3%A9mon&abilityType=Pok%C3%A9mon%20Power"
    apiURL = paste0(apiURL, pageQueryParam)
    apiURL = paste0(apiURL, pageSizeQueryParam)
    
    print(paste0("Getting pokemon abilities from: ", apiURL))
    pokemonData = fromJSON(apiURL)$cards
    if (length(pokemonData$ability$text) < pageSize) {
      morePagesAvailable = FALSE
    }
    pokemonAbilities = append(pokemonAbilities, pokemonData$ability$text)
  }
```

Once the while loop is finished we return the vector of pokemon abilities.
```{r, eval=FALSE}
  return(pokemonAbilities)
```

### Example use:
```{r results='hide', message=FALSE, warning=FALSE}
pokemonAbilities = getPokemonAbilities(5)
```
```{r}
pokemonAbilities
```

## BBC Articles
We use BBC news as another resource. We gather articles links from bbc rss feeds and then get the article content from those links using the article html.
For getting data, we have the method called **getBBCArticls**. Takes one parameter:

1. **minimumCount** - The minimum number of ability summeries we need.

```{r}
getBBCArticls = function(minimumCount) {
  getArticleURLsFromFeed = function(feedURL) {
    print(paste0("Getting article URLs from feed from: ", feedURL))
    rssFeed = GET(feedURL)
    doc = xmlParse(rssFeed, asText = TRUE)
    articleURLs = xpathSApply(doc, "//item/link",xmlValue)
    return(articleURLs)
  }
  getArticleContent = function(articleURL) {
    print(paste0("Getting BBC html from: ", articleURL))
    html = GET(articleURL)
    document = htmlParse(html, asText=TRUE)
    plainText = xpathSApply(document, "//div[@class='story-body__inner']/p",xmlValue)
    plainText = paste(plainText, collapse = "\n")
    return(plainText)
  }
  
  bbcRSSFeeds = c('http://feeds.bbci.co.uk/news/world/europe/rss.xml', 'http://feeds.bbci.co.uk/news/world/middle_east/rss.xml', 'http://feeds.bbci.co.uk/news/video_and_audio/politics/rss.xml')
  
  bbcArticleURLs = unlist(lapply(bbcRSSFeeds, getArticleURLsFromFeed))
  bbcArticleURLs = head(bbcArticleURLs, minimumCount)
  bbcArticls = unlist(lapply(bbcArticleURLs, getArticleContent))
  bbcArticls = Filter(function(article) { return(nchar(article) > 700) }, bbcArticls)
  
  return(bbcArticls)
}
```

### getTriviaFacts in depth
*getArticleURLsFromFeed* is a helper method get a list of news links from a single rss feed.  
We download the xml file for the feed and parse it.  
Once parsing is done, we extract from the document the value for <link> tags that are inside an <item> tag.
```{r, eval=FALSE}
getArticleURLsFromFeed = function(feedURL) {
  print(paste0("Getting article URLs from feed from: ", feedURL))
  rssFeed = GET(feedURL)
  doc = xmlParse(rssFeed, asText = TRUE)
  articleURLs = xpathSApply(doc, "//item/link",xmlValue)
  return(articleURLs)
}
```

*getArticleContent* is a helper method that takes a bbc article url as the parameter and returns the article contents.  
It first downloads the html and parses it.  
We later extract the contents of each <div> that has the 'story-body__inner' class.  
```{r, eval=FALSE}
getArticleContent = function(articleURL) {
  print(paste0("Getting BBC html from: ", articleURL))
  html = GET(articleURL)
  document = htmlParse(html, asText=TRUE)
  plainText = xpathSApply(document, "//div[@class='story-body__inner']/p",xmlValue)
  plainText = paste(plainText, collapse = "\n")
  return(plainText)
}
```

*bbcRSSFeeds* is the list of feeds that we want to download from.  
We apply *getArticleURLsFromFeed* on this list and then for each url that is return we apply *getArticleContent*.  
We filter articles that have less than 700 chars.
```{r, eval=FALSE}
  bbcRSSFeeds = c('http://feeds.bbci.co.uk/news/world/europe/rss.xml', 'http://feeds.bbci.co.uk/news/world/middle_east/rss.xml', 'http://feeds.bbci.co.uk/news/video_and_audio/politics/rss.xml')
  
  bbcArticleURLs = unlist(lapply(bbcRSSFeeds, getArticleURLsFromFeed))
  bbcArticls = unlist(lapply(bbcArticleURLs, getArticleContent))
  bbcArticls = Filter(function(article) { return(nchar(article) > 700) }, bbcArticls)
```

### Example use:
```{r results='hide', message=FALSE, warning=FALSE}
bbcArticls = getBBCArticls(1)
```
```{r}
bbcArticls
```