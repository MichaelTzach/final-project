---
title: "Gather text resources"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***

<center><h2><a href="http://htmlpreview.github.io/?https://github.com/MichaelTzach/final-project/blob/master/gather/README.html">HTML PREVIEW</a> </h2></center>

***

Import the needed libraries
```{r}
if(!require(jsonlite)) {
  install.packages('jsonlite')
}
library(jsonlite)
if(!require(XML)) {
  install.packages('XML')
}
library(XML)
if(!require(httr)) {
  install.packages('httr')
}
library(httr)
```

# Gathering Data
We want to gather text data from different resources.
We prefer gathering the data using APIs instead of using web scrapping to have consistency in the data produced.
Some of the APIs have random results. Some have constant results.

## Wikipedia
We use wikipedia as a resource. To gather data from Wikipedia we input a title for an article and then get the related articles for that article recursively.  
Documentation for the API can be found here: [MediaPedia REST Documentation](https://www.mediawiki.org/api/rest_v1/)  
For getting data from Wikipedia, we have the method called **getWikipediaSummaries** which takes two parameters:

1. **minimumCount** - The minimum number of wikipedia entry summeries we need
2. **startingTitle** - The initial title we want the related articles to be from. You can choose any title and the results will be related to that title.

```{r}
getWikipediaSummaries = function(minimumCount, startingTitle) {
  githubTitlesVector = c(startingTitle)
  isUniqueTitle = function(title) {
    return(title %in% githubTitlesVector == FALSE)
  }
  
  relatedSitesAlreadyQueried = c()
  didNotGetRelatedTitlesForTitle = function(title) {
    return(title %in% relatedSitesAlreadyQueried == FALSE)
  }
  
  getRelatedTitlesToTitlesList = function(title) {
    relatedSitesAPIURL = paste0("https://en.wikipedia.org/api/rest_v1/page/related/", title)
    print(paste0("Getting related Wikipedia titles from: ", relatedSitesAPIURL))
    relatedTitles = fromJSON(relatedSitesAPIURL)$pages$title
    return(relatedTitles)
  }
  getWikipediaSummary = function(title) {
    summaryAPIURL = paste0("https://en.wikipedia.org/api/rest_v1/page/html/", title)
    print(paste0("Getting Wikipedia title html from: ", summaryAPIURL))
    html = GET(summaryAPIURL)
    document = htmlParse(html, asText=TRUE)
    plainText = xpathSApply(document, "//p", xmlValue)
    plainText = gsub("\\[\\d+\\]", '', plainText)
    plainText = paste(plainText, collapse = "\n")
    return(c(plainText))
  }
  
  safeGetWikipediaSummary = function(title) {
    result = tryCatch({
      getWikipediaSummary(title)
    }, error = function(e) {
      c()
    })
    return(result)
  }
  
  safeGetRelatedWikipedia = function(title) {
    result = tryCatch({
      getRelatedTitlesToTitlesList(title)
    }, error = function(e) {
      c()
    })
    return(result)
  }
  
  while (length(githubTitlesVector) < minimumCount) {
    titlesDidntSearchRelatedFor = Filter(didNotGetRelatedTitlesForTitle, githubTitlesVector)
    relatedSitesAlreadyQueried = append(relatedSitesAlreadyQueried, titlesDidntSearchRelatedFor)
    relatedTitles = unique(unlist(lapply(titlesDidntSearchRelatedFor, safeGetRelatedWikipedia)))
    uniqueRelatedTitles = Filter(isUniqueTitle, relatedTitles)
    githubTitlesVector = append(githubTitlesVector, uniqueRelatedTitles)
  }
  
  wikipediaSummaries = unlist(lapply(githubTitlesVector, safeGetWikipediaSummary))
  
  return(wikipediaSummaries)
}
```

### getWikipediaSummaries in depth
We start with the starting title. *githubTitlesVector* is where we store it and all the related titles.  
*isUniqueTitle* is a helper method to make sure that we only store unique values.
```{r, eval=FALSE}
githubTitlesVector = c(startingTitle)
isUniqueTitle = function(title) {
  return(title %in% githubTitlesVector == FALSE)
}
```

When we are collecting the related titles recursively, we almost always return to previously visited titles. We store them in *relatedSitesAlreadyQueried* so we don't request for related for them again.
*didNotGetRelatedTitlesForTitle* is a helper method to get only the titles that were not previously checked.
```{r, eval=FALSE}
relatedSitesAlreadyQueried = c()
didNotGetRelatedTitlesForTitle = function(title) {
  return(title %in% relatedSitesAlreadyQueried == FALSE)
}
```

*getRelatedTitlesToTitlesList* is a helper method that gets a title and returns a vector of related titles to that title.
```{r, eval=FALSE}
getRelatedTitlesToTitlesList = function(title) {
  relatedSitesAPIURL = paste0("https://en.wikipedia.org/api/rest_v1/page/related/", title)
  print(paste0("Getting related Wikipedia titles from: ", relatedSitesAPIURL))
  relatedTitles = fromJSON(relatedSitesAPIURL)$pages$title
  return(relatedTitles)
}
```

*getWikipediaSummary* is a helper method that gets a title and returns the summary for that title.  
It downloads the html and then cleans it up by grabbing text in <p> tags and then removing annotations for citations (for example: "[12]")
We wrap this method with *safeGetWikipediaSummary* because it could raise errors.
```{r, eval=FALSE}
  getWikipediaSummary = function(title) {
    summaryAPIURL = paste0("https://en.wikipedia.org/api/rest_v1/page/html/", title)
    print(paste0("Getting Wikipedia title html from: ", summaryAPIURL))
    html = GET(summaryAPIURL)
    document = htmlParse(html, asText=TRUE)
    plainText = xpathSApply(document, "//p", xmlValue)
    plainText = gsub("\\[\\d+\\]", '', plainText)
    plainText = paste(plainText, collapse = "\n")
    return(c(plainText))
  }

safeGetWikipediaSummary = function(title) {
  result = tryCatch({
    getWikipediaSummary(title)
  }, error = function(e) {
    c()
  })
  return(result)
}
```

The main while loop collects titles using a recursive related search.  
For each title that we have in *githubTitlesVector* that hasn't yet been searched, we request the related titles and then append the unique ones back to *githubTitlesVector*.  
This while loop finished when we have at least *minimumCount* titles in *githubTitlesVector*.
```{r, eval=FALSE}
while (length(githubTitlesVector) < minimumCount) {
  titlesDidntSearchRelatedFor = Filter(didNotGetRelatedTitlesForTitle, githubTitlesVector)
  relatedSitesAlreadyQueried = append(relatedSitesAlreadyQueried, titlesDidntSearchRelatedFor)
  relatedTitles = unique(unlist(lapply(titlesDidntSearchRelatedFor, getRelatedTitlesToTitlesList)))
  uniqueRelatedTitles = Filter(isUniqueTitle, relatedTitles)
  githubTitlesVector = append(githubTitlesVector, uniqueRelatedTitles)
}
```

We map each title to its summary using *safeGetWikipediaSummary* and return it.
```{r, eval=FALSE}
  wikipediaSummaries = unlist(lapply(githubTitlesVector, safeGetWikipediaSummary))
  
  return(wikipediaSummaries)
```

### Example use:
```{r results='hide', message=FALSE, warning=FALSE}
wikipediaSummaries = getWikipediaSummaries(1, "Swift_(programming_language)")  
```
```{r}
wikipediaSummaries
```

## Trip advisor blogs
We use trip advisor blogs as another source of data. We use an unpublished api that returns links to blogs posts in trip advisor. This api is the api used by the site for paginating the list of blog posts shown there.  
For getting data, we have the method called **getTripAdvisorBlogPosts** which takes one parameter:

1. **minimumCount** - The minimum number of blog posts that we want. The function will download three times that number and will return the longest *minimumCount* blog posts it found.

```{r}
getTripAdvisorBlogPosts = function(minimumCount) {
  getBlogPostURLs = function() {
    travalStoriesListJSONURL = paste0("https://www.tripadvisor.com/blog/api/posts/?locale=en&posts_per_page=", toString(minimumCount * 3))
    print(paste0("Getting blog post URLs from: ", travalStoriesListJSONURL))
    travalStoriesListJSON = fromJSON(travalStoriesListJSONURL)
    travelBlogURLs = travalStoriesListJSON$data$posts$link
    return(travelBlogURLs)
  }
  getBlogPostContent = function(blogpostURL) {
    print(paste0("Getting blog post from: ", blogpostURL))
    html = GET(blogpostURL)
    document = htmlParse(html, asText = TRUE)
    plainText = xpathSApply(document, "//div[@class='ec__post-body']/p",xmlValue)
    plainText = paste(plainText, collapse = "\n")
    return(plainText)
  }

  blogPostURLs = getBlogPostURLs()
  blogPosts = unlist(lapply(blogPostURLs, getBlogPostContent))
  blogPosts = rev(blogPosts[order(nchar(blogPosts))])
  blogPosts = head(blogPosts, minimumCount)

  return(blogPosts)
}
```

### getTripAdvisorBlogPosts in depth

*getBlogPostURLs* is a helper method that returns blog post links three times the number of minimum blog posts we need.  
It does so by downloading from trip advisor's undocumented api and then taking the links from the JSON returned.
```{r, eval=FALSE}
getBlogPostURLs = function() {
  travalStoriesListJSONURL = paste0("https://www.tripadvisor.com/blog/api/posts/?locale=en&posts_per_page=", toString(minimumCount * 3))
  print(paste0("Getting blog post URLs from: ", travalStoriesListJSONURL))
  travalStoriesListJSON = fromJSON(travalStoriesListJSONURL)
  travelBlogURLs = travalStoriesListJSON$data$posts$link
  return(travelBlogURLs)
}
```

*getBlogPostContent* is another helper method that takes a single blog post link as a parameter and then downloads it and parses the HTML.  
Once the HTML is parsed, we take all <p> tags that sit under <div> tags that have the 'ec__post-body' class which is the class for the blog post.  
```{r, eval=FALSE}
getBlogPostContent = function(blogpostURL) {
  print(paste0("Getting blog post from: ", blogpostURL))
  html = GET(blogpostURL)
  document = htmlParse(html, asText = TRUE)
  plainText = xpathSApply(document, "//div[@class='ec__post-body']/p",xmlValue)
  plainText = paste(plainText, collapse = "\n")
  return(plainText)
}
```

Once the function runs, it first creates a var with the blog posts urls *blogPostURLs*.  
After that it downloads each blog post and reverse sorts it by blog post length. Once it is sorted, we return the longest *minimumCount* blog posts.
```{r, eval=FALSE}
blogPostURLs = getBlogPostURLs()
blogPosts = unlist(lapply(blogPostURLs, getBlogPostContent))
blogPosts = rev(blogPosts[order(nchar(blogPosts))])
blogPosts = head(blogPosts, minimumCount)
```

### Example use:
```{r results='hide', message=FALSE, warning=FALSE}
tripAdvisorBlogPosts = getTripAdvisorBlogPosts(1)
```
```{r}
tripAdvisorBlogPosts
```

## BBC Articles
We use BBC news as another resource. We gather articles links from bbc rss feeds and then get the article content from those links using the article html.
For getting data, we have the method called **getBBCArticls**. Takes one parameter:

1. **minimumCount** - The minimum number of ability summeries we need.

```{r}
getBBCArticls = function(minimumCount) {
  getArticleURLsFromFeed = function(feedURL) {
    print(paste0("Getting article URLs from feed from: ", feedURL))
    rssFeed = GET(feedURL)
    doc = xmlParse(rssFeed, asText = TRUE)
    articleURLs = xpathSApply(doc, "//item/link",xmlValue)
    return(articleURLs)
  }
  getArticleContent = function(articleURL) {
    print(paste0("Getting BBC html from: ", articleURL))
    html = GET(articleURL)
    document = htmlParse(html, asText=TRUE)
    plainText = xpathSApply(document, "//div[@class='story-body__inner']/p",xmlValue)
    plainText = paste(plainText, collapse = "\n")
    return(plainText)
  }
  
  bbcRSSFeeds = c('http://feeds.bbci.co.uk/news/world/europe/rss.xml', 'http://feeds.bbci.co.uk/news/world/middle_east/rss.xml', 'http://feeds.bbci.co.uk/news/video_and_audio/politics/rss.xml')
  
  bbcArticleURLs = unlist(lapply(bbcRSSFeeds, getArticleURLsFromFeed))
  bbcArticleURLs = head(bbcArticleURLs, minimumCount)
  bbcArticls = unlist(lapply(bbcArticleURLs, getArticleContent))
  bbcArticls = Filter(function(article) { return(nchar(article) > 700) }, bbcArticls)
  
  return(bbcArticls)
}
```

### getBBCArticls in depth
*getArticleURLsFromFeed* is a helper method get a list of news links from a single rss feed.  
We download the xml file for the feed and parse it.  
Once parsing is done, we extract from the document the value for <link> tags that are inside an <item> tag.
```{r, eval=FALSE}
getArticleURLsFromFeed = function(feedURL) {
  print(paste0("Getting article URLs from feed from: ", feedURL))
  rssFeed = GET(feedURL)
  doc = xmlParse(rssFeed, asText = TRUE)
  articleURLs = xpathSApply(doc, "//item/link",xmlValue)
  return(articleURLs)
}
```

*getArticleContent* is a helper method that takes a bbc article url as the parameter and returns the article contents.  
It first downloads the html and parses it.  
We later extract the contents of each <div> that has the 'story-body__inner' class.  
```{r, eval=FALSE}
getArticleContent = function(articleURL) {
  print(paste0("Getting BBC html from: ", articleURL))
  html = GET(articleURL)
  document = htmlParse(html, asText=TRUE)
  plainText = xpathSApply(document, "//div[@class='story-body__inner']/p",xmlValue)
  plainText = paste(plainText, collapse = "\n")
  return(plainText)
}
```

*bbcRSSFeeds* is the list of feeds that we want to download from.  
We apply *getArticleURLsFromFeed* on this list and then for each url that is return we apply *getArticleContent*.  
We filter articles that have less than 700 chars.
```{r, eval=FALSE}
  bbcRSSFeeds = c('http://feeds.bbci.co.uk/news/world/europe/rss.xml', 'http://feeds.bbci.co.uk/news/world/middle_east/rss.xml', 'http://feeds.bbci.co.uk/news/video_and_audio/politics/rss.xml')
  
  bbcArticleURLs = unlist(lapply(bbcRSSFeeds, getArticleURLsFromFeed))
  bbcArticls = unlist(lapply(bbcArticleURLs, getArticleContent))
  bbcArticls = Filter(function(article) { return(nchar(article) > 700) }, bbcArticls)
```

### Example use:
```{r results='hide', message=FALSE, warning=FALSE}
bbcArticls = getBBCArticls(1)
```
```{r}
bbcArticls
```